#
msgid ""
msgstr ""

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) PyTorch BackendConfig Tutorial"
msgstr "(原型) PyTorch BackendConfig 教程"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author**: `Andrew Or <https://github.com/andrewor14>`_"
msgstr "**作者**: `Andrew Or <https://github.com/andrewor14>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The BackendConfig API enables developers to integrate their backends with "
"PyTorch quantization. It is currently only supported in FX graph mode "
"quantization, but support may be extended to other modes of quantization in "
"the future. In this tutorial, we will demonstrate how to use this API to "
"customize quantization support for specific backends. For more information "
"on the motivation and implementation details behind BackendConfig, please "
"refer to this `README "
"<https://github.com/pytorch/pytorch/tree/master/torch/ao/quantization/backend_config>`__."
msgstr ""
"BackendConfig API 使开发者能够将其后端集成到 PyTorch 的量化功能中。目前仅支持 FX "
"图模式量化，但未来可能会扩展到其他量化模式。在本教程中，我们将演示如何使用此 API 为特定后端自定义量化支持。有关 BackendConfig "
"背后的动机和实现细节的更多信息，请参阅此 `README "
"<https://github.com/pytorch/pytorch/tree/master/torch/ao/quantization/backend_config>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Suppose we are a backend developer and we wish to integrate our backend with"
" PyTorch's quantization APIs. Our backend consists of two ops only: "
"quantized linear and quantized conv-relu. In this section, we will walk "
"through how to achieve this by quantizing an example model using a custom "
"BackendConfig through `prepare_fx` and `convert_fx`."
msgstr ""
"假设我们是后端开发者，希望将我们的后端与 PyTorch 的量化 API 集成。我们的后端仅包含两个运算符：量化线性和量化 conv-"
"relu。在本节中，我们将通过使用自定义 BackendConfig 对示例模型进行量化，演示如何实现这一点，使用 `prepare_fx` 和 "
"`convert_fx`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Derive reference pattern for each quantized operator"
msgstr "1. 为每个量化运算符推导参考模式"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For quantized linear, suppose our backend expects the reference pattern "
"`[dequant - fp32_linear - quant]` and lowers it into a single quantized "
"linear op. The way to achieve this is to first insert quant-dequant ops "
"before and after the float linear op, such that we produce the following "
"reference model::"
msgstr ""
"对于量化线性，假设我们的后端期待参考模式 `[dequant - fp32_linear - quant]` "
"并将其降低为单个量化线性运算符。实现这一目标的方式是首先在浮点线性运算符之前和之后插入量化-反量化操作，从而产生以下参考模型::"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Similarly, for quantized conv-relu, we wish to produce the following "
"reference model, where the reference pattern in the square brackets will be "
"lowered into a single quantized conv-relu op::"
msgstr "类似地，对于量化 conv-relu，我们希望产生以下参考模型，方括号中的参考模式将被降低为单个量化 conv-relu 运算符::"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Set DTypeConfigs with backend constraints"
msgstr "2. 使用后端约束设置 DTypeConfigs"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In the reference patterns above, the input dtype specified in the "
"DTypeConfig will be passed as the dtype argument to quant1, while the output"
" dtype will be passed as the dtype argument to quant2. If the output dtype "
"is fp32, as in the case of dynamic quantization, then the output quant-"
"dequant pair will not be inserted. This example also shows how to specify "
"restrictions on quantization and scale ranges on a particular dtype."
msgstr ""
"在上述参考模式中，在 DTypeConfig 中指定的输入数据类型将作为 dtype 参数传递给 quant1，而输出数据类型将作为 dtype "
"参数传递给 quant2。如果输出数据类型是 fp32，如动态量化的情况，那么输出量化-"
"反量化对将不会被插入。此示例还展示了如何在特定数据类型上指定量化和比例范围的限制。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Set up fusion for conv-relu"
msgstr "3. 设置 conv-relu 的融合"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that the original user model contains separate conv and relu ops, so we"
" need to first fuse the conv and relu ops into a single conv-relu op "
"(`fp32_conv_relu`), and then quantize this op similar to how the linear op "
"is quantized. We can set up fusion by defining a function that accepts 3 "
"arguments, where the first is whether or not this is for QAT, and the "
"remaining arguments refer to the individual items of the fused pattern."
msgstr ""
"注意，原始用户模型包含分离的 conv 和 relu 运算符，因此我们需要首先将 conv 和 relu 运算符融合为单个 conv-relu 运算符 "
"(`fp32_conv_relu`)，然后以类似于线性运算符量化的方式对该运算符进行量化。通过定义一个接受三个参数的函数，可以设置融合，其中第一个参数指示是否用于"
" QAT，其余参数指代融合模式的单个项。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "4. Define the BackendConfig"
msgstr "4. 定义 BackendConfig"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now we have all the necessary pieces, so we go ahead and define our "
"BackendConfig. Here we use different observers (will be renamed) for the "
"input and output for the linear op, so the quantization params passed to the"
" two quantize ops (quant1 and quant2) will be different. This is commonly "
"the case for weighted ops like linear and conv."
msgstr ""
"现在我们有了所有必要的部分，因此继续定义我们的 "
"BackendConfig。在这里，我们为线性运算符的输入和输出使用不同的观察者（将会重命名），因此传递给两个量化操作（quant1 和 "
"quant2）的量化参数将会有所不同。这种情况在加权操作（如线性和卷积）中是常见的。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For the conv-relu op, the observation type is the same. However, we need two"
" BackendPatternConfigs to support this op, one for fusion and one for "
"quantization. For both conv-relu and linear, we use the DTypeConfig defined "
"above."
msgstr ""
"对于 conv-relu 运算符，观察类型是相同的。然而，我们需要两个 BackendPatternConfig "
"来支持该运算符，一个用于融合，一个用于量化。对于 conv-relu 和线性，我们使用上面定义的 DTypeConfig。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "5. Set up QConfigMapping that satisfies the backend constraints"
msgstr "5. 设置满足后端约束的 QConfigMapping"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In order to use the ops defined above, the user must define a QConfig that "
"satisfies the constraints specified in the DTypeConfig. For more detail, see"
" the documentation for `DTypeConfig "
"<https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.DTypeConfig.html>`__."
" We will then use this QConfig for all the modules used in the patterns we "
"wish to quantize."
msgstr ""
"为了使用上面定义的运算符，用户必须定义一个满足 DTypeConfig 中指定约束的 QConfig。有关更多详细信息，请参阅 `DTypeConfig"
" 的文档 "
"<https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.DTypeConfig.html>`__。然后我们将使用此"
" QConfig 应用于我们希望量化的模式中所使用的所有模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "6. Quantize the model through prepare and convert"
msgstr "6. 通过准备和转换对模型进行量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Finally, we quantize the model by passing the BackendConfig we defined into "
"prepare and convert. This produces a quantized linear module and a fused "
"quantized conv-relu module."
msgstr ""
"最后，我们通过将定义的 BackendConfig 传递给准备和转换来量化模型。这将生成一个量化线性模块和一个融合的量化 conv-relu 模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(7. Experiment with faulty BackendConfig setups)"
msgstr "(7. 试验有问题的 BackendConfig 设置)"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As an experiment, here we modify the model to use conv-bn-relu instead of "
"conv-relu, but use the same BackendConfig, which doesn't know how to "
"quantize conv-bn-relu. As a result, only linear is quantized, but conv-bn-"
"relu is neither fused nor quantized."
msgstr ""
"作为试验，我们将模型修改为使用 conv-bn-relu 而不是 conv-relu，但使用相同的 BackendConfig，该配置不支持量化 "
"conv-bn-relu。因此，只有线性被量化，而 conv-bn-relu 既没有融合也没有被量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As another experiment, here we use the default QConfigMapping that doesn't "
"satisfy the dtype constraints specified in the backend. As a result, nothing"
" is quantized since the QConfigs are simply ignored."
msgstr ""
"作为另一个试验，我们使用了不满足后端中指定数据类型约束的默认 QConfigMapping。因此，由于 QConfig 被忽略，没有任何内容被量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Built-in BackendConfigs"
msgstr "内置 BackendConfigs"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"PyTorch quantization supports a few built-in native BackendConfigs under the"
" ``torch.ao.quantization.backend_config`` namespace:"
msgstr ""
"PyTorch 的量化在``torch.ao.quantization.backend_config``命名空间下支持一些内置的本地 "
"BackendConfigs:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`get_fbgemm_backend_config "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/fbgemm.py>`__:"
" for server target settings"
msgstr ""
"`get_fbgemm_backend_config "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/fbgemm.py>`__:"
" 用于服务器目标设置"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`get_qnnpack_backend_config "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/qnnpack.py>`__:"
" for mobile and edge device target settings, also supports XNNPACK quantized"
" ops"
msgstr ""
"`get_qnnpack_backend_config "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/qnnpack.py>`__:"
" 用于移动和边缘设备目标设置，也支持 XNNPACK 量化运算符"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`get_native_backend_config "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/native.py>`__"
" (default): a BackendConfig that supports a union of the operator patterns "
"supported in the FBGEMM and QNNPACK BackendConfigs"
msgstr ""
"`get_native_backend_config "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/native.py>`__"
" (默认): 一个支持 FBGEMM 和 QNNPACK BackendConfigs 中支持的运算符模式联合的 BackendConfig"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"There are also other BackendConfigs under development (e.g. for TensorRT and"
" x86), but these are still mostly experimental at the moment. If the user "
"wishes to integrate a new, custom backend with PyTorch’s quantization API, "
"they may define their own BackendConfigs using the same set of APIs used to "
"define the natively supported ones as in the example above."
msgstr ""
"还有其他 BackendConfigs 正在开发中（例如，用于 TensorRT 和 "
"x86），但目前这些大多仍处于实验阶段。如果用户希望将新的自定义后端集成到 PyTorch 的量化 API "
"中，他们可以使用与定义本地支持的后端配置所使用的 API 的相同集合来定义自己的 BackendConfigs，如上面的示例。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Further Reading"
msgstr "进一步阅读"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"How BackendConfig is used in FX graph mode quantization: "
"https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/fx/README.md"
msgstr ""
"BackendConfig 在 FX 图模式量化中的使用: "
"https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/fx/README.md"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Motivation and implementation details behind BackendConfig: "
"https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/README.md"
msgstr ""
"BackendConfig 的动机和实现细节: "
"https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/README.md"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Early design of BackendConfig: "
"https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-"
"Quantization-to-Custom-Backends.md"
msgstr ""
"BackendConfig 的早期设计: "
"https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-"
"Quantization-to-Custom-Backends.md"

#: ../../prototype/vulkan_workflow.rst:250
msgid "edit"
msgstr "编辑"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Introduction to Context Parallel"
msgstr "Context Parallel 简介"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Authors**: `Xilun Wu <https://github.com/XilunWu>`_, `Chien-Chin Huang "
"<https://github.com/fegin>`__"
msgstr ""
"**作者**: `Xilun Wu <https://github.com/XilunWu>`_, `Chien-Chin Huang "
"<https://github.com/fegin>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"|edit| View and edit this tutorial in `GitHub "
"<https://github.com/pytorch/tutorials/blob/main/prototype_source/context_parallel.rst>`__."
msgstr ""
"|编辑| 在 `GitHub "
"<https://github.com/pytorch/tutorials/blob/main/prototype_source/context_parallel.rst>`__"
" 中查看和编辑本教程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"What you will learn"
msgstr ""
"您将学习的内容"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Context Parallel APIs "
"<https://pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.experimental.context_parallel>`__"
msgstr ""
"`Context Parallel API "
"<https://pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.experimental.context_parallel>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`1M sequence training in TorchTitan with Context Parallel "
"<https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-"
"training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-"
"parallel/215082>`__"
msgstr ""
"`在 TorchTitan 中使用 Context Parallel 进行 1M 序列训练 "
"<https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-"
"training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-"
"parallel/215082>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Prerequisites"
msgstr ""
"前置条件"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch 2.7 or later"
msgstr "PyTorch 2.7 或更高版本"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Introduction"
msgstr "简介"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Context Parallel is an approach used in large language model training to "
"reduce peak activation size by sharding the long input sequence across "
"multiple devices. It breaks the constraint on input sequence length "
"resulting from peak memory usage on storing activations in Transformer "
"blocks."
msgstr ""
"Context Parallel 是在大型语言模型训练中使用的一种方法，通过将长输入序列分片到多个设备上来减少峰值激活大小。它突破了因 "
"Transformer 块中激活存储的峰值内存使用约束而导致的输入序列长度限制。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Ring Attention, a novel parallel implementation of the Attention layer, is "
"critical to performant Context Parallel. Ring Attention shuffles the KV "
"shards and calculates the partial attention scores, repeats until all KV "
"shards have been used on each device. Two Ring Attention variants have been "
"implemented: `the all-gather based pass-KV "
"<https://arxiv.org/abs/2407.21783>`__ and `the all-to-all based pass-KV "
"<https://openreview.net/forum?id=WsRHpHH4s0>`__:"
msgstr ""
"Ring Attention 是一种新颖的 Attention 层并行实现，对于高效的 Context Parallel 至关重要。Ring "
"Attention 通过对 KV 分片进行洗牌并计算部分 Attention 分数，一直重复直到每台设备都使用了所有 KV 分片。已实现两种 Ring "
"Attention 的变体：`基于全收集的传递 KV <https://arxiv.org/abs/2407.21783>`__ 和 `基于全交换的传递"
" KV <https://openreview.net/forum?id=WsRHpHH4s0>`__："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The all-gather based pass-KV algorithm is used in Llama3 training, which "
"initially performs an all-gather on the key and value tensors, followed by "
"computing the attention output for the local query tensor chunk. Our "
"modified all-gather based pass-KV algorithm concurrently all-gathers KV "
"shards and computes attention output for the local query tensor chunk using "
"local key and value tensor chunks, followed by a final computation of "
"attention output for the local query tensor and remaining KV shards. This "
"allows some degree of overlap between the attention computation and the all-"
"gather collective. For example, in the case of Llama3 training, we also "
"shard ``freq_cis`` over the sequence dimension."
msgstr ""
"基于全收集的传递 KV 算法用于 Llama3 "
"训练，该算法最初对关键和价值张量执行全收集，然后计算本地查询张量块的注意力输出。我们修改后的基于全收集的传递 KV 算法同时全收集 KV "
"分片，并使用本地关键和价值张量块计算本地查询张量块的注意力输出，然后对本地查询张量和剩余 KV "
"分片进行最终计算。这允许一定程度的注意力计算和全收集集合之间的重叠。例如，在 Llama3 "
"训练的情况下，我们还对序列维度上的``freq_cis``进行分片。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The all-to-all approach uses interleaved all-to-all collectives to ring "
"shuffle KV shards to overlap the SDPA (Scaled Dot Product Attention) "
"computation and the all-to-all communication necessary for the next SDPA."
msgstr "全交换方法使用交错的全交换集合对 KV 分片进行环状洗牌，以重叠 SDPA（缩放点积注意力）计算和下一次 SDPA 所需的全交换通信。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The Context Parallel APIs consist of two parts:"
msgstr "Context Parallel API 包含两个部分:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``context_parallel()`` allows users to create a Python context where the "
"SDPA function (``torch.nn.functional.scaled_dot_product_attention``) will be"
" automatically replaced with Ring Attention. To shard Tensors along a "
"dimension, simply pass the Tensors and their sharding dimensions to argument"
" ``buffers`` and ``buffer_seq_dims`` respectively. We recommend that users "
"add tensors computing along the sequence dimension to ``buffers`` and shard "
"them along this dimension. Taking Llama3 training as an example, missing "
"``freq_cis`` in ``buffers`` will result in a miscalculated rotary embedding."
msgstr ""
"``context_parallel()`` 允许用户创建一个 Python 上下文，其中 SDPA "
"函数（``torch.nn.functional.scaled_dot_product_attention``）将自动替换为 Ring "
"Attention。要沿某个维度对张量进行切分，只需将张量及其切分的维度分别传递给参数 ``buffers`` 和 "
"``buffer_seq_dims``。我们建议用户将沿序列维度计算的张量添加到 ``buffers`` 并沿此维度进行切分。以 Llama3 "
"训练为例，如果 ``buffers`` 中缺少 ``freq_cis``，将导致旋转嵌入的计算错误。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``set_rotate_method()`` allows users to choose between the all-gather based "
"pass-KV approach and the all-to-all based pass-KV approach."
msgstr ""
"``set_rotate_method()`` 允许用户在基于全收集的 pass-KV 方法和基于全到全的 pass-KV 方法之间进行选择。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Setup"
msgstr "设置"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With ``torch.distributed.tensor.experimental.context_parallel()``, users can"
" easily shard the Tensor input and parallelize the execution of the SDPA "
"function. To better demonstrate the usage of this API, we start with a "
"simple code snippet doing SDPA and then parallelize it using the API:"
msgstr ""
"通过使用 "
"``torch.distributed.tensor.experimental.context_parallel()``，用户可以轻松地切分张量输入，并并行化"
" SDPA 函数的执行。为了更好地展示此 API 的使用，我们从一个执行 SDPA 的简单代码片段开始，然后使用 API 并行化它："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Enable Context Parallel"
msgstr "启用上下文并行"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, let's first adapt it to a distributed program where each rank has the "
"same tensor input. Then we apply the context parallel API to shard to input "
"and distribute the computation across ranks:"
msgstr ""
"现在，让我们首先将其调整为一个分布式程序，其中每个 rank 都有相同的张量输入。然后我们应用上下文并行 API 来切分输入并在各个 rank "
"间分配计算："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"You can use the command ``torchrun --standalone --nnodes=1 --nproc-per-"
"node=4 cp_sdpa_example.py`` to launch the above context parallel SDPA on 4 "
"GPUs. We demonstrate the numeric correctness by comparing the output of Ring"
" Attention to that of SDPA on a single GPU."
msgstr ""
"您可以使用命令 ``torchrun --standalone --nnodes=1 --nproc-per-node=4 "
"cp_sdpa_example.py`` 在 4 个 GPU 上启动上述的上下文并行 SDPA。通过将 Ring Attention 的输出与单个 "
"GPU 上 SDPA 的输出进行比较，我们展示了其数值准确性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Select Rotation Approach"
msgstr "选择旋转方法"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"You can choose the desired shards rotation approach in Ring Attention by "
"using "
"``torch.distributed.tensor.experimental._attention.set_rotate_method()``:"
msgstr ""
"您可以通过使用 "
"``torch.distributed.tensor.experimental._attention.set_rotate_method()`` 在 "
"Ring Attention 中选择所需的切分旋转方法："

#: ../../prototype/vulkan_workflow.rst:250
msgid "The default rotation approach is the all-gather based pass-KV."
msgstr "默认的旋转方法是基于全收集的 pass-KV。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Conclusion"
msgstr "总结"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have learned how to parallelize the SDPA computation "
"along the sequence dimension easily with our Context Parallel APIs. For "
"design and implementation details, performance analysis, and an end-to-end "
"training example in `TorchTitan <https://github.com/pytorch/torchtitan>`__, "
"see our post on `PyTorch native long-context training "
"<https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-"
"training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-"
"parallel/215082>`__."
msgstr ""
"在本教程中，我们学习了如何使用 Context Parallel API 来轻松地沿序列维度并行化 SDPA 计算。有关设计和实现细节、性能分析以及在 "
"`TorchTitan <https://github.com/pytorch/torchtitan>`__ 中的端到端训练示例，请参阅我们在 "
"`PyTorch 原生长上下文训练 <https://discuss.pytorch.org/t/distributed-w-torchtitan-"
"breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-"
"pytorch-using-context-parallel/215082>`__ 上的帖子。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Profiling PyTorch RPC-Based Workloads"
msgstr "基于 PyTorch RPC 工作负载的性能分析"

#: ../../prototype/vulkan_workflow.rst:250
msgid "This tutorial has been deprecated."
msgstr "本教程已被弃用。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Redirecting to homepage..."
msgstr "正在重定向到主页..."

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) Flight Recorder for Debugging Stuck Jobs"
msgstr "（原型）用于调试卡住任务的飞行记录器"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Chirag Pandya <https://github.com/c-p-i-o>`_, `Junjie Wang "
"<https://github.com/fduwjj>`_"
msgstr ""
"**作者**: `Chirag Pandya <https://github.com/c-p-i-o>`_, `Junjie Wang "
"<https://github.com/fduwjj>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid "What you will learn"
msgstr "您将学到什么"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Learn about a new tool for debugging stuck jobs during distributed training."
msgstr "了解一种新的工具，可用于调试分布式训练过程中卡住的任务。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Learn how you can enable the tool and use the collected data for analyzing "
"stuck jobs."
msgstr "了解如何启用该工具并使用收集到的数据分析卡住的任务。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Prerequisites"
msgstr "先决条件"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch version 2.5 or later."
msgstr "PyTorch 版本 2.5 或更高。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`tabulate <https://pypi.org/project/tabulate/>`__. You can install by "
"running ``pip install tabulate``."
msgstr ""
"`tabulate <https://pypi.org/project/tabulate/>`__。您可以通过运行 ``pip install "
"tabulate`` 来安装。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Overview"
msgstr "概述"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"An AI distributed training job refers to the process of training a machine "
"learning model using multiple devices, such as GPUs or CPUs, connected in a "
"network. This approach allows for faster and more efficient training of "
"large models that require significant computational resources. An engineer’s"
" goal is to complete an AI training job as quickly as possible and make "
"continuous improvements so that subsequent training can be done faster. A "
"trained, usable model is the final desired outcome. One of the biggest "
"impediment to completing training is the concept of a *stuck job*."
msgstr ""
"AI 分布式训练任务是指使用多个设备（如 GPU 或 "
"CPU）连接到网络中来训练机器学习模型的过程。这种方法可以更快、更高效地训练需要大量计算资源的大型模型。工程师的目标是尽可能快地完成 AI "
"训练任务，并持续改进以便随后能更快地进行训练。经过训练、可用的模型是最终的目标成果。完成训练的一个最大障碍就是“卡住任务”的概念。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"A distributed AI training job is considered `stuck` when it stops making "
"meaningful progress for an extended period of time."
msgstr "当分布式 AI 训练任务在长时间内无法取得有意义的进展时，它被认为是`卡住`了。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "A job can get stuck for various reasons:"
msgstr "任务可能因多种原因而卡住："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Data Starvation:** This occurs when the training job is not receiving data"
" at the expected rate, possibly due to issues with the data pipeline or the "
"data source."
msgstr "**数据匮乏**：这发生在训练任务未按照预期速率接收数据，可能是由于数据管道或数据源的问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Resource Constraints:** If the system running the job does not have enough"
" computational resources (such as CPU, GPU, or memory), the job might not be"
" able to proceed."
msgstr "**资源限制**：如果运行该任务的系统没有足够的计算资源（如 CPU、GPU 或内存），任务可能无法继续。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Network Issues:** In a distributed training setup, different parts of the "
"model or data may be processed on different devices. If there are network "
"issues, communication between these devices may be disrupted, causing the "
"job to get stuck."
msgstr ""
"**网络问题**：在分布式训练设置中，模型或数据的不同部分可能在不同的设备上处理。如果出现网络问题，这些设备之间的通信可能会中断，导致任务卡住。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Software Bugs or Errors:** Errors in the training code or the underlying "
"libraries and frameworks can also cause a job to get stuck."
msgstr "**软件错误或错误**：训练代码或底层库和框架中的错误也可能导致任务卡住。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Synchronization Issues:** In distributed training, different parts of the "
"computation are often run in parallel and need to be synchronized at certain"
" points. If this synchronization fails, the job can get stuck. For example, "
"a deadlock can occur if one or more ranks fail to join a collective while "
"the remaining ranks have joined. This results in an indefinite wait for the "
"job to progress."
msgstr ""
"**同步问题**：在分布式训练中，不同部分的计算通常是并行运行的，需要在某些点进行同步。如果这种同步失败，任务可能会卡住。例如，如果一个或多个 rank"
" 未能加入一个集合，而其余 rank 已经加入，就会出现死锁，这会导致任务无限期地等待进展。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Flight Recorder, as the name suggests, captures diagnostics information as "
"collectives run. The captured diagnostic information is used to help "
"identify the root causes of issues when jobs become stuck. Flight Recorder "
"consists of two core parts:"
msgstr "正如名称所示，飞行记录器可以在运行集合时捕获诊断信息。捕获的诊断信息用来帮助识别任务卡住时问题的根本原因。飞行记录器由两个核心部分组成："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The collection portion: when enabled, information about collectives is "
"recorded in an in-memory circular buffer. Upon job timeout, or on demand, "
"the in-memory buffer can be retrieved or dumped to file."
msgstr "收集部分：启用后，有关集合的信息会记录在内存中的循环缓冲区内。在任务超时或按需读取时，可以检索或将内存中的缓冲区写入文件。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"An analyzer script is available in the `tools/flight_recorder "
"<https://github.com/pytorch/pytorch/tree/main/tools/flight_recorder>`__ "
"directory (details below)."
msgstr ""
"分析脚本位于 `tools/flight_recorder "
"<https://github.com/pytorch/pytorch/tree/main/tools/flight_recorder>`__ "
"目录中（详见下文）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The analyzer script runs known heuristics using the collected data and "
"attempts to automatically identify the underlying issue that caused the job "
"to stall."
msgstr "分析脚本利用已知的启发式方法处理收集到的数据，并尝试自动识别导致任务停滞的潜在问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Enabling Flight Recorder"
msgstr "启用飞行记录器"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"There are three required environment variables to get the initial version of"
" Flight Recorder working."
msgstr "设置飞行记录器工作的初始版本需要三个必需的环境变量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``TORCH_NCCL_TRACE_BUFFER_SIZE = (0, N)``: Setting ``N`` to a positive "
"number enables collection. ``N`` represents the number of entries that will "
"be kept internally in a circular buffer. We recommended to set this value at"
" *2000*. The default value is ``2000``."
msgstr ""
"``TORCH_NCCL_TRACE_BUFFER_SIZE = (0, N)``：将 ``N`` 设置为正数以启用收集。``N`` "
"表示将保存在内部循环缓冲区中的条目数量。建议将此值设置为 *2000*。默认值为 ``2000``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``TORCH_NCCL_DUMP_ON_TIMEOUT = (true, false)``: Setting this to ``true`` "
"will write out diagnostic files to disk on job timeout. If enabled, there "
"will be one file per rank output in the job's running directory. The default"
" value is ``false``."
msgstr ""
"``TORCH_NCCL_DUMP_ON_TIMEOUT = (true, false)``：将此项设置为 ``true`` "
"会在任务超时时将诊断文件写入磁盘。如果启用，则每个 rank 都将在任务的运行目录中生成一个文件。默认值为 ``false``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``TORCH_NCCL_DEBUG_INFO_TEMP_FILE``: Setting the path where the flight "
"recorder will be dumped with file prefix. One file per rank. The default "
"value is ``/tmp/nccl_trace_rank_``."
msgstr ""
"``TORCH_NCCL_DEBUG_INFO_TEMP_FILE``：设置飞行记录器转储的文件前缀路径。每个 rank 一个文件。默认值为 "
"``/tmp/nccl_trace_rank_``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Optional settings:**"
msgstr "**可选设置：**"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``TORCH_NCCL_TRACE_CPP_STACK = (true, false)``: Setting this to true enables"
" C++ stack traces to be captured in Flight Recorder. C++ stack traces can be"
" useful in providing the exact code path from a PyTorch Python call down to "
"the primitive C++ implementation. Also see ``TORCH_SYMBOLIZE_MODE`` in "
"additional settings."
msgstr ""
"``TORCH_NCCL_TRACE_CPP_STACK = (true, false)``：将此项设置为 true 可在飞行记录器中捕获 C++ "
"堆栈踪迹。C++ 堆栈踪迹有助于提供从 PyTorch Python 调用到原始 C++ 实现的确切代码路径。另见 "
"``TORCH_SYMBOLIZE_MODE`` 中的附加设置。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``TORCH_NCCL_ENABLE_TIMING = (true, false)``: Setting this to ``true`` will "
"enable additional cuda events at the start of each collective and records "
"the *duration* of each collective. This may incur some CPU overhead. In the "
"collected data, the *duration* field indicates how long each collective took"
" to execute."
msgstr ""
"``TORCH_NCCL_ENABLE_TIMING = (true, false)``：将此项设置为 ``true`` 会在每个集合的开始添加额外的 "
"cuda 事件，并记录每个集合的*持续时间*。这可能会导致一定的 CPU 开销。在收集的数据中，*持续时间*字段表示每个集合的执行时间。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Additional Settings"
msgstr "附加设置"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``TORCH_SYMBOLIZE_MODE = (dladdr, addr2line, fast)``: This setting "
"determines the program used to retrieve C++ traces from a running program."
msgstr ""
"``TORCH_SYMBOLIZE_MODE = (dladdr, addr2line, fast)``：此设置确定要用来从运行程序中检索 C++ "
"堆栈踪迹的程序。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The default setting is ``addr2line``."
msgstr "默认设置为 ``addr2line``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``fast`` is a new experimental mode that is shown to be much faster than the"
" traditional ``addr2line``. Use this setting in conjunction with "
"``TORCH_NCCL_TRACE_CPP_STACK`` to collect C++ traces in the Flight Recorder "
"data."
msgstr ""
"``fast`` 是一种新的实验模式，被证明比传统的 ``addr2line`` 快得多。请将此设置与 "
"``TORCH_NCCL_TRACE_CPP_STACK`` 结合使用以在飞行记录器数据中收集 C++ 堆栈踪迹。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If you prefer not to have the flight recorder data dumped into the local "
"disk but rather onto your own storage, you can define your own writer class."
" This class should inherit from class ``::c10d::DebugInfoWriter`` `(code) "
"<https://github.com/pytorch/pytorch/blob/release/2.5/torch/csrc/distributed/c10d/NCCLUtils.hpp#L237>`__"
" and then register the new writer using "
"``::c10d::DebugInfoWriter::registerWriter`` `(code) "
"<https://github.com/pytorch/pytorch/blob/release/2.5/torch/csrc/distributed/c10d/NCCLUtils.hpp#L242>`__"
" before we initiate PyTorch distributed."
msgstr ""
"如果您希望将飞行记录器数据存储到您的自定义存储而不是本地磁盘，您可以定义自己的写入类。此类应继承自类 "
"``::c10d::DebugInfoWriter`` `(代码) "
"<https://github.com/pytorch/pytorch/blob/release/2.5/torch/csrc/distributed/c10d/NCCLUtils.hpp#L237>`__，然后在启动"
" PyTorch 分布式之前使用 ``::c10d::DebugInfoWriter::registerWriter`` `(代码) "
"<https://github.com/pytorch/pytorch/blob/release/2.5/torch/csrc/distributed/c10d/NCCLUtils.hpp#L242>`__"
" 注册新写入器。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Retrieving Flight Recorder Data via an API"
msgstr "通过 API 获取飞行记录器数据"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"You can also retrieve Flight Recorder data with an API call. The API with "
"the default arguments is shown below:"
msgstr "您也可以通过 API 调用检索飞行记录器数据。使用默认参数的 API 如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid "To view the data, you can ``unpickle`` it as shown below:"
msgstr "要查看数据，可以按照以下方式 ``unpickle`` 它："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Flight Recorder File Formats"
msgstr "飞行记录器文件格式"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Flight Recorder files are dumped in ``pickle`` format. Files are written to "
"local disks or mounted shared NFS folders."
msgstr "飞行记录器文件以 ``pickle`` 格式转储。文件写入本地磁盘或已挂载的共享 NFS 文件夹。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The contents of a Flight Recorder ``unpickled`` file are shown below:"
msgstr "飞行记录器 ``unpickled`` 文件的内容如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Analyzing Flight Recorder Dumps"
msgstr "分析飞行记录器转储文件"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We have convenient scripts available in `pytorch/tools/flight_recorder` "
"directory for analyzing captured data."
msgstr "我们在 `pytorch/tools/flight_recorder` 目录中提供了分析捕获数据的便捷脚本。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "To run the convenience script, follow these steps:"
msgstr "要运行便捷脚本，请按以下步骤操作："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Copy all files from a rank into a single directory."
msgstr "将每个 rank 的所有文件复制到一个目录中。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "To run the script, use this command:"
msgstr "要运行脚本，请使用以下命令："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If you install the PyTorch nightly build or build from scratch with "
"``USE_DISTRIBUTED=1``, you can directly use the following command directly:"
msgstr "如果您安装了 PyTorch 的最新夜间版本或从头构建并带有 ``USE_DISTRIBUTED=1``，您可以直接使用以下命令："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Currently, we support two modes for the analyzer script. The first mode "
"allows the script to apply some heuristics to the parsed flight recorder "
"dumps to generate a report identifying potential culprits for the timeout. "
"The second mode is simply outputs the raw dumps. By default, the script "
"prints flight recoder dumps for all ranks and all ``ProcessGroups``(PGs). "
"This can be narrowed down to certain ranks and PGs using the *--selected-"
"ranks* argument for ranks and *--pg-filters* argument for PGs. An example "
"command is:"
msgstr ""
"目前，我们为分析脚本支持两种模式。第一种模式允许脚本对解析的飞行记录器转储文件应用一些启发式方法，以生成识别超时潜在原因的报告。第二种模式则仅输出原始转储文件。默认情况下，脚本打印所有"
" rank 和所有 ``ProcessGroups``（PG）的飞行记录器转储。这可以通过 *--selected-ranks* 参数（指定 "
"rank）和 *--pg-filters* 参数（指定 PG）缩小范围。例如命令如下："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Caveat: tabulate module is needed, so you might need pip install it first."
msgstr "注意：tabulate 模块是必须的，因此您可能需要先安装它。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "An End-to-End Example"
msgstr "端到端示例"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To demonstrate the use of Flight Recorder, we will use a small program where"
" we induce mismatched collectives. In this example, ``rank0`` is programmed "
"to do an additional collective. The Flight Recorder dump files are saved to "
"the ``/tmp`` directory. For demonstration purposes, we named this program "
"``crash.py``."
msgstr ""
"为了演示飞行记录器的用法，我们将编写一个引发不匹配集合的小程序。在此示例中，``rank0`` 被编程为执行一个额外的集合。飞行记录器转储文件被保存在 "
"``/tmp`` 目录中。为了演示目的，我们将此程序命名为 ``crash.py``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Please note that this is a simplified example. In real-world scenarios, the "
"process would involve more complexities."
msgstr "请注意，这是一个简化的示例。在实际场景中，过程会更加复杂。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "To run this program, use ``torchrun``:"
msgstr "要运行此程序，使用 ``torchrun``："

#: ../../prototype/vulkan_workflow.rst:250
msgid "You should see two files in the ``/tmp`` directory:"
msgstr "您将在 ``/tmp`` 目录中看到两个文件："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Finally, to analyze these two files, we use the ``torchfrtrace`` command:"
msgstr "最后，我们使用 ``torchfrtrace`` 命令分析这两个文件："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The output from the trace command is meant to be human-readable. It includes"
" information about the set of collectives that caused a failure. The output "
"for the command above is shown below. We can clearly see that rank 1 did not"
" join the \"all_reduce\" collective."
msgstr ""
"跟踪命令的输出旨在使人类易于阅读。它包含导致失败的集合的信息。上述命令的输出如下所示。我们可以清楚地看到 Rank 1 未加入 "
"\"all_reduce\" 集合。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have learned about a new PyTorch diagnostic tool called"
" Flight Recorder. We have discussed how to enable Flight Recorder to collect"
" diagnostic data from a machine. Additionally, we explored how to analyze "
"the data captured from the Flight Recorder using a convenience script "
"located in the `tools/flight_recorder "
"<https://github.com/pytorch/pytorch/tree/main/tools/flight_recorder>`__ "
"directory of the PyTorch repository."
msgstr ""
"在本教程中，我们学习了一种新的PyTorch诊断工具，名为Flight Recorder。我们讨论了如何启用Flight "
"Recorder以从机器收集诊断数据。此外，我们还探索了如何使用位于PyTorch代码库`tools/flight_recorder "
"<https://github.com/pytorch/pytorch/tree/main/tools/flight_recorder>`__目录中的便捷脚本来分析从Flight"
" Recorder捕获的数据。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_fx_graph_mode_ptq_dynamic.py>`"
" to download the full example code"
msgstr ""
"点击:ref:`这里 "
"<sphx_glr_download_prototype_fx_graph_mode_ptq_dynamic.py>`下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) FX Graph Mode Post Training Dynamic Quantization"
msgstr "（原型）FX Graph模式后训练动态量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author**: `Jerry Zhang <https://github.com/jerryzh168>`_"
msgstr "**作者**：`Jerry Zhang <https://github.com/jerryzh168>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the steps to do post training dynamic quantization "
"in graph mode based on ``torch.fx``. We have a separate tutorial for `FX "
"Graph Mode Post Training Static Quantization "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`_, "
"comparison between FX Graph Mode Quantization and Eager Mode Quantization "
"can be found in the `quantization docs "
"<https://pytorch.org/docs/master/quantization.html#quantization-api-"
"summary>`_"
msgstr ""
"本教程介绍了基于``torch.fx``进行图模式后训练动态量化的步骤。我们有一个单独的教程，介绍`FX 图模式后训练静态量化 "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`_，FX图模式量化与Eager模式量化的比较可以在"
" `量化文档 <https://pytorch.org/docs/master/quantization.html#quantization-api-"
"summary>`_ 中找到。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"tldr; The FX Graph Mode API for dynamic quantization looks like the "
"following:"
msgstr "简单总结：FX图模式API动态量化如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we’ll apply dynamic quantization to an LSTM-based next "
"word-prediction model, closely following the word language model from the "
"PyTorch examples. We will copy the code from `Dynamic Quantization on an "
"LSTM Word Language Model "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`_"
" and omit the descriptions."
msgstr ""
"在本教程中，我们将对基于LSTM的下一个单词预测模型应用动态量化，紧密跟随PyTorch示例中的词语言模型。我们将从`Dynamic "
"Quantization on an LSTM Word Language Model "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`_复制代码并省略描述。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Define the Model, Download Data and Model"
msgstr "1. 定义模型，下载数据和模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Download the `data "
"<https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip>`_"
" and unzip to data folder"
msgstr ""
"下载`数据 "
"<https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip>`_并解压到数据文件夹"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Download model to the data folder:"
msgstr "将模型下载到数据文件夹："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Define the model:"
msgstr "定义模型："

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Post Training Dynamic Quantization"
msgstr "2. 后训练动态量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now we can dynamically quantize the model. We can use the same function as "
"post training static quantization but with a dynamic qconfig."
msgstr "现在我们可以动态量化模型。我们可以使用后训练静态量化的相同函数，但使用动态qconfig。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For dynamically quantized objects, we didn't do anything in ``prepare_fx`` "
"for modules, but will insert observers for weight for dynamically "
"quantizable forunctionals and torch ops. We also fuse the modules like Conv "
"+ Bn, Linear + ReLU."
msgstr ""
"对于动态量化的对象，我们在``prepare_fx``中没有对模块做任何操作，而是会为动态可量化的函数和torch操作插入权重观察者。我们还会将模块融合，比如Conv"
" + Bn、Linear + ReLU。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In convert we'll convert the float modules to dynamically quantized modules "
"and convert float ops to dynamically quantized ops. We can see in the "
"example model, ``nn.Embedding``, ``nn.Linear`` and ``nn.LSTM`` are "
"dynamically quantized."
msgstr ""
"在转换阶段，我们将浮点模块转换为动态量化模块，并将浮点操作转换为动态量化操作。在示例模型中，我们可以看到``nn.Embedding``、``nn.Linear``和``nn.LSTM``被动态量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Now we can compare the size and runtime of the quantized model."
msgstr "现在我们可以比较量化后的模型的大小和运行时。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"There is a 4x size reduction because we quantized all the weights in the "
"model (nn.Embedding, nn.Linear and nn.LSTM) from float (4 bytes) to "
"quantized int (1 byte)."
msgstr ""
"因模型中的所有权重（nn.Embedding、nn.Linear和nn.LSTM）从浮点数（4字节）量化为量化整数（1字节），模型大小减少了4倍。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"There is a roughly 2x speedup for this model. Also note that the speedup may"
" vary depending on model, device, build, input batch sizes, threading etc."
msgstr "对于该模型，大约有2倍的加速。而且请注意，加速可能因模型、设备、构建、输入批大小、线程等因素而有所不同。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Conclusion"
msgstr "3. 结论"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the api for post training dynamic quantization in "
"FX Graph Mode, which dynamically quantizes the same modules as Eager Mode "
"Quantization."
msgstr "本教程介绍了FX图模式后训练动态量化的API，它动态地量化与Eager模式量化相同的模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Total running time of the script:** ( 0 minutes  0.000 seconds)"
msgstr "**脚本总运行时间:** (0分钟 0.000秒)"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: fx_graph_mode_ptq_dynamic.py "
"<fx_graph_mode_ptq_dynamic.py>`"
msgstr ""
":download:`下载Python源代码: fx_graph_mode_ptq_dynamic.py "
"<fx_graph_mode_ptq_dynamic.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: fx_graph_mode_ptq_dynamic.ipynb "
"<fx_graph_mode_ptq_dynamic.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: fx_graph_mode_ptq_dynamic.ipynb "
"<fx_graph_mode_ptq_dynamic.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`画廊由Sphinx-Gallery生成 <https://sphinx-gallery.github.io>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) FX Graph Mode Post Training Static Quantization"
msgstr "（原型）FX图模式后训练静态量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Jerry Zhang <https://github.com/jerryzh168>`_ **Edited by**: "
"`Charles Hernandez <https://github.com/HDCharles>`_"
msgstr ""
"**作者**：`Jerry Zhang <https://github.com/jerryzh168>`_ **编辑者**：`Charles "
"Hernandez <https://github.com/HDCharles>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the steps to do post training static quantization "
"in graph mode based on `torch.fx "
"<https://github.com/pytorch/pytorch/blob/master/torch/fx/__init__.py>`_. The"
" advantage of FX graph mode quantization is that we can perform quantization"
" fully automatically on the model. Although there might be some effort "
"required to make the model compatible with FX Graph Mode Quantization "
"(symbolically traceable with ``torch.fx``), we'll have a separate tutorial "
"to show how to make the part of the model we want to quantize compatible "
"with FX Graph Mode Quantization. We also have a tutorial for `FX Graph Mode "
"Post Training Dynamic Quantization "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html>`_. "
"tldr; The FX Graph Mode API looks like the following:"
msgstr ""
"本教程介绍了基于`torch.fx "
"<https://github.com/pytorch/pytorch/blob/master/torch/fx/__init__.py>`_进行图模式后训练静态量化的步骤。FX图模式量化的优势在于我们可以在模型上完全自动执行量化。尽管可能需要一些努力使模型兼容FX图模式量化（可符号跟踪的``torch.fx``），但我们会有一个单独的教程展示如何让我们想要量化的模型部分与FX图模式量化兼容。我们还提供了一个教程介绍`FX"
" 图模式后训练动态量化 "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html>`_。简单总结：FX图模式API如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Motivation of FX Graph Mode Quantization"
msgstr "1. FX图模式量化的动机"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Currently, PyTorch only has eager mode quantization as an alternative: "
"`Static Quantization with Eager Mode in PyTorch "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_."
msgstr ""
"目前，PyTorch只有一种替代方案：Eager模式静态量化：`PyTorch中的Eager模式静态量化 "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can see there are multiple manual steps involved in the eager mode "
"quantization process, including:"
msgstr "可以看到，在Eager模式量化过程中涉及多个手动步骤，包括："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Explicitly quantize and dequantize activations-this is time consuming when "
"floating point and quantized operations are mixed in a model."
msgstr "显式量化和反量化激活-当一个模型混合了浮点和量化操作时，这会非常耗时。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Explicitly fuse modules-this requires manually identifying the sequence of "
"convolutions, batch norms and relus and other fusion patterns."
msgstr "显式融合模块-需要手动识别卷积、批归一化和ReLU等融合模式的序列。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Special handling is needed for pytorch tensor operations (like add, concat "
"etc.)"
msgstr "对于PyTorch张量操作（如加法、连接等）需要特殊处理。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Functionals did not have first class support (functional.conv2d and "
"functional.linear would not get quantized)"
msgstr "函数没有一流支持（functional.conv2d和functional.linear不会被量化）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Most of these required modifications comes from the underlying limitations "
"of eager mode quantization. Eager mode works in module level since it can "
"not inspect the code that is actually run (in the forward function), "
"quantization is achieved by module swapping, and we don’t know how the "
"modules are used in forward function in eager mode, so it requires users to "
"insert QuantStub and DeQuantStub manually to mark the points they want to "
"quantize or dequantize. In graph mode, we can inspect the actual code that’s"
" been executed in forward function (e.g. aten function calls) and "
"quantization is achieved by module and graph manipulations. Since graph mode"
" has full visibility of the code that is run, our tool is able to "
"automatically figure out things like which modules to fuse and where to "
"insert observer calls, quantize/dequantize functions etc., we are able to "
"automate the whole quantization process."
msgstr ""
"这些大多数需要的修改都来源于Eager模式量化的基本限制。Eager模式作用于模块级别，因为它无法检查实际运行的代码（forward函数中），通过模块交换实现量化，而且在Eager模式下我们不知道模块在forward函数中的使用情况，因此需要用户手动插入QuantStub和DeQuantStub标记他们想要量化或反量化的位置。在图模式中，我们可以检查forward函数中实际执行代码的情况（例如aten函数调用），通过模块和图操作实现量化。由于图模式具有代码运行的完全可见性，我们的工具可以自动识别要融合的模块和插入观察者调用的位置、量化/反量化函数等，从而实现整个量化过程的自动化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Advantages of FX Graph Mode Quantization are:"
msgstr "FX图模式量化的优点是："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Simple quantization flow, minimal manual steps"
msgstr "简单的量化流程，手动步骤最少"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Unlocks the possibility of doing higher level optimizations like automatic "
"precision selection"
msgstr "解锁执行更高层次优化的可能性，例如自动精度选择"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Define Helper Functions and Prepare Dataset"
msgstr "2. 定义助手函数并准备数据集"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We’ll start by doing the necessary imports, defining some helper functions "
"and prepare the data. These steps are identitcal to `Static Quantization "
"with Eager Mode in PyTorch "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_."
msgstr ""
"我们将首先进行必要的导入，定义一些助手函数并准备数据。这些步骤与`PyTorch中的Eager模式静态量化 "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_中的步骤完全相同。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To run the code in this tutorial using the entire ImageNet dataset, first "
"download imagenet by following the instructions at here `ImageNet Data "
"<http://www.image-net.org/download>`_. Unzip the downloaded file into the "
"'data_path' folder."
msgstr ""
"要使用整个ImageNet数据集运行本教程中的代码，请首先通过以下说明`ImageNet Data <http://www.image-"
"net.org/download>`_下载imagenet。 将下载的文件解压到&apos;data_path&apos;文件夹。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Download the `torchvision resnet18 model "
"<https://download.pytorch.org/models/resnet18-f37072fd.pth>`_ and rename it "
"to ``data/resnet18_pretrained_float.pth``."
msgstr ""
"下载`torchvision resnet18 模型 "
"<https://download.pytorch.org/models/resnet18-f37072fd.pth>`_ "
"并将其重命名为``data/resnet18_pretrained_float.pth``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Set model to eval mode"
msgstr "3. 设置模型为评估模式"

#: ../../prototype/vulkan_workflow.rst:250
msgid "For post training quantization, we'll need to set model to eval mode."
msgstr "对于后训练量化，我们需要设置模型为评估模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "4. Specify how to quantize the model with ``QConfigMapping``"
msgstr "4. 使用``QConfigMapping``指定如何量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We use the same qconfig used in eager mode quantization, ``qconfig`` is just"
" a named tuple of the observers for activation and weight. "
"``QConfigMapping`` contains mapping information from ops to qconfigs:"
msgstr ""
"我们使用Eager模式量化中使用的qconfig，``qconfig``只是用于激活和权重的观察者的命名元组。``QConfigMapping``包含从操作到qconfig的映射信息："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Utility functions related to ``qconfig`` can be found in the `qconfig "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/qconfig.py>`_"
" file while those for ``QConfigMapping`` can be found in the "
"`qconfig_mapping "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/qconfig_mapping_utils.py>`"
msgstr ""
"与``qconfig``相关的实用功能可在`qconfig "
"<https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/qconfig.py>`_文件中找到，``QConfigMapping``的相关功能可见`qconfig_mapping"
" "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/qconfig_mapping_utils.py>`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "5. Prepare the Model for Post Training Static Quantization"
msgstr "5. 为后训练静态量化准备模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"prepare_fx folds BatchNorm modules into previous Conv2d modules, and insert "
"observers in appropriate places in the model."
msgstr "prepare_fx将BatchNorm模块折叠到前面的Conv2d模块中，并在模型中的适当位置插入观察者。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "6. Calibration"
msgstr "6. 校准"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Calibration function is run after the observers are inserted in the model. "
"The purpose for calibration is to run through some sample examples that is "
"representative of the workload (for example a sample of the training data "
"set) so that the observers in the model are able to observe the statistics "
"of the Tensors and we can later use this information to calculate "
"quantization parameters."
msgstr ""
"校准功能在模型中插入观察者后运行。校准的目的是运行一些能代表工作负载的样本示例（例如训练数据集的样本），以便模型中的观察者能够观察张量的统计信息，以后可以使用这些信息计算量化参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "7. Convert the Model to a Quantized Model"
msgstr "7. 将模型转换为量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``convert_fx`` takes a calibrated model and produces a quantized model."
msgstr "``convert_fx``将校准后的模型转换为量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "8. Evaluation"
msgstr "8. 评估"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We can now print the size and accuracy of the quantized model."
msgstr "我们现在可以打印量化模型的大小和准确性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If you want to get better accuracy or performance,  try changing the "
"`qconfig_mapping`. We plan to add support for graph mode in the Numerical "
"Suite so that you can easily determine the sensitivity towards quantization "
"of different modules in a model. For more information, see `PyTorch Numeric "
"Suite Tutorial "
"<https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html>`_"
msgstr ""
"如果您想获得更好的准确性或性能，可以尝试更改`qconfig_mapping`。我们计划在数值套件中添加对图模式的支持，以便您可以轻松确定模型中不同模块对量化的敏感性。有关更多信息，请参阅`PyTorch"
" Numeric Suite教程 "
"<https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "9. Debugging Quantized Model"
msgstr "9. 调试量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can also print the weight for quantized a non-quantized convolution op to"
" see the difference, we'll first call fuse explicitly to fuse the "
"convolution and batch norm in the model: Note that ``fuse_fx`` only works in"
" eval mode."
msgstr ""
"我们还可以打印量化和非量化卷积操作的权重以查看差异，我们将首先显式调用融合以融合模型中的卷积和批归一化操作：注意``fuse_fx``仅在评估模式下工作。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "10. Comparison with Baseline Float Model and Eager Mode Quantization"
msgstr "10. 与基线浮点模型和Eager模式量化的比较"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this section, we compare the model quantized with FX graph mode "
"quantization with the model quantized in eager mode. FX graph mode and eager"
" mode produce very similar quantized models, so the expectation is that the "
"accuracy and speedup are similar as well."
msgstr ""
"在本节中，我们将比较使用FX图模式量化的模型与使用Eager模式量化的模型。FX图模式和Eager模式产生非常相似的量化模型，因此预计它们的准确性和加速效果也类似。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can see that the model size and accuracy of FX graph mode and eager mode "
"quantized model are pretty similar."
msgstr "我们可以看到，FX图模式和Eager模式量化模型的模型大小和准确性非常相似。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Running the model in AIBench (with single threading) gives the following "
"result:"
msgstr "在AIBench中运行模型（单线程）得到以下结果："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As we can see for resnet18 both FX graph mode and eager mode quantized model"
" get similar speedup over the floating point model, which is around 2-4x "
"faster than the floating point model. But the actual speedup over floating "
"point model may vary depending on model, device, build, input batch sizes, "
"threading etc."
msgstr ""
"如我们所见，对于resnet18，FX图模式和Eager模式量化模型相较浮点模型都获得了类似的加速度，大约比浮点模型快2-4倍。但是，相较于浮点模型的实际加速可能因模型、设备、构建、输入批大小、线程等因素而有所不同。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) FX Graph Mode Quantization User Guide"
msgstr "（原型）FX图模式量化用户指南"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"FX Graph Mode Quantization requires a symbolically traceable model. We use "
"the FX framework to convert a symbolically traceable nn.Module instance to "
"IR, and we operate on the IR to execute the quantization passes. Please post"
" your question about symbolically tracing your model in `PyTorch Discussion "
"Forum <https://discuss.pytorch.org/c/quantization/17>`_"
msgstr ""
"FX图模式量化需要一个可符号跟踪的模型。我们使用FX框架将一个可符号跟踪的nn.Module实例转换为IR，并操作IR以执行量化过程。如有关于符号跟踪模型的问题，请在`PyTorch讨论论坛"
" <https://discuss.pytorch.org/c/quantization/17>`_中发布您的问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Quantization will only work on the symbolically traceable parts of your "
"model. The data dependent control flow-if statements / for loops, and so on "
"using symbolically traced values-are one common pattern which is not "
"supported. If your model is not symbolically traceable end to end, you have "
"a couple of options to enable FX Graph Mode Quantization only on a part of "
"the model. You can use any combination of these options:"
msgstr ""
"量化只适用于模型中可以符号化跟踪的部分。基于数据的控制流（if语句/for循环等使用符号化跟踪值）是常见的一种模式，但不被支持。如果您的模型无法端到端符号化跟踪，您有以下几个选项可以仅对模型的一部分启用FX图模式量化。您可以使用以下选项的任意组合："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Non traceable code doesn’t need to be quantized"
msgstr "不可跟踪的代码不需要量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Symbolically trace only the code that needs to be quantized"
msgstr "只符号化跟踪需要量化的代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Skip symbolic tracing the non-traceable code"
msgstr "跳过符号化跟踪不可跟踪的代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Non traceable code needs to be quantized"
msgstr "不可跟踪的代码需要量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Refactor your code to make it symbolically traceable"
msgstr "重构代码使其可符号化跟踪"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Write your own observed and quantized submodule"
msgstr "编写自己的已观察和量化的子模块"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If the code that is not symbolically traceable does not need to be "
"quantized, we have the following two options to run FX Graph Mode "
"Quantization:"
msgstr "如果不可符号化跟踪的代码不需要量化，我们有以下两个选项来运行FX图模式量化："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When the whole model is not symbolically traceable but the submodule we want"
" to quantize is symbolically traceable, we can run quantization only on that"
" submodule."
msgstr "当整个模型不可符号化跟踪，但我们想要量化的子模块可符号化跟踪时，我们可以仅对该子模块运行量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "before:"
msgstr "之前："

#: ../../prototype/vulkan_workflow.rst:250
msgid "after:"
msgstr "之后："

#: ../../prototype/vulkan_workflow.rst:250
msgid "quantization code:"
msgstr "量化代码："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note if original model needs to be preserved, you will have to copy it "
"yourself before calling the quantization APIs."
msgstr "注意如果需要保留原始模型，您需要在调用量化API之前自行复制。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Skip symbolically trace the non-traceable code"
msgstr "跳过符号化跟踪不可跟踪的代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When we have some non-traceable code in the module, and this part of code "
"doesn’t need to be quantized, we can factor out this part of the code into a"
" submodule and skip symbolically trace that submodule."
msgstr "当我们在模块中有一些不可跟踪的代码，并且这部分代码不需要量化时，我们可以将这部分代码拆分为一个子模块并跳过符号化跟踪该子模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "before"
msgstr "之前"

#: ../../prototype/vulkan_workflow.rst:250
msgid "after, non-traceable parts moved to a module and marked as a leaf"
msgstr "之后，将不可跟踪的部分移到一个模块并标记为叶子"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If the code that is not symbolically traceable needs to be quantized, we "
"have the following two options:"
msgstr "如果不可符号化跟踪的代码需要量化，我们有以下两个选项："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If it is easy to refactor the code and make the code symbolically traceable,"
" we can refactor the code and remove the use of non-traceable constructs in "
"python."
msgstr "如果重构代码并使其可符号化跟踪较容易，那么我们可以重构代码并去除在Python中使用不可跟踪的结构。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"More information about symbolic tracing support can be found `here "
"<https://pytorch.org/docs/stable/fx.html#limitations-of-symbolic-tracing>`_."
msgstr ""
"关于符号化跟踪支持的更多信息可以查看 `这里 <https://pytorch.org/docs/stable/fx.html#limitations-"
"of-symbolic-tracing>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This is not symbolically traceable because in x.view(*new_x_shape) unpacking"
" is not supported, however, it is easy to remove the unpacking since x.view "
"also supports list input."
msgstr ""
"此处不可符号化跟踪，因为x.view(*new_x_shape)的解包操作不被支持，然而，我们可以轻松移除解包操作，因为x.view也支持列表输入。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This can be combined with other approaches and the quantization code depends"
" on the model."
msgstr "这可以与其他方法结合使用，而量化代码依赖于模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If the non-traceable code can’t be refactored to be symbolically traceable, "
"for example it has some loops that can’t be eliminated, like nn.LSTM, we’ll "
"need to factor out the non-traceable code to a submodule (we call it "
"CustomModule in fx graph mode quantization) and define the observed and "
"quantized version of the submodule (in post training static quantization or "
"quantization aware training for static quantization) or define the quantized"
" version (in post training dynamic and weight only quantization)"
msgstr ""
"如果不可跟踪的代码不能重构为可符号化跟踪，例如其中有些循环无法消除（例如 "
"nn.LSTM），我们需要将不可跟踪的代码拆分成一个子模块（在FX图模式量化中我们称其为 "
"CustomModule），并定义该子模块的已观察和量化版本（用于静态后训练量化或静态量化感知训练），或者定义量化版本（用于动态后训练量化和仅权重量化）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"1. Factor out non_traceable_code to FP32NonTraceable non-traceable logic, "
"wrapped in a module"
msgstr "1. 将不可跟踪代码拆分为FP32NonTraceable不可跟踪逻辑，封装在一个模块中"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Define observed version of FP32NonTraceable"
msgstr "2. 定义FP32NonTraceable的已观察版本"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"3. Define statically quantized version of FP32NonTraceable and a class "
"method \"from_observed\" to convert from ObservedNonTraceable to "
"StaticQuantNonTraceable"
msgstr ""
"3. "
"定义FP32NonTraceable的静态量化版本以及一个类方法“from_observed”用于从ObservedNonTraceable转换为StaticQuantNonTraceable"

#: ../../prototype/vulkan_workflow.rst:250
msgid "calibrate / train (not shown)"
msgstr "校准/训练（未显示）"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"post training dynamic/weight only quantization in these two modes we don't "
"need to observe the original model, so we only need to define thee quantized"
" model"
msgstr "动态后训练量化/仅权重量化在这两种模式中，我们不需要观察原始模型，因此我们只需定义量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"You can also find examples for custom modules in test "
"``test_custom_module_class`` in "
"``torch/test/quantization/test_quantize_fx.py``."
msgstr ""
"您还可以在“torch/test/quantization/test_quantize_fx.py”的``test_custom_module_class``测试中找到自定义模块的示例。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_gpu_direct_storage.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`here <sphx_glr_download_prototype_gpu_direct_storage.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"(prototype) Accelerating ``torch.save`` and ``torch.load`` with GPUDirect "
"Storage"
msgstr "（原型）使用GPUDirect Storage加速``torch.save``和``torch.load``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"GPUDirect Storage enables a direct data path for direct memory access "
"transfers between GPU memory and storage, avoiding a bounce buffer through "
"the CPU."
msgstr "GPUDirect Storage为GPU内存和存储之间的数据直接路径提供直接内存访问传输，避免通过CPU的中间缓冲区。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In version **2.7**, we introduced new prototype APIs to ``torch.cuda.gds`` "
"that serve as thin wrappers around the `cuFile APIs "
"<https://docs.nvidia.com/gpudirect-storage/api-reference-"
"guide/index.html#cufile-io-api>`_ that can be used with ``torch.Tensor`` to "
"achieve improved I/O performance."
msgstr ""
"在版本**2.7**中，我们针对``torch.cuda.gds``引入了新的原型API，这些API作为`cuFile API "
"<https://docs.nvidia.com/gpudirect-storage/api-reference-"
"guide/index.html#cufile-io-api>`_的轻量封装，可与``torch.Tensor``一起使用以实现更高的I/O性能。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we will demonstrate how to use the ``torch.cuda.gds`` APIs"
" in conjunction with checkpoints generated by ``torch.save`` and "
"``torch.load`` on local filesystem."
msgstr ""
"在本教程中，我们将演示如何结合``torch.save``和``torch.load``在本地文件系统上使用``torch.cuda.gds``API。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Understand how to use the ``torch.cuda.gds`` APIs in conjunction with "
"checkpoints generated by ``torch.save`` and ``torch.load`` on local "
"filesystem"
msgstr "了解如何结合``torch.save``和``torch.load``在本地文件系统上使用``torch.cuda.gds``API"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch v.2.7.0 or later"
msgstr "需要PyTorch v.2.7.0或更高版本"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"GPUDirect Storage must be installed per `the documentation "
"<https://docs.nvidia.com/gpudirect-storage/troubleshooting-"
"guide/contents.html>`_"
msgstr ""
"GPUDirect Storage必须安装，具体步骤参见`文档 <https://docs.nvidia.com/gpudirect-"
"storage/troubleshooting-guide/contents.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Ensure that the filesystem that you are saving/loading to supports GPUDirect"
" Storage."
msgstr "确保您保存/加载到的文件系统支持GPUDirect Storage。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Using GPUDirect Storage with ``torch.save`` and ``torch.load``"
msgstr "结合``torch.save``和``torch.load``使用GPUDirect Storage"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"GPUDirect Storage requires a storage alignment of 4KB. You can toggle this "
"by using ``torch.utils.serialization.config.save.storage_alignment``:"
msgstr ""
"GPUDirect "
"Storage需要4KB的存储对齐。您可以通过使用``torch.utils.serialization.config.save.storage_alignment``进行切换："

#: ../../prototype/vulkan_workflow.rst:250
msgid "The steps involved in the process are as follows:"
msgstr "具体步骤如下："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Write the checkpoint file without any actual data. This reserves the space "
"on disk."
msgstr "写入检查点文件但不包含任何实际数据。这会在磁盘上保留空间。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Read the offsets for the storage associated with each tensor in the "
"checkpoint using ``FakeTensor``."
msgstr "使用``FakeTensor``读取检查点中与每个张量相关的存储的偏移量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Use ``GDSFile`` to write the appropriate data at these offsets."
msgstr "使用``GDSFile``将适当的数据写入这些偏移量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Given a state dictionary of tensors that are on the GPU, one can use the "
"``torch.serialization.skip_data`` context manager to save a checkpoint that "
"contains all relevant metadata except the storage bytes. For each "
"``torch.Storage`` in the state dictionary, space will be reserved within the"
" checkpoint for the storage bytes."
msgstr ""
"在GPU上的张量状态字典情况下，可以使用``torch.serialization.skip_data``上下文管理器保存仅包含相关元数据但不包括存储字节的检查点。对于状态字典中的每个``torch.Storage``，空间会在检查点中为存储字节保留。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can get the offsets that each storage should be written to within the "
"checkpoint by loading under a ``FakeTensorMode``. A FakeTensor is a tensor "
"that has metadata (such as sizes, strides, dtype, device) information about "
"the tensor but does not have any storage bytes. The following snippet will "
"not materialize any data but will tag each ``FakeTensor`` with the offset "
"within the checkpoint that corresponds to the tensor."
msgstr ""
"我们可以通过在``FakeTensorMode``下加载来获得每个存储应写入检查点的偏移量。FakeTensor "
"是一种具有元数据（例如大小、步幅、数据类型、设备）信息的张量，但没有存储字节。以下代码段不会实际化任何数据，但会将每个``FakeTensor``标记为检查点中张量对应的偏移量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If you are continuously saving the same state dictionary during training, "
"you would only need to obtain the offsets once and the same offsets can be "
"re-used. Similarly if tensor is going to be saved or loaded to repeatedly "
"you can use the ``torch.cuda.gds.gds_register_buffer`` which wraps "
"``cuFileBufRegister`` to register the storages as GDS buffers."
msgstr ""
"如果您在训练期间持续保存相同的状态字典，您只需获取一次偏移量即可，并且这些偏移量可以重复使用。同样，如果张量将被重复保存或加载，您可以使用``torch.cuda.gds.gds_register_buffer``包装``cuFileBufRegister``以将存储注册为GDS缓冲区。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that ``torch.cuda.gds.GdsFile.save_storage`` binds to the synchronous "
"``cuFileWrite`` API, so no synchronization is needed afterwards."
msgstr ""
"注意，``torch.cuda.gds.GdsFile.save_storage``绑定到同步``cuFileWrite``API，因此之后不需要任何同步。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We verify correctness of the saved checkpoint by ``torch.load`` and "
"comparing."
msgstr "我们通过``torch.load``及对比验证保存的检查点的正确性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The loading flow is the inverse: you can use ``torch.load`` with the "
"``torch.serialization.skip_data`` context manager to load everything except "
"the storage bytes. This means that any tensors in the checkpoint will be "
"created but their storages will be empty (as if the tensors were created via"
" ``torch.empty``)."
msgstr ""
"加载流程是相反的：您可以使用``torch.load``与``torch.serialization.skip_data``上下文管理器加载所有数据（不包括存储字节）。这意味着检查点中的任何张量会被创建，但其存储为空（就像张量通过``torch.empty``创建一样）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We once again use the ``FakeTensorMode`` to get the checkpoint offsets and "
"ascertain that the loaded checkpoint is the same as the saved checkpoint."
msgstr "我们再次使用``FakeTensorMode``获取检查点偏移量并确认加载的检查点与保存的检查点相同。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Similar to  ``torch.cuda.gds.GdsFile.save_storage``, "
"``torch.cuda.gds.GdsFile.load_storage`` binds to the synchronous "
"``cuFileRead`` API, so no synchronization is needed afterwards."
msgstr ""
"与``torch.cuda.gds.GdsFile.save_storage``类似，``torch.cuda.gds.GdsFile.load_storage``绑定到同步``cuFileRead``API，因此之后不需要任何同步。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial we have demonstrated how to use the prototype "
"``torch.cuda.gds`` APIs in conjunction with ``torch.save`` and "
"``torch.load`` on local filesystem. Please file an issue in the PyTorch "
"GitHub repo if you have any feedback."
msgstr ""
"在本教程中，我们演示了如何结合``torch.save``和``torch.load``在本地文件系统上使用原型``torch.cuda.gds``API。如果您有任何反馈，请在PyTorch"
" GitHub仓库中提交问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: gpu_direct_storage.py "
"<gpu_direct_storage.py>`"
msgstr ":download:`下载Python源代码：gpu_direct_storage.py <gpu_direct_storage.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: gpu_direct_storage.ipynb "
"<gpu_direct_storage.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：gpu_direct_storage.ipynb "
"<gpu_direct_storage.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here "
"<sphx_glr_download_prototype_gpu_quantization_torchao_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`here "
"<sphx_glr_download_prototype_gpu_quantization_torchao_tutorial.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) GPU Quantization with TorchAO"
msgstr "（原型）使用TorchAO进行GPU量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author**: `HDCharles <https://github.com/HDCharles>`_"
msgstr "**作者**: `HDCharles <https://github.com/HDCharles>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we will walk you through the quantization and optimization"
" of the popular `segment anything model "
"<https://github.com/facebookresearch/segment-anything>`_. These steps will "
"mimic some of those taken to develop the `segment-anything-fast "
"<https://github.com/pytorch-labs/segment-anything-"
"fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L15>`_ repo. "
"This step-by-step guide demonstrates how you can apply these techniques to "
"speed up your own models, especially those that use transformers. To that "
"end, we will focus on widely applicable techniques, such as optimizing "
"performance with ``torch.compile`` and quantization and measure their "
"impact."
msgstr ""
"在本教程中，我们将指导您对流行的`segment anything模型 "
"<https://github.com/facebookresearch/segment-"
"anything>`_进行量化和优化。这些步骤模仿了开发`segment-anything-fast "
"<https://github.com/pytorch-labs/segment-anything-"
"fast/blob/main/segment_anything_fast/modeling/image_encoder.py#L15>`_仓库时采取的一些步骤。该逐步指南演示了如何应用这些技术来加速您的模型，特别是那些使用Transformer的模型。为此，我们将专注于广泛适用的技术，例如使用``torch.compile``优化性能以及量化，并衡量其影响。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Set up Your Environment"
msgstr "配置您的环境"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"First, let's configure your environment. This guide was written for CUDA "
"12.1. We have run this tutorial on an A100-PG509-200 power limited to 330.00"
" W. If you are using a different hardware, you might see different "
"performance numbers."
msgstr ""
"首先，让我们配置您的环境。本指南为CUDA "
"12.1撰写。我们在功率限制为330.00W的A100-PG509-200上运行了本教程。如果您使用不同的硬件，看到的性能数字可能会有所不同。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Segment Anything Model checkpoint setup:"
msgstr "Segment Anything模型检查点设置："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Go to the `segment-anything repo checkpoint "
"<https://github.com/facebookresearch/segment-anything/tree/main#model-"
"checkpoints>`_ and download the ``vit_h`` checkpoint. Alternatively, you can"
" use ``wget`` (for example, ``wget "
"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth "
"--directory-prefix=<path>``)."
msgstr ""
"访问`segment-anything仓库中的检查点 <https://github.com/facebookresearch/segment-"
"anything/tree/main#model-"
"checkpoints>`_并下载``vit_h``检查点。或者，您可以使用``wget``（例如``wget "
"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth "
"--directory-prefix=<path>``）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Pass in that directory by editing the code below to say:"
msgstr "通过编辑以下代码传入该目录，以说明："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we focus on quantizing the ``image_encoder`` because the "
"inputs to it are statically sized while the prompt encoder and mask decoder "
"have variable sizes which makes them harder to quantize."
msgstr ""
"在本教程中，我们专注于对``image_encoder``进行量化，因为它的输入是静态大小，而prompt encoder和mask "
"decoder是可变大小的，这使得它们更难量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We’ll focus on just a single block at first to make the analysis easier."
msgstr "我们将首先专注于一个模块以使分析更加容易。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Let's start by measuring the baseline runtime."
msgstr "让我们从测量基线运行时间开始。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can achieve an instant performance boost by converting the model to "
"bfloat16. The reason we opt for bfloat16 over fp16 is due to its dynamic "
"range, which is comparable to that of fp32. Both bfloat16 and fp32 possess 8"
" exponential bits, whereas fp16 only has 4. This larger dynamic range helps "
"protect us from overflow errors and other issues that can arise when scaling"
" and rescaling tensors due to quantization."
msgstr ""
"通过将模型转换为bfloat16，我们可以立即实现性能提升。与fp16相比，我们选择bfloat16的原因是由于其动态范围类似于fp32。bfloat16和fp32均具有8个指数位，而fp16仅有4个指数位。这种较大的动态范围有助于保护我们免受溢出错误和由于量化时张量缩放调整而可能出现的其他问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Just this quick change improves runtime by a factor of ~7x in the tests we "
"have conducted (186.16ms to 25.43ms)."
msgstr "仅此快速更改在我们进行的测试中将运行时间提高了约7倍（从186.16ms缩短到25.43ms）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Next, let's use ``torch.compile`` with our model to see how much the "
"performance improves."
msgstr "接下来，让我们在模型中使用``torch.compile``，以查看性能提高了多少。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The first time this is run, you should see a sequence of ``AUTOTUNE`` "
"outputs which occurs when inductor compares the performance between various "
"kernel parameters for a kernel. This only happens once (unless you delete "
"your cache) so if you run the cell again you should just get the benchmark "
"output."
msgstr ""
"第一次运行时，您应该看到一系列``AUTOTUNE``输出，这是由于inductor比较内核参数间的性能时发生的。这仅发生一次（除非您删除了您的缓存），因此如果您再次运行单元格，您应该仅看到基准测试输出。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``torch.compile`` yields about another 27% improvement. This brings the "
"model to a reasonable baseline where we now have to work a bit harder for "
"improvements."
msgstr "``torch.compile``提供了约27%的性能改进。这将模型带到了一个合理的基线，此时我们需要更努力地进行改进。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Next, let's apply quantization. Quantization for GPUs comes in three main "
"forms in `torchao <https://github.com/pytorch-labs/ao>`_ which is just "
"native pytorch+python code. This includes:"
msgstr ""
"接下来，让我们应用量化。针对GPU的量化主要有三种形式，在`torchao <https://github.com/pytorch-"
"labs/ao>`_中实现，它只是原生的pytorch+python代码。这包括："

#: ../../prototype/vulkan_workflow.rst:250
msgid "int8 dynamic quantization"
msgstr "int8动态量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "int8 weight-only quantization"
msgstr "int8仅权重量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "int4 weight-only quantization"
msgstr "int4仅权重量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Different models, or sometimes different layers in a model can require "
"different techniques. For models which are heavily compute bound, dynamic "
"quantization tends to work the best since it swaps the normal expensive "
"floating point matmul ops with integer versions. Weight-only quantization "
"works better in memory bound situations where the benefit comes from loading"
" less weight data, rather than doing less computation. The torchao APIs:"
msgstr ""
"不同的模型，或有时模型中的不同层可能需要不同的技术。对于计算密集型的模型，动态量化一般效果最好，因为它将正常昂贵的浮点矩阵乘法操作替换为整数版本。在内存受限的情况下，仅权重量化效果会更好，因为其优势在于加载更少的权重数据而不是进行更少的计算。torchao的API："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``int8_dynamic_activation_int8_weight()``, ``int8_weight_only()`` or "
"``int4_weight_only()``"
msgstr ""
"``int8_dynamic_activation_int8_weight()``, "
"``int8_weight_only()``或``int4_weight_only()``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"can be used to easily apply the desired quantization technique and then once"
" the model is compiled with ``torch.compile`` with ``max-autotune``, "
"quantization is complete and we can see our speedup."
msgstr ""
"可以轻松应用所需的量化技术，然后一旦使用``torch.compile``中的``max-"
"autotune``编译模型，量化就完成了，我们可以看到性能得到提升。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"You might experience issues with these on older versions of PyTorch. If you "
"run into an issue, you can use ``apply_dynamic_quant`` and "
"``apply_weight_only_int8_quant`` instead as drop in replacement for the two "
"above (no replacement for int4)."
msgstr ""
"在旧版本的PyTorch中使用这些可能会遇到问题。如果碰到问题，可以使用``apply_dynamic_quant``和``apply_weight_only_int8_quant``作为上述两个函数的下拉替代品（int4没有替代品）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The difference between the two APIs is that ``int8_dynamic_activation`` API "
"alters the weight tensor of the linear module so instead of doing a normal "
"linear, it does a quantized operation. This is helpful when you have non-"
"standard linear ops that do more than one thing. The ``apply`` APIs directly"
" swap the linear modules for a quantized module which works on older "
"versions but doesn’t work with non-standard linear modules."
msgstr ""
"两个API的不同之处在于，``int8_dynamic_activation`` "
"API更改线性模块的权重张量，使其不是执行普通线性操作，而是执行量化操作。当您拥有非标准线性操作（执行多个任务）时，这非常有用。而``apply`` "
"API会直接将线性模块替换为量化模块，该模块适用于旧版本，但不适用于非标准线性模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this case Segment Anything is compute-bound so we’ll use dynamic "
"quantization:"
msgstr "在这种情况下，Segment Anything是计算密集型的，所以我们将使用动态量化："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With quantization, we have improved performance a bit more but memory usage "
"increased significantly."
msgstr "通过量化，我们性能得到了进一步提升，但内存使用显著增加。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "This is for two reasons:"
msgstr "这是由于两种原因："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Quantization adds overhead to the model since we need to quantize and "
"dequantize the input and output. For small batch sizes this overhead can "
"actually make the model go slower."
msgstr "量化为模型增加了额外开销，因为我们需要对输入和输出进行量化和反量化。对于小批量，这种开销实际上可能使模型变慢。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Even though we are doing a quantized matmul, such as ``int8 x int8``, the "
"result of the multiplication gets stored in an int32 tensor which is twice "
"the size of the result from the non-quantized model. If we can avoid "
"creating this int32 tensor, our memory usage will improve a lot."
msgstr ""
"即使我们正在执行量化的矩阵乘法，如``int8 x "
"int8``，乘法的结果存储在int32张量中，该张量大小是非量化模型结果的两倍。如果我们能避免创建这个int32张量，我们的内存使用将明显改善。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can fix #2 by fusing the integer matmul with the subsequent rescale "
"operation since the final output will be bf16, if we immediately convert the"
" int32 tensor to bf16 and instead store that we’ll get better performance in"
" terms of both runtime and memory."
msgstr ""
"我们可以通过将整数矩阵乘法与后续的比例操作融合来解决第二个问题，因为最终输出将是bf16，如果我们立即将int32张量转换为bf16并存储该结果，运行时和内存性能将得到改善。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The way to do this, is to enable the option ``force_fuse_int_mm_with_mul`` "
"in the inductor config."
msgstr "实现这一点的方法是在inductor配置中启用选项``force_fuse_int_mm_with_mul``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The fusion improves performance by another small bit (about 6% over the "
"baseline in total) and removes almost all the memory increase, the remaining"
" amount (2.37GB quantized vs 2.24GB unquantized) is due to quantization "
"overhead which cannot be helped."
msgstr ""
"这种融合进一步提升性能（比基线总体提升约6%），并几乎消除了所有的内存增长，剩余部分（2.37GB量化模型对比2.24GB非量化模型）来自量化带来的不可避免的额外开销。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We’re still not done though, we can apply a few general purpose "
"optimizations to get our final best-case performance."
msgstr "但我们还没有完成，我们可以应用一些通用优化来获得最终的最佳性能。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can sometimes improve performance by disabling epilogue fusion since the "
"autotuning process can be confused by fusions and choose bad kernel "
"parameters."
msgstr "通过禁用后续融合有时可以改善性能，因为自动调优过程可能因融合而混淆，选择了不佳的内核参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can apply coordinate descent tuning in all directions to enlarge the "
"search area for kernel parameters."
msgstr "我们可以在所有方向上应用坐标下降调优，扩大内核参数搜索区域。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As you can see, we’ve squeezed another small improvement from the model, "
"taking our total improvement to over 10x compared to our original. To get a "
"final estimate of the impact of quantization lets do an apples to apples "
"comparison on the full model since the actual improvement will differ block "
"by block depending on the shapes involved."
msgstr ""
"如您所见，我们从模型中挤出了另一个小的改进，总体提升超过了原始模型的10倍。为了最终估算量化的影响，我们可以在整个模型上进行类似比较，因为实际改进会因块的形状不同而有所差异。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have learned about the quantization and optimization "
"techniques on the example of the segment anything model."
msgstr "在本教程中，我们通过Segment Anything模型的案例学习了量化和优化技术。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In the end, we achieved a full-model apples to apples quantization speedup "
"of about 7.7% on batch size 16 (677.28ms to 729.65ms). We can push this a "
"bit further by increasing the batch size and optimizing other parts of the "
"model. For example, this can be done with some form of flash attention."
msgstr ""
"最终，我们实现了全模型的类似比较量化性能提升约7.7%（批量大小16，从677.28ms到729.65ms）。我们可以通过增加批量大小及优化模型其他部分进一步推进性能提升。例如，可以通过某种形式的闪存注意力实现。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For more information visit `torchao <https://github.com/pytorch-labs/ao>`_ "
"and try it on your own models."
msgstr ""
"有关更多信息，请访问`torchao <https://github.com/pytorch-labs/ao>`_并尝试应用到您自己的模型上。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: gpu_quantization_torchao_tutorial.py"
" <gpu_quantization_torchao_tutorial.py>`"
msgstr ""
":download:`下载Python源码：gpu_quantization_torchao_tutorial.py "
"<gpu_quantization_torchao_tutorial.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: "
"gpu_quantization_torchao_tutorial.ipynb "
"<gpu_quantization_torchao_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：gpu_quantization_torchao_tutorial.ipynb "
"<gpu_quantization_torchao_tutorial.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) Graph Mode Dynamic Quantization on BERT"
msgstr "(原型) 使用BERT图模式动态量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author**: `Supriya Rao <https://github.com/supriyar>`_"
msgstr "**作者**：`Supriya Rao <https://github.com/supriyar>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the steps to do post training Dynamic Quantization "
"with Graph Mode Quantization. Dynamic quantization converts a float model to"
" a quantized model with static int8 data types for the weights and dynamic "
"quantization for the activations. The activations are quantized dynamically "
"(per batch) to int8 while the weights are statically quantized to int8. "
"Graph Mode Quantization flow operates on the model graph and requires "
"minimal user intervention to quantize the model. To be able to use graph "
"mode, the float model needs to be either traced or scripted first."
msgstr ""
"本教程介绍了使用图模式量化进行训练后动态量化的步骤。动态量化将浮点模型转换为具有静态int8数据类型权重和动态量化激活的量化模型。激活根据批次动态量化为int8，而权重量化为静态int8。图模式量化流程基于模型图运行，用户干预量化操作的步骤最少。要使用图模式量化，浮点模型需要先进行追踪或脚本化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Advantages of graph mode quantization are:"
msgstr "图模式量化的优势包括："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In graph mode, we can inspect the code that is executed in forward function "
"(e.g. aten function calls) and quantization is achieved by module and graph "
"manipulations."
msgstr "在图模式下，我们可以检查前向函数中执行的代码（例如aten函数调用），通过模块和图的操作来实现量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Simple quantization flow, minimal manual steps."
msgstr "简单的量化流程，手动步骤最少。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Unlocks the possibility of doing higher level optimizations like automatic "
"precision selection."
msgstr "解锁进行高级优化的可能，例如自动精度选择。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For additional details on Graph Mode Quantization please refer to the `Graph"
" Mode Static Quantization Tutorial "
"<https://pytorch.org/tutorials/prototype/graph_mode_static_quantization_tutorial.html>`_."
msgstr ""
"有关图模式静态量化的更多详细信息，请参阅`图模式静态量化教程 "
"<https://pytorch.org/tutorials/prototype/graph_mode_static_quantization_tutorial.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"tl;dr The Graph Mode Dynamic `Quantization API "
"<https://pytorch.org/docs/master/quantization.html#torch-quantization>`_:"
msgstr ""
"简而言之，图模式动态`量化API <https://pytorch.org/docs/master/quantization.html#torch-"
"quantization>`_："

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Quantizing BERT Model"
msgstr "1. 对BERT模型进行量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The installaion steps and details about the model are identical to the steps"
" in the Eager Mode Tutorial. Please refer to the tutorial `here "
"<https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html#install-"
"pytorch-and-huggingface-transformers>`_ for more details."
msgstr ""
"安装步骤和模型相关的细节与Eager模式教程中的步骤一致。有关详细信息，请参阅`此教程 "
"<https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html#install-"
"pytorch-and-huggingface-transformers>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.1 Setup"
msgstr "1.1 设置"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Once all the necesessary packages are downloaded and installed we setup the "
"code. We first start with the necessary imports and setup for the model."
msgstr "下载并安装所有必要的包后，我们开始配置代码。首先从必要的导入和模型配置开始。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.2 Download GLUE dataset"
msgstr "1.2 下载GLUE数据集"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Before running MRPC tasks we download the GLUE data by running this script "
"and unpack it to a directory glue_data."
msgstr "在运行MRPC任务之前，我们通过运行此脚本下载GLUE数据并将其解压到一个名为glue_data的目录中。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.3 Set global BERT configurations"
msgstr "1.3 设置全局BERT配置"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To run this experiment we first need a fine tuned BERT model. We provide the"
" fined-tuned BERT model for MRPC task `here "
"<https://download.pytorch.org/tutorial/MRPC.zip>`_. To save time, you can "
"download the model file (~400 MB) directly into your local folder $OUT_DIR."
msgstr ""
"要运行此实验，我们首先需要一个微调过的BERT模型。我们为MRPC任务提供了微调的BERT模型`此处 "
"<https://download.pytorch.org/tutorial/MRPC.zip>`_。为了节省时间，您可以直接将模型文件（约400 "
"MB）下载到本地文件夹$OUT_DIR。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.4 Quantizing BERT model with Graph Mode Quantization"
msgstr "1.4 使用图模式量化BERT模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.4.1 Script/Trace the model"
msgstr "1.4.1 将模型脚本化或追踪"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The input for graph mode quantization is a TorchScript model, so you'll need"
" to either script or trace the model first. Currently, scripting the BERT "
"model is not supported so we trace the model here."
msgstr ""
"图模式量化的输入是一个TorchScript模型，因此您需要先对模型进行脚本化或追踪。目前，BERT模型不支持脚本化，所以在这里我们选择追踪。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We first identify the inputs to be passed to the model. Here, we trace the "
"model with the largest possible input size that will be passed during the "
"evaluation. We choose a batch size of 8 and sequence lenght of 128 based on "
"the input sizes passed in during the evaluation step below. Using the max "
"possible shape during inference while tracing is a limitation of the "
"huggingface BERT model as mentioned `here "
"<https://huggingface.co/transformers/v2.3.0/torchscript.html#dummy-inputs-"
"and-standard-lengths>`_."
msgstr ""
"我们首先确定需要传递给模型的输入。在此，我们选择最大的可能输入尺寸进行追踪，这将是评估期间传递的最大尺寸。我们选择批量大小为8，序列长度为128，基于以下评估步骤中传递的输入。在模型追踪期间使用推理时的最大可能形状，是如`此处"
" <https://huggingface.co/transformers/v2.3.0/torchscript.html#dummy-inputs-"
"and-standard-lengths>`_所述的huggingface BERT模型的限制。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We trace the model using ``torch.jit.trace``."
msgstr "我们使用``torch.jit.trace``对模型进行追踪。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.4.2 Specify qconfig_dict"
msgstr "1.4.2 指定qconfig_dict"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"qconfig is a named tuple of the observers for activation and weight. For "
"dynamic quantization we use a dummy activation observer to mimic the dynamic"
" quantization process that happens in the operator during runtime. For the "
"weight tensors we recommend using per-channel quantization which helps "
"improve the final accuracy. ``qconfig_dict`` is a dictionary with names of "
"sub modules as key and qconfig for that module as value, empty key means the"
" qconfig will be applied to whole model unless it’s overwritten by more "
"specific configurations, the qconfig for each module is either found in the "
"dictionary or fallback to the qconfig of parent module."
msgstr ""
"qconfig是一个命名元组，包含激活和权重的观察者。对于动态量化，我们使用一个虚拟激活观察器来模拟运行时操作中的动态量化过程。对于权重张量，我们推荐使用每通道量化，有助于提升最终准确性。``qconfig_dict``是一个字典，子模块的名称为键，模块的qconfig为值。空键表示qconfig将应用于整个模型，除非被更具体的配置覆盖。对于每个模块的qconfig，字典中优先寻找，若不存在则使用父模块的qconfig作为回退。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Right now qconfig_dict is the only way to configure how the model is "
"quantized, and it is done in the granularity of module, that is, we only "
"support one type of qconfig for each module, and the qconfig for sub module "
"will override the qconfig for parent module. For example, if we have"
msgstr ""
"目前，qconfig_dict是配置模型量化方式的唯一方式，它以模块为细粒度进行配置，也就是我们仅支持每个模块一种类型的qconfig，子模块的qconfig会覆盖父模块的qconfig。例如，如果我们有"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Module ``sub.fc1`` will be configured with ``qconfig_fc``, and all other "
"child modules in ``sub`` will be configured with ``qconfig_sub`` and "
"``sub.fc2`` will not be quantized. All other modules in the model will be "
"quantized with qconfig_global"
msgstr ""
"模块``sub.fc1``将使用``qconfig_fc``配置，而``sub``中的所有其他子模块将使用``qconfig_sub``配置，``sub.fc2``不会被量化。模型中所有其他模块将使用qconfig_global进行量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.4.3 Quantize the model (one-line API)"
msgstr "1.4.3 量化模型（单行API）"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We call the one line API (similar to eager mode) to perform quantization as "
"follows."
msgstr "我们调用类似于Eager模式的单行API，以如下方式执行量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Evaluation"
msgstr "2. 评估"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We reuse the tokenize and evaluation function from Huggingface."
msgstr "我们重用Huggingface的分词和评估函数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2.1 Check Model Size"
msgstr "2.1 检查模型大小"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We print the model size to account for wins from quantization"
msgstr "我们打印模型大小，以衡量量化带来的优势。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2.2 Run the evaluation"
msgstr "2.2 运行评估"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We evaluate the FP32 and quantized model and compare the F1 score. Note that"
" the performance numbers below are on a dev machine and they would likely "
"improve on a production server."
msgstr "我们评估FP32模型和量化模型并比较F1分数。请注意，以下性能数据是在开发机器上测得的，在生产服务器上可能会有所改善。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Debugging the Quantized Model"
msgstr "3. 调试量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We can debug the quantized model by passing in the debug option."
msgstr "我们通过传递调试选项来调试量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "If debug is set to True:"
msgstr "如果调试选项设置为True："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can access the attributes of the quantized model the same way as in a "
"torchscript model, e.g. model.fc1.weight (might be harder if you use a "
"module list or sequential)."
msgstr "我们可以像在torchscript模型中一样访问量化模型的属性，例如model.fc1.weight（如果使用模块列表或顺序可能会更难）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The arithmetic operations all occur in floating point with the numerics "
"being identical to the final quantized model, allowing for debugging."
msgstr "所有算术操作都以浮点数进行，与最终量化模型的数值完全一致，支持调试。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Calling ``quantize_dynamic_jit`` is equivalent to calling "
"``prepare_dynamic_jit`` followed by ``convert_dynamic_jit``. Usage of the "
"one-line API is recommended. But if you wish to debug or analyze the model "
"after each step, the multi-line API comes into use."
msgstr ""
"调用``quantize_dynamic_jit``等价于依次调用``prepare_dynamic_jit``和``convert_dynamic_jit``。推荐使用单行API。但如果希望在每个步骤后调试或分析模型，则可使用多行API。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3.1. Evaluate the Debug Model"
msgstr "3.1 评估调试模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that the accuracy of the debug version is close to, but not exactly the"
" same as the non-debug version as the debug version uses floating point ops "
"to emulate quantized ops and the numerics match is approximate. This is the "
"case only for per-channel quantization (we are working on improving this). "
"Per-tensor quantization (using default_dynamic_qconfig) has exact numerics "
"match between debug and non-debug version."
msgstr ""
"请注意，调试版本的精度接近但并不完全等同于非调试版本，因为调试版本使用浮点操作模拟量化操作，数值匹配是近似的。这种情况仅适用于每通道量化（我们正在努力改进这一点）。每张量的量化（使用默认的动态量化配置）在调试和非调试版本之间具有精确的数值匹配。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Snippet of the graph printed -"
msgstr "打印的图示片段 -"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can see that there is no ``quantized::linear_dynamic`` in the model, but "
"the numerically equivalent pattern of ``aten::_choose_qparams_per_tensor`` -"
" ``aten::quantize_per_tensor`` - ``aten::dequantize`` - ``aten::linear``."
msgstr ""
"我们可以看到模型中没有``quantized::linear_dynamic``，而是与之数值等效的模式为``aten::_choose_qparams_per_tensor``"
" - ``aten::quantize_per_tensor`` - ``aten::dequantize`` - ``aten::linear``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Size of the debug model is the close to the floating point model because all"
" the weights are in float and not yet quantized and frozen, this allows "
"people to inspect the weight. You may access the weight attributes directly "
"in the torchscript model. Accessing the weight in the debug model is the "
"same as accessing the weight in a TorchScript model:"
msgstr ""
"调试模型的大小接近浮点模型，因为所有权重都是浮点数，还没有进行量化和固定，这使得用户可以检查权重。您可以直接访问torchscript模型中的权重属性。在调试模型中访问权重与在TorchScript模型中访问权重的方式相同："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Accessing the scale and zero_point for the corresponding weight can be done "
"as follows -"
msgstr "可以按以下方式访问相应权重的scale和zero_point -"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Since we use per-channel quantization, we get per-channel scales tensor."
msgstr "由于我们使用每通道量化，我们获得的是每通道的scale张量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Zero-point tensor -"
msgstr "Zero-point张量 -"

#: ../../prototype/vulkan_workflow.rst:250
msgid "4. Comparing Results with Eager Mode"
msgstr "4. 与Eager模式的结果比较"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Following results show the F1 score and model size for Eager Mode "
"Quantization of the same model by following the steps mentioned in the "
"`tutorial "
"<https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html#evaluate-"
"the-inference-accuracy-and-time>`_. Results show that Eager and Graph Mode "
"Quantization on the model produce identical results."
msgstr ""
"以下结果显示，通过遵循“教程 "
"<https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html#evaluate-"
"the-inference-accuracy-and-"
"time>`_中提到的步骤，对同一模型进行Eager模式量化的F1得分和模型大小。结果表明，模型的Eager与图模式量化产生相同的结果。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "5. Benchmarking the Model"
msgstr "5. 模型基准测试"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We benchmark the model with dummy input and compare the Float model with "
"Eager and Graph Mode Quantized Model on a production server machine."
msgstr "我们使用虚拟输入进行基准测试，并在一个生产服务器机器上将浮点模型与Eager模式和图模式量化模型进行比较。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we demonstrated how to convert a well-known state-of-the-"
"art NLP model like BERT into dynamic quantized model using graph mode with "
"same performance as eager mode. Dynamic quantization can reduce the size of "
"the model while only having a limited implication on accuracy."
msgstr ""
"在此教程中，我们演示了如何使用图模式将一个著名的先进NLP模型如BERT转换为动态量化模型，并且性能与Eager模式相同。动态量化可以在对精度只造成有限影响的情况下减小模型大小。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Thanks for reading! As always, we welcome any feedback, so please create an "
"issue `here <https://github.com/pytorch/pytorch/issues>`_ if you have any."
msgstr ""
"感谢阅读！一如既往，我们欢迎任何反馈，如果您有任何问题，请在`此处 "
"<https://github.com/pytorch/pytorch/issues>`_创建一个问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Inductor C++ Wrapper Tutorial"
msgstr "Inductor C++ 包装器教程"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Chunyuan Wu <https://github.com/chunyuan-w>`_, `Bin Bao "
"<https://github.com/desertfire>`__, `Jiong Gong "
"<https://github.com/jgong5>`__"
msgstr ""
"**作者**: `Chunyuan Wu <https://github.com/chunyuan-w>`_, `Bin Bao "
"<https://github.com/desertfire>`_, `Jiong Gong <https://github.com/jgong5>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Prerequisites:"
msgstr "先决条件："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`torch.compile and TorchInductor concepts in PyTorch "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__"
msgstr ""
"`PyTorch中的torch.compile和TorchInductor概念 "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Python, as the primary interface of PyTorch, is easy to use and efficient "
"for development and debugging. The Inductor's default wrapper generates "
"Python code to invoke generated kernels and external kernels. However, in "
"deployments requiring high performance, Python, as an interpreted language, "
"runs relatively slower compared to compiled languages."
msgstr ""
"Python作为PyTorch的主要接口，使用起来简单且便于开发和调试。Inductor的默认包装器生成Python代码以调用生成的内核和外部内核。然而，在需要高性能的部署中，Python作为一种解释语言，相较于编译语言运行较慢。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We implemented an Inductor C++ wrapper by leveraging the PyTorch C++ APIs to"
" generate pure C++ code that combines the generated and external kernels. "
"This allows for the execution of each captured Dynamo graph in pure C++, "
"thereby reducing the Python overhead within the graph."
msgstr ""
"我们通过利用PyTorch的C++ API实现了一个Inductor "
"C++包装器来生成结合生成和外部内核的纯C++代码。这允许在纯C++中执行每个捕获的Dynamo图，从而减少图中Python的开销。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Enabling the API"
msgstr "启用接口"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This feature is still in prototype stage. To activate this feature, add the "
"following to your code:"
msgstr "此功能仍处于原型阶段。要激活此功能，请在您的代码中添加以下内容："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This will speed up your models by reducing the Python overhead of the "
"Inductor wrapper."
msgstr "这将通过减少Inductor包装器的Python开销来加速您的模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Example code"
msgstr "示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We will use the below frontend code as an example:"
msgstr "我们将使用以下前端代码作为示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid "**For CPU**"
msgstr "**对于CPU**"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The main part of Inductor-generated code with the default Python wrapper "
"will look like this:"
msgstr "默认Python包装器生成的Inductor代码主要部分将如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"By turning on the C++ wrapper, the generated code for the ``call`` function "
"becomes a C++ function ``inductor_entry_cpp`` of the C++ extension "
"``module``:"
msgstr ""
"打开C++包装器后，``call``函数的生成代码将变为C++扩展``module``的C++函数``inductor_entry_cpp``："

#: ../../prototype/vulkan_workflow.rst:250
msgid "**For GPU**"
msgstr "**对于GPU**"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Based on the same example code, the generated code for GPU will look like "
"this:"
msgstr "基于相同示例代码，GPU生成的代码将如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With the C++ wrapper turned on, the below equivalent C++ code will be "
"generated:"
msgstr "打开C++包装器后，将生成以下等效的C++代码："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we introduced a new C++ wrapper in TorchInductor to speed "
"up your models with just two lines of code changes. We explained the "
"motivation of this new feature and walked through the easy-to-use API to "
"activate this experimental feature. Furthermore, we demonstrated the "
"Inductor-generated code using the default Python wrapper and the new C++ "
"wrapper on both CPU and GPU to visually showcase the difference between "
"these two wrappers."
msgstr ""
"在本教程中，我们介绍了TorchInductor中的一个新C++包装器，只需两行代码的更改即可加速您的模型。我们解释了此新功能的动机，并讲解了用于激活此实验功能的易于使用的API。此外，我们展示了在CPU和GPU上使用默认Python包装器和新C++包装器生成的Inductor代码，并以直观的方式展示了这两个包装器之间的区别。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This feature is still in prototype stage. If you have any feature requests "
"or run into any issues, please file a bug report at `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_."
msgstr ""
"此功能仍处于原型阶段。如果您有任何功能请求或遇到任何问题，请在`GitHub问题 "
"<https://github.com/pytorch/pytorch/issues>`_中提交错误报告。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "How to use ``torch.compile`` on Windows CPU/XPU"
msgstr "如何在Windows CPU/XPU上使用``torch.compile``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Zhaoqiong Zheng <https://github.com/ZhaoqiongZ>`_, `Xu, Han "
"<https://github.com/xuhancn>`_"
msgstr ""
"**作者**: `Zhaoqiong Zheng <https://github.com/ZhaoqiongZ>`_, `Xu, Han "
"<https://github.com/xuhancn>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"TorchInductor is the new compiler backend that compiles the FX Graphs "
"generated by TorchDynamo into optimized C++/Triton kernels."
msgstr "TorchInductor是新的编译器后端，将TorchDynamo生成的FX图编译为优化的C++/Triton内核。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the steps for using TorchInductor via "
"``torch.compile`` on Windows CPU/XPU."
msgstr "本教程介绍了在Windows CPU/XPU上通过``torch.compile``使用TorchInductor的步骤。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Software Installation"
msgstr "软件安装"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, we will walk you through a step-by-step tutorial for how to use "
"``torch.compile`` on Windows CPU/XPU."
msgstr "现在，我们将向您讲解如何在Windows CPU/XPU上使用``torch.compile``的逐步教程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Install a Compiler"
msgstr "安装编译器"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"C++ compiler is required for TorchInductor optimization, let's take "
"Microsoft Visual C++ (MSVC) as an example."
msgstr "TorchInductor优化需要C++编译器，这里以Microsoft Visual C++ (MSVC)为例。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Download and install `MSVC "
"<https://visualstudio.microsoft.com/downloads/>`_."
msgstr "下载并安装`MSVC <https://visualstudio.microsoft.com/downloads/>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"During Installation, select **Workloads** and then **Desktop & Mobile**. "
"Select a checkmark on **Desktop Development with C++** and install."
msgstr "安装时，选择**工作负载**，然后选择**桌面与移动应用**。勾选**使用C++进行桌面开发**并安装。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Windows CPU inductor also support C++ compiler `LLVM Compiler "
"<https://github.com/llvm/llvm-project/releases>`_ and `Intel Compiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-"
"compiler-download.html>`_ for better performance. Please check `Alternative "
"Compiler for better performance on CPU <#alternative-compiler-for-better-"
"performance>`_."
msgstr ""
"Windows CPU Inductor还支持C++编译器`LLVM Compiler <https://github.com/llvm/llvm-"
"project/releases>`_和`Intel Compiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-"
"compiler-download.html>`_以获得更好的性能。请参考`提高CPU性能的替代编译器 <#alternative-compiler-"
"for-better-performance>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Set Up Environment"
msgstr "设置环境"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Next, let's configure our environment."
msgstr "接下来，让我们配置我们的环境。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Open a command line environment via cmd.exe."
msgstr "通过cmd.exe打开命令行界面。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Activate ``MSVC`` via below command::"
msgstr "通过以下命令激活``MSVC``：："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Create and activate a virtual environment: ::"
msgstr "创建并激活虚拟环境：："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Install `PyTorch 2.5 <https://pytorch.org/get-started/locally/>`_ or later "
"for CPU Usage. Install PyTorch 2.7 or later refer to `Getting Started on "
"Intel GPU <https://pytorch.org/docs/main/notes/get_start_xpu.html>`_ for XPU"
" usage."
msgstr ""
"安装`PyTorch 2.5版本 <https://pytorch.org/get-"
"started/locally/>`_或更高版本以使用CPU。欲使用XPU，请通过`Intel GPU入门指南 "
"<https://pytorch.org/docs/main/notes/get_start_xpu.html>`_安装PyTorch "
"2.7或更高版本。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Here is an example of how to use TorchInductor on Windows:"
msgstr "以下是如何在Windows上使用TorchInductor的示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Below is the output of the above example::"
msgstr "以下是上述示例的输出：："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Alternative Compiler for better performance on CPU"
msgstr "提高CPU性能的替代编译器"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To enhance performance for inductor on Windows CPU, you can use the Intel "
"Compiler or LLVM Compiler. However, they rely on the runtime libraries from "
"Microsoft Visual C++ (MSVC). Therefore, your first step should be to install"
" MSVC."
msgstr ""
"为了提高Windows CPU上Inductor的性能，您可以使用Intel Compiler或LLVM "
"Compiler。然而，它们依赖于Microsoft Visual C++ (MSVC)的运行时库。因此，第一步应该是安装MSVC。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Intel Compiler"
msgstr "Intel Compiler"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Download and install `Intel Compiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-"
"compiler-download.html>`_ with Windows version."
msgstr ""
"下载并安装适用于Windows版本的`Intel Compiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-"
"compiler-download.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Set Windows Inductor Compiler via environment variable ``set CXX=icx-cl``."
msgstr "通过环境变量``set CXX=icx-cl``设置Windows Inductor编译器。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "LLVM Compiler"
msgstr "LLVM Compiler"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Download and install `LLVM Compiler <https://github.com/llvm/llvm-"
"project/releases>`_ and choose win64 version."
msgstr ""
"下载并安装`LLVM Compiler <https://github.com/llvm/llvm-"
"project/releases>`_并选择win64版本。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Set Windows Inductor Compiler via environment variable ``set CXX=clang-cl``."
msgstr "通过环境变量``set CXX=clang-cl``设置Windows Inductor编译器。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we introduce how to use Inductor on Windows CPU with "
"PyTorch 2.5 or later, and on Windows XPU with PyTorch 2.7 or later. We can "
"also use Intel Compiler or LLVM Compiler to get better performance on CPU."
msgstr ""
"在本教程中，我们介绍了如何在Windows CPU上使用PyTorch 2.5或更高版本，以及在Windows XPU上使用PyTorch "
"2.7或更高版本的Inductor。我们还可以使用Intel Compiler或LLVM Compiler在CPU上获得更好的性能。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/tutorials/prototype/inductor_windows.html."
msgstr "此教程已迁移至https://pytorch.org/tutorials/prototype/inductor_windows.html。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Redirecting in 3 seconds..."
msgstr "3秒后跳转..."

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Prototype) Convert Mobilenetv2 to Core ML"
msgstr "（原型）将Mobilenetv2转换为Core ML"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"PyTorch Mobile is no longer actively supported. Please check out `ExecuTorch"
" <https://github.com/pytorch/executorch>`__."
msgstr ""
"PyTorch Mobile不再积极支持。请查看`ExecuTorch "
"<https://github.com/pytorch/executorch>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Prototype) Use iOS GPU in PyTorch"
msgstr "（原型）在PyTorch中使用iOS GPU"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Prototype) Introduce lite interpreter workflow in Android and iOS"
msgstr "（原型）介绍Android和iOS中轻量解释器工作流程"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/tutorials/recipes/mobile_interpreter.html"
msgstr "本教程已迁移至https://pytorch.org/tutorials/recipes/mobile_interpreter.html"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_maskedtensor_adagrad.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_prototype_maskedtensor_adagrad.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"(Prototype) Efficiently writing \"sparse\" semantics for Adagrad with "
"MaskedTensor"
msgstr "（原型）高效地为Adagrad编写带\"稀疏\"语义的MaskedTensor"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Before working through this tutorial, please review the MaskedTensor "
"`Overview "
"<https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`__ and "
"`Sparsity "
"<https://pytorch.org/tutorials/prototype/maskedtensor_sparsity.html>`__ "
"tutorials."
msgstr ""
"在阅读本教程之前，请查看MaskedTensor的`概述 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`_和`稀疏性 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_sparsity.html>`_教程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Introduction and Motivation"
msgstr "介绍与动机"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Issue 1369 <https://github.com/pytorch/pytorch/issues/1369>`__ discussed "
"the additional lines of code that were introduced while writing \"sparse\" "
"semantics for Adagrad, but really, the code uses sparsity as a proxy for "
"masked semantics rather than the intended use case of sparsity: a "
"compression and optimization technique. Previously, we worked around the "
"lack of formal masked semantics by introducing one-off semantics and "
"operators while forcing users to be aware of storage details such as indices"
" and values."
msgstr ""
"`问题1369 "
"<https://github.com/pytorch/pytorch/issues/1369>`_讨论了在为Adagrad编写\"稀疏\"语义时引入的额外代码行，但实际上，代码使用稀疏性作为遮罩语义的代理，而不是稀疏性的预期应用场景：一种压缩和优化技术。之前，我们通过引入一次性语义和操作符来绕过正式遮罩语义的缺失，同时强制用户了解索引和值等存储细节。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now that we have masked semantics, we are better equipped to point out when "
"sparsity is used as a semantic extension. We'll also compare and contrast "
"this with equivalent code written using MaskedTensor. In the end the code "
"snippets are repeated without additional comments to show the difference in "
"brevity."
msgstr ""
"现在我们有了遮罩语义，我们可以更好地指出何时稀疏性被用作语义扩展。我们还将与使用MaskedTensor编写的等效代码进行比较和对比。最后，代码片段将在没有额外注释的情况下重复，以显示简洁性上的差异。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Preparation"
msgstr "准备工作"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Simpler Code with MaskedTensor"
msgstr "使用MaskedTensor的更简单代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Before we get too far in the weeds, let's introduce the problem a bit more "
"concretely. We will be taking a look into the `Adagrad (functional) "
"<https://github.com/pytorch/pytorch/blob/6c2f235d368b697072699e5ca9485fd97d0b9bcc/torch/optim/_functional.py#L16-L51>`__"
" implementation in PyTorch with the ultimate goal of simplifying and more "
"faithfully representing the masked approach."
msgstr ""
"在深入细节之前，让我们更具体地介绍一下问题。我们将研究PyTorch中的`Adagrad（函数式） "
"<https://github.com/pytorch/pytorch/blob/6c2f235d368b697072699e5ca9485fd97d0b9bcc/torch/optim/_functional.py#L16-L51>`_实现，其最终目标是简化并更忠实地代表遮罩方法。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For reference, this is the regular, dense code path without masked gradients"
" or sparsity:"
msgstr "供参考，这是没有遮罩梯度或稀疏性的常规密集代码路径："

#: ../../prototype/vulkan_workflow.rst:250
msgid "The vanilla tensor implementation for sparse is:"
msgstr "稀疏的普通张量实现为："

#: ../../prototype/vulkan_workflow.rst:250
msgid "while :class:`MaskedTensor` minimizes the code to the snippet:"
msgstr "而 :class:`MaskedTensor` 将代码简化为以下片段："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we will go through each implementation line by line, but "
"at first glance, we can notice (1) how much shorter the MaskedTensor "
"implementation is, and (2) how it avoids conversions between dense and "
"sparse tensors."
msgstr ""
"在本教程中，我们会逐行浏览每个实现，但大致来看，我们可以注意到（1）MaskedTensor实现有多短，以及（2）它如何避免在密集张量和稀疏张量之间进行转换。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Original Sparse Implementation"
msgstr "原始稀疏实现"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Now, let's break down the code with some inline comments:"
msgstr "现在，让我们带上一些内联注释来分解代码："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The third to last line -- `std = state_sum.sparse_mask(grad)` -- is where we"
" have a very important divergence."
msgstr "倒数第三行--`std = state_sum.sparse_mask(grad)`--是我们发生非常重要分歧的地方。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The addition of eps should technically be applied to all values but instead "
"is only applied to specified values. Here we're using sparsity as a semantic"
" extension and to enforce a certain pattern of defined and undefined values."
" If parts of the values of the gradient are zero, they are still included if"
" materialized even though they could be compressed by other sparse storage "
"layouts. This is theoretically quite brittle! That said, one could argue "
"that eps is always very small, so it might not matter so much in practice."
msgstr ""
"虽然技术上应将eps应用于所有值，但实际上仅应用于指定的值。在这里，我们使用稀疏性作为语义扩展，并强制执行某种定义值和未定义值的模式。如果梯度的部分值为零，它们仍会被包含在内，即使它们可以通过其他稀疏存储布局进行压缩。这在理论上相当脆弱！不过，可以认为eps总是非常小，所以在实际操作中可能影响不大。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Moreover, an implementation `add_` for sparsity as a storage layout and "
"compression scheme should cause densification, but we force it not to for "
"performance. For this one-off case it is fine.. until we want to introduce "
"new compression scheme, such as `CSC "
"<https://pytorch.org/docs/master/sparse.html#sparse-csc-docs>`__, `BSR "
"<https://pytorch.org/docs/master/sparse.html#sparse-bsr-docs>`__, or `BSC "
"<https://pytorch.org/docs/master/sparse.html#sparse-bsc-docs>`__. We will "
"then need to introduce separate Tensor types for each and write variations "
"for gradients compressed using different storage formats, which is "
"inconvenient and not quite scalable nor clean."
msgstr ""
"此外，为稀疏性作为存储布局和压缩方案实现的`add_`应导致密化，但我们为了性能强制不进行密化。对于这种一次性的情况是可以的......直到我们希望引入新的压缩方案，例如`CSC"
" <https://pytorch.org/docs/master/sparse.html#sparse-csc-docs>`__、`BSR "
"<https://pytorch.org/docs/master/sparse.html#sparse-bsr-docs>`__或`BSC "
"<https://pytorch.org/docs/master/sparse.html#sparse-bsc-"
"docs>`__。到那时，我们需要为每种情况引入单独的Tensor类型，并为使用不同存储格式压缩的梯度编写变体，这既不方便也不太可扩展且不够简洁。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "MaskedTensor Sparse Implementation"
msgstr "MaskedTensor稀疏实现"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We've been conflating sparsity as an optimization with sparsity as a "
"semantic extension to PyTorch. MaskedTensor proposes to disentangle the "
"sparsity optimization from the semantic extension; for example, currently we"
" can't have dense semantics with sparse storage or masked semantics with "
"dense storage. MaskedTensor enables these ideas by purposefully separating "
"the storage from the semantics."
msgstr ""
"我们一直在将稀疏性作为一种优化与稀疏性作为PyTorch的语义扩展相混淆。MaskedTensor建议将稀疏性优化从语义扩展中分离出来；例如，目前我们无法以稠密语义和稀疏存储或以稠密存储实现掩码语义。MaskedTensor通过故意将存储与语义分开来实现这些想法。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Consider the above example using a masked gradient:"
msgstr "考虑使用掩码梯度的上述示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that the implementations look quite similar, but the MaskedTensor "
"implementation is shorter and simpler. In particular, much of the "
"boilerplate code around ``_make_sparse`` (and needing to have a separate "
"implementation per layout) is handled for the user with "
":class:`MaskedTensor`."
msgstr ""
"注意，实现看起来很相似，但MaskedTensor的实现更短更简单。尤其是关于``_make_sparse``的大量样板代码（以及需要为每个布局进行单独实现）由:class:`MaskedTensor`为用户处理。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"At this point, let's print both this version and original version for easier"
" comparison:"
msgstr "在这里，我们打印出这个版本和原版，以便更容易比较："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we've discussed how native masked semantics can enable a "
"cleaner developer experience for Adagrad's existing implementation in "
"PyTorch, which used sparsity as a proxy for writing masked semantics. But "
"more importantly, allowing masked semantics to be a first class citizen "
"through MaskedTensor removes the reliance on sparsity or unreliable hacks to"
" mimic masking, thereby allowing for proper independence and development, "
"while enabling sparse semantics, such as this one."
msgstr ""
"在本教程中，我们讨论了原生掩码语义如何帮助改善Adagrad在PyTorch中的现有实现，从而使用稀疏性作为编写掩码语义的代理。但更重要的是，允许通过MaskedTensor使掩码语义成为一等公民，消除了对稀疏性或模拟掩码的不可靠技巧的依赖，从而实现真正的独立性和开发，同时支持稀疏语义，例如这种情况。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To continue learning more, you can find our final review (for now) on "
"`MaskedTensor Advanced Semantics "
"<https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html>`__"
" to see some of the differences in design decisions between "
":class:`MaskedTensor` and NumPy's MaskedArray, as well as reduction "
"semantics."
msgstr ""
"要继续学习更多内容，您可以查看我们现在的最终评论`MaskedTensor高级语义 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html>`__，以了解:class:`MaskedTensor`与NumPy&apos;s"
" MaskedArray之间设计决策的某些差异，以及归约语义。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: maskedtensor_adagrad.py "
"<maskedtensor_adagrad.py>`"
msgstr ""
":download:`下载Python源码：maskedtensor_adagrad.py <maskedtensor_adagrad.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: maskedtensor_adagrad.ipynb "
"<maskedtensor_adagrad.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：maskedtensor_adagrad.ipynb "
"<maskedtensor_adagrad.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here "
"<sphx_glr_download_prototype_maskedtensor_advanced_semantics.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`此处<sphx_glr_download_prototype_maskedtensor_advanced_semantics.py>`下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Prototype) MaskedTensor Advanced Semantics"
msgstr "(原型) MaskedTensor高级语义"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Before working on this tutorial, please make sure to review our "
"`MaskedTensor Overview tutorial "
"<https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`."
msgstr ""
"开始学习本教程之前，请确保已查看我们的`MaskedTensor概述教程 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The purpose of this tutorial is to help users understand how some of the "
"advanced semantics work and how they came to be. We will focus on two "
"particular ones:"
msgstr "本教程的目的是帮助用户了解一些高级语义的工作原理及其生成过程。我们会重点介绍其中两种语义："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"*. Differences between MaskedTensor and `NumPy's MaskedArray "
"<https://numpy.org/doc/stable/reference/maskedarray.html>`__ *. Reduction "
"semantics"
msgstr ""
"*. MaskedTensor与`NumPy&apos;s MaskedArray "
"<https://numpy.org/doc/stable/reference/maskedarray.html>`__之间的差异 *. 归约语义"

#: ../../prototype/vulkan_workflow.rst:250
msgid "MaskedTensor vs NumPy's MaskedArray"
msgstr "MaskedTensor与NumPy&apos;s MaskedArray"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"NumPy's ``MaskedArray`` has a few fundamental semantics differences from "
"MaskedTensor."
msgstr "NumPy&apos;s ``MaskedArray``与MaskedTensor存在一些基本语义差异。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"*. Their factory function and basic definition inverts the mask (similar to "
"``torch.nn.MHA``); that is, MaskedTensor"
msgstr "*. 它们的工厂函数和基本定义会反转掩码（类似于``torch.nn.MHA``）；也就是说，MaskedTensor"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"uses ``True`` to denote \"specified\" and ``False`` to denote "
"\"unspecified\", or \"valid\"/\"invalid\", whereas NumPy does the opposite. "
"We believe that our mask definition is not only more intuitive, but it also "
"aligns more with the existing semantics in PyTorch as a whole."
msgstr ""
"使用``True``表示“指定”而``False``表示“未指定”或“有效”/“无效”，而NumPy则相反。我们认为我们的掩码定义不仅更直观，而且与PyTorch整体上的现有语义更加一致。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"*. Intersection semantics. In NumPy, if one of two elements are masked out, "
"the resulting element will be"
msgstr "*. 交集语义。在NumPy中，如果其中一个元素被掩盖，结果元素将"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"masked out as well -- in practice, they `apply the logical_or operator "
"<https://github.com/numpy/numpy/blob/68299575d8595d904aff6f28e12d21bf6428a4ba/numpy/ma/core.py#L1016-L1024>`__."
msgstr ""
"同样被掩盖——实际上，它们`应用逻辑或操作符 "
"<https://github.com/numpy/numpy/blob/68299575d8595d904aff6f28e12d21bf6428a4ba/numpy/ma/core.py#L1016-L1024>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Meanwhile, MaskedTensor does not support addition or binary operators with "
"masks that don't match -- to understand why, please find the :ref:`section "
"on reductions <reduction-semantics>`."
msgstr ""
"与此同时，MaskedTensor不支持掩码不匹配时的加法或二元操作符——要理解原因，请查看:ref:`归约章节<reduction-"
"semantics>`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"However, if this behavior is desired, MaskedTensor does support these "
"semantics by giving access to the data and masks and conveniently converting"
" a MaskedTensor to a Tensor with masked values filled in using "
":func:`to_tensor`. For example:"
msgstr ""
"不过，如果需要这种行为，MaskedTensor确实支持这些语义，可以访问数据和掩码，同时通过使用:func:`to_tensor`方便地将MaskedTensor转换为用掩码值填充的Tensor。例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that the mask is `mt0.get_mask() & mt1.get_mask()` since "
":class:`MaskedTensor`'s mask is the inverse of NumPy's."
msgstr ""
"注意，掩码是`mt0.get_mask() & "
"mt1.get_mask()`，因为:class:`MaskedTensor`的掩码是NumPy&apos;s的反转版本。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Reduction Semantics"
msgstr "归约语义"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Recall in `MaskedTensor's Overview tutorial "
"<https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`__ we "
"discussed \"Implementing missing torch.nan* ops\". Those are examples of "
"reductions -- operators that remove one (or more) dimensions from a Tensor "
"and then aggregate the result. In this section, we will use reduction "
"semantics to motivate our strict requirements around matching masks from "
"above."
msgstr ""
"回想在`MaskedTensor&apos;s概述教程 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`__中我们讨论过的“实现缺失的torch.nan*操作”。这些是归约的例子——运算符从Tensor中移除一个（或多个）维度并聚合结果。在本节中，我们将使用归约语义来说明我们之前严格要求匹配掩码的原因。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Fundamentally, :class:`MaskedTensor`s perform the same reduction operation "
"while ignoring the masked out (unspecified) values. By way of example:"
msgstr "基本上，:class:`MaskedTensor`在忽略掩盖（未指定）值的情况下执行相同的归约操作。以一个例子说明："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Now, the different reductions (all on dim=1):"
msgstr "现在，不同的归约（所有在dim=1上）："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Of note, the value under a masked out element is not guaranteed to have any "
"specific value, especially if the row or column is entirely masked out (the "
"same is true for normalizations). For more details on masked semantics, you "
"can find this `RFC <https://github.com/pytorch/rfcs/pull/27>`__."
msgstr ""
"需要注意的是，被掩盖的元素下的值未必保证具有任何特定的值，特别是当整行或整列被完全掩盖时（归一化也是如此）。关于掩码语义的更多细节，可以查阅此`RFC "
"<https://github.com/pytorch/rfcs/pull/27>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, we can revisit the question: why do we enforce the invariant that masks"
" must match for binary operators? In other words, why don't we use the same "
"semantics as ``np.ma.masked_array``? Consider the following example:"
msgstr ""
"现在，我们可以重新审视这个问题：为什么我们会在二元操作上强制执行掩码必须匹配的约束？换句话说，为什么我们不使用``np.ma.masked_array``的相同语义？请看以下示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Now, let's try addition:"
msgstr "现在，我们尝试加法操作："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Sum and addition should clearly be associative, but with NumPy's semantics, "
"they are not, which can certainly be confusing for the user."
msgstr "求和和加法显然应该具有结合性，但使用NumPy&apos;s的语义时，却不具有，这可能会给用户带来困惑。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":class:`MaskedTensor`, on the other hand, will simply not allow this "
"operation since `mask0 != mask1`. That being said, if the user wishes, there"
" are ways around this (for example, filling in the MaskedTensor's undefined "
"elements with 0 values using :func:`to_tensor` like shown below), but the "
"user must now be more explicit with their intentions."
msgstr ""
":class:`MaskedTensor`则不会允许这种操作，因为`mask0 != "
"mask1`。话虽如此，如果用户愿意，可以通过一些方式规避这一点（例如，如下图所示，使用:func:`to_tensor`将MaskedTensor的未定义元素填充为0值），但用户现在必须更加明确自己的意图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have learned about the different design decisions "
"behind MaskedTensor and NumPy's MaskedArray, as well as reduction semantics."
" In general, MaskedTensor is designed to avoid ambiguity and confusing "
"semantics (for example, we try to preserve the associative property amongst "
"binary operations), which in turn can necessitate the user to be more "
"intentional with their code at times, but we believe this to be the better "
"move. If you have any thoughts on this, please `let us know "
"<https://github.com/pytorch/pytorch/issues>`__!"
msgstr ""
"在本教程中，我们了解了MaskedTensor与NumPy&apos;s "
"MaskedArray背后的不同设计决策，以及归约语义。总体而言，MaskedTensor旨在避免歧义和令人困惑的语义（例如，我们尝试在二元操作之间保留结合性属性），这反过来可能会使用户有时需要更加有意图地编写代码，但我们认为这是更好的选择。如果您对此有任何想法，请`告诉我们"
" <https://github.com/pytorch/pytorch/issues>`__！"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: maskedtensor_advanced_semantics.py "
"<maskedtensor_advanced_semantics.py>`"
msgstr ""
":download:`下载Python源码：maskedtensor_advanced_semantics.py "
"<maskedtensor_advanced_semantics.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: maskedtensor_advanced_semantics.ipynb "
"<maskedtensor_advanced_semantics.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：maskedtensor_advanced_semantics.ipynb "
"<maskedtensor_advanced_semantics.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_maskedtensor_overview.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`此处<sphx_glr_download_prototype_maskedtensor_overview.py>`下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Prototype) MaskedTensor Overview"
msgstr "(原型) MaskedTensor概述"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial is designed to serve as a starting point for using "
"MaskedTensors and discuss its masking semantics."
msgstr "本教程旨在作为使用MaskedTensor的起点，并讨论其掩码语义。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"MaskedTensor serves as an extension to :class:`torch.Tensor` that provides "
"the user with the ability to:"
msgstr "MaskedTensor作为:class:`torch.Tensor`的扩展，为用户提供以下能力："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"use any masked semantics (for example, variable length tensors, nan* "
"operators, etc.)"
msgstr "使用任何掩码语义（例如，变量长度张量、nan*操作等）"

#: ../../prototype/vulkan_workflow.rst:250
msgid "differentiation between 0 and NaN gradients"
msgstr "区分0和NaN梯度"

#: ../../prototype/vulkan_workflow.rst:250
msgid "various sparse applications (see tutorial below)"
msgstr "各种稀疏应用（请参阅下方教程）"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For a more detailed introduction on what MaskedTensors are, please find the "
"`torch.masked documentation "
"<https://pytorch.org/docs/master/masked.html>`__."
msgstr ""
"有关MaskedTensor更详细的介绍，请参阅`torch.masked文档 "
"<https://pytorch.org/docs/master/masked.html>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Using MaskedTensor"
msgstr "使用MaskedTensor"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this section we discuss how to use MaskedTensor including how to "
"construct, access, the data and mask, as well as indexing and slicing."
msgstr "在本节中，我们讨论如何使用MaskedTensor，包括如何构造、访问数据和掩码，以及如何进行索引和切片。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We'll begin by doing the necessary setup for the tutorial:"
msgstr "我们将首先为教程做必要的设置："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Construction"
msgstr "构造"

#: ../../prototype/vulkan_workflow.rst:250
msgid "There are a few different ways to construct a MaskedTensor:"
msgstr "构造MaskedTensor有几种不同的方法："

#: ../../prototype/vulkan_workflow.rst:250
msgid "The first way is to directly invoke the MaskedTensor class"
msgstr "第一种方法是直接调用MaskedTensor类"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The second (and our recommended way) is to use :func:`masked.masked_tensor` "
"and :func:`masked.as_masked_tensor` factory functions, which are analogous "
"to :func:`torch.tensor` and :func:`torch.as_tensor`"
msgstr ""
"第二种（也是我们推荐的方法）是使用:func:`masked.masked_tensor`和:func:`masked.as_masked_tensor`工厂函数，这与:func:`torch.tensor`和:func:`torch.as_tensor`类似"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Throughout this tutorial, we will be assuming the import line: `from "
"torch.masked import masked_tensor`."
msgstr "在本教程中，我们将假设如下的导入行：`from torch.masked import masked_tensor`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Accessing the data and mask"
msgstr "访问数据和掩码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The underlying fields in a MaskedTensor can be accessed through:"
msgstr "MaskedTensor中的底层字段可以通过以下方式访问："

#: ../../prototype/vulkan_workflow.rst:250
msgid "the :meth:`MaskedTensor.get_data` function"
msgstr ":meth:`MaskedTensor.get_data`函数"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"the :meth:`MaskedTensor.get_mask` function. Recall that ``True`` indicates "
"\"specified\" or \"valid\" while ``False`` indicates \"unspecified\" or "
"\"invalid\"."
msgstr ""
":meth:`MaskedTensor.get_mask`函数。请记住，``True``表示“指定”或“有效”，而``False``表示“未指定”或“无效”。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In general, the underlying data that is returned may not be valid in the "
"unspecified entries, so we recommend that when users require a Tensor "
"without any masked entries, that they use :meth:`MaskedTensor.to_tensor` (as"
" shown above) to return a Tensor with filled values."
msgstr ""
"总的来说，返回的底层数据在未指定的条目中可能无效，因此我们建议当用户需要一个没有任何掩码条目的Tensor时，使用:meth:`MaskedTensor.to_tensor`（如上所示）返回一个填充值的Tensor。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Indexing and slicing"
msgstr "索引和切片"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":class:`MaskedTensor` is a Tensor subclass, which means that it inherits the"
" same semantics for indexing and slicing as :class:`torch.Tensor`. Below are"
" some examples of common indexing and slicing patterns:"
msgstr ""
":class:`MaskedTensor`是Tensor的子类，这意味着它继承了与:class:`torch.Tensor`相同的索引和切片语义。以下是一些常见的索引和切片模式的示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Why is MaskedTensor useful?"
msgstr "为什么MaskedTensor有用？"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Because of :class:`MaskedTensor`'s treatment of specified and unspecified "
"values as a first-class citizen instead of an afterthought (with filled "
"values, nans, etc.), it is able to solve for several of the shortcomings "
"that regular Tensors are unable to; indeed, :class:`MaskedTensor` was born "
"in a large part due to these recurring issues."
msgstr ""
"因为:class:`MaskedTensor``对指定和未指定值的处理是作为一等公民而不是事后补充（通过填充值、nans等），它能够解决常规Tensor无法解决的多个不足之处；事实上，:class:`MaskedTensor`的诞生很大程度上是由于这些重复性问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Below, we will discuss some of the most common issues that are still "
"unresolved in PyTorch today and illustrate how :class:`MaskedTensor` can "
"solve these problems."
msgstr "以下我们将讨论PyTorch中至今未解决的一些常见问题，并说明如何用:class:`MaskedTensor`解决这些问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Distinguishing between 0 and NaN gradient"
msgstr "区分0和NaN梯度"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"One issue that :class:`torch.Tensor` runs into is the inability to "
"distinguish between gradients that are undefined (NaN) vs. gradients that "
"are actually 0. Because PyTorch does not have a way of marking a value as "
"specified/valid vs. unspecified/invalid, it is forced to rely on NaN or 0 "
"(depending on the use case), leading to unreliable semantics since many "
"operations aren't meant to handle NaN values properly. What is even more "
"confusing is that sometimes depending on the order of operations, the "
"gradient could vary (for example, depending on how early in the chain of "
"operations a NaN value manifests)."
msgstr ""
"一个问题是 :class:`torch.Tensor` 面临无法区分梯度是未定义（NaN）还是实际为 0 的困境。由于 PyTorch "
"没有一种方法标记一个值是已指定/有效还是未指定/无效，它不得不依赖 NaN 或 0（取决于具体情况），导致不可靠的语义，因为许多操作不能正确处理 NaN"
" 值。更混乱的是，有时梯度会因操作顺序不同而有所变化（例如，取决于链操作中 NaN 值出现的早晚）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ":class:`MaskedTensor` is the perfect solution for this!"
msgstr ":class:`MaskedTensor` 是解决这个问题的完美方案！"

#: ../../prototype/vulkan_workflow.rst:250
msgid "torch.where"
msgstr "torch.where"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In `Issue 10729 <https://github.com/pytorch/pytorch/issues/10729>`__, we "
"notice a case where the order of operations can matter when using "
":func:`torch.where` because we have trouble differentiating between if the 0"
" is a real 0 or one from undefined gradients. Therefore, we remain "
"consistent and mask out the results:"
msgstr ""
"在 `Issue 10729 <https://github.com/pytorch/pytorch/issues/10729>`__ "
"中，我们注意到使用 :func:`torch.where` 时操作顺序可能会产生影响，因为我们无法区分 0 是实际的 0 "
"还是来自未定义梯度的结果。因此，我们保持一致并屏蔽结果："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Current result:"
msgstr "当前结果："

#: ../../prototype/vulkan_workflow.rst:250
msgid ":class:`MaskedTensor` result:"
msgstr ":class:`MaskedTensor` 结果："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The gradient here is only provided to the selected subset. Effectively, this"
" changes the gradient of `where` to mask out elements instead of setting "
"them to zero."
msgstr "这里的梯度仅提供给选定的子集。实际上，这会更改 `where` 的梯度以屏蔽掉元素，而不是将它们设置为零。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Another torch.where"
msgstr "另一个 torch.where"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Issue 52248 <https://github.com/pytorch/pytorch/issues/52248>`__ is another"
" example."
msgstr ""
"`Issue 52248 <https://github.com/pytorch/pytorch/issues/52248>`__ 是另一个例子。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This issue is similar (and even links to the next issue below) in that it "
"expresses frustration with unexpected behavior because of the inability to "
"differentiate \"no gradient\" vs \"zero gradient\", which in turn makes "
"working with other ops difficult to reason about."
msgstr ""
"此问题类似（甚至链接到下面的下一个问题），因为它表达了对意外行为的挫败感，原因是无法区分“无梯度”和“零梯度”，这反过来使处理其他操作变得难以理解。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "When using mask, x/0 yields NaN grad"
msgstr "使用掩码时，x/0 产生 NaN 梯度"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In `Issue 4132 <https://github.com/pytorch/pytorch/issues/4132>`__, the user"
" proposes that `x.grad` should be `[0, 1]` instead of the `[nan, 1]`, "
"whereas :class:`MaskedTensor` makes this very clear by masking out the "
"gradient altogether."
msgstr ""
"在 `Issue 4132 <https://github.com/pytorch/pytorch/issues/4132>`__ 中，用户提出 "
"`x.grad` 应为 `[0, 1]` 而不是 `[nan, 1]`，而 :class:`MaskedTensor` "
"通过完全屏蔽梯度使这一点变得非常清晰。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ":func:`torch.nansum` and :func:`torch.nanmean`"
msgstr ":func:`torch.nansum` 和 :func:`torch.nanmean`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In `Issue 67180 <https://github.com/pytorch/pytorch/issues/67180>`__, the "
"gradient isn't calculate properly (a longstanding issue), whereas "
":class:`MaskedTensor` handles it correctly."
msgstr ""
"在 `Issue 67180 <https://github.com/pytorch/pytorch/issues/67180>`__ "
"中，梯度未正确计算（一个长期存在的问题），而 :class:`MaskedTensor` 正确处理了它。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Safe Softmax"
msgstr "安全 Softmax"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Safe softmax is another great example of `an issue "
"<https://github.com/pytorch/pytorch/issues/55056>`__ that arises frequently."
" In a nutshell, if there is an entire batch that is \"masked out\" or "
"consists entirely of padding (which, in the softmax case, translates to "
"being set `-inf`), then this will result in NaNs, which can lead to training"
" divergence."
msgstr ""
"安全 softmax 是另一个经常出现的很好的例子，问题详见 `issue "
"<https://github.com/pytorch/pytorch/issues/55056>`__。简而言之，如果整个批次被“屏蔽”或完全由填充组成（在"
" softmax 情况下，表示设置为 `-inf`），这会导致 NaN，进而可能导致训练发散。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Luckily, :class:`MaskedTensor` has solved this issue. Consider this setup:"
msgstr "幸运的是，:class:`MaskedTensor` 已解决该问题。考虑以下设置："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For example, we want to calculate the softmax along `dim=0`. Note that the "
"second column is \"unsafe\" (i.e. entirely masked out), so when the softmax "
"is calculated, the result will yield `0/0 = nan` since `exp(-inf) = 0`. "
"However, what we would really like is for the gradients to be masked out "
"since they are unspecified and would be invalid for training."
msgstr ""
"例如，我们希望沿着 `dim=0` 计算 softmax。注意第二列是“不安全的”（即完全被屏蔽），因此计算 softmax 时结果会产生 `0/0 ="
" nan`，因为 `exp(-inf) = 0`。然而，我们真正希望的是屏蔽梯度，因为它们未被指定且对训练无效。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch result:"
msgstr "PyTorch 结果："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Implementing missing torch.nan* operators"
msgstr "实现缺失的 torch.nan* 操作符"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In `Issue 61474 <https://github.com/pytorch/pytorch/issues/61474>`__, there "
"is a request to add additional operators to cover the various `torch.nan*` "
"applications, such as ``torch.nanmax``, ``torch.nanmin``, etc."
msgstr ""
"在 `Issue 61474 <https://github.com/pytorch/pytorch/issues/61474>`__ "
"中，有添加操作符以覆盖各种 `torch.nan*` 应用的请求，例如 ``torch.nanmax``、``torch.nanmin`` 等。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In general, these problems lend themselves more naturally to masked "
"semantics, so instead of introducing additional operators, we propose using "
":class:`MaskedTensor` instead. Since `nanmean has already landed "
"<https://github.com/pytorch/pytorch/issues/21987>`__, we can use it as a "
"comparison point:"
msgstr ""
"一般来说，这些问题更自然地适配屏蔽语义，因此与其引入额外的操作符，我们建议使用 :class:`MaskedTensor`。由于 `nanmean "
"已经实现 <https://github.com/pytorch/pytorch/issues/21987>`__，我们可以将其用作比较点："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In the above example, we've constructed a `y` and would like to calculate "
"the mean of the series while ignoring the zeros. `torch.nanmean` can be used"
" to do this, but we don't have implementations for the rest of the "
"`torch.nan*` operations. :class:`MaskedTensor` solves this issue by being "
"able to use the base operation, and we already have support for the other "
"operations listed in the issue. For example:"
msgstr ""
"在上述示例中，我们构造了一个 `y`，并希望计算序列的平均值，同时忽略零值。`torch.nanmean` 可以用于该操作，但其余的 "
"`torch.nan*` 操作缺乏实现。:class:`MaskedTensor` "
"通过能够使用基础操作解决了这个问题，我们已经支持该问题中列出的其他操作。例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Indeed, the index of the minimum argument when ignoring the 0's is the 1 in "
"index 1."
msgstr "确实，当忽略零值时，最低参数的索引为索引 1 的 1。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":class:`MaskedTensor` can also support reductions when the data is fully "
"masked out, which is equivalent to the case above when the data Tensor is "
"completely ``nan``. ``nanmean`` would return ``nan`` (an ambiguous return "
"value), while MaskedTensor would more accurately indicate a masked out "
"result."
msgstr ""
":class:`MaskedTensor` 还可以支持当数据完全被屏蔽时的归约操作，相当于上述情况，当数据张量完全为 ``nan`` "
"时。``nanmean`` 会返回 ``nan``（一个模糊的返回值），而 MaskedTensor 更准确地指示一个屏蔽结果。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This is a similar problem to safe softmax where `0/0 = nan` when what we "
"really want is an undefined value."
msgstr "这是一个类似于安全 softmax 的问题，其中 `0/0 = nan` 而我们真正需要的是一个未定义的值。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we've introduced what MaskedTensors are, demonstrated how "
"to use them, and motivated their value through a series of examples and "
"issues that they've helped resolve."
msgstr "在本教程中，我们介绍了 MaskedTensors 是什么，演示了如何使用它们，并通过一系列示例和它们帮助解决的问题阐明了它们的价值。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To continue learning more, you can find our `MaskedTensor Sparsity tutorial "
"<https://pytorch.org/tutorials/prototype/maskedtensor_sparsity.html>`__ to "
"see how MaskedTensor enables sparsity and the different storage formats we "
"currently support."
msgstr ""
"要继续学习更多内容，可以查看我们的 `MaskedTensor Sparsity 教程 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_sparsity.html>`__，了解 "
"MaskedTensor 如何实现稀疏性以及我们目前支持的不同存储格式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: maskedtensor_overview.py "
"<maskedtensor_overview.py>`"
msgstr ""
":download:`下载 Python 源代码: maskedtensor_overview.py "
"<maskedtensor_overview.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: maskedtensor_overview.ipynb "
"<maskedtensor_overview.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本: maskedtensor_overview.ipynb "
"<maskedtensor_overview.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_maskedtensor_sparsity.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_prototype_maskedtensor_sparsity.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Prototype) MaskedTensor Sparsity"
msgstr "(原型) MaskedTensor 稀疏性"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Sparsity has been an area of rapid growth and importance within PyTorch; if "
"any sparsity terms are confusing below, please refer to the `sparsity "
"tutorial <https://pytorch.org/docs/stable/sparse.html>`__ for additional "
"details."
msgstr ""
"稀疏性在 PyTorch 中一直是快速增长和重要的领域；如果下面的稀疏术语有任何困惑，请参阅 `稀疏教程 "
"<https://pytorch.org/docs/stable/sparse.html>`__ 了解更多细节。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Sparse storage formats have been proven to be powerful in a variety of ways."
" As a primer, the first use case most practitioners think about is when the "
"majority of elements are equal to zero (a high degree of sparsity), but even"
" in cases of lower sparsity, certain formats (e.g. BSR) can take advantage "
"of substructures within a matrix."
msgstr ""
"稀疏存储格式已经在多种方面被证明是强大的。作为基础，大多数实践者首先想到的用例是当多数元素为零时（高稀疏度），但即使在较低稀疏度的情况下，某些格式（如 "
"BSR）可以利用矩阵中的子结构。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"At the moment, MaskedTensor supports COO and CSR tensors with plans to "
"support additional formats (such as BSR and CSC) in the future. If you have "
"any requests for additional formats, please file a feature request `here "
"<https://github.com/pytorch/pytorch/issues>`__!"
msgstr ""
"目前，MaskedTensor 支持 COO 和 CSR 张量，未来计划支持其他格式（如 BSR 和 CSC）。如果您有任何对其他格式的需求，请在 "
"`这里 <https://github.com/pytorch/pytorch/issues>`__ 提交功能请求！"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Principles"
msgstr "原则"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When creating a :class:`MaskedTensor` with sparse tensors, there are a few "
"principles that must be observed:"
msgstr "使用稀疏张量创建 :class:`MaskedTensor` 时，需要遵循以下原则："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``data`` and ``mask`` must have the same storage format, whether that's "
":attr:`torch.strided`, :attr:`torch.sparse_coo`, or :attr:`torch.sparse_csr`"
msgstr ""
"``data`` 和 ``mask`` 必须具有相同的存储格式，无论是 "
":attr:`torch.strided`、:attr:`torch.sparse_coo` 还是 :attr:`torch.sparse_csr`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``data`` and ``mask`` must have the same size, indicated by :func:`size()`"
msgstr "``data`` 和 ``mask`` 必须具有相同的大小，由 :func:`size()` 指示"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Sparse COO tensors"
msgstr "稀疏 COO 张量"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In accordance with Principle #1, a sparse COO MaskedTensor is created by "
"passing in two sparse COO tensors, which can be initialized by any of its "
"constructors, for example :func:`torch.sparse_coo_tensor`."
msgstr ""
"根据原则 #1，稀疏 COO MaskedTensor 是通过传递两个稀疏 COO 张量创建的，可以通过其任何构造函数初始化，例如 "
":func:`torch.sparse_coo_tensor`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As a recap of `sparse COO tensors "
"<https://pytorch.org/docs/stable/sparse.html#sparse-coo-tensors>`__, the COO"
" format stands for \"coordinate format\", where the specified elements are "
"stored as tuples of their indices and the corresponding values. That is, the"
" following are provided:"
msgstr ""
"回顾 `稀疏 COO 张量 <https://pytorch.org/docs/stable/sparse.html#sparse-coo-"
"tensors>`__ 的内容，COO 格式代表“坐标格式”，指定的元素被存储为其索引元组及对应值。即提供以下内容："

#: ../../prototype/vulkan_workflow.rst:250
msgid "``indices``: array of size ``(ndim, nse)`` and dtype ``torch.int64``"
msgstr "``indices``: 大小 ``(ndim, nse)`` 和 dtype ``torch.int64`` 的数组"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``values``: array of size `(nse,)` with any integer or floating point dtype"
msgstr "``values``: 大小 `(nse,)` 且具有任意整数或浮点 dtype 的数组"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"where ``ndim`` is the dimensionality of the tensor and ``nse`` is the number"
" of specified elements."
msgstr "其中 ``ndim`` 是张量的维数，``nse`` 是指定的元素数量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For both sparse COO and CSR tensors, you can construct a "
":class:`MaskedTensor` by doing either:"
msgstr "对于稀疏 COO 和 CSR 张量，可以通过以下方式构造 :class:`MaskedTensor`："

#: ../../prototype/vulkan_workflow.rst:250
msgid "``masked_tensor(sparse_tensor_data, sparse_tensor_mask)``"
msgstr "``masked_tensor(sparse_tensor_data, sparse_tensor_mask)``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``dense_masked_tensor.to_sparse_coo()`` or "
"``dense_masked_tensor.to_sparse_csr()``"
msgstr ""
"``dense_masked_tensor.to_sparse_coo()`` 或 "
"``dense_masked_tensor.to_sparse_csr()``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The second method is easier to illustrate so we've shown that below, but for"
" more on the first and the nuances behind the approach, please read the "
":ref:`Sparse COO Appendix <sparse-coo-appendix>`."
msgstr ""
"第二种方法更容易说明，因此我们在下面展示了该方法，但有关第一种方法和方法细节的更多信息，请阅读 :ref:`Sparse COO 附录 <sparse-"
"coo-appendix>`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Sparse CSR tensors"
msgstr "稀疏 CSR 张量"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Similarly, :class:`MaskedTensor` also supports the `CSR (Compressed Sparse "
"Row) <https://pytorch.org/docs/stable/sparse.html#sparse-csr-tensor>`__ "
"sparse tensor format. Instead of storing the tuples of the indices like "
"sparse COO tensors, sparse CSR tensors aim to decrease the memory "
"requirements by storing compressed row indices. In particular, a CSR sparse "
"tensor consists of three 1-D tensors:"
msgstr ""
"类似地，:class:`MaskedTensor` 还支持 `CSR (压缩稀疏行) "
"<https://pytorch.org/docs/stable/sparse.html#sparse-csr-tensor>`__ "
"稀疏张量格式。与稀疏 COO 张量存储索引元组不同，稀疏 CSR 张量通过存储压缩行索引来减少内存需求。具体来说，CSR "
"稀疏张量由以下三个一维张量组成："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``crow_indices``: array of compressed row indices with size ``(size[0] + "
"1,)``. This array indicates which row a given entry in values lives in. The "
"last element is the number of specified elements, while `crow_indices[i+1] -"
" crow_indices[i]` indicates the number of specified elements in row i."
msgstr ""
"``crow_indices``: 包含压缩行索引且大小为 ``(size[0] + 1,)`` "
"的数组。此数组指示给定值条目在哪一行。最后一个元素是指定元素的数量，`crow_indices[i+1] - crow_indices[i]` 表示第 "
"i 行的指定元素数量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``col_indices``: array of size ``(nnz,)``. Indicates the column indices for "
"each value."
msgstr "``col_indices``: 大小为 ``(nnz,)`` 的数组。指示每个值的列索引。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``values``: array of size ``(nnz,)``. Contains the values of the CSR tensor."
msgstr "``values``: 大小为 ``(nnz,)`` 的数组。包含 CSR 张量的值。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Of note, both sparse COO and CSR tensors are in a `beta "
"<https://pytorch.org/docs/stable/index.html>`__ state."
msgstr ""
"值得注意的是，稀疏 COO 和 CSR 张量均处于 `beta "
"<https://pytorch.org/docs/stable/index.html>`__ 阶段。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "By way of example:"
msgstr "例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Supported Operations"
msgstr "支持的操作"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Unary"
msgstr "一元运算"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"All `unary operators <https://pytorch.org/docs/master/masked.html#unary-"
"operators>`__ are supported, e.g.:"
msgstr ""
"所有 `一元运算符 <https://pytorch.org/docs/master/masked.html#unary-operators>`__ "
"均受支持，例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Binary"
msgstr "二元运算"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Binary operators <https://pytorch.org/docs/master/masked.html#unary-"
"operators>`__ are also supported, but the input masks from the two masked "
"tensors must match. For more information on why this decision was made, "
"please find our `MaskedTensor: Advanced Semantics tutorial "
"<https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html>`__."
msgstr ""
"`二元运算符 <https://pytorch.org/docs/master/masked.html#binary-operators>`__ "
"也受支持，但两个 masked 张量的输入屏蔽必须匹配。有关为什么做出此决定的更多信息，请参阅我们的 `MaskedTensor: 高级语义教程 "
"<https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Please find an example below:"
msgstr "请参见下面的示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Reductions"
msgstr "归约"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Finally, `reductions "
"<https://pytorch.org/docs/master/masked.html#reductions>`__ are supported:"
msgstr ""
"最后，`归约 <https://pytorch.org/docs/master/masked.html#reductions>`__ 受支持："

#: ../../prototype/vulkan_workflow.rst:250
msgid "MaskedTensor Helper Methods"
msgstr "MaskedTensor 辅助方法"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For convenience, :class:`MaskedTensor` has a number of methods to help "
"convert between the different layouts and identify the current layout:"
msgstr "为了便捷，:class:`MaskedTensor` 提供了一些方法帮助在不同布局之间转换并识别当前布局："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Setup:"
msgstr "设置："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":meth:`MaskedTensor.to_sparse_coo()` / :meth:`MaskedTensor.to_sparse_csr()` "
"/ :meth:`MaskedTensor.to_dense()` to help convert between the different "
"layouts."
msgstr ""
":meth:`MaskedTensor.to_sparse_coo()` / :meth:`MaskedTensor.to_sparse_csr()` "
"/ :meth:`MaskedTensor.to_dense()` 用于帮助在不同布局之间转换。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":meth:`MaskedTensor.is_sparse` -- this will check if the "
":class:`MaskedTensor`'s layout matches any of the supported sparse layouts "
"(currently COO and CSR)."
msgstr ""
":meth:`MaskedTensor.is_sparse` -- 检查 :class:`MaskedTensor`&apos; "
"的布局是否匹配任何支持的稀疏布局（目前为 COO 和 CSR）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ":meth:`MaskedTensor.is_sparse_coo()`"
msgstr ":meth:`MaskedTensor.is_sparse_coo()`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ":meth:`MaskedTensor.is_sparse_csr()`"
msgstr ":meth:`MaskedTensor.is_sparse_csr()`"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Appendix"
msgstr "附录"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Sparse COO Construction"
msgstr "稀疏 COO 构造"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Recall in our :ref:`original example <sparse-coo-tensors>`, we created a "
":class:`MaskedTensor` and then converted it to a sparse COO MaskedTensor "
"with :meth:`MaskedTensor.to_sparse_coo`."
msgstr ""
"回顾我们的 :ref:`原始示例 <sparse-coo-tensors>`，我们创建了一个 :class:`MaskedTensor`，然后通过 "
":meth:`MaskedTensor.to_sparse_coo` 将其转换为一个稀疏 COO MaskedTensor。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Alternatively, we can also construct a sparse COO MaskedTensor directly by "
"passing in two sparse COO tensors:"
msgstr "另外，我们还可以通过传入两个稀疏 COO 张量直接构造稀疏 COO MaskedTensor："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Instead of using :meth:`torch.Tensor.to_sparse`, we can also create the "
"sparse COO tensors directly, which brings us to a warning:"
msgstr "除了使用 :meth:`torch.Tensor.to_sparse`，我们还可以直接创建稀疏 COO 张量，这引出了一个警告："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When using a function like :meth:`MaskedTensor.to_sparse_coo` (analogous to "
":meth:`Tensor.to_sparse`), if the user does not specify the indices like in "
"the above example, then the 0 values will be \"unspecified\" by default."
msgstr ""
"使用类似 :meth:`MaskedTensor.to_sparse_coo` 的函数（类似于 "
":meth:`Tensor.to_sparse`），如果用户未像上面的示例那样指定索引，则默认情况下 0 值会被视为“未指定”。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Below, we explicitly specify the 0's:"
msgstr "下面，我们明确指定了 0&apos;："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that ``mt`` and ``mt2`` look identical on the surface, and in the vast "
"majority of operations, will yield the same result. But this brings us to a "
"detail on the implementation:"
msgstr "注意 ``mt`` 和 ``mt2`` 在表面上看起来是相同的，并且在绝大多数操作中会产生相同结果。但这引出了有关实现细节的一点："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``data`` and ``mask`` -- only for sparse MaskedTensors -- can have a "
"different number of elements (:func:`nnz`) **at creation**, but the indices "
"of ``mask`` must then be a subset of the indices of ``data``. In this case, "
"``data`` will assume the shape of ``mask`` by ``data = "
"data.sparse_mask(mask)``; in other words, any of the elements in ``data`` "
"that are not ``True`` in ``mask`` (that is, not specified) will be thrown "
"away."
msgstr ""
"``data`` 和 ``mask`` - 仅针对稀疏 MaskedTensors - 在创建时可以具有不同数量的元素（:func:`nnz`），但此时"
" ``mask`` 的索引必须是 ``data`` 的索引的子集。在这种情况下，通过 ``data = "
"data.sparse_mask(mask)``，``data`` 将采用 ``mask`` 的形状；换句话说，``mask`` 中不是 "
"``True`` 的元素（即未指定的元素）将在 ``data`` 中被丢弃。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Therefore, under the hood, the data looks slightly different; ``mt2`` has "
"the \"4\" value masked out and ``mt`` is completely without it. Their "
"underlying data has different shapes, which would make operations like ``mt "
"+ mt2`` invalid."
msgstr ""
"因此，从内部来看，数据看起来稍有不同；``mt2`` 遮盖了 \"4\" 的值，而 ``mt`` 则完全没有它。它们的底层数据有不同的形状，这会使类似 "
"``mt + mt2`` 的操作无效。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Sparse CSR Construction"
msgstr "稀疏 CSR 构造"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can also construct a sparse CSR MaskedTensor using sparse CSR tensors, "
"and like the example above, this results in a similar treatment under the "
"hood."
msgstr "我们还可以使用稀疏 CSR 张量构造稀疏 CSR MaskedTensor，与上面的示例类似，这会导致内部数据处理方式的相似性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have introduced how to use :class:`MaskedTensor` with "
"sparse COO and CSR formats and discussed some of the subtleties under the "
"hood in case users decide to access the underlying data structures directly."
" Sparse storage formats and masked semantics indeed have strong synergies, "
"so much so that they are sometimes used as proxies for each other (as we "
"will see in the next tutorial). In the future, we certainly plan to invest "
"and continue developing in this direction."
msgstr ""
"在本教程中，我们介绍了如何使用具有稀疏 COO 和 CSR 格式的 "
":class:`MaskedTensor`，并讨论了如果用户决定直接访问底层数据结构时的一些细微差别。稀疏存储格式和掩码语义确实具有很强的协同作用，以至于它们有时被用作彼此的代理（我们将在下一个教程中看到）。未来，我们肯定计划投资并继续在此方面进行开发。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To continue learning more, you can find our `Efficiently writing \"sparse\" "
"semantics for Adagrad with MaskedTensor tutorial "
"<https://pytorch.org/tutorials/prototype/maskedtensor_adagrad.html>`__ to "
"see an example of how MaskedTensor can simplify existing workflows with "
"native masking semantics."
msgstr ""
"要继续学习，您可以查看我们的 `Efficiently writing \"sparse\" semantics for Adagrad with "
"MaskedTensor tutorial "
"<https://pytorch.org/tutorials/prototype/maskedtensor_adagrad.html>`__ "
"，以了解如何使用 MaskedTensor 将现有工作流简化为原生掩码语义的示例。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: maskedtensor_sparsity.py "
"<maskedtensor_sparsity.py>`"
msgstr ""
":download:`下载 Python 源代码: maskedtensor_sparsity.py "
"<maskedtensor_sparsity.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: maskedtensor_sparsity.ipynb "
"<maskedtensor_sparsity.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: maskedtensor_sparsity.ipynb "
"<maskedtensor_sparsity.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Using Max-Autotune Compilation on CPU for Better Performance"
msgstr "在 CPU 上使用最高自动调优编译以提高性能"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Jiong Gong <https://github.com/jgong5>`__, `Leslie Fang "
"<https://github.com/leslie-fang-intel>`__, `Chunyuan Wu "
"<https://github.com/chunyuan-w>`__"
msgstr ""
"**作者**: `Jiong Gong <https://github.com/jgong5>`__，`Leslie Fang "
"<https://github.com/leslie-fang-intel>`__，`Chunyuan Wu "
"<https://github.com/chunyuan-w>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, you will learn how to boost your PyTorch models' "
"performance on CPU by leveraging the max-autotune mode in the Inductor CPU "
"backend. Explore the activation process, understand the differences from "
"traditional methods, and integrate max-autotune into your code for enhanced "
"computational efficiency. Dive into the use of advanced GEMM templates for "
"faster processing and superior runtime performance."
msgstr ""
"在本教程中，您将学习如何通过利用 Inductor CPU 后端的最高自动调优模式来提高 PyTorch 模型在 CPU "
"上的性能。探索激活过程，了解与传统方法的区别，并将最大自动调优集成到您的代码中以增强计算效率。深入了解使用高级 GEMM "
"模板进行更快的处理和更高的运行时性能。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The ``max-autotune`` mode for the Inductor CPU backend in ``torch.compile`` "
"(`RFC link <https://github.com/pytorch/pytorch/issues/125683>`_) profiles "
"multiple implementations of operations at compile time and selects the best-"
"performing one, trading longer compilation times for improved runtime "
"performance. This enhancement is particularly beneficial for GEMM-related "
"operations. In the Inductor CPU backend, we’ve introduced a C++ template-"
"based GEMM implementation as an alternative to the ATen-based approach that "
"relies on oneDNN and MKL libraries. This is similar to the max-autotune mode"
" on CUDA, where implementations from ATen, Triton, and CUTLASS are "
"considered."
msgstr ""
"Inductor CPU 后端的 ``max-autotune`` 模式在 ``torch.compile`` (`RFC 链接 "
"<https://github.com/pytorch/pytorch/issues/125683>`_) "
"中会在编译时对操作的多个实现进行分析，并选择性能最佳的一个，牺牲较长的编译时间以提高运行时性能。这种增强尤其有利于 GEMM 相关的操作。我们在 "
"Inductor CPU 后端引入了基于 C++ 模板的 GEMM 实现，作为依赖 oneDNN 和 MKL 库的基于 ATen 方法的替代方案。这与 "
"CUDA 上的 max-autotune 模式类似，其中考虑了来自 ATen、Triton 和 CUTLASS 的实现。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We have covered most popular data types, including FP32, BF16, FP16, and "
"INT8, with epilogue fusions for x86 CPUs."
msgstr "我们涵盖了大多数流行的数据类型，包括 FP32、BF16、FP16 和 INT8，带有 x86 CPU 的后处理融合。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"While the development is still in progress, we have already seen promising "
"speedups over pure ATen-based GEMMs as measured by the three benchmark "
"suites and the inference of LLMs."
msgstr "尽管开发仍在进行中，我们已经在通过三个基准套件和 LLM 的推理测得的纯基于 ATen 的 GEMM 上看到了令人鼓舞的加速效果。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Activating the ``max-autotune`` mode"
msgstr "激活 ``max-autotune`` 模式"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To activate the ``max-autotune`` mode in PyTorch, set the ``mode`` argument "
"to ``max-autotune`` when compiling your model using ``torch.compile``. If "
"you prefer to bypass the tuning process and always use the C++ template "
"implementations, you can configure this via an environment variable: "
"``export TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS=CPP``."
msgstr ""
"要在 PyTorch 中激活 ``max-autotune`` 模式，使用 ``torch.compile`` 编译模型时将 ``mode`` "
"参数设置为 ``max-autotune``。如果您更喜欢跳过调优过程并始终使用 C++ 模板实现，则可以通过环境变量进行配置：``export "
"TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS=CPP``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Example"
msgstr "示例"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The below code is an example of using the ``max-autotune`` mode on a simple "
"neural network with a linear layer followed by a ReLU activation."
msgstr "下面的代码示例展示了在一个简单的神经网络上使用 ``max-autotune`` 模式，其中包括一个线性层跟随一个 ReLU 激活。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In the C++ template-based GEMM implementation, we will pre-pack the weight "
"for good cache usage. In the case of inference which is the primary scenario"
" of CPU AI workloads, model weights are constant and we pack them upfront "
"during compilation so that the data accesses are contiguous within the cache"
" blocks. Thus, We only support frozen model with ``torch.no_grad`` or the "
"inference mode. You need to set the environment variable ``export "
"TORCHINDUCTOR_FREEZING=1`` and ensure that both the compilation and "
"inference steps are executed within the ``torch.no_grad`` context."
msgstr ""
"在基于 C++ 模板的 GEMM 实现中，我们会预先打包权重以实现良好的缓存使用。在推理的情况下（这是 CPU AI "
"工作负载的主要场景），模型权重是恒定的，我们在编译期间预先打包它们，以便数据访问是在缓存块内连续的。因此，我们仅支持使用 "
"``torch.no_grad`` 或推理模式的冻结模型。您需要设置环境变量 ``export TORCHINDUCTOR_FREEZING=1`` "
"并确保编译和推理步骤都在 ``torch.no_grad`` 的上下文中执行。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When running the above code snippet, you will see the autotuning result (the"
" performance numbers are for demonstration purposes). In this example, C++ "
"template outperforms ATen kernel so that it will be selected."
msgstr "运行上述代码片段时，您将看到自动调优结果（性能数字仅为演示用途）。在此示例中，C++ 模板优于 ATen 内核，因此将被选择。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We could check the generated output code by setting ``export "
"TORCH_LOGS=\"+output_code\"``. When C++ template is selected, we won't have "
"``torch.ops.mkldnn._linear_pointwise.default`` (for bfloat16) or "
"``torch.ops.mkl._mkl_linear.default`` (for float32) in the generated code "
"anymore, instead, we'll find kernel based on CPP GEMM template "
"``cpp_fused__to_copy_relu_1`` (only part of the code is demonstrated below "
"for simplicity) with the bias and relu epilogues fused inside the C++ GEMM "
"template kernel."
msgstr ""
"我们可以通过设置 ``export TORCH_LOGS=\"+output_code\"`` 检查生成的输出代码。当选择 C++ "
"模板时，生成的代码中不再有 ``torch.ops.mkldnn._linear_pointwise.default``（用于 bfloat16）或 "
"``torch.ops.mkl._mkl_linear.default``（用于 float32），而是基于 C++ GEMM 模板的内核 "
"``cpp_fused__to_copy_relu_1``（为了简化，仅展示部分代码）并在 C++ GEMM 模板内核中将 bias 和 relu "
"后处理融合。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The generated code differs by CPU architecture and is implementation-"
"specific, which is subject to change."
msgstr "生成的代码因 CPU 架构不同而有所不同，并且是实现特定的，可能会发生变化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we introduced max-autotune support on CPU with GEMM "
"template. We explained the API to activate this feature, and demonstrated "
"the generated code of the GEMM template."
msgstr ""
"在本教程中，我们介绍了 CPU 上的 GEMM 模板的 max-autotune支持。我们解释了激活此功能的 API，并演示了 GEMM "
"模板生成的代码。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This feature is in prototype stage. If you have any feature requests or run "
"into any issues, please file a bug report at `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_."
msgstr ""
"此功能处于原型阶段。如果您有任何功能请求或遇到任何问题，请在 `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`__ 提交错误报告。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_nestedtensor.py>` to download "
"the full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_prototype_nestedtensor.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Getting Started with Nested Tensors"
msgstr "嵌套张量入门"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Nested tensors generalize the shape of regular dense tensors, allowing for "
"representation of ragged-sized data."
msgstr "嵌套张量对常规密集张量的形状进行泛化，支持表示大小不一的数据。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "for a regular tensor, each dimension is regular and has a size"
msgstr "对于常规张量，每个维度是规则的并且具有一个大小"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"for a nested tensor, not all dimensions have regular sizes; some of them are"
" ragged"
msgstr "对于嵌套张量，并非所有维度都有规则大小；有些维度是大小不均的"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Nested tensors are a natural solution for representing sequential data "
"within various domains:"
msgstr "嵌套张量是表示各个领域内顺序数据的自然解决方案："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"in NLP, sentences can have variable lengths, so a batch of sentences forms a"
" nested tensor"
msgstr "在自然语言处理（NLP）中，句子可以具有不同的长度，因此句子的批量形成嵌套张量"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"in CV, images can have variable shapes, so a batch of images forms a nested "
"tensor"
msgstr "在计算机视觉（CV）中，图像可以具有不同的形状，因此图像的批量形成嵌套张量"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we will demonstrate basic usage of nested tensors and "
"motivate their usefulness for operating on sequential data of varying "
"lengths with a real-world example. In particular, they are invaluable for "
"building transformers that can efficiently operate on ragged sequential "
"inputs. Below, we present an implementation of multi-head attention using "
"nested tensors that, combined usage of ``torch.compile``, out-performs "
"operating naively on tensors with padding."
msgstr ""
"在本教程中，我们将展示嵌套张量的基本用法，并通过一个真实的示例说明它们在处理可变长度的顺序数据操作上的用处。特别是在构建可以有效处理不规则顺序输入的变形金刚（Transformer）时，它们是不可或缺的工具。下面我们展示了使用嵌套张量实现多头注意力的一个示例，该方法结合使用了"
" ``torch.compile``，比直接对带有填充的张量操作更加高效。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Nested tensors are currently a prototype feature and are subject to change."
msgstr "嵌套张量目前是一个原型功能，可能会发生变化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Nested tensor initialization"
msgstr "嵌套张量的初始化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"From the Python frontend, a nested tensor can be created from a list of "
"tensors. We denote nt[i] as the ith tensor component of a nestedtensor."
msgstr "从 Python 前端，可以通过张量列表创建嵌套张量。我们用 nt[i] 表示嵌套张量的第 i 个张量组件。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"By padding every underlying tensor to the same shape, a nestedtensor can be "
"converted to a regular tensor."
msgstr "通过将每个底层张量填充为相同的形状，嵌套张量可以转换为常规张量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "All tensors posses an attribute for determining if they are nested;"
msgstr "所有张量都具有一个属性，用于确定它们是否是嵌套张量；"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"It is common to construct nestedtensors from batches of irregularly shaped "
"tensors. i.e. dimension 0 is assumed to be the batch dimension. Indexing "
"dimension 0 gives back the first underlying tensor component."
msgstr "通常从形状不规则的张量批量构造嵌套张量。例如，第 0 维被假定为批量维度。索引第 0 维返回第一个底层张量组件。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"An important note is that slicing in dimension 0 has not been supported yet."
" Which means it not currently possible to construct a view that combines the"
" underlying tensor components."
msgstr "需要注意的是，目前还不支持在第 0 维进行切片。这意味着当前还无法构造合并底层张量组件的视图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Nested Tensor Operations"
msgstr "嵌套张量操作"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As each operation must be explicitly implemented for nestedtensors, "
"operation coverage for nestedtensors is currently narrower than that of "
"regular tensors. For now, only basic operations such as index, dropout, "
"softmax, transpose, reshape, linear, bmm are covered. However, coverage is "
"being expanded. If you need certain operations, please file an `issue "
"<https://github.com/pytorch/pytorch>`__ to help us prioritize coverage."
msgstr ""
"由于每个操作都必须针对嵌套张量显式实现，目前嵌套张量的操作覆盖率比常规张量低。现在，仅支持基本操作，比如索引、dropout、softmax、转置、变形、线性操作、bmm。然而，覆盖范围正在扩展。如果您需要某些操作，请提交一个"
" `问题 <https://github.com/pytorch/pytorch>`__ 来帮助我们优先考虑覆盖。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**reshape**"
msgstr "**变形**"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The reshape op is for changing the shape of a tensor. Its full semantics for"
" regular tensors can be found `here "
"<https://pytorch.org/docs/stable/generated/torch.reshape.html>`__. For "
"regular tensors, when specifying the new shape, a single dimension may be "
"-1, in which case it is inferred from the remaining dimensions and the "
"number of elements."
msgstr ""
"变形操作用于改变张量的形状。有关常规张量的完整语义，请参考 `这里 "
"<https://pytorch.org/docs/stable/generated/torch.reshape.html>`__。对于常规张量，在指定新形状时，单个维度可以是"
" -1，这种情况下会根据剩余维度和元素数量进行推断。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The semantics for nestedtensors are similar, except that -1 no longer "
"infers. Instead, it inherits the old size (here 2 for ``nt[0]`` and 3 for "
"``nt[1]``). -1 is the only legal size to specify for a jagged dimension."
msgstr ""
"嵌套张量的语义类似，除了 -1 不再进行推断，而是继承旧大小（对于 ``nt[0]`` 是 2，对于 ``nt[1]`` 是 3）。-1 "
"是指定不规则维度的唯一合法大小。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**transpose**"
msgstr "**转置**"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The transpose op is for swapping two dimensions of a tensor. Its full "
"semantics can be found `here "
"<https://pytorch.org/docs/stable/generated/torch.transpose.html>`__. Note "
"that for nestedtensors dimension 0 is special; it is assumed to be the batch"
" dimension, so transposes involving nestedtensor dimension 0 are not "
"supported."
msgstr ""
"转置操作用于交换张量的两个维度。其完整语义请参考 `这里 "
"<https://pytorch.org/docs/stable/generated/torch.transpose.html>`__。请注意，对于嵌套张量，第"
" 0 维是特殊的；它被假定为批量维度，因此涉及嵌套张量第 0 维的转置不受支持。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**others**"
msgstr "**其他**"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Other operations have the same semantics as for regular tensors. Applying "
"the operation on a nestedtensor is equivalent to applying the operation to "
"the underlying tensor components, with the result being a nestedtensor as "
"well."
msgstr "其他操作的语义与常规张量相同。对嵌套张量应用操作相当于对底层张量组件应用操作，结果也是一个嵌套张量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Why Nested Tensor"
msgstr "为什么使用嵌套张量"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When data is sequential, it is often the case that each sample has a "
"different length. For example, in a batch of sentences, each sentence has a "
"different number of words. A common technique for handling varying sequences"
" is to manually pad each data tensor to the same shape in order to form a "
"batch. For example, we have 2 sentences with different lengths and a "
"vocabulary In order to represent his as single tensor we pad with 0 to the "
"max length in the batch."
msgstr ""
"当数据是顺序时，通常每个样本具有不同的长度。例如，在句子批量中，每个句子具有不同的单词数量。处理不规则序列的一种常用技术是手动将每个数据张量填充为相同的形状，以形成一个批量。例如，我们有"
" 2 个句子，长度不同，并有一个词汇表。为了将其表示为一个张量，我们用 0 填充到批量中的最大长度。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This technique of padding a batch of data to its max length is not optimal. "
"The padded data is not needed for computation and wastes memory by "
"allocating larger tensors than necessary. Further, not all operations have "
"the same semnatics when applied to padded data. For matrix multiplications "
"in order to ignore the padded entries, one needs to pad with 0 while for "
"softmax one has to pad with -inf to ignore specific entries. The primary "
"objective of nested tensor is to facilitate operations on ragged data using "
"the standard PyTorch tensor UX, thereby eliminating the need for inefficient"
" and complex padding and masking."
msgstr ""
"这种将数据批次填充到最大长度的技术并不理想。填充的数据不需要用于计算，并通过分配超过必要大小的张量浪费了内存。此外，并不是所有操作在应用于填充数据时都具有相同语义。对于矩阵乘法，为忽略填充条目，需要用"
" 0 填充，而对于 softmax，则需要用 -inf 填充以忽略特定条目。嵌套张量的主要目标是使用标准 PyTorch 张量 UX "
"方便地对不规则数据进行操作，从而消除低效和复杂的填充和掩码需求。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Let us take a look at a practical example: the multi-head attention "
"component utilized in `Transformers "
"<https://arxiv.org/pdf/1706.03762.pdf>`__. We can implement this in such a "
"way that it can operate on either padded or nested tensors."
msgstr ""
"让我们来看一个实际例子：在《Transformers》论文中使用的多头注意力组件。我们可以以这种方式实现它，使其可以在填充或嵌套张量上运行。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"set hyperparameters following `the Transformer paper "
"<https://arxiv.org/pdf/1706.03762.pdf>`__"
msgstr "按照《Transformers》论文设置超参数"

#: ../../prototype/vulkan_workflow.rst:250
msgid "except for dropout probability: set to 0 for correctness check"
msgstr "除了dropout概率：为了正确性验证，设置为0"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Let us generate some realistic fake data from Zipf's law."
msgstr "让我们根据Zipf定律生成一些真实的假数据。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Create nested tensor batch inputs"
msgstr "创建嵌套张量批量输入"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Generate padded forms of query, key, value for comparison"
msgstr "生成用于比较的查询、键、值的填充形式"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Construct the model"
msgstr "构建模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Check correctness and performance"
msgstr "检查正确性和性能"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that without ``torch.compile``, the overhead of the python subclass "
"nested tensor can make it slower than the equivalent computation on padded "
"tensors. However, once ``torch.compile`` is enabled, operating on nested "
"tensors gives a multiple x speedup. Avoiding wasted computation on padding "
"becomes only more valuable as the percentage of padding in the batch "
"increases."
msgstr ""
"请注意，在没有``torch.compile``的情况下，Python子类嵌套张量的开销可能使其比在填充张量上的等效计算更慢。然而，一旦启用``torch.compile``，在嵌套张量上操作可以提供数倍的加速。随着批量中的填充百分比增加，避免填充上的无用计算变得更加重要。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have learned how to perform basic operations with "
"nested tensors and how implement multi-head attention for transformers in a "
"way that avoids computation on padding. For more information, check out the "
"docs for the `torch.nested <https://pytorch.org/docs/stable/nested.html>`__ "
"namespace."
msgstr ""
"在本教程中，我们学会了如何使用嵌套张量进行基本操作，以及如何以避免填充上的计算的方式实现多头注意力用于Transformers。有关更多信息，请查看`torch.nested"
" <https://pytorch.org/docs/stable/nested.html>`__命名空间的文档。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "See Also"
msgstr "另见"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Accelerating PyTorch Transformers by replacing nn.Transformer with Nested "
"Tensors and torch.compile "
"<https://docs.pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`__"
msgstr ""
"`通过替换nn.Transformer为嵌套张量和torch.compile加速PyTorch Transformers "
"<https://docs.pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: nestedtensor.py <nestedtensor.py>`"
msgstr ":download:`下载Python源代码：nestedtensor.py <nestedtensor.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: nestedtensor.ipynb "
"<nestedtensor.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：nestedtensor.ipynb <nestedtensor.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(Beta) Convert MobileNetV2 to NNAPI"
msgstr "（测试版）将MobileNetV2转换为NNAPI"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_numeric_suite_tutorial.py>` to"
" download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_prototype_numeric_suite_tutorial.py>` "
"下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch Numeric Suite Tutorial"
msgstr "PyTorch数值工具套件教程"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Quantization is good when it works, but it’s difficult to know what's wrong "
"when it doesn't satisfy the accuracy we expect. Debugging the accuracy issue"
" of quantization is not easy and time consuming."
msgstr "量化很好，但只有在工作时才好；当它没有达到我们期望的准确性时，很难知道问题出在哪里。调试量化的准确性问题不易且耗时。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"One important step of debugging is to measure the statistics of the float "
"model and its corresponding quantized model to know where are they differ "
"most. We built a suite of numeric tools called PyTorch Numeric Suite in "
"PyTorch quantization to enable the measurement of the statistics between "
"quantized module and float module to support quantization debugging efforts."
" Even for the quantized model with good accuracy, PyTorch Numeric Suite can "
"still be used as the profiling tool to better understand the quantization "
"error within the model and provide the guidance for further optimization."
msgstr ""
"调试的重要一步是测量浮点模型及其对应的量化模型的统计数据，以了解它们的最大差异点。我们在PyTorch量化中构建了一套称为PyTorch Numeric"
" Suite的数值工具，用于测量量化模块和浮点模块之间的统计数据，以支持量化调试工作。即使对具有良好准确性的量化模型，PyTorch Numeric "
"Suite仍然可以作为剖析工具来更好地理解模型中的量化误差，为进一步优化提供指导。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"PyTorch Numeric Suite currently supports models quantized through both "
"static quantization and dynamic quantization with unified APIs."
msgstr "PyTorch Numeric Suite目前支持通过静态量化和动态量化完成的模型，并具有统一的API。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial we will first use ResNet18 as an example to show how to use"
" PyTorch Numeric Suite to measure the statistics between static quantized "
"model and float model in eager mode. Then we will use LSTM based sequence "
"model as an example to show the usage of PyTorch Numeric Suite for dynamic "
"quantized model."
msgstr ""
"在本教程中，我们将首先使用ResNet18作为示例展示如何使用PyTorch Numeric "
"Suite测量静态量化模型和浮点模型之间的统计数据（即时模式下）。然后我们将使用基于LSTM的序列模型作为示例展示如何在动态量化模型中使用PyTorch"
" Numeric Suite。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Numeric Suite for Static Quantization"
msgstr "用于静态量化的数值工具套件"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We’ll start by doing the necessary imports:"
msgstr "我们先完成必要的导入操作："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Then we load the pretrained float ResNet18 model, and quantize it into "
"qmodel. We cannot compare two arbitrary models, only a float model and the "
"quantized model derived from it can be compared."
msgstr "接着我们加载预训练的浮点ResNet18模型，并将其量化为qmodel。我们无法比较两个任意的模型，只能比较来自浮点模型的量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Compare the weights of float and quantized models"
msgstr "1. 比较浮点模型和量化模型的权重"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The first thing we usually want to compare are the weights of quantized "
"model and float model. We can call ``compare_weights()`` from PyTorch "
"Numeric Suite to get a dictionary ``wt_compare_dict`` with key corresponding"
" to module names and each entry is a dictionary with two keys 'float' and "
"'quantized', containing the float and quantized weights. "
"``compare_weights()`` takes in floating point and quantized state dict and "
"returns a dict, with keys corresponding to the floating point weights and "
"values being a dictionary of floating point and quantized weights"
msgstr ""
"我们通常首先想要比较的是量化模型和浮点模型的权重。我们可以调用PyTorch Numeric "
"Suite中的``compare_weights()``获得一个字典``wt_compare_dict``，其中的键对应于模块名称，每个条目都是一个字典，拥有两个键&apos;float&apos;和&apos;quantized&apos;，分别包含浮点和量化权重。``compare_weights()``接收浮点和量化状态字典并返回一个字典，键对应于浮点权重，值是一个包含浮点和量化权重的字典。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Once get ``wt_compare_dict``, users can process this dictionary in whatever "
"way they want. Here as an example we compute the quantization error of the "
"weights of float and quantized models as following. Compute the Signal-to-"
"Quantization-Noise Ratio (SQNR) of the quantized tensor ``y``. The SQNR "
"reflects the relationship between the maximum nominal signal strength and "
"the quantization error introduced in the quantization. Higher SQNR "
"corresponds to lower quantization error."
msgstr ""
"获得``wt_compare_dict``后，用户可以随意处理这个字典。下面作为示例，我们计算浮点模型和量化模型权重的量化误差。计算量化张量``y``的信号量化噪声比（SQNR）。SQNR反映了最大名义信号强度与量化中引入的量化误差之间的关系。更高的SQNR对应于更低的量化误差。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As another example ``wt_compare_dict`` can also be used to plot the "
"histogram of the weights of floating point and quantized models."
msgstr "作为另一个示例，``wt_compare_dict``也可用于绘制浮点模型和量化模型权重的直方图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Compare float point and quantized models at corresponding locations"
msgstr "2. 在对应位置比较浮点模型和量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The second tool allows for comparison of weights and activations between "
"float and quantized models at corresponding locations for the same input as "
"shown in the figure below. Red arrows indicate the locations of the "
"comparison."
msgstr "第二个工具允许在相同输入情况下比较浮点和量化模型的权重和激活输出，并对应位置如图所示。红色箭头表示比较位置。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We call ``compare_model_outputs()`` from PyTorch Numeric Suite to get the "
"activations in float model and quantized model at corresponding locations "
"for the given input data. This API returns a dict with module names being "
"keys. Each entry is itself a dict with two keys 'float' and 'quantized' "
"containing the activations."
msgstr ""
"我们调用PyTorch Numeric "
"Suite中的``compare_model_outputs()``以获取给定输入数据下在对应位置的浮点模型和量化模型的激活输出。此API返回一个字典，模块名称为键，每个条目本身是一个字典，拥有两个键&apos;float&apos;和&apos;quantized&apos;，分别包含激活输出。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This dict can be used to compare and compute the quantization error of the "
"activations of float and quantized models as following."
msgstr "该字典可用于比较浮点模型和量化模型激活输出的量化误差，如下所示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If we want to do the comparison for more than one input data, we can do the "
"following. Prepare the model by attaching the logger to both floating point "
"module and quantized module if they are in the ``white_list``. Default "
"logger is ``OutputLogger``, and default white_list is "
"``DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_WHITE_LIST``"
msgstr ""
"如果我们想对多个输入数据进行比较，可以如下操作。通过在``white_list``中的情况下将记录器附加到浮点模块和量化模块来准备模型。默认记录器是``OutputLogger``，默认白名单是``DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_WHITE_LIST``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The default logger used in above APIs is ``OutputLogger``, which is used to "
"log the outputs of the modules. We can inherit from base ``Logger`` class "
"and create our own logger to perform different functionalities. For example "
"we can make a new ``MyOutputLogger`` class as below."
msgstr ""
"上述API中使用的默认记录器是``OutputLogger``，用于记录模块的输出。我们可以继承基类``Logger``并创建自己的记录器以执行不同的功能。例如，我们可以创建一个新的``MyOutputLogger``类，如下所示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "And then we can pass this logger into above APIs such as:"
msgstr "然后可以将此记录器传递给上述API，例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid "or:"
msgstr "或："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"3. Compare a module in a quantized model with its float point equivalent, "
"with the same input data"
msgstr "3. 将量化模型中的一个模块与其对应浮点版本进行比较，并使用相同输入数据"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The third tool allows for comparing a quantized module in a model with its "
"float point counterpart, feeding both of them the same input and comparing "
"their outputs as shown below."
msgstr "第三个工具允许将模型中量化模块与其对应的浮点模块进行比较，使用相同输入进行比较输出，如下图所示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In practice we call prepare_model_with_stubs() to swap the quantized module "
"that we want to compare with the Shadow module, which is illustrated as "
"below:"
msgstr "实际上，我们调用``prepare_model_with_stubs()``来将我们想要比较的量化模块替换为Shadow模块，如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The Shadow module takes quantized module, float module and logger as input, "
"and creates a forward path inside to make the float module to shadow "
"quantized module sharing the same input tensor."
msgstr "Shadow模块以量化模块、浮点模块和记录器作为输入，并创建内部执行的前向路径，使浮点模块在共享相同输入张量的情况下模拟量化模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The logger can be customizable, default logger is ``ShadowLogger`` and it "
"will save the outputs of the quantized module and float module that can be "
"used to compute the module level quantization error."
msgstr "记录器可以自定义，默认记录器是``ShadowLogger``，它将保存量化模块和浮点模块的输出，可用于计算模块级别的量化误差。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Notice before each call of ``compare_model_outputs()`` and "
"``compare_model_stub()`` we need to have clean float and quantized model. "
"This is because ``compare_model_outputs()`` and ``compare_model_stub()`` "
"modify float and quantized model inplace, and it will cause unexpected "
"results if call one right after another."
msgstr ""
"注意在每次调用``compare_model_outputs()``和``compare_model_stub()``之前，我们需要有一个干净的浮点模型和量化模型。这是因为``compare_model_outputs()``和``compare_model_stub()``会内修改浮点模型和量化模型，并且如果一个紧接着另一个调用会导致意外结果。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In the following example we call ``compare_model_stub()`` from PyTorch "
"Numeric Suite to compare ``QuantizableBasicBlock`` module with its float "
"point equivalent. This API returns a dict with key corresponding to module "
"names and each entry being a dictionary with two keys 'float' and "
"'quantized', containing the output tensors of quantized and its matching "
"float shadow module."
msgstr ""
"在以下示例中，我们调用PyTorch Numeric "
"Suite中的``compare_model_stub()``以比较``QuantizableBasicBlock``模块与其对应浮点版本。此API返回一个字典，其中的键对应于模块名称，每个条目是一个字典，具有两个键&apos;float&apos;和&apos;quantized&apos;，分别包含量化模块及其匹配浮点影子模块的输出张量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This dict can be then used to compare and compute the module level "
"quantization error."
msgstr "然后可以使用该字典比较和计算模块级别的量化误差。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If we want to do the comparison for more than one input data, we can do the "
"following."
msgstr "如果我们想对多个输入数据进行比较，可以如下操作。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The default logger used in above APIs is ``ShadowLogger``, which is used to "
"log the outputs of the quantized module and its matching float shadow "
"module. We can inherit from base ``Logger`` class and create our own logger "
"to perform different functionalities. For example we can make a new "
"``MyShadowLogger`` class as below."
msgstr ""
"上述API中使用的默认记录器是``ShadowLogger``，用于记录量化模块及其匹配浮点影子模块的输出。我们可以继承基类``Logger``并创建自己的记录器以执行不同的功能。例如，我们可以创建一个新的``MyShadowLogger``类，如下所示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Numeric Suite for Dynamic Quantization"
msgstr "用于动态量化的数值工具套件"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Numeric Suite APIs are designed in such as way that they work for both "
"dynamic quantized model and static quantized model. We will use a model with"
" both LSTM and Linear modules to demonstrate the usage of Numeric Suite on "
"dynamic quantized model. This model is the same one used in the tutorial of "
"dynamic quantization on LSTM word language model [1]."
msgstr ""
"Numeric Suite API设计成既适用于动态量化模型也适用于静态量化模型。我们将使用一个包含LSTM和线性模块的模型来展示Numeric "
"Suite在动态量化模型上的使用。这个模型与动态量化LSTM语言模型教程中使用的模型一致 [1]。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"First we define the model as below. Notice that within this model only "
"``nn.LSTM`` and ``nn.Linear`` modules will be quantized dynamically and "
"``nn.Embedding`` will remain as floating point module after quantization."
msgstr ""
"首先我们定义如下模型。注意在此模型中，只有``nn.LSTM``和``nn.Linear``模块会被动态量化，而``nn.Embedding``在量化后仍保持浮点模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Then we create the ``float_model`` and quantize it into qmodel."
msgstr "随后我们创建``float_model``并将其量化为qmodel。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We first call ``compare_weights()`` from PyTorch Numeric Suite to get a "
"dictionary ``wt_compare_dict`` with key corresponding to module names and "
"each entry is a dictionary with two keys 'float' and 'quantized', containing"
" the float and quantized weights."
msgstr ""
"我们首先调用PyTorch Numeric "
"Suite中的``compare_weights()``以获得一个字典``wt_compare_dict``，其中的键对应于模块名称，每个条目都是拥有两个键&apos;float&apos;和&apos;quantized&apos;的字典，分别包含浮点和量化权重。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Once we get ``wt_compare_dict``, it can be used to compare and compute the "
"quantization error of the weights of float and quantized models as "
"following."
msgstr "获得``wt_compare_dict``后，可以用于比较浮点模型和量化模型权重的量化误差，如下所示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The Inf value in ``encoder.weight`` entry above is because encoder module is"
" not quantized and the weights are the same in both floating point and "
"quantized models."
msgstr "上述``encoder.weight``条目中的Inf值是因为编码器模块未被量化，并且权重在浮点和量化模型中是相同的。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Then we call ``compare_model_outputs()`` from PyTorch Numeric Suite to get "
"the activations in float model and quantized model at corresponding "
"locations for the given input data. This API returns a dict with module "
"names being keys. Each entry is itself a dict with two keys 'float' and "
"'quantized' containing the activations. Notice that this sequence model has "
"two inputs, and we can pass both inputs into ``compare_model_outputs()`` and"
" ``compare_model_stub()``."
msgstr ""
"然后我们调用PyTorch Numeric "
"Suite中的``compare_model_outputs()``以获取给定输入数据下在对应位置的浮点模型和量化模型的激活输出。此API返回一个字典，模块名称为键，每个条目本身是一个字典，拥有两个键&apos;float&apos;和&apos;quantized&apos;，分别包含激活输出。注意此序列模型有两个输入，我们可以将两输入都传递给``compare_model_outputs()``和``compare_model_stub()``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This dict can be used to compare and compute the quantization error of the "
"activations of float and quantized models as following. The LSTM module in "
"this model has two outputs, in this example we compute the error of the "
"first output."
msgstr "该字典可用于比较浮点模型和量化模型激活输出的量化误差，如下所示。此模型中的LSTM模块有两个输出，在这个例子中我们计算第一个输出的误差。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Next we call ``compare_model_stub()`` from PyTorch Numeric Suite to compare "
"LSTM and Linear module with its float point equivalent. This API returns a "
"dict with key corresponding to module names and each entry being a "
"dictionary with two keys 'float' and 'quantized', containing the output "
"tensors of quantized and its matching float shadow module."
msgstr ""
"接下来，我们调用 PyTorch 数字套件中的 ``compare_model_stub()`` 来比较 LSTM 和 Linear "
"模块与其浮点等效模块。该 API 返回一个字典，其键对应于模块名称，每个条目都是一个包含两个键的字典：'float' 和 "
"'quantized'，其中包含量化模块及其对应浮点影子模块的输出张量。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We reset the model first."
msgstr "我们先重置模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"SQNR of 40 dB is high and this is a situation where we have very good "
"numerical alignment between the floating point and quantized model."
msgstr "40 dB 的 SQNR 是很高的，这表明浮点模型与量化模型之间有非常好的数值对齐。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we demonstrated how to use PyTorch Numeric Suite to "
"measure and compare the statistics between quantized model and float model "
"in eager mode with unified APIs for both static quantization and dynamic "
"quantization."
msgstr ""
"在本教程中，我们演示了如何使用 PyTorch 数字套件在动态模式下，通过统一的 API "
"测量和比较量化模型与浮点模型之间的统计数据，支持静态量化和动态量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "References"
msgstr "参考文献"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"[1] `DYNAMIC QUANTIZATION ON AN LSTM WORD LANGUAGE MODEL "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`_."
msgstr ""
"[1] `LSTM词语言模型的动态量化 "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: numeric_suite_tutorial.py "
"<numeric_suite_tutorial.py>`"
msgstr ""
":download:`下载Python源码：numeric_suite_tutorial.py <numeric_suite_tutorial.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: numeric_suite_tutorial.ipynb "
"<numeric_suite_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：numeric_suite_tutorial.ipynb "
"<numeric_suite_tutorial.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch 2 Export Quantization for OpenVINO torch.compile Backend"
msgstr "PyTorch 2 导出量化到 OpenVINO torch.compile 后端"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Authors**: `Daniil Lyakhov <https://github.com/daniil-lyakhov>`_,  `Aamir "
"Nazir <https://github.com/anzr299>`_,  `Alexander Suslov "
"<https://github.com/alexsu52>`_, `Yamini Nimmagadda "
"<https://github.com/ynimmaga>`_, `Alexander Kozlov "
"<https://github.com/AlexKoff88>`_"
msgstr ""
"**作者**: `Daniil Lyakhov <https://github.com/daniil-lyakhov>`_,  `Aamir Nazir"
" <https://github.com/anzr299>`_,  `Alexander Suslov "
"<https://github.com/alexsu52>`_, `Yamini Nimmagadda "
"<https://github.com/ynimmaga>`_, `Alexander Kozlov "
"<https://github.com/AlexKoff88>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`PyTorch 2 Export Post Training Quantization "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_"
msgstr ""
"`PyTorch 2 导出后训练量化 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`How to Write a Quantizer for PyTorch 2 Export Quantization "
"<https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`_"
msgstr ""
"`如何为 PyTorch 2 导出的量化编写量化器 "
"<https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This is an experimental feature, the quantization API is subject to change."
msgstr "这是一个实验性功能，量化 API 可能会更改。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial demonstrates how to use ``OpenVINOQuantizer`` from `Neural "
"Network Compression Framework (NNCF) "
"<https://github.com/openvinotoolkit/nncf/tree/develop>`_ in PyTorch 2 Export"
" Quantization flow to generate a quantized model customized for the "
"`OpenVINO torch.compile backend <https://docs.openvino.ai/2024/openvino-"
"workflow/torch-compile.html>`_ and explains how to lower the quantized model"
" into the `OpenVINO <https://docs.openvino.ai/2024/index.html>`_ "
"representation. ``OpenVINOQuantizer`` unlocks the full potential of low-"
"precision OpenVINO kernels due to the placement of quantizers designed "
"specifically for the OpenVINO."
msgstr ""
"本教程演示如何在 PyTorch 2 导出量化流程中使用 `Neural Network Compression Framework (NNCF) "
"<https://github.com/openvinotoolkit/nncf/tree/develop>`_ 的 "
"``OpenVINOQuantizer`` 生成专为 `OpenVINO torch.compile 后端 "
"<https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html>`_ "
"定制的量化模型，并解释如何将量化模型转为 `OpenVINO <https://docs.openvino.ai/2024/index.html>`_ "
"表示形式。由于 ``OpenVINOQuantizer`` 特定于 OpenVINO 的量化器布局，它释放了低精度 OpenVINO 内核的全部潜力。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The PyTorch 2 export quantization flow uses ``torch.export`` to capture the "
"model into a graph and performs quantization transformations on top of the "
"ATen graph. This approach is expected to have significantly higher model "
"coverage, improved flexibility, and a simplified UX. OpenVINO backend "
"compiles the FX Graph generated by TorchDynamo into an optimized OpenVINO "
"model."
msgstr ""
"PyTorch 2 导出量化流程使用 ``torch.export`` 将模型捕获为图，并基于 ATen "
"图执行量化转换。这种方法预计具有显著更高的模型覆盖率、更好的灵活性和简化的用户体验。OpenVINO 后端将 TorchDynamo 生成的 FX "
"图编译为优化的 OpenVINO 模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The quantization flow mainly includes four steps:"
msgstr "量化流程主要包括四个步骤："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 1: Capture the FX Graph from the eager Model based on the `torch export"
" mechanism <https://pytorch.org/docs/main/export.html>`_."
msgstr ""
"步骤1: 基于 `torch 导出机制 <https://pytorch.org/docs/main/export.html>`_ 从动态模型捕获 FX"
" 图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 2: Apply the PyTorch 2 Export Quantization flow with OpenVINOQuantizer "
"based on the captured FX Graph."
msgstr "步骤2: 基于捕获的 FX 图使用 OpenVINOQuantizer 应用 PyTorch 2 导出量化流程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Lower the quantized model into OpenVINO representation with the "
"`torch.compile <https://docs.openvino.ai/2024/openvino-workflow/torch-"
"compile.html>`_ API."
msgstr ""
"步骤3: 通过 `torch.compile <https://docs.openvino.ai/2024/openvino-"
"workflow/torch-compile.html>`_ API 将量化模型降低为 OpenVINO 表示形式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Optional step 4: : Improve quantized model metrics via `quantize_pt2e "
"<https://openvinotoolkit.github.io/nncf/autoapi/nncf/experimental/torch/fx/index.html#nncf.experimental.torch.fx.quantize_pt2e>`_"
" method."
msgstr ""
"可选步骤4: 通过 `quantize_pt2e "
"<https://openvinotoolkit.github.io/nncf/autoapi/nncf/experimental/torch/fx/index.html#nncf.experimental.torch.fx.quantize_pt2e>`_"
" 方法提升量化模型指标。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The high-level architecture of this flow could look like this:"
msgstr "该流程的高级架构可以如下图所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Post Training Quantization"
msgstr "后训练量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, we will walk you through a step-by-step tutorial for how to use it with"
" `torchvision resnet18 model "
"<https://download.pytorch.org/models/resnet18-f37072fd.pth>`_ for post "
"training quantization."
msgstr ""
"现在，我们将逐步向您介绍如何在 `torchvision resnet18 模型 "
"<https://download.pytorch.org/models/resnet18-f37072fd.pth>`_ 上执行后训练量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Prerequisite: OpenVINO and NNCF installation"
msgstr "前提条件：安装 OpenVINO 和 NNCF"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"OpenVINO and NNCF could be easily installed via `pip distribution "
"<https://docs.openvino.ai/2024/get-started/install-openvino.html>`_:"
msgstr ""
"OpenVINO 和 NNCF 可通过 `pip 分发方式 <https://docs.openvino.ai/2024/get-"
"started/install-openvino.html>`_ 轻松安装："

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Capture FX Graph"
msgstr "1. 捕获 FX 图"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We will start by performing the necessary imports, capturing the FX Graph "
"from the eager module."
msgstr "我们将从必要的导入开始，从动态模块中捕获 FX 图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Apply Quantization"
msgstr "2. 应用量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After we capture the FX Module to be quantized, we will import the "
"OpenVINOQuantizer."
msgstr "捕获要量化的 FX 模块后，我们将导入 OpenVINOQuantizer。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``OpenVINOQuantizer`` has several optional parameters that allow tuning the "
"quantization process to get a more accurate model. Below is the list of "
"essential parameters and their description:"
msgstr "``OpenVINOQuantizer`` 有若干可选参数，可用于调整量化过程以获得更准确的模型。下面是基本参数及其描述的列表："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``preset`` - defines quantization scheme for the model. Two types of presets"
" are available:"
msgstr "``preset`` - 定义模型的量化方案。可用两种类型的预设："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``PERFORMANCE`` (default) - defines symmetric quantization of weights and "
"activations"
msgstr "``PERFORMANCE``（默认）- 定义权重和激活的对称量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``MIXED`` - weights are quantized with symmetric quantization and the "
"activations are quantized with asymmetric quantization. This preset is "
"recommended for models with non-ReLU and asymmetric activation functions, "
"e.g. ELU, PReLU, GELU, etc."
msgstr ""
"``MIXED`` - 权重采用对称量化，激活采用非对称量化。此预设推荐用于具有非 ReLU 和非对称激活函数的模型，例如 ELU，PReLU，GELU"
" 等。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``model_type`` - used to specify quantization scheme required for specific "
"type of the model. Transformer is the only supported special quantization "
"scheme to preserve accuracy after quantization of Transformer models (BERT, "
"Llama, etc.). None is default, i.e. no specific scheme is defined."
msgstr ""
"``model_type`` - 用于指定特定类型模型所需的量化方案。Transformer 是唯一支持的特殊量化方案，用于在 Transformer "
"模型（如 BERT、Llama 等）量化后保持准确性。默认是 None，即未定义特定方案。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``ignored_scope`` - this parameter can be used to exclude some layers from "
"the quantization process to preserve the model accuracy.  For example, when "
"you want to exclude the last layer of the model from quantization.  Below "
"are some examples of how to use this parameter:"
msgstr ""
"``ignored_scope`` - "
"此参数可用于排除某些层的量化过程以保持模型准确性。例如，当您想排除模型的最后一层的量化时。以下是一些如何使用此参数的示例："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``target_device`` - defines the target device, the specificity of which will"
" be taken into account during optimization. The following values are "
"supported: ``ANY`` (default), ``CPU``, ``CPU_SPR``, ``GPU``, and ``NPU``."
msgstr ""
"``target_device`` - "
"定义目标设备，其特性将在优化过程中考虑。支持的值包括：``ANY``（默认）、``CPU``、``CPU_SPR``、``GPU`` 和 "
"``NPU``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For further details on `OpenVINOQuantizer` please see the `documentation "
"<https://openvinotoolkit.github.io/nncf/autoapi/nncf/experimental/torch/fx/index.html#nncf.experimental.torch.fx.OpenVINOQuantizer>`_."
msgstr ""
"有关 `OpenVINOQuantizer` 的详细信息，请参阅 `文档 "
"<https://openvinotoolkit.github.io/nncf/autoapi/nncf/experimental/torch/fx/index.html#nncf.experimental.torch.fx.OpenVINOQuantizer>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After we import the backend-specific Quantizer, we will prepare the model "
"for post-training quantization. ``prepare_pt2e`` folds BatchNorm operators "
"into preceding Conv2d operators, and inserts observers in appropriate places"
" in the model."
msgstr ""
"导入特定后端的 Quantizer 之后，我们将为后训练量化准备模型。``prepare_pt2e`` 将 BatchNorm 操作折叠到前面的 "
"Conv2d 操作中，并在模型中的适当位置插入观察器。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, we will calibrate the ``prepared_model`` after the observers are "
"inserted in the model."
msgstr "现在，我们将在模型插入观察器后校准 ``prepared_model``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Finally, we will convert the calibrated Model to a quantized Model. "
"``convert_pt2e`` takes a calibrated model and produces a quantized model."
msgstr "最后，我们将校准后的模型转换为量化模型。``convert_pt2e`` 接收一个已校准的模型并生成量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After these steps, we finished running the quantization flow, and we will "
"get the quantized model."
msgstr "完成这些步骤后，我们完成了量化流程，可以获得量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Lower into OpenVINO representation"
msgstr "3. 转为 OpenVINO 表示形式"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After that the FX Graph can utilize OpenVINO optimizations using "
"`torch.compile(…, backend=”openvino”) "
"<https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html>`_ "
"functionality."
msgstr ""
"之后 FX 图可以使用 `torch.compile(…, backend=”openvino”) "
"<https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html>`_ 功能利用 "
"OpenVINO 优化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The optimized model is using low-level kernels designed specifically for "
"Intel CPU. This should significantly speed up inference time in comparison "
"with the eager model."
msgstr "优化后的模型使用专为英特尔 CPU 设计的低级内核。相比动态模型，这应该显著加快推理时间。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "4. Optional: Improve quantized model metrics"
msgstr "4. 可选：提升量化模型指标"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"NNCF implements advanced quantization algorithms like `SmoothQuant "
"<https://arxiv.org/abs/2211.10438>`_ and `BiasCorrection "
"<https://arxiv.org/abs/1906.04721>`_, which help to improve the quantized "
"model metrics while minimizing the output discrepancies between the original"
" and compressed models. These advanced NNCF algorithms can be accessed via "
"the NNCF `quantize_pt2e` API:"
msgstr ""
"NNCF 实现了像 `SmoothQuant <https://arxiv.org/abs/2211.10438>`_ 和 "
"`BiasCorrection <https://arxiv.org/abs/1906.04721>`_ "
"这样的高级量化算法，这些算法在尽量减少原始模型与压缩模型之间输出差异的同时帮助提升量化模型的指标。这些高级 NNCF 算法可以通过 NNCF 的 "
"`quantize_pt2e` API 访问："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For further details, please see the `documentation "
"<https://openvinotoolkit.github.io/nncf/autoapi/nncf/experimental/torch/fx/index.html#nncf.experimental.torch.fx.quantize_pt2e>`_"
" and a complete `example on Resnet18 quantization "
"<https://github.com/openvinotoolkit/nncf/blob/develop/examples/post_training_quantization/torch_fx/resnet18/README.md>`_."
msgstr ""
"有关详细信息，请参阅 `文档 "
"<https://openvinotoolkit.github.io/nncf/autoapi/nncf/experimental/torch/fx/index.html#nncf.experimental.torch.fx.quantize_pt2e>`_"
" 和 Resnet18 量化完整示例 `示例 "
"<https://github.com/openvinotoolkit/nncf/blob/develop/examples/post_training_quantization/torch_fx/resnet18/README.md>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces how to use torch.compile with the OpenVINO backend "
"and the OpenVINO quantizer. For more details on NNCF and the NNCF "
"Quantization Flow for PyTorch models, refer to the `NNCF Quantization Guide "
"<https://docs.openvino.ai/2025/openvino-workflow/model-optimization-"
"guide/quantizing-models-post-training/basic-quantization-flow.html.>`_. For "
"additional information, check out the `OpenVINO Deployment via torch.compile"
" Documentation <https://docs.openvino.ai/2024/openvino-workflow/torch-"
"compile.html>`_."
msgstr ""
"本教程介绍如何使用 torch.compile 和 OpenVINO 后端及 OpenVINO 量化器。有关 NNCF 和 NNCF 针对 "
"PyTorch 模型的量化流程的更多细节，请参考 `NNCF 量化指南 <https://docs.openvino.ai/2025/openvino-"
"workflow/model-optimization-guide/quantizing-models-post-training/basic-"
"quantization-flow.html>`_ 。有关更多信息，请查阅 `通过 torch.compile 进行 OpenVINO 部署文档 "
"<https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch Prototype Recipes"
msgstr "PyTorch 原型示例"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Prototype features are not available as part of binary distributions like "
"PyPI or Conda (except maybe behind run-time flags). To test these features "
"we would, depending on the feature, recommend building from master or using "
"the nightly wheels that are made available on `pytorch.org "
"<https://pytorch.org>`_."
msgstr ""
"原型功能在二进制分发（例如 PyPI 或 Conda）中不可用（可能除非启用运行时标志）。为了测试这些功能，根据不同的功能，我们建议从主版本构建或使用在"
" `pytorch.org <https://pytorch.org>`_ 上提供的 nightly wheels。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"*Level of commitment*: We are committing to gathering high bandwidth "
"feedback only on these features. Based on this feedback and potential "
"further engagement between community members, we as a community will decide "
"if we want to upgrade the level of commitment or to fail fast."
msgstr "*承诺等级*: 我们承诺只对这些功能收集高带宽反馈。基于此反馈以及社区成员的进一步参与，我们作为一个社区将决定是否升级承诺等级或快速失败。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) PyTorch 2 Export Post Training Quantization"
msgstr "（原型）PyTorch 2 导出后训练量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the steps to do post training static quantization "
"in graph mode based on `torch._export.export "
"<https://pytorch.org/docs/main/export.html>`_. Compared to `FX Graph Mode "
"Quantization "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`_, "
"this flow is expected to have significantly higher model coverage (`88% on "
"14K models "
"<https://github.com/pytorch/pytorch/issues/93667#issuecomment-1601171596>`_),"
" better programmability, and a simplified UX."
msgstr ""
"本教程介绍了如何基于 `torch._export.export "
"<https://pytorch.org/docs/main/export.html>`_ 在图模式中执行后训练静态量化的步骤。相比 `FX 图模式量化"
" "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`_，此流程预计具有显著更高的模型覆盖率（`88%"
" 在 14K 模型中 "
"<https://github.com/pytorch/pytorch/issues/93667#issuecomment-1601171596>`_），更便于编程，以及简化的用户体验。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Exportable by `torch.export.export` is a prerequisite to use the flow, you "
"can find what are the constructs that's supported in `Export DB "
"<https://pytorch.org/docs/main/generated/exportdb/index.html>`_."
msgstr ""
"使用此流程的前提条件是支持 `torch.export.export` 导出功能，您可以在 `导出数据库 "
"<https://pytorch.org/docs/main/generated/exportdb/index.html>`_ 中找到支持的结构。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The high level architecture of quantization 2 with quantizer could look like"
" this:"
msgstr "具有量化器的量化流程 2 的高层架构如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid "The PyTorch 2 export quantization API looks like this:"
msgstr "PyTorch 2 导出量化 API 看起来如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Motivation of PyTorch 2 Export Quantization"
msgstr "PyTorch 2 导出量化的动机"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In PyTorch versions prior to 2, we have FX Graph Mode Quantization that uses"
" `QConfigMapping "
"<https://pytorch.org/docs/main/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html>`_"
" and `BackendConfig "
"<https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html>`_"
" for customizations. ``QConfigMapping`` allows modeling users to specify how"
" they want their model to be quantized, ``BackendConfig`` allows backend "
"developers to specify the supported ways of quantization in their backend. "
"While that API covers most use cases relatively well, it is not fully "
"extensible. There are two main limitations for the current API:"
msgstr ""
"在 PyTorch 2 之前的版本中，我们有 FX 图模式量化，使用 `QConfigMapping "
"<https://pytorch.org/docs/main/generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html>`_"
" 和 `BackendConfig "
"<https://pytorch.org/docs/stable/generated/torch.ao.quantization.backend_config.BackendConfig.html>`_"
" 来进行定制。``QConfigMapping`` 允许模型用户指定如何对模型进行量化，``BackendConfig`` "
"允许后端开发者指定后端支持的量化方式。虽然该 API 在大多数情况下覆盖得相对较好，但它并不是完全可扩展的。当前 API 存在两个主要限制："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Limitation around expressing quantization intentions for complicated "
"operator patterns (how an operator pattern should be observed/quantized) "
"using existing objects: ``QConfig`` and ``QConfigMapping``."
msgstr ""
"表达复杂操作模式的量化意图（如何观察/量化操作模式）的局限性：使用现有对象``QConfig`` 和 ``QConfigMapping`` 。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Limited support on how user can express their intention of how they want "
"their model to be quantized. For example, if users want to quantize the "
"every other linear in the model, or the quantization behavior has some "
"dependency on the actual shape of the Tensor (for example, only "
"observe/quantize inputs and outputs when the linear has a 3D input), backend"
" developer or modeling users need to change the core quantization API/flow."
msgstr ""
"用户在表达他们希望如何对模型进行量化的意图时支持有限。例如，如果用户希望每隔一个线性层进行量化，或者量化行为与张量的实际形状有某种依赖关系（例如，当线性层具有3D输入时仅观察/量化输入和输出），后台开发者或建模用户需要更改核心量化API/流程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "A few improvements could make the existing flow better:"
msgstr "一些改进可以使现有流程更好："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We use ``QConfigMapping`` and ``BackendConfig`` as separate objects, "
"``QConfigMapping`` describes user’s intention of how they want their model "
"to be quantized, ``BackendConfig`` describes what kind of quantization a "
"backend supports. ``BackendConfig`` is backend-specific, but "
"``QConfigMapping`` is not, and the user can provide a ``QConfigMapping`` "
"that is incompatible with a specific ``BackendConfig``, this is not a great "
"UX. Ideally, we can structure this better by making both configuration "
"(``QConfigMapping``) and quantization capability (``BackendConfig``) "
"backend-specific, so there will be less confusion about incompatibilities."
msgstr ""
"我们将“QConfigMapping”和“BackendConfig”作为独立的对象，“QConfigMapping”描述了用户希望如何对模型进行量化的意图，“BackendConfig”描述了后端支持哪种量化。“BackendConfig”是后端特定的，但“QConfigMapping”不是，用户可以提供与特定“BackendConfig”不兼容的“QConfigMapping”，这不是一个很好的用户体验。理想情况下，我们可以通过使配置(“QConfigMapping”)和量化能力(“BackendConfig”)都更具后端特性来更好地构造它们，从而减少关于不兼容的困惑。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In ``QConfig`` we are exposing observer/ ``fake_quant`` observer classes as "
"an object for the user to configure quantization, this increases the things "
"that the user may need to care about. For example, not only the ``dtype`` "
"but also how the observation should happen, these could potentially be "
"hidden from the user so that the user flow is simpler."
msgstr ""
"在“QConfig”中，我们将观察者/“假量化”观察者类作为对象暴露给用户以配置量化，这增加了用户需要关心的事项。例如，不仅是“dtype”，还有观察应该如何进行，这些可以潜在地隐藏起来，使用户流程更简单。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Here is a summary of the benefits of the new API:"
msgstr "以下是新API的优点摘要："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Programmability** (addressing 1. and 2.): When a user’s quantization needs"
" are not covered by available quantizers, users can build their own "
"quantizer and compose it with other quantizers as mentioned above."
msgstr "**可编程性**（解决问题1和问题2）：当用户的量化需求未被现有量化器覆盖时，用户可以构建自己的量化器并像上面提到的那样与其他量化器组合。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Simplified UX** (addressing 3.): Provides a single instance with which "
"both backend and users interact. Thus you no longer have the user facing "
"quantization config mapping to map users intent and a separate quantization "
"config that backends interact with to configure what backend support. We "
"will still have a method for users to query what is supported in a "
"quantizer. With a single instance, composing different quantization "
"capabilities also becomes more natural than previously."
msgstr ""
"**简化用户体验**（解决问题3）：提供一个单一实例，与后端和用户交互。因此，用户不再需要面对量化配置映射以映射用户意图，以及与后端交互以配置后端支持的单独量化配置。我们仍然会有一种方法让用户查询量化器支持的内容。通过单一实例，组合不同的量化能力也比以前更自然。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For example XNNPACK does not support ``embedding_byte`` and we have natively"
" support for this in ExecuTorch. Thus, if we had ``ExecuTorchQuantizer`` "
"that only quantized ``embedding_byte``, then it can be composed with "
"``XNNPACKQuantizer``. (Previously, this used to be concatenating the two "
"``BackendConfig`` together and since options in ``QConfigMapping`` are not "
"backend specific, user also need to figure out how to specify the "
"configurations by themselves that matches the quantization capabilities of "
"the combined backend. With a single quantizer instance, we can compose two "
"quantizers and query the composed quantizer for capabilities, which makes it"
" less error prone and cleaner, for example, "
"``composed_quantizer.quantization_capabilities())``."
msgstr ""
"例如，XNNPACK不支持“embedding_byte”，而我们在ExecuTorch中本地支持它。因此，如果我们有一个只量化“embedding_byte”的“ExecuTorchQuantizer”，则可以与“XNNPACKQuantizer”组合。（之前，这通常是将两个“BackendConfig”连接在一起，由于“QConfigMapping”中的选项不是后端特定的，用户还需要自行弄清楚如何指定与组合后端的量化能力匹配的配置。通过单一量化器实例，我们可以组合两个量化器并查询组合量化器的能力，这使得它更少易出错且更清晰，例如，“composed_quantizer.quantization_capabilities()”。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Separation of concerns** (addressing 4.): As we design the quantizer API, "
"we also decouple specification of quantization, as expressed in terms of "
"``dtype``, min/max (# of bits), symmetric, and so on, from the observer "
"concept. Currently, the observer captures both quantization specification "
"and how to observe (Histogram vs MinMax observer). Modeling users are freed "
"from interacting with observer and fake quant objects with this change."
msgstr ""
"**关注点分离**（解决问题4）：在我们设计量化器API时，我们还将量化的规范与观察者概念解耦。当前，观察者既捕获了量化规范又捕获了如何观察（直方图vs最小最大观察者）。通过更改，建模用户不再与观察者和假量化对象交互。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Define Helper Functions and Prepare Dataset"
msgstr "定义辅助函数并准备数据集"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To run the code in this tutorial using the entire ImageNet dataset, first "
"download Imagenet by following the instructions at here `ImageNet Data "
"<http://www.image-net.org/download>`_. Unzip the downloaded file into the "
"``data_path`` folder."
msgstr ""
"要使用整个ImageNet数据集运行本教程中的代码，请首先按照此处的说明下载ImageNet`ImageNet Data "
"<http://www.image-net.org/download>`_。将下载的文件解压到``data_path``文件夹中。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Set the model to eval mode"
msgstr "将模型设为评估模式"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For post training quantization, we'll need to set the model to the eval "
"mode."
msgstr "对于后训练量化，我们需要将模型设置为评估模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Export the model with torch.export"
msgstr "使用torch.export导出模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Here is how you can use ``torch.export`` to export the model:"
msgstr "以下是如何使用``torch.export``导出模型："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Import the Backend Specific Quantizer and Configure how to Quantize the "
"Model"
msgstr "导入特定后端的量化器并配置如何对模型进行量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The following code snippets describes how to quantize the model:"
msgstr "以下代码片段描述了如何对模型进行量化："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``Quantizer`` is backend specific, and each ``Quantizer`` will provide their"
" own way to allow users to configure their model. Just as an example, here "
"is the different configuration APIs supported by ``XNNPackQuantizer``:"
msgstr ""
"``Quantizer``是后端特定的，每个``Quantizer``将提供自己的方式来允许用户配置其模型。作为示例，以下是``XNNPackQuantizer``支持的不同配置API："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Check out our `tutorial "
"<https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`_ that "
"describes how to write a new ``Quantizer``."
msgstr ""
"查看我们的`教程 "
"<https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`_，描述如何编写新的``Quantizer``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Prepare the Model for Post Training Quantization"
msgstr "为后训练量化准备模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``prepare_pt2e`` folds ``BatchNorm`` operators into preceding ``Conv2d`` "
"operators, and inserts observers in appropriate places in the model."
msgstr "``prepare_pt2e``将``BatchNorm``操作折叠到前面的``Conv2d``操作中，并在模型中的适当位置插入观察者。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Calibration"
msgstr "校准"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The calibration function is run after the observers are inserted in the "
"model. The purpose for calibration is to run through some sample examples "
"that is representative of the workload (for example a sample of the training"
" data set) so that the observers in themodel are able to observe the "
"statistics of the Tensors and we can later use this information to calculate"
" quantization parameters."
msgstr ""
"校准功能在观察者插入到模型中后运行。校准的目的是通过一些代表工作负载的示例（例如训练数据集的一个样本）运行，以便模型中的观察者能够观察张量的统计信息，我们可以稍后使用这些信息计算量化参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Convert the Calibrated Model to a Quantized Model"
msgstr "将校准后的模型转换为量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``convert_pt2e`` takes a calibrated model and produces a quantized model."
msgstr "``convert_pt2e``接受一个校准后的模型并生成一个量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"At this step, we currently have two representations that you can choose "
"from, but exact representation we offer in the long term might change based "
"on feedback from PyTorch users."
msgstr "在这一步，我们目前有两种表示方式可供选择，但长期提供的确切表示可能根据PyTorch用户的反馈而有所改变。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Q/DQ Representation (default)"
msgstr "Q/DQ表示（默认）"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Previous documentation for `representations "
"<https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-"
"Quantization-to-Custom-Backends.md>`_ all quantized operators are "
"represented as ``dequantize -> fp32_op -> qauntize``."
msgstr ""
"之前的文档`表示 <https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-"
"PyTorch-Quantization-to-Custom-Backends.md>`_中，所有量化操作都表示为``dequantize -> "
"fp32_op -> quantize``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Reference Quantized Model Representation"
msgstr "参考量化模型表示"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We will have a special representation for selected ops, for example, "
"quantized linear. Other ops are represented as ``dq -> float32_op -> q`` and"
" ``q/dq`` are decomposed into more primitive operators. You can get this "
"representation by using ``convert_pt2e(..., "
"use_reference_representation=True)``."
msgstr ""
"我们将对选定的操作（例如量化线性）提供特殊表示。其他操作表示为``dq -> float32_op -> "
"q``且``q/dq``分解为更原始的操作。您可以通过使用``convert_pt2e(..., "
"use_reference_representation=True)``获得这种表示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"See `here "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pt2e/representation/rewrite.py>`_"
" for the most up-to-date reference representations."
msgstr ""
"查看`这里 "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pt2e/representation/rewrite.py>`_以获取最新的参考表示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Checking Model Size and Accuracy Evaluation"
msgstr "检查模型大小和准确性评估"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Now we can compare the size and model accuracy with baseline model."
msgstr "现在我们可以将模型的大小和准确性与基准模型进行比较。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We can't do performance evaluation now since the model is not lowered to "
"target device, it's just a representation of quantized computation in ATen "
"operators."
msgstr "由于模型尚未降低到目标设备，我们目前无法进行性能评估，它仅是ATen操作中量化计算的表示。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The weights are still in fp32 right now, we may do constant propagation for "
"quantize op to get integer weights in the future."
msgstr "权重现在仍是fp32，我们以后可能会对量化操作进行常量传播以获取整数权重。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If you want to get better accuracy or performance,  try configuring "
"``quantizer`` in different ways, and each ``quantizer`` will have its own "
"way of configuration, so please consult the documentation for the quantizer "
"you are using to learn more about how you can have more control over how to "
"quantize a model."
msgstr ""
"如果您希望获得更好的准确性或性能，请尝试以不同方式配置``quantizer``，每个``quantizer``都会有自己的配置方式，因此请查阅您正在使用的量化器的文档以了解更多关于如何控制模型量化的信息。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Save and Load Quantized Model"
msgstr "保存和加载量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We'll show how to save and load the quantized model."
msgstr "我们将展示如何保存和加载量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Output:"
msgstr "输出："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Debugging the Quantized Model"
msgstr "调试量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"You can use `Numeric Suite <https://pytorch.org/docs/stable/quantization-"
"accuracy-debugging.html#numerical-debugging-tooling-prototype>`_ that can "
"help with debugging in eager mode and FX graph mode. The new version of "
"Numeric Suite working with PyTorch 2 Export models is still in development."
msgstr ""
"您可以使用`Numeric Suite <https://pytorch.org/docs/stable/quantization-accuracy-"
"debugging.html#numerical-debugging-tooling-"
"prototype>`_，它可以帮助以急切模式和FX图模式进行调试。与PyTorch 2 Export模型一起工作的新版本Numeric "
"Suite仍在开发中。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Lowering and Performance Evaluation"
msgstr "降低和性能评估"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The model produced at this point is not the final model that runs on the "
"device, it is a reference quantized model that captures the intended "
"quantized computation from the user, expressed as ATen operators and some "
"additional quantize/dequantize operators, to get a model that runs on real "
"devices, we'll need to lower the model. For example, for the models that run"
" on edge devices, we can lower with delegation and ExecuTorch runtime "
"operators."
msgstr ""
"目前生成的模型不是在设备上运行的最终模型，它是一个参考量化模型，捕获用户意图的量化计算，表示为ATen操作和一些额外的量化/去量化操作。为了获得在真实设备上运行的模型，我们需要降低模型。例如，对于在边缘设备上运行的模型，我们可以使用委托和ExecuTorch运行时操作进行降低。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we went through the overall quantization flow in PyTorch 2"
" Export Quantization using ``XNNPACKQuantizer`` and got a quantized model "
"that could be further lowered to a backend that supports inference with "
"XNNPACK backend. To use this for your own backend, please first follow the "
"`tutorial <https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`__ "
"and implement a ``Quantizer`` for your backend, and then quantize the model "
"with that ``Quantizer``."
msgstr ""
"在本教程中，我们使用``XNNPACKQuantizer``完成了PyTorch 2 "
"Export量化的整体量化流程并获得了一个可以进一步降低到支持XNNPACK后端推理的后端的量化模型。要将其用于您自己的后端，请首先查看`教程 "
"<https://pytorch.org/tutorials/prototype/pt2e_quantizer.html>`__并为您的后端实现一个``Quantizer``，然后使用该``Quantizer``进行模型量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Quantization in PyTorch 2.0 Export Tutorial"
msgstr "PyTorch 2.0导出量化教程"

#: ../../prototype/vulkan_workflow.rst:250
msgid "This tutorial has been moved."
msgstr "本教程已被移除。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) PyTorch 2 Export Quantization-Aware Training (QAT)"
msgstr "（原型）PyTorch 2导出量化感知训练（QAT）"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial shows how to perform quantization-aware training (QAT) in "
"graph mode based on `torch.export.export "
"<https://pytorch.org/docs/main/export.html>`_. For more details about "
"PyTorch 2 Export Quantization in general, refer to the `post training "
"quantization tutorial "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_."
msgstr ""
"本教程展示了如何基于`torch.export.export "
"<https://pytorch.org/docs/main/export.html>`_在图模式下进行量化感知训练（QAT）。有关PyTorch 2 "
"Export量化的一般详细信息，请参考`后训练量化教程 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The PyTorch 2 Export QAT flow looks like the following—it is similar to the "
"post training quantization (PTQ) flow for the most part:"
msgstr "PyTorch 2 Export QAT流如下——它在大多数方面与后训练量化（PTQ）流相似："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that calling ``model.eval()`` or ``model.train()`` after program "
"capture is not allowed, because these methods no longer correctly change the"
" behavior of certain ops like dropout and batch normalization. Instead, "
"please use ``torch.ao.quantization.move_exported_model_to_eval()`` and "
"``torch.ao.quantization.move_exported_model_to_train()`` (coming soon) "
"respectively."
msgstr ""
"请注意，在程序捕获后调用``model.eval()``或``model.train()``是不允许的，因为这些方法不再正确改变某些操作（如dropout和批量归一化）的行为。相反，请分别使用``torch.ao.quantization.move_exported_model_to_eval()``和``torch.ao.quantization.move_exported_model_to_train()``（即将推出）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Define Helper Functions and Prepare the Dataset"
msgstr "定义辅助函数并准备数据集"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To run the code in this tutorial using the entire ImageNet dataset, first "
"download ImageNet by following the instructions in `ImageNet Data "
"<http://www.image-net.org/download>`_. Unzip the downloaded file into the "
"``data_path`` folder."
msgstr ""
"要使用整个ImageNet数据集运行本教程中的代码，请首先按照此处的说明下载ImageNet`ImageNet Data "
"<http://www.image-net.org/download>`_。将下载的文件解压到``data_path``文件夹中。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Next, download the `torchvision resnet18 model "
"<https://download.pytorch.org/models/resnet18-f37072fd.pth>`_ and rename it "
"to ``data/resnet18_pretrained_float.pth``."
msgstr ""
"接下来，下载`torchvision resnet18模型 "
"<https://download.pytorch.org/models/resnet18-f37072fd.pth>`_并将其重命名为``data/resnet18_pretrained_float.pth``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We’ll start by doing the necessary imports, defining some helper functions "
"and prepare the data. These steps are very similar to the ones defined in "
"the `static eager mode post training quantization tutorial "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_:"
msgstr ""
"我们将从必要的导入、定义一些辅助函数以及准备数据开始。这些步骤与`静态急切模式后训练量化教程 "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_定义的步骤非常相似："

#: ../../prototype/vulkan_workflow.rst:250
msgid "The following code snippets describe how to quantize the model:"
msgstr "以下代码片段描述了如何对模型进行量化："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``Quantizer`` is backend specific, and each ``Quantizer`` will provide their"
" own way to allow users to configure their model."
msgstr "``Quantizer``是后端特定的，每个``Quantizer``将提供自己的方式来允许用户配置其模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Prepare the Model for Quantization-Aware Training"
msgstr "为量化感知训练准备模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``prepare_qat_pt2e`` inserts fake quantizes in appropriate places in the "
"model and performs the appropriate QAT \"fusions\", such as ``Conv2d`` + "
"``BatchNorm2d``, for better training accuracies. The fused operations are "
"represented as a subgraph of ATen ops in the prepared graph."
msgstr ""
"``prepare_qat_pt2e``在模型中的适当位置插入假量化操作并执行适当的QAT“融合”，例如``Conv2d`` + "
"``BatchNorm2d``，以获得更好的训练准确性。融合操作在准备好的图中表示为ATen操作子图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"If your model contains batch normalization, the actual ATen ops you get in "
"the graph depend on the model's device when you export the model. If the "
"model is on CPU, then you'll get "
"``torch.ops.aten._native_batch_norm_legit``. If the model is on CUDA, then "
"you'll get ``torch.ops.aten.cudnn_batch_norm``. However, this is not "
"fundamental and may be subject to change in the future."
msgstr ""
"如果您的模型包含批规范化，在导出模型时，您在图中获得的实际 ATen 操作取决于模型所在的设备。如果模型在 CPU 上，那么您将获得 "
"``torch.ops.aten._native_batch_norm_legit``。如果模型在 CUDA 上，那么您将获得 "
"``torch.ops.aten.cudnn_batch_norm``。然而，这并不是固定的，将来可能会有所变化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Between these two ops, it has been shown that "
"``torch.ops.aten.cudnn_batch_norm`` provides better numerics on models like "
"MobileNetV2. To get this op, either call ``model.cuda()`` before export, or "
"run the following after prepare to manually swap the ops:"
msgstr ""
"在这两个操作之间，已经证明 ``torch.ops.aten.cudnn_batch_norm`` 在像 MobileNetV2 "
"这样的模型上提供了更好的数值性能。要获取此操作，可以在导出之前调用 ``model.cuda()``，或者在准备之后运行以下代码以手动交换操作："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In the future, we plan to consolidate the batch normalization ops such that "
"the above will no longer be necessary."
msgstr "未来，我们计划整合批规范化操作，以使上述步骤不再必要。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Training Loop"
msgstr "训练循环"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The training loop is similar to the ones in previous versions of QAT. To "
"achieve better accuracies, you may optionally disable observers and updating"
" batch normalization statistics after a certain number of epochs, or "
"evaluate the QAT or the quantized model trained so far every ``N`` epochs."
msgstr ""
"训练循环与以前版本的 QAT 中的训练循环类似。为了获得更好的准确性，您可以选择在一定数量的训练周期后禁用观察者和更新批规范化统计数据，或者每隔 "
"``N`` 个周期评估迄今为止训练的 QAT 或量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Saving and Loading Model Checkpoints"
msgstr "保存和加载模型检查点"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Model checkpoints for the PyTorch 2 Export QAT flow are the same as in any "
"other training flow. They are useful for pausing training and resuming it "
"later, recovering from failed training runs, and performing inference on "
"different machines at a later time. You can save model checkpoints during or"
" after training as follows:"
msgstr ""
"PyTorch 2 导出 QAT "
"流的模型检查点与其他训练流相同。它们对于暂停训练并稍后恢复训练、从失败的训练运行中恢复以及稍后在不同机器上进行推理很有用。您可以在训练期间或训练后保存模型检查点，如下所示："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To load the checkpoints, you must export and prepare the model the exact "
"same way it was initially exported and prepared. For example:"
msgstr "要加载检查点，必须以与最初导出和准备时完全相同的方式导出和准备模型。例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Convert the Trained Model to a Quantized Model"
msgstr "将训练的模型转换为量化模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``convert_pt2e`` takes a calibrated model and produces a quantized model. "
"Note that, before inference, you must first call "
"``torch.ao.quantization.move_exported_model_to_eval()`` to ensure certain "
"ops like dropout behave correctly in the eval graph. Otherwise, we would "
"continue to incorrectly apply dropout in the forward pass during inference, "
"for example."
msgstr ""
"``convert_pt2e`` 接受一个经过校准的模型并生成一个量化模型。请注意，在推理之前，您必须首先调用 "
"``torch.ao.quantization.move_exported_model_to_eval()`` 以确保像 dropout "
"这样的操作在评估图中表现正确。否则，例如在推理期间，我们会在前向传播中继续错误地应用 dropout。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we demonstrated how to run Quantization-Aware Training "
"(QAT) flow in PyTorch 2 Export Quantization. After convert, the rest of the "
"flow is the same as Post-Training Quantization (PTQ); the user can "
"serialize/deserialize the model and further lower it to a backend that "
"supports inference with XNNPACK backend. For more detail, follow the `PTQ "
"tutorial <https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_."
msgstr ""
"在本教程中，我们演示了如何运行 PyTorch 2 导出量化感知训练 (QAT) 流程。在转换之后，其余流程与后训练量化 (PTQ) "
"相同；用户可以序列化/反序列化模型，并进一步将其降低到支持基于 XNNPACK 后端推理的后端。有关更多详细信息，请参阅 `PTQ 教程 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch 2 Export Quantization with X86 Backend through Inductor"
msgstr "通过 Inductor 使用 X86 后端的 PyTorch 2 导出量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Leslie Fang <https://github.com/leslie-fang-intel>`_, `Weiwen "
"Xia <https://github.com/Xia-Weiwen>`_, `Jiong Gong "
"<https://github.com/jgong5>`_, `Jerry Zhang "
"<https://github.com/jerryzh168>`_"
msgstr ""
"**作者**: `Leslie Fang <https://github.com/leslie-fang-intel>`_, `Weiwen Xia "
"<https://github.com/Xia-Weiwen>`_, `Jiong Gong "
"<https://github.com/jgong5>`_, `Jerry Zhang "
"<https://github.com/jerryzh168>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`PyTorch 2 Export Quantization-Aware Training "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html>`_"
msgstr ""
"`PyTorch 2 导出量化感知训练 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`TorchInductor and torch.compile concepts in PyTorch "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"
msgstr ""
"`PyTorch 中的 TorchInductor 和 torch.compile 概念 "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Inductor C++ Wrapper concepts "
"<https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html>`_"
msgstr ""
"`Inductor C++ Wrapper 概念 "
"<https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces the steps for utilizing the PyTorch 2 Export "
"Quantization flow to generate a quantized model customized for the x86 "
"inductor backend and explains how to lower the quantized model into the "
"inductor."
msgstr ""
"本教程介绍了利用 PyTorch 2 导出量化流程生成定制化的量化模型以适配 x86 Inductor 后端的步骤，并说明如何将量化模型降低到 "
"Inductor。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The pytorch 2 export quantization flow uses the torch.export to capture the "
"model into a graph and perform quantization transformations on top of the "
"ATen graph. This approach is expected to have significantly higher model "
"coverage, better programmability, and a simplified UX. TorchInductor is the "
"new compiler backend that compiles the FX Graphs generated by TorchDynamo "
"into optimized C++/Triton kernels."
msgstr ""
"PyTorch 2 导出量化流程使用 torch.export 捕获模型到图中，并在 ATen "
"图的基础上执行量化转换。这种方法预计具有显著更高的模型覆盖率、更好的可编程性，以及简化的用户体验。TorchInductor 是新的编译器后端，可将 "
"TorchDynamo 生成的 FX 图编译为优化的 C++/Triton 内核。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This flow of quantization 2 with Inductor supports both static and dynamic "
"quantization. Static quantization works best for CNN models, like ResNet-50."
" And dynamic quantization is more suitable for NLP models, like RNN and "
"BERT. For the difference between the two quantization types, please refer to"
" the `following page "
"<https://pytorch.org/docs/stable/quantization.html#quantization-mode-"
"support>`__."
msgstr ""
"Inductor 的这种量化流程支持静态和动态量化。静态量化对 CNN 模型（如 ResNet-50）效果最佳，动态量化更适合于 NLP 模型（如 "
"RNN 和 BERT）。有关两种量化类型之间的区别，请参阅 `相关页面 "
"<https://pytorch.org/docs/stable/quantization.html#quantization-mode-"
"support>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The quantization flow mainly includes three steps:"
msgstr "量化流程主要包括三个步骤："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 2: Apply the Quantization flow based on the captured FX Graph, "
"including defining the backend-specific quantizer, generating the prepared "
"model with observers, performing the prepared model's calibration or "
"quantization-aware training, and converting the prepared model into the "
"quantized model."
msgstr ""
"步骤 2：基于捕获的 FX "
"图应用量化流程，包括定义特定后端量化器、生成带观察者的准备模型、执行准备模型的校准或量化感知训练，以及将准备模型转换为量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Lower the quantized model into inductor with the API "
"``torch.compile``."
msgstr "步骤 3：使用 ``torch.compile`` API 将量化模型降低到 Inductor。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Combining Quantization in PyTorch 2 Export and TorchInductor, we have "
"flexibility and productivity with the new Quantization frontend and "
"outstanding out-of-box performance with the compiler backend. Especially on "
"Intel fourth generation (SPR) Xeon processors which can further boost the "
"models' performance by leveraging the `advanced-matrix-extensions "
"<https://www.intel.com/content/www/us/en/products/docs/accelerator-"
"engines/advanced-matrix-extensions/overview.html>`_ feature."
msgstr ""
"将 PyTorch 2 导出的量化与 TorchInductor "
"相结合，我们通过新的量化前端获得灵活性和生产力，通过编译器后端获得突出的开箱即用性能。特别是在 Intel 第四代 (SPR) Xeon "
"处理器上，通过利用 `高级矩阵扩展 "
"<https://www.intel.com/content/www/us/en/products/docs/accelerator-"
"engines/advanced-matrix-extensions/overview.html>`_ 功能可以进一步提升模型性能。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Next, we will have the FX Module to be quantized."
msgstr "接下来，我们将获得需要量化的 FX 模块。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After we capture the FX Module to be quantized, we will import the Backend "
"Quantizer for X86 CPU and configure how to quantize the model."
msgstr "在捕获需要量化的 FX 模块后，我们将导入用于 X86 CPU 的后端量化器，并配置如何对模型进行量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The default quantization configuration in ``X86InductorQuantizer`` uses "
"8-bits for both activations and weights."
msgstr "``X86InductorQuantizer`` 的默认量化配置对激活和权重均使用 8 位。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When Vector Neural Network Instruction is not available, the oneDNN backend "
"silently chooses kernels that assume `multiplications are 7-bit x 8-bit "
"<https://oneapi-"
"src.github.io/oneDNN/dev_guide_int8_computations.html#inputs-of-mixed-"
"type-u8-and-s8>`_. In other words, potential numeric saturation and accuracy"
" issue may happen when running on CPU without Vector Neural Network "
"Instruction."
msgstr ""
"当 Vector Neural Network 指令不可用时，oneDNN 后端会默默选择假设 `乘法是 7 位与 8 位的内核 "
"<https://oneapi-"
"src.github.io/oneDNN/dev_guide_int8_computations.html#inputs-of-mixed-"
"type-u8-and-s8>`_。换句话说，在没有 Vector Neural Network 指令的 CPU "
"上运行时，可能会发生潜在的数值饱和和准确性问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The quantization config is for static quantization by default. To apply "
"dynamic quantization, add an argument ``is_dynamic=True`` when getting the "
"config."
msgstr "默认情况下，量化配置是用于静态量化的。要应用动态量化，请在获取配置时添加参数 ``is_dynamic=True``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, we will calibrate the ``prepared_model`` after the observers are "
"inserted in the model. This step is needed for static quantization only."
msgstr "现在，当模型中插入了观察者后，我们将校准 ``prepared_model``。此步骤仅适用于静态量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After these steps, we finished running the quantization flow and we will get"
" the quantized model."
msgstr "完成这些步骤后，我们完成了量化流程，并将获得量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Lower into Inductor"
msgstr "3. 降低至 Inductor"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After we get the quantized model, we will further lower it to the inductor "
"backend. The default Inductor wrapper generates Python code to invoke both "
"generated kernels and external kernels. Additionally, Inductor supports C++ "
"wrapper that generates pure C++ code. This allows seamless integration of "
"the generated and external kernels, effectively reducing Python overhead. In"
" the future, leveraging the C++ wrapper, we can extend the capability to "
"achieve pure C++ deployment. For more comprehensive details about C++ "
"Wrapper in general, please refer to the dedicated tutorial on `Inductor C++ "
"Wrapper Tutorial "
"<https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html>`_."
msgstr ""
"获得量化模型后，我们将进一步将其降低到 Inductor 后端。默认的 Inductor 包装器会生成 Python代码 "
"来调用生成的内核和外部内核。此外，Inductor 支持生成纯 C++ 代码的 C++ 包装器。这允许生成的内核和外部内核的无缝集成，有效地减少了 "
"Python 开销。将来，利用 C++ 包装器，我们可以扩展能力以实现纯 C++ 部署。有关 C++ 包装器的更全面细节，请参阅关于 `Inductor"
" C++ 包装器教程 "
"<https://pytorch.org/tutorials/prototype/inductor_cpp_wrapper_tutorial.html>`_"
" 的专用教程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In a more advanced scenario, int8-mixed-bf16 quantization comes into play. "
"In this instance, a Convolution or GEMM operator produces BFloat16 output "
"data type instead of Float32 in the absence of a subsequent quantization "
"node. Subsequently, the BFloat16 tensor seamlessly propagates through "
"subsequent pointwise operators, effectively minimizing memory usage and "
"potentially enhancing performance. The utilization of this feature mirrors "
"that of regular BFloat16 Autocast, as simple as wrapping the script within "
"the BFloat16 Autocast context."
msgstr ""
"在更高级的场景中，int8 混合 bf16 量化派上用场。在这种情况下，当缺少后续量化节点时，卷积或 GEMM 操作会生成 BFloat16 "
"输出数据类型而不是 Float32。随后，BFloat16 "
"张量可以无缝地通过后续的点操作传播，有效地减少内存使用，并可能提高性能。使用此功能的方式与常规 BFloat16 自动转型相同，只需将脚本包裹在 "
"BFloat16 自动转型上下文中即可。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Put all these codes together, we will have the toy example code. Please note"
" that since the Inductor ``freeze`` feature does not turn on by default yet,"
" run your example code with ``TORCHINDUCTOR_FREEZING=1``."
msgstr ""
"将所有这些代码放在一起，我们将得到一个示例代码。请注意，由于 Inductor ``freeze`` 功能尚未默认启用，运行示例代码时需要设置 "
"``TORCHINDUCTOR_FREEZING=1``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "For example:"
msgstr "例如："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With PyTorch 2.1 release, all CNN models from TorchBench test suite have "
"been measured and proven effective comparing with Inductor FP32 inference "
"path. Please refer to `this document <https://dev-"
"discuss.pytorch.org/t/torchinductor-update-6-cpu-backend-performance-update-"
"and-new-features-in-pytorch-2-1/1514#int8-inference-with-post-training-"
"static-quantization-3>`_ for detail benchmark number."
msgstr ""
"随着 PyTorch 2.1 的发布，所有来自 TorchBench 测试套件的 CNN 模型都已被测量并证明与 Inductor FP32 "
"推理路径相比效果良好。有关详细的基准指标，请参阅 `本文档 <https://dev-"
"discuss.pytorch.org/t/torchinductor-update-6-cpu-backend-performance-update-"
"and-new-features-in-pytorch-2-1/1514#int8-inference-with-post-training-"
"static-quantization-3>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Quantization Aware Training"
msgstr "量化感知训练"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The PyTorch 2 Export Quantization-Aware Training (QAT) is now supported on "
"X86 CPU using X86InductorQuantizer, followed by the subsequent lowering of "
"the quantized model into Inductor. For a more in-depth understanding of PT2 "
"Export Quantization-Aware Training, we recommend referring to the dedicated "
"`PyTorch 2 Export Quantization-Aware Training "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html>`_."
msgstr ""
"PyTorch 2 导出量化感知训练 (QAT) 现已支持使用 X86InductorQuantizer 在 X86 CPU "
"上运行，随后将量化模型降低到 Inductor。有关 PT2 导出量化感知训练的更多深入理解，我们建议参考专门的 `PyTorch 2 "
"导出量化感知训练教程 <https://pytorch.org/tutorials/prototype/pt2e_quant_qat.html>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The PyTorch 2 Export QAT flow is largely similar to the PTQ flow:"
msgstr "PyTorch 2 导出 QAT 流与 PTQ 流基本类似："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Please note that the Inductor ``freeze`` feature is not enabled by default. "
"To use this feature, you need to run example code with "
"``TORCHINDUCTOR_FREEZING=1``."
msgstr ""
"请注意，Inductor ``freeze`` 功能默认未启用。要使用此功能，您需要运行示例代码并设置 "
"``TORCHINDUCTOR_FREEZING=1``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With this tutorial, we introduce how to use Inductor with X86 CPU in PyTorch"
" 2 Quantization. Users can learn about how to use ``X86InductorQuantizer`` "
"to quantize a model and lower it into the inductor with X86 CPU devices."
msgstr ""
"通过本教程，我们介绍了如何在 PyTorch 2 量化中使用 X86 CPU 的 Inductor。用户可以学习如何使用 "
"``X86InductorQuantizer`` 对模型进行量化，并将其降低到适用于 X86 CPU 设备的 Inductor。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch 2 Export Quantization with Intel GPU Backend through Inductor"
msgstr "通过 Inductor 使用 Intel GPU 后端的 PyTorch 2 导出量化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Yan Zhiwei <https://github.com/ZhiweiYan-96>`_, `Wang Eikan "
"<https://github.com/EikanWang>`_, `Zhang Liangang "
"<https://github.com/liangan1>`_, `Liu River "
"<https://github.com/riverliuintel>`_, `Cui Yifeng "
"<https://github.com/CuiYifeng>`_"
msgstr ""
"**作者**: `Yan Zhiwei <https://github.com/ZhiweiYan-96>`_, `Wang Eikan "
"<https://github.com/EikanWang>`_, `Zhang Liangang "
"<https://github.com/liangan1>`_, `Liu River "
"<https://github.com/riverliuintel>`_, `Cui Yifeng "
"<https://github.com/CuiYifeng>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces ``XPUInductorQuantizer``, which aims to serve "
"quantized models for inference on Intel GPUs. ``XPUInductorQuantizer`` uses "
"the PyTorch Export Quantization flow and lowers the quantized model into the"
" inductor."
msgstr ""
"本教程介绍了 ``XPUInductorQuantizer``，旨在为 Intel GPU "
"上的推理服务量化模型。``XPUInductorQuantizer`` 使用 PyTorch 导出量化流程并将量化模型降低到 Inductor。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The Pytorch 2 Export Quantization flow uses `torch.export` to capture the "
"model into a graph and perform quantization transformations on top of the "
"ATen graph. This approach is expected to have significantly higher model "
"coverage with better programmability and a simplified user experience. "
"TorchInductor  is a compiler backend that transforms FX Graphs generated by "
"``TorchDynamo`` into optimized C++/Triton kernels."
msgstr ""
"PyTorch 2 导出量化流程使用 `torch.export` 捕获模型到图中，并在 ATen "
"图的基础上执行量化转换。这种方法预计具有显著更高的模型覆盖率、更好的可编程性，以及简化的用户体验。TorchInductor 是一个编译器后端，将 "
"``TorchDynamo`` 生成的 FX 图转化为优化的 C++/Triton 内核。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The quantization flow has three steps:"
msgstr "量化流程共有三个步骤："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 1: Capture the FX Graph from the eager model based on the `torch export"
" mechanism <https://pytorch.org/docs/main/export.html>`_."
msgstr ""
"步骤 1：基于 `torch export 机制 <https://pytorch.org/docs/main/export.html>`_ "
"从即时模式模型中捕获 FX 图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 2: Apply the quantization flow based on the captured FX Graph, "
"including defining the backend-specific quantizer, generating the prepared "
"model with observers, performing the prepared model's calibration, and "
"converting the prepared model into the quantized model."
msgstr ""
"步骤 2：基于捕获的 FX 图应用量化流程，包括定义特定后端量化器、生成带观察者的准备模型、执行准备模型的校准，以及将准备模型转换为量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Lower the quantized model into inductor with the API "
"``torch.compile``, which would call Triton kernels or oneDNN "
"GEMM/Convolution kernels."
msgstr ""
"步骤 3：使用 ``torch.compile`` API 将量化模型降低到 Inductor，该 API 会调用 Triton 内核或 oneDNN "
"GEMM/卷积内核。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Static quantization is the only method we currently support."
msgstr "目前我们仅支持静态量化方法。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The following dependencies are recommended to be installed through the Intel"
" GPU channel:"
msgstr "建议通过Intel GPU通道安装以下依赖项："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Please note that since the inductor ``freeze`` feature does not turn on by "
"default yet, you must run your example code with "
"``TORCHINDUCTOR_FREEZING=1``."
msgstr ""
"请注意，由于Inductor的``freeze``功能尚未默认开启，您需要使用``TORCHINDUCTOR_FREEZING=1``运行您的示例代码。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Next, we will quantize the FX Module."
msgstr "接下来，我们将对FX模块进行量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After we capture the FX Module, we will import the Backend Quantizer for "
"Intel GPU and configure it to quantize the model."
msgstr "在获取FX模块后，我们将导入用于Intel GPU的后端量化器，并将其配置为对模型进行量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The default quantization configuration in ``XPUInductorQuantizer`` uses "
"signed 8-bits for both activations and weights. The tensors are per-tensor "
"quantized, whereas the weights are signed 8-bit per-channel quantized."
msgstr ""
"``XPUInductorQuantizer``中的默认量化配置使用有符号的8位表示激活和权重。张量使用每张量量化，而权重则是有符号的8位按通道量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Optionally, in addition to the default quantization configuration using "
"asymmetric quantized activation, signed 8-bits symmetric quantized "
"activation is also supported, which has the potential to provide better "
"performance."
msgstr "可选地，除了默认的使用非对称量化激活的量化配置，还支持有符号的8位对称量化激活，这可能提供更好的性能。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After the backend-specific quantizer is imported, prepare the model for "
"post-training quantization. ``prepare_pt2e`` folds ``BatchNorm`` operators "
"into preceding Conv2d operators, and inserts observers into appropriate "
"places in the model."
msgstr ""
"在导入特定后端的量化器后，为后训练量化准备模型。``prepare_pt2e``将``BatchNorm``运算符折叠到前面的Conv2d运算符中，并在模型中的适当位置插入观察器。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**(For static quantization only)** Calibrate the ``prepared_model`` after "
"the observers are inserted into the model."
msgstr "（仅适用于静态量化）当观察器被插入模型后，对``prepared_model``进行校准。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Finally, convert the calibrated model to a quantized model. ``convert_pt2e``"
" takes a calibrated model and produces a quantized model."
msgstr "最后，将校准后的模型转换为量化模型。``convert_pt2e``接收一个校准模型并生成一个量化模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After these steps, the quantization flow has been completed and the "
"quantized model is available."
msgstr "完成上述步骤后，量化流程即告完成，量化模型已可用。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The quantized model will then be lowered into the inductor backend."
msgstr "然后量化模型将被降低到Inductor后端。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In a more advanced scenario, int8-mixed-bf16 quantization comes into play. "
"In this instance, a convolution or GEMM operator produces the output in "
"BFloat16 instead of Float32 in the absence of a subsequent quantization "
"node. Subsequently, the BFloat16 tensor seamlessly propagates through "
"subsequent pointwise operators, effectively minimizing memory usage and "
"potentially enhancing performance. The utilization of this feature mirrors "
"that of regular BFloat16 Autocast, as simple as wrapping the script within "
"the BFloat16 Autocast context."
msgstr ""
"在更高级的场景中，会涉及到int8混合bf16的量化。在这种情况下，当缺少后续量化节点时，卷积或GEMM运算符将输出BFloat16而不是Float32。随后，BFloat16张量将在后续逐点运算符中无缝传播，有效减少内存使用并可能提高性能。此功能的使用与常规BFloat16自动类型转换类似，只需将脚本包装在BFloat16自动类型转换上下文中即可。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have learned how to utilize the "
"``XPUInductorQuantizer`` to perform post-training quantization on models for"
" inference on Intel GPUs, leveraging PyTorch 2's Export Quantization flow. "
"We covered the step-by-step process of capturing an FX Graph, applying "
"quantization, and lowering the quantized model into the inductor backend "
"using ``torch.compile``. Additionally, we explored the benefits of using "
"int8-mixed-bf16 quantization for improved memory efficiency and potential "
"performance gains, especially when using ``BFloat16`` autocast."
msgstr ""
"在本教程中，我们学习了如何利用``XPUInductorQuantizer``对模型进行后训练量化，以便在Intel GPU上推理，利用PyTorch "
"2的导出量化流程。我们涵盖了捕获FX图、应用量化以及使用``torch.compile``将量化模型降低到Inductor后端的分步骤过程。此外，我们还探讨了使用int8混合bf16量化以提高内存效率和可能的性能提升的好处，尤其是在使用``BFloat16``自动类型转换时。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "How to Write a ``Quantizer`` for PyTorch 2 Export Quantization"
msgstr "如何为PyTorch 2导出量化编写``Quantizer``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"**Author**: `Leslie Fang <https://github.com/leslie-fang-intel>`_, `Weiwen "
"Xia <https://github.com/Xia-Weiwen>`__, `Jiong Gong "
"<https://github.com/jgong5>`__, `Kimish Patel "
"<https://github.com/kimishpatel>`__, `Jerry Zhang "
"<https://github.com/jerryzh168>`__"
msgstr ""
"作者：`Leslie Fang <https://github.com/leslie-fang-intel>`_, `Weiwen Xia "
"<https://github.com/Xia-Weiwen>`__, `Jiong Gong "
"<https://github.com/jgong5>`__, `Kimish Patel "
"<https://github.com/kimishpatel>`__, `Jerry Zhang "
"<https://github.com/jerryzh168>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Required:"
msgstr "必需："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Torchdynamo concepts in PyTorch "
"<https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html>`__"
msgstr ""
"`PyTorch中的Torchdynamo概念 "
"<https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`Quantization concepts in PyTorch "
"<https://pytorch.org/docs/master/quantization.html#quantization-api-"
"summary>`__"
msgstr ""
"`PyTorch中的量化概念 "
"<https://pytorch.org/docs/master/quantization.html#quantization-api-"
"summary>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`(prototype) PyTorch 2 Export Post Training Quantization "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`__"
msgstr ""
"`(原型) PyTorch 2导出的后训练量化 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Optional:"
msgstr "可选："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`FX Graph Mode post training static quantization "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`__"
msgstr ""
"`FX图模式后训练静态量化 "
"<https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`BackendConfig in PyTorch Quantization FX Graph Mode "
"<https://pytorch.org/tutorials/prototype/backend_config_tutorial.html?highlight=backend>`__"
msgstr ""
"`PyTorch量化FX图模式中的BackendConfig "
"<https://pytorch.org/tutorials/prototype/backend_config_tutorial.html?highlight=backend>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`QConfig and QConfigMapping in PyTorch Quantization FX Graph Mode "
"<https://pytorch.org/tutorials/prototype/backend_config_tutorial.html#set-"
"up-qconfigmapping-that-satisfies-the-backend-constraints>`__"
msgstr ""
"`PyTorch量化FX图模式中的QConfig和QConfigMapping "
"<https://pytorch.org/tutorials/prototype/backend_config_tutorial.html#set-"
"up-qconfigmapping-that-satisfies-the-backend-constraints>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`(prototype) PyTorch 2 Export Post Training Quantization "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`__ introduced "
"the overall API for pytorch 2 export quantization, main difference from fx "
"graph mode quantization in terms of API is that we made it explicit that "
"quantiation is targeting a specific backend. So to use the new flow, backend"
" need to implement a ``Quantizer`` class that encodes: (1). What is "
"supported quantized operator or patterns in the backend (2). How can users "
"express the way they want their floating point model to be quantized, for "
"example, quantized the whole model to be int8 symmetric quantization, or "
"quantize only linear layers etc."
msgstr ""
"`(原型) PyTorch 2导出的后训练量化 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`__介绍了PyTorch "
"2导出量化的整体API，与FX图模式量化在API上的主要区别是我们明确表示量化是针对特定后端的。因此，要使用新的流程，后端需要实现一个``Quantizer``类，该类编码：(1)后端支持的量化运算符或模式；(2)用户如何表达他们希望将浮点模型量化的方式，比如将整个模型量化为int8对称量化，或仅量化线性层等。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Please see `here "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html#motivation-of-"
"pytorch-2-export-quantization>`__ For motivations for the new API and "
"``Quantizer``."
msgstr ""
"有关新API和``Quantizer``的动机，请参见`此处 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html#motivation-of-"
"pytorch-2-export-quantization>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"An existing quantizer object defined for ``XNNPACK`` is in `QNNPackQuantizer"
" "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pt2e/quantizer/xnnpack_quantizer.py>`__"
msgstr ""
"为``XNNPACK``定义的现有量化器对象位于`QNNPackQuantizer "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pt2e/quantizer/xnnpack_quantizer.py>`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Annotation API"
msgstr "注释API"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``Quantizer`` uses annotation API to convey quantization intent for "
"different operators/patterns. Annotation API mainly consists of "
"`QuantizationSpec "
"<https://github.com/pytorch/pytorch/blob/1ca2e993af6fa6934fca35da6970308ce227ddc7/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L38>`__"
" and `QuantizationAnnotation "
"<https://github.com/pytorch/pytorch/blob/07104ca99c9d297975270fb58fda786e60b49b38/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L144>`__."
msgstr ""
"``Quantizer``使用注释API来传递对不同运算符/模式的量化意图。注释API主要包括`QuantizationSpec "
"<https://github.com/pytorch/pytorch/blob/1ca2e993af6fa6934fca35da6970308ce227ddc7/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L38>`__和`QuantizationAnnotation"
" "
"<https://github.com/pytorch/pytorch/blob/07104ca99c9d297975270fb58fda786e60b49b38/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L144>`__。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``QuantizationSpec`` is used to convey intent of how a tensor will be "
"quantized, e.g. dtype, bitwidth, min, max values, symmetric vs. asymmetric "
"etc. Furthermore, ``QuantizationSpec`` also allows quantizer to specify how "
"a tensor value should be observed, e.g. ``MinMaxObserver``, or "
"``HistogramObserver`` , or some customized observer."
msgstr ""
"``QuantizationSpec``用于传递张量将如何量化的意图，例如数据类型、位宽、最小值、最大值、对称量化或非对称量化等。此外，``QuantizationSpec``还允许量化器指定如何观察张量值，例如``MinMaxObserver``、``HistogramObserver``或一些自定义观察器。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``QuantizationAnnotation`` composed of ``QuantizationSpec`` objects is used "
"to annotate input tensors and output tensor of a pattern. Annotating input "
"tensors is equivalent of annotating input edges, while annotating output "
"tensor is equivalent of annotating node. ``QuantizationAnnotation`` is a "
"``dataclass`` with several fields:"
msgstr ""
"由``QuantizationSpec``对象组成的``QuantizationAnnotation``用于注释模式的输入张量和输出张量。注释输入张量相当于注释输入边缘，而注释输出张量相当于注释节点。``QuantizationAnnotation``是一个具有几个字段的``dataclass``："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``input_qspec_map`` field is of class ``Dict`` to map each input tensor (as "
"input edge) to a ``QuantizationSpec``."
msgstr ""
"``input_qspec_map``字段是一个``Dict``类，用于将每个输入张量（作为输入边缘）映射到一个``QuantizationSpec``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``output_qspec`` field expresses the ``QuantizationSpec`` used to annotate "
"the output tensor;"
msgstr "``output_qspec``字段表示用于注释输出张量的``QuantizationSpec``；"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``_annotated`` field indicates if this node has already been annotated by "
"quantizer."
msgstr "``_annotated``字段指示此节点是否已被量化器注释。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To conclude, annotation API requires quantizer to annotate edges (input "
"tensors) or nodes (output tensor) of the graph. Now, we will have a step-by-"
"step tutorial for how to use the annotation API with different types of "
"``QuantizationSpec``."
msgstr ""
"总之，注释API要求量化器注释图的边缘（输入张量）或节点（输出张量）。接下来，我们将逐步介绍如何使用具有不同类型``QuantizationSpec``的注释API。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1. Annotate Common Operator Patterns"
msgstr "1. 注释常见的运算符模式"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In order to use the quantized pattern/operators, e.g. ``quantized add``, "
"backend developers will have intent to quantize (as expressed by "
"``QuantizationSpec``) inputs, output of the pattern. Following is an example"
" flow (take ``add`` operator as example) of how this intent is conveyed in "
"the quantization workflow with annotation API."
msgstr ""
"为了使用量化的模式/运算符，例如``quantized "
"add``，后端开发人员将希望（通过``QuantizationSpec``表达的）量化模式的输入和输出。以下是一个示例流程（以``add``运算符为例），展示如何在量化工作流中通过注释API传达这一意图。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 1: Identify the original floating point pattern in the FX graph. There "
"are several ways to identify this pattern: Quantizer may use a pattern "
"matcher to match the operator pattern; Quantizer may go through the nodes "
"from start to the end and compare the node's target type to match the "
"operator pattern. In this example, we can use the `get_source_partitions "
"<https://github.com/pytorch/pytorch/blob/07104ca99c9d297975270fb58fda786e60b49b38/torch/fx/passes/utils/source_matcher_utils.py#L51>`__"
" to match this pattern. The original floating point ``add`` pattern only "
"contain a single ``add`` node."
msgstr ""
"步骤1：在FX图中识别原始浮点模式。有几种识别此模式的方法：量化器可以使用模式匹配器来匹配运算符模式；量化器可以从头至尾遍历节点并比较节点的目标类型以匹配运算符模式。在该示例中，我们可以使用`get_source_partitions"
" "
"<https://github.com/pytorch/pytorch/blob/07104ca99c9d297975270fb58fda786e60b49b38/torch/fx/passes/utils/source_matcher_utils.py#L51>`__匹配此模式。原始浮点``add``模式只包含一个``add``节点。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 2: Define the ``QuantizationSpec`` for inputs and output of the "
"pattern. ``QuantizationSpec`` defines the ``data type``, ``qscheme``, and "
"other quantization parameters about users' intent of how to observe or fake "
"quantize a tensor."
msgstr ""
"步骤2：为模式的输入和输出定义``QuantizationSpec``。``QuantizationSpec``定义了用户关于如何观察或假量化张量的意图，例如“数据类型”、“qscheme”和其他量化参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Annotate the inputs and output of the pattern with "
"``QuantizationAnnotation``. In this example, we will create the "
"``QuantizationAnnotation`` object with the ``QuantizationSpec`` created in "
"above step 2 for two inputs and one output of the ``add`` node."
msgstr ""
"步骤3：使用``QuantizationAnnotation``注释模式的输入和输出。在此示例中，我们将使用步骤2中为``add``节点的两个输入和一个输出创建的``QuantizationSpec``创建``QuantizationAnnotation``对象。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After we annotate the ``add`` node like this, in the following up "
"quantization flow, ``HistogramObserver`` will be inserted at its two input "
"nodes and one output node in prepare phase. And ``HistogramObserver`` will "
"be substituted with ``quantize`` node and ``dequantize`` node in the convert"
" phase."
msgstr ""
"在我们对``add``节点进行这样的注释后，在后续的量化流程中，``HistogramObserver``将在准备阶段插入到其两个输入节点和一个输出节点。而在转换阶段，``HistogramObserver``将被``quantize``节点和``dequantize``节点替代。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2. Annotate Operators that Shares Quantization Params"
msgstr "2. 注释共享量化参数的运算符"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"It is natural that users want to annotate a quantized model where "
"quantization parameters can be shared among some tensors explicitly. Two "
"typical use cases are:"
msgstr "用户显然希望注释一个量化模型，其中量化参数可以在一些张量之间显式共享。两个典型的用例为："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Example 1: One example is for ``add`` where having both inputs sharing "
"quantization parameters makes operator implementation much easier. Without "
"using of `SharedQuantizationSpec "
"<https://github.com/pytorch/pytorch/blob/1ca2e993af6fa6934fca35da6970308ce227ddc7/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L90>`__,"
" we must annotate ``add`` as example in above section 1, in which two inputs"
" of ``add`` has different quantization parameters."
msgstr ""
"示例1：“add”的一个示例是，为了使运算符实现更容易，两个输入共享量化参数。若不使用`SharedQuantizationSpec "
"<https://github.com/pytorch/pytorch/blob/1ca2e993af6fa6934fca35da6970308ce227ddc7/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L90>`__，我们必须按照上方第1节中的示例为``add``进行注释，其中``add``的两个输入具有不同的量化参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Example 2: Another example is that of sharing quantization parameters "
"between inputs and output. This typically results from operators such as "
"``maxpool``, ``average_pool``, ``concat`` etc."
msgstr ""
"示例2：另一个示例是输入与输出共享量化参数。这通常由``maxpool``、``average_pool``、``concat``等运算符导致。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``SharedQuantizationSpec`` is designed for this use case to annotate tensors"
" whose quantization parameters are shared with other tensors. Input of "
"``SharedQuantizationSpec`` is an ``EdgeOrNode`` object which can be an input"
" edge or an output value."
msgstr ""
"``SharedQuantizationSpec``专为此用例设计，以注释其量化参数与其他张量共享的张量。``SharedQuantizationSpec``的输入为一个``EdgeOrNode``对象，可以是输入边缘或一个输出值。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Sharing is transitive"
msgstr "共享是可传递的"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Some tensors might be effectively using shared quantization spec due to:"
msgstr "由于以下原因，一些张量可能有效地使用共享量化规范："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Two nodes/edges are configured to use ``SharedQuantizationSpec``."
msgstr "两个节点/边缘被配置为使用``SharedQuantizationSpec``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "There is existing sharing of some nodes."
msgstr "存在一些节点的共享。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For example, let's say we have two ``conv`` nodes ``conv1`` and ``conv2``, "
"and both of them are fed into a ``cat`` node: ``cat([conv1_out, conv2_out], "
"...)``. Let's say the output of ``conv1``, ``conv2``, and the first input of"
" ``cat`` are configured with the same configurations of "
"``QuantizationSpec``. The second input of ``cat`` is configured to use "
"``SharedQuantizationSpec`` with the first input."
msgstr ""
"例如，假设我们有两个``conv``节点``conv1``和``conv2``，它们都被馈送到一个``cat``节点：``cat([conv1_out,"
" conv2_out], "
"...)``。假设``conv1``、``conv2``的输出以及``cat``的第一个输入被配置为具有相同的``QuantizationSpec``配置。而``cat``的第二个输入被配置为使用第一个输入的``SharedQuantizationSpec``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"First of all, the output of ``conv1`` is implicitly sharing quantization "
"parameters (and observer object) with the first input of ``cat``, and the "
"same is true for the output of ``conv2`` and the second input of ``cat``. "
"Therefore, since the user configures the two inputs of ``cat`` to share "
"quantization parameters, by transitivity, ``conv2_out`` and ``conv1_out`` "
"will also be sharing quantization parameters. In the observed graph, you "
"will see the following:"
msgstr ""
"首先，``conv1`` 的输出隐式地与 ``cat`` 的第一个输入共享量化参数（和观测器对象），同样，``conv2`` 的输出也与 ``cat``"
" 的第二个输入共享量化参数。因此，由于用户配置了 ``cat`` 的两个输入共享量化参数，传递性使得 ``conv2_out`` 和 "
"``conv1_out`` 也会共享量化参数。在观测到的图中，您将看到以下内容："

#: ../../prototype/vulkan_workflow.rst:250
msgid "and both ``obs`` will be the same observer instance."
msgstr "而两个 ``obs`` 都将是相同的观察器实例。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Input edge is the connection between input node and the node consuming the "
"input, so it's a ``Tuple[Node, Node]``."
msgstr "输入边是输入节点与消费输入的节点之间的连接，因此它是一个 ``Tuple[Node, Node]``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Output value is an FX ``Node``."
msgstr "输出值是一个FX ``Node``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now, if we want to rewrite ``add`` annotation example with "
"``SharedQuantizationSpec`` to indicate two input tensors as sharing "
"quantization parameters. We can define its ``QuantizationAnnotation`` as "
"this:"
msgstr ""
"现在，如果我们想用 ``SharedQuantizationSpec`` 重写 ``add`` 注释示例以指示两个输入张量共享量化参数，我们可以定义其 "
"``QuantizationAnnotation`` 如下："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 1: Identify the original floating point pattern in the FX graph. We can"
" use the same methods introduced in ``QuantizationSpec`` example to identify"
" the ``add`` pattern."
msgstr ""
"步骤1：在FX图中识别原始浮点模式。我们可以使用 ``QuantizationSpec`` 示例中介绍的相同方法识别 ``add`` 模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Step 2: Annotate input_act0 of ``add`` with ``QuantizationSpec``."
msgstr "步骤2：用 ``QuantizationSpec`` 注释 ``add`` 的输入 ``input_act0``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Create a ``SharedQuantizationSpec`` object with input edge defined "
"as ``(input_act0, add_node)`` which means to share the observer used for "
"this edge. Then, user can annotate input_act1 with this "
"``SharedQuantizationSpec`` object."
msgstr ""
"步骤3：创建一个定义为 ``(input_act0, add_node)`` 的输入边的 ``SharedQuantizationSpec`` "
"对象，这意味着共享用于该边的观察器。然后，用户可以使用该 ``SharedQuantizationSpec`` 对象注释 ``input_act1``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "3. Annotate Operators with Fixed Quantization Parameters"
msgstr "3. 用固定量化参数注释操作"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Another typical use case to annotate a quantized model is for tensors whose "
"quantization parameters are known beforehand. For example, operator like "
"``sigmoid``, which has predefined and fixed scale/zero_point at input and "
"output tensors. `FixedQParamsQuantizationSpec "
"<https://github.com/pytorch/pytorch/blob/1ca2e993af6fa6934fca35da6970308ce227ddc7/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L90>`__"
" is designed for this use case. To use ``FixedQParamsQuantizationSpec``, "
"users need to pass in parameters of ``scale`` and ``zero_point`` explicitly."
msgstr ""
"另一种典型用例是为提前知道量化参数的张量注释量化模型。例如，对于像 ``sigmoid`` "
"这样的操作符，其输入和输出张量具有预定义的固定比例/零点。`FixedQParamsQuantizationSpec`_ 专为此用例设计。要使用 "
"``FixedQParamsQuantizationSpec``，用户需要显式传递 ``scale`` 和 ``zero_point`` 参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 1: Identify the original floating point pattern in the FX graph. We can"
" use the same methods introduced in ``QuantizationSpec`` example to identify"
" the ``sigmoid`` pattern."
msgstr ""
"步骤1：在FX图中识别原始浮点模式。我们可以使用 ``QuantizationSpec`` 示例中介绍的相同方法识别 ``sigmoid`` 模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 2: Create ``FixedQParamsQuantizationSpec`` object with inputs of fixed "
"``scale``, ``zero_point`` value. These values will be used to create the "
"``quantize`` node and ``dequantize`` node in the convert phase."
msgstr ""
"步骤2：使用固定的 ``scale`` 和 ``zero_point`` 值创建 ``FixedQParamsQuantizationSpec`` "
"对象。这些值将在转换阶段用于创建 ``quantize`` 节点和 ``dequantize`` 节点。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Annotate inputs and output to use this "
"``FixedQParamsQuantizationSpec`` object."
msgstr "步骤3：注释输入和输出以使用此 ``FixedQParamsQuantizationSpec`` 对象。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "4. Annotate Tensors with Derived Quantization Parameters"
msgstr "4. 用派生量化参数注释张量"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Another use case is to define the constraint for tensors whose quantization "
"parameters are derived from other tensors. For example, if we want to "
"annotate a convolution node, and define the ``scale`` of its bias input "
"tensor as product of the activation tensor's ``scale`` and weight tensor's "
"``scale``. We can use `DerivedQuantizationSpec "
"<https://github.com/pytorch/pytorch/blob/1ca2e993af6fa6934fca35da6970308ce227ddc7/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L102>`__"
" to annotate this conv node."
msgstr ""
"另一种用例是定义张量量化参数的约束，其量化参数是从其他张量派生的。例如，如果我们想要注释卷积节点，并定义其偏置输入张量的 ``scale`` "
"是激活张量的 ``scale`` 和权重张量的 ``scale`` 的乘积。我们可以使用 `DerivedQuantizationSpec`_ "
"来注释该卷积节点。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 1: Identify the original floating point pattern in the FX graph. We can"
" use the same methods introduced in ``QuantizationSpec`` example to identify"
" the ``convolution`` pattern."
msgstr ""
"步骤1：在FX图中识别原始浮点模式。我们可以使用 ``QuantizationSpec`` 示例中介绍的相同方法识别 ``convolution`` "
"模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 2: Define ``derive_qparams_fn`` function, it accepts list of "
"``ObserverOrFakeQuantize`` ( `ObserverBase "
"<https://github.com/pytorch/pytorch/blob/07104ca99c9d297975270fb58fda786e60b49b38/torch/ao/quantization/observer.py#L124>`__"
" or `FakeQuantizeBase "
"<https://github.com/pytorch/pytorch/blob/07104ca99c9d297975270fb58fda786e60b49b38/torch/ao/quantization/fake_quantize.py#L60>`__)"
" as input. From each ``ObserverOrFakeQuantize`` object, user can get the "
"``scale``, ``zero point`` value. User can define its heuristic about how to "
"derive new ``scale``, ``zero point`` value based on the quantization "
"parameters calculated from the observer or fake quant instances."
msgstr ""
"步骤2：定义 ``derive_qparams_fn`` 函数，接受 ``ObserverOrFakeQuantize`` 的列表 "
"(`ObserverBase`_ 或 `FakeQuantizeBase`_) 作为输入。从每个 ``ObserverOrFakeQuantize`` "
"对象中，用户可以获取 ``scale`` 和 ``zero point`` 值。用户可以定义其启发式方法来根据从观察器或伪量实例计算的量化参数派生新的 "
"``scale`` 和 ``zero point`` 值。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 3: Define ``DerivedQuantizationSpec`` obejct, it accepts inputs of: "
"list of ``EdgeOrNode`` objects. The observer corresponding to each "
"``EdgeOrNode`` object will be passed into the ``derive_qparams_fn`` "
"function; ``derive_qparams_fn`` function; several other quantization "
"parameters such as ``dtype``, ``qscheme``."
msgstr ""
"步骤3：定义 ``DerivedQuantizationSpec`` 对象，接受以下输入： ``EdgeOrNode`` 对象的列表。对应于每个 "
"``EdgeOrNode`` 对象的观察器将传递到 ``derive_qparams_fn`` 函数； ``derive_qparams_fn`` "
"函数；以及其他几个量化参数，比如 ``dtype``、 ``qscheme``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Step 4: Annotate the inputs and output of this conv node with "
"``QuantizationAnnotation``."
msgstr "步骤4：用 ``QuantizationAnnotation`` 注释该卷积节点的输入和输出。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "5. A Toy Example with Resnet18"
msgstr "5. 使用Resnet18的一个示例"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After above annotation methods defined with ``QuantizationAnnotation API``, "
"we can now put them together to construct a ``BackendQuantizer`` and run a "
"`toy example <https://gist.github.com/leslie-fang-"
"intel/b78ed682aa9b54d2608285c5a4897cfc>`__ with ``Torchvision Resnet18``. To"
" better understand the final example, here are the classes and utility "
"functions that are used in the example:"
msgstr ""
"定义了上述 ``QuantizationAnnotation API`` 的注释方法之后，我们现在可以将它们组合在一起构建一个 "
"``BackendQuantizer`` 并运行一个 `玩具示例`_ 使用 ``Torchvision "
"Resnet18``。为了更好地理解最终示例，以下是示例中使用的类和实用函数："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`QuantizationConfig "
"<https://github.com/pytorch/pytorch/blob/73fd7235ad25ff061c087fa4bafc6e8df4d9c299/torch/ao/quantization/_pt2e/quantizer/quantizer.py#L103-L109>`__"
" consists of ``QuantizationSpec`` for activation, weight, and bias "
"separately."
msgstr "`QuantizationConfig`_ 包含激活、权重和偏差单独的 ``QuantizationSpec``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When annotating the model, `get_input_act_qspec "
"<https://github.com/pytorch/pytorch/blob/47cfcf566ab76573452787335f10c9ca185752dc/torch/ao/quantization/_pt2e/quantizer/utils.py#L10>`__,"
" `get_output_act_qspec "
"<https://github.com/pytorch/pytorch/blob/47cfcf566ab76573452787335f10c9ca185752dc/torch/ao/quantization/_pt2e/quantizer/utils.py#L23>`__,"
" `get_weight_qspec "
"<https://github.com/pytorch/pytorch/blob/47cfcf566ab76573452787335f10c9ca185752dc/torch/ao/quantization/_pt2e/quantizer/utils.py#L36>`__,"
" and `get_bias_qspec "
"<https://github.com/pytorch/pytorch/blob/47cfcf566ab76573452787335f10c9ca185752dc/torch/ao/quantization/_pt2e/quantizer/utils.py#L53>`__"
" can be used to get the ``QuantizationSpec`` from ``QuantizationConfig`` for"
" a specific pattern."
msgstr ""
"在模型注释时，`get_input_act_qspec`_，`get_output_act_qspec`_，`get_weight_qspec`_ 和 "
"`get_bias_qspec`_ 可以用于从 ``QuantizationConfig`` 中获取特定模式的 "
"``QuantizationSpec``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "A Note on IR for PT2E Quantization Flow"
msgstr "关于PT2E量化流程中的IR的说明"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"IR means the intermediate representation of the model, for example, "
"``torch`` IR (``torch.nn`` modules, ``torch.nn.functional`` ops) or ``aten``"
" IR (``torch.ops.aten.linear``, ...). PT2E Quantization Flow is using pre "
"autograd aten IR (the output of `torch.export` API) so that we support "
"training. As is shown before, we need to match the operator or operator "
"patterns before we can attach annotations on them, So the question is how do"
" we match the pattern?"
msgstr ""
"IR指模型的中间表示，例如 ``torch`` IR（``torch.nn`` 模块，``torch.nn.functional`` 操作）或 "
"``aten`` IR（``torch.ops.aten.linear``，……）。PT2E量化流程使用 pre autograd aten "
"IR（`torch.export` API 的输出）以便支持训练。如前所述，我们需要在附加注释之前匹配操作或操作模式，那么问题是我们如何匹配模式？"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Motivation: Problem of Matching ``aten`` IR directly"
msgstr "动机：直接匹配 ``aten`` IR 的问题"

#: ../../prototype/vulkan_workflow.rst:250
msgid "The most straightforward way might be matching ``aten`` IR directly."
msgstr "最直接的方法可能是直接匹配 ``aten`` IR。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Example::"
msgstr "示例::"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"However one problem for using this IR is that the representation might "
"change if the PyTorch implementation for modules or functional ops changed. "
"But this could be unexpected since modeling users typically assume that when"
" the eager mode model code doesn't change, they should get the same model "
"representation after program capture as well. One concrete effect for this "
"problem is that if a ``Quantizer`` do annotations based on recognizing "
"``aten`` IR patterns, then it may fail to recognzing the pattern after "
"PyTorch version update, and the same eager mode floating point may be left "
"unquantized."
msgstr ""
"然而，使用此IR的问题在于，如果PyTorch模块或功能操作的实现发生变化，表示可能会改变。但这一点可能是意料之外的，因为建模用户通常假设当即时模式模型代码保持不变时，在程序捕获之后也应该获得相同的模型表示。此问题的具体影响在于，如果"
" ``Quantizer`` 基于识别 ``aten`` IR "
"模式进行注释，那么在PyTorch版本更新后可能无法识别模式，并且相同的即时模式浮点可能被留为未量化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Recommendation: Use ``SubgraphMatcherWithNameNodeMap`` for pattern matching"
msgstr "建议：使用 ``SubgraphMatcherWithNameNodeMap`` 进行模式匹配"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Because of this, we recommend people to recognize the pattern through "
"``SubgraphMatcherWithNameNodeMap`` (an improved version of "
"``SubgraphMatcher`` that makes it easier to query the nodes that people want"
" to annotate), through capturing a ``torch`` IR pattern (with the same "
"program capture used for capturing the floating point model), instead of "
"using the ``aten`` IR pattern directly."
msgstr ""
"鉴于此，我们建议通过 ``SubgraphMatcherWithNameNodeMap``（一种优化版的 "
"``SubgraphMatcher``，使您更容易查询需要注释的节点）识别模式，通过捕获一个 ``torch`` IR "
"模式（使用相同的程序捕获来捕获浮点模型），而不是直接使用 ``aten`` IR 模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With this, the ``Quantizer`` will still be valid even when the "
"implementation for nn modules and functionals changes, the ``aten`` IR for "
"floating point model will change, but since we capture the pattern again "
"instead of hardcoding the ``aten`` IR for the pattern, we'll get the updated"
" ``aten`` IR as well and will still be able to match the pattern."
msgstr ""
"有了这个，即使对nn模块和函数的实现发生了变化，``Quantizer`` 仍然有效，浮点模型的 ``aten`` IR "
"会发生变化，但由于我们再次捕获模式而不是硬编码模式的 ``aten`` IR，我们将获得更新的 ``aten`` IR 并仍然能够匹配模式。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"One caveat is that if inputs of the pattern has multiple users, we don't "
"have a good way to identify which user node we want to annotate except for "
"checking the aten op target."
msgstr "一个注意事项是，如果模式的输入有多个用户，我们没有好的方法标记我们想要注释的用户节点，除了检查aten操作目标。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Another caveat is that we need to make sure we have an exhaustive list of "
"examples (e.g. 2D, 3D, 4D inputs, real v.s. symbolic inputs, training=True "
"v.s. training=False etc.) for the pattern to make sure cover different "
"possible ``aten`` IR outcomes captured from the ``torch`` IR pattern."
msgstr ""
"另一个注意事项是，我们需要确保拥有一个详尽的示例列表（例如，2D、3D、4D输入，现实与符号输入，training=True与training=False等）以确保覆盖从"
" ``torch`` IR 模式捕获的不同可能 ``aten`` IR 结果。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note: We may provide some (pattern, list of example_inputs) or some pre-"
"generated matcher object so people can just use them directly in the future."
msgstr "注意：我们可能会提供一些（模式，示例输入列表）或一些预生成的匹配器对象，使人们可以直接使用它们。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With this tutorial, we introduce the new quantization path in PyTorch 2. "
"Users can learn about how to define a ``BackendQuantizer`` with the "
"``QuantizationAnnotation API`` and integrate it into the PyTorch 2 Export "
"Quantization flow. Examples of ``QuantizationSpec``, "
"``SharedQuantizationSpec``, ``FixedQParamsQuantizationSpec``, and "
"``DerivedQuantizationSpec`` are given for specific annotation use case. You "
"can use `XNNPACKQuantizer "
"<https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/quantizer/xnnpack_quantizer.py>`_"
" as an example to start implementing your own ``Quantizer``. After that "
"please follow `this tutorial "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_ to actually "
"quantize your model."
msgstr ""
"通过本教程，我们介绍了PyTorch 2中的新的量化路径。用户可以了解如何使用 ``QuantizationAnnotation API`` 定义 "
"``BackendQuantizer`` 并将其集成到PyTorch 2导出量化流程中。为特定的注释用例提供了 "
"``QuantizationSpec``、 ``SharedQuantizationSpec``、 "
"``FixedQParamsQuantizationSpec`` 和 ``DerivedQuantizationSpec`` 的示例。您可以使用 "
"`XNNPACKQuantizer`_ 作为示例开始实现自己的 ``Quantizer``。在此之后，请遵循 `本教程 "
"<https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html>`_ 实际量化您的模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Autoloading Out-of-Tree Extension"
msgstr "自动加载树外扩展"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author:** `Yuanhao Ji <https://github.com/shink>`__"
msgstr "**作者：** `Yuanhao Ji`__"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The extension autoloading mechanism enables PyTorch to automatically load "
"out-of-tree backend extensions without explicit import statements. This "
"feature is beneficial for users as it enhances their experience and enables "
"them to follow the familiar PyTorch device programming model without having "
"to explicitly load or import device-specific extensions. Additionally, it "
"facilitates effortless adoption of existing PyTorch applications with zero-"
"code changes on out-of-tree devices. For further details, refer to the "
"`[RFC] Autoload Device Extension "
"<https://github.com/pytorch/pytorch/issues/122468>`_."
msgstr ""
"扩展自动加载机制使PyTorch可以在没有显式导入语句的情况下自动加载树外后端扩展。此功能为用户带来了便利，使他们能够遵循熟悉的PyTorch设备编程模型，而无需显式加载或导入特定于设备的扩展。此外，这可以在树外设备上零代码变更轻松采用现有PyTorch应用程序。有关详情，请参阅"
" `[RFC] 自动加载设备扩展 <https://github.com/pytorch/pytorch/issues/122468>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "How to use out-of-tree extension autoloading in PyTorch"
msgstr "如何在PyTorch中使用树外扩展自动加载"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Review examples with Intel Gaudi HPU, Huawei Ascend NPU"
msgstr "查看使用Intel Gaudi HPU、华为Ascend NPU的示例"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch v2.5 or later"
msgstr "PyTorch v2.5或更新版本"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This feature is enabled by default and can be disabled by using ``export "
"TORCH_DEVICE_BACKEND_AUTOLOAD=0``. If you get an error like this: \"Failed "
"to load the backend extension\", this error is independent with PyTorch, you"
" should disable this feature and ask the out-of-tree extension maintainer "
"for help."
msgstr ""
"此功能默认启用，可以通过使用 ``export TORCH_DEVICE_BACKEND_AUTOLOAD=0`` "
"禁用。如果遇到类似“无法加载后端扩展”的错误，此错误与PyTorch无关，应禁用此功能并向树外扩展维护者求助。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "How to apply this mechanism to out-of-tree extensions?"
msgstr "如何将此机制应用于树外扩展？"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For instance, suppose you have a backend named ``foo`` and a corresponding "
"package named ``torch_foo``. Ensure that your package is compatible with "
"PyTorch 2.5 or later and includes the following snippet in its "
"``__init__.py`` file:"
msgstr ""
"例如，假设你有一个名为``foo``的后端，以及对应的包名为``torch_foo``。请确保你的包兼容PyTorch "
"2.5或更高版本，并在其``__init__.py``文件中包含以下代码片段："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Then, the only thing you need to do is define an entry point within your "
"Python package:"
msgstr "然后，你需要做的唯一事情就是在你的Python包中定义一个入口点："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now you can import the ``torch_foo`` module by simply adding the ``import "
"torch`` statement without the need to add ``import torch_foo``:"
msgstr ""
"现在你可以通过简单地添加``import torch``语句来导入``torch_foo``模块，而无需添加``import torch_foo``："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In some cases, you might encounter issues with circular imports. The "
"examples below demonstrate how you can address them."
msgstr "在某些情况下，你可能会遇到循环导入问题。下面的示例演示了如何解决这些问题。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Examples"
msgstr "示例"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this example, we will be using Intel Gaudi HPU and Huawei Ascend NPU to "
"determine how to integrate your out-of-tree extension with PyTorch using the"
" autoloading feature."
msgstr ""
"在这个示例中，我们将使用Intel Gaudi HPU和华为Ascend "
"NPU来确定如何使用PyTorch的自动加载功能将你的扩展集成到PyTorch中。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`habana_frameworks.torch`_ is a Python package that enables users to run "
"PyTorch programs on Intel Gaudi by using the PyTorch ``HPU`` device key."
msgstr ""
"`habana_frameworks.torch`_是一个Python包，它允许用户通过使用PyTorch的``HPU``设备键在Intel "
"Gaudi上运行PyTorch程序。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``habana_frameworks.torch`` is a submodule of ``habana_frameworks``, we add "
"an entry point to ``__autoload()`` in ``habana_frameworks/setup.py``:"
msgstr ""
"``habana_frameworks.torch``是``habana_frameworks``的一个子模块，我们在``habana_frameworks/setup.py``中向``__autoload()``添加一个入口点："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In ``habana_frameworks/init.py``, we use a global variable to track if our "
"module has been loaded:"
msgstr "在``habana_frameworks/init.py``中，我们使用一个全局变量来跟踪我们的模块是否已被加载："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In ``habana_frameworks/torch/init.py``, we prevent circular imports by "
"updating the state of the global variable:"
msgstr "在``habana_frameworks/torch/init.py``中，我们通过更新全局变量的状态来防止循环导入："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`torch_npu`_ enables users to run PyTorch programs on Huawei Ascend NPU, it "
"leverages the ``PrivateUse1`` device key and exposes the device name as "
"``npu`` to the end users."
msgstr ""
"``torch_npu``使用户能够在华为Ascend "
"NPU上运行PyTorch程序，它使用``PrivateUse1``设备键，并向终端用户公开设备名为``npu``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "We define an entry point in `torch_npu/setup.py`_:"
msgstr "我们在`torch_npu/setup.py`_中定义了一个入口点："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Unlike ``habana_frameworks``, ``torch_npu`` uses the environment variable "
"``TORCH_DEVICE_BACKEND_AUTOLOAD`` to control the autoloading process. For "
"example, we set it to ``0`` to disable autoloading to prevent circular "
"imports:"
msgstr ""
"不同于``habana_frameworks``，``torch_npu``使用环境变量``TORCH_DEVICE_BACKEND_AUTOLOAD``来控制自动加载过程。例如，我们可以将其设置为``0``以禁用自动加载以防止循环导入："

#: ../../prototype/vulkan_workflow.rst:250
msgid "How it works"
msgstr "工作原理"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Autoloading implementation"
msgstr "自动加载实现"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Autoloading is implemented based on Python's `Entrypoints "
"<https://packaging.python.org/en/latest/specifications/entry-points/>`_ "
"mechanism. We discover and load all of the specific entry points in "
"``torch/__init__.py`` that are defined by out-of-tree extensions."
msgstr ""
"自动加载是基于Python的`Entrypoints "
"<https://packaging.python.org/en/latest/specifications/entry-"
"points/>`_机制实现的。我们在``torch/__init__.py``中发现并加载所有由扩展定义的特定入口点。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"As shown above, after installing ``torch_foo``, your Python module can be "
"imported when loading the entrypoint that you have defined, and then you can"
" do some necessary work when calling it."
msgstr "如上所示，安装``torch_foo``后，在加载你定义的入口点时，你的Python模块可以被导入，然后你可以在调用时做一些必要的操作。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"See the implementation in this pull request: `[RFC] Add support for device "
"extension autoloading <https://github.com/pytorch/pytorch/pull/127074>`_."
msgstr ""
"请参阅此拉取请求中的实现：`[RFC] Add support for device extension autoloading "
"<https://github.com/pytorch/pytorch/pull/127074>`_。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we learned about the out-of-tree extension autoloading "
"mechanism in PyTorch, which automatically loads backend extensions "
"eliminating the need to add additional import statements. We also learned "
"how to apply this mechanism to out-of-tree extensions by defining an entry "
"point and how to prevent circular imports. We also reviewed an example on "
"how to use the autoloading mechanism with Intel Gaudi HPU and Huawei Ascend "
"NPU."
msgstr ""
"在本教程中，我们学习了PyTorch中的扩展自动加载机制，该机制自动加载后端扩展，无需添加额外的导入语句。我们还学习了如何通过定义一个入口点将此机制应用于扩展，并如何防止循环导入。此外，我们还回顾了如何在Intel"
" Gaudi HPU和华为Ascend NPU上使用自动加载机制的示例。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "(prototype) Accelerating BERT with semi-structured (2:4) sparsity"
msgstr "(原型)使用半结构化(2:4)稀疏性加速BERT"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author**: `Jesse Cai <https://github.com/jcaip>`_"
msgstr "**作者**: `Jesse Cai <https://github.com/jcaip>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Like other forms of sparsity, **semi-structured sparsity** is a model "
"optimization technique that seeks to reduce the memory overhead and latency "
"of a neural network at the expense of some model accuracy. It is also known "
"as **fine-grained structured sparsity** or **2:4 structured sparsity**."
msgstr ""
"与其他形式的稀疏性一样，**半结构化稀疏性**是一种模型优化技术，旨在通过牺牲一定的模型准确性来降低神经网络的内存开销和延迟。它也被称为**细粒度结构稀疏性**或**2:4结构稀疏性**。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Semi-structured sparsity derives its name from its unique sparsity pattern, "
"where n out of every 2n elements are pruned. We most often see n=2, hence "
"2:4 sparsity Semi-structured sparsity is particularly interesting because it"
" can be efficiently accelerated on GPUs and doesn't degrade model accuracy "
"as much as other sparsity patterns."
msgstr ""
"半结构化稀疏性因其独特的稀疏模式而得名，其中2n个元素中的n个元素被裁剪。我们通常看到n=2，因此称为2:4稀疏性。半结构化稀疏性特别有趣，因为它可以在GPU上高效加速，并且它对模型准确性的损害不像其他稀疏性模式那么大。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"With the introduction of `semi-structured sparsity support "
"<https://pytorch.org/docs/2.1/sparse.html#sparse-semi-structured-tensors>`_,"
" it is possible to prune and accelerate a semi-structured sparse model "
"without leaving PyTorch. We will explain this process in this tutorial."
msgstr ""
"随着`半结构化稀疏性支持 <https://pytorch.org/docs/2.1/sparse.html#sparse-semi-"
"structured-tensors>`_的引入，可以在不离开PyTorch的情况下裁剪并加速半结构化稀疏模型。我们将在本教程中解释这一过程。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"By the end of this tutorial, we will have sparsified a BERT question-"
"answering model to be 2:4 sparse, fine-tuning it to recover nearly all F1 "
"loss (86.92 dense vs 86.48 sparse). Finally, we will accelerate this 2:4 "
"sparse model for inference, yielding a 1.3x speedup."
msgstr ""
"通过本教程结束时，我们将使一个BERT问答模型变为2:4稀疏，并对其进行微调以恢复几乎所有的F1损失(86.92密集 vs "
"86.48稀疏)。最后，我们将加速这个2:4稀疏模型进行推理，带来1.3倍的加速。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Requirements"
msgstr "要求"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch >= 2.1."
msgstr "PyTorch >= 2.1。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"A NVIDIA GPU with semi-structured sparsity support (Compute Capability "
"8.0+)."
msgstr "支持半结构化稀疏性的NVIDIA GPU (计算能力8.0+)。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial is designed for beginners to semi-structured sparsity / "
"sparsity in general. For users with existing 2:4 sparse models, accelerating"
" ``nn.Linear`` layers for inference with ``to_sparse_semi_structured`` is as"
" easy as:"
msgstr ""
"本教程是为半结构化稀疏性/稀疏性新手设计的。对于已有2:4稀疏模型的用户，使用``to_sparse_semi_structured``加速推理的``nn.Linear``层非常简单："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"On an A100 80GB, we see: `Dense: 0.870ms Sparse: 0.630ms | Speedup: 1.382x`"
msgstr "在A100 80GB上，我们看到了：`密集：0.870ms 稀疏：0.630ms | 加速比：1.382x`"

#: ../../prototype/vulkan_workflow.rst:250
msgid "What problem does semi-structured sparsity solve?"
msgstr "半结构化稀疏性解决了什么问题？"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The general motivation behind sparsity is simple: if there are zeros in your"
" network, you can avoid storing / doing compute with those parameters. "
"However, the specifics of sparsity are tricky. Zeroing out parameters "
"doesn't affect the latency / memory overhead of our model out of the box."
msgstr ""
"稀疏性的总体动机很简单：如果网络中存在零值参数，你可以避免存储/计算这些参数。然而，稀疏性本身的细节是复杂的。将参数置零不是直接带来模型的延迟/内存开销的减少。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This is because the dense tensor still contains the pruned (zero) elements, "
"which the dense matrix multiplication kernel will still operate on this "
"elements. In order to realize performance gains, we need to swap out dense "
"kernels for sparse kernels, which skip calculation involving pruned "
"elements."
msgstr ""
"这是因为稠密张量中仍然包含被裁剪（零值）的元素，稠密矩阵乘法内核仍然对这些元素执行操作。为了实现性能提升，我们需要将稠密内核替换为稀疏内核，从而跳过涉及被裁剪元素的计算。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To do this, these kernels work on sparse matrices, which do not store the "
"pruned elements and store the specified elements in a compressed format."
msgstr "为实现这一点，这些内核使用稀疏矩阵，这些矩阵不存储被裁剪的元素，并以压缩格式存储指定的元素。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For semi-structured sparsity, we store exactly half of the original "
"parameters along with some compressed metadata about how the elements were "
"arranged."
msgstr "对于半结构化稀疏性，我们精确保存了原始参数的一半以及关于这些元素如何排列的一些压缩元数据。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"There are many different sparse layouts, each with their own benefits and "
"drawbacks. The 2:4 semi-structured sparse layout is particularly interesting"
" for two reasons: 1. Unlike previous sparse formats, semi-structured "
"sparsity was designed to be efficiently accelerated on GPUs."
msgstr ""
"有许多不同的稀疏布局，每种布局都有其自己的优点和缺点。2:4半结构化稀疏布局因两个原因特别有趣：1. "
"与之前的稀疏格式不同，半结构化稀疏性是专门为在GPU上高效加速而设计的。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In 2020, NVIDIA introduced hardware support for semi-structured sparsity "
"with their Ampere architecture, and have also released fast sparse kernels "
"via CUTLASS/`cuSPARSELt "
"<https://docs.nvidia.com/cuda/cusparselt/index.html>`_."
msgstr ""
"2020年NVIDIA通过其Ampere架构引入了硬件支持半结构化稀疏性，并通过CUTLASS/`cuSPARSELt "
"<https://docs.nvidia.com/cuda/cusparselt/index.html>`_发布了快速的稀疏内核。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"At the same time, semi-structured sparsity tends to have a milder impact on "
"model accuracy compared to other sparse formats, especially when accounting "
"for more advanced pruning / fine-tuning methods. NVIDIA has shown in their "
"`white paper <https://arxiv.org/abs/2104.08378>`_ that a simple paradigm of "
"magnitude pruning once to be 2:4 sparse and then retraining the model yields"
" nearly identical model accuracies."
msgstr ""
"同时，与其他稀疏格式相比，半结构化稀疏性对模型准确性的影响往往较小，尤其是在考虑更高级裁剪/微调方法时。NVIDIA在其`白皮书 "
"<https://arxiv.org/abs/2104.08378>`_中显示一次简单的范式：基于幅值裁剪将模型裁剪为2:4稀疏，然后重新训练模型，从而实现几乎相同的模型准确性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Semi-structured exists in a sweet spot, providing a 2x (theoretical) speedup"
" at a much lower sparsity level (50%), while still being granular enough to "
"preserve model accuracy."
msgstr "半结构化稀疏性处于一个甜蜜点，提供了2倍（理论）加速，同时保持较低的稀疏水平（50%），而仍然足够细粒度以保持模型准确性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Network"
msgstr "网络"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Data Set"
msgstr "数据集"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Metric"
msgstr "指标"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Dense FP16"
msgstr "稠密FP16"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Sparse FP16"
msgstr "稀疏FP16"

#: ../../prototype/vulkan_workflow.rst:250
msgid "ResNet-50"
msgstr "ResNet-50"

#: ../../prototype/vulkan_workflow.rst:250
msgid "ImageNet"
msgstr "ImageNet"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Top-1"
msgstr "Top-1"

#: ../../prototype/vulkan_workflow.rst:250
msgid "76.1"
msgstr "76.1"

#: ../../prototype/vulkan_workflow.rst:250
msgid "76.2"
msgstr "76.2"

#: ../../prototype/vulkan_workflow.rst:250
msgid "ResNeXt-101_32x8d"
msgstr "ResNeXt-101_32x8d"

#: ../../prototype/vulkan_workflow.rst:250
msgid "79.3"
msgstr "79.3"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Xception"
msgstr "Xception"

#: ../../prototype/vulkan_workflow.rst:250
msgid "79.2"
msgstr "79.2"

#: ../../prototype/vulkan_workflow.rst:250
msgid "SSD-RN50"
msgstr "SSD-RN50"

#: ../../prototype/vulkan_workflow.rst:250
msgid "COCO2017"
msgstr "COCO2017"

#: ../../prototype/vulkan_workflow.rst:250
msgid "bbAP"
msgstr "bbAP"

#: ../../prototype/vulkan_workflow.rst:250
msgid "24.8"
msgstr "24.8"

#: ../../prototype/vulkan_workflow.rst:250
msgid "MaskRCNN-RN50"
msgstr "MaskRCNN-RN50"

#: ../../prototype/vulkan_workflow.rst:250
msgid "37.9"
msgstr "37.9"

#: ../../prototype/vulkan_workflow.rst:250
msgid "FairSeq Transformer"
msgstr "FairSeq Transformer"

#: ../../prototype/vulkan_workflow.rst:250
msgid "EN-DE WMT14"
msgstr "EN-DE WMT14"

#: ../../prototype/vulkan_workflow.rst:250
msgid "BLEU"
msgstr "BLEU"

#: ../../prototype/vulkan_workflow.rst:250
msgid "28.2"
msgstr "28.2"

#: ../../prototype/vulkan_workflow.rst:250
msgid "28.5"
msgstr "28.5"

#: ../../prototype/vulkan_workflow.rst:250
msgid "BERT-Large"
msgstr "BERT-Large"

#: ../../prototype/vulkan_workflow.rst:250
msgid "SQuAD v1.1"
msgstr "SQuAD v1.1"

#: ../../prototype/vulkan_workflow.rst:250
msgid "F1"
msgstr "F1"

#: ../../prototype/vulkan_workflow.rst:250
msgid "91.9"
msgstr "91.9"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Semi-structured sparsity has an additional advantage from a workflow "
"perspective. Because the sparsity level is fixed at 50%, it is easier to "
"decompose the problem of sparsifying a model into two distinct subproblems:"
msgstr "从工作流程上看，半结构化稀疏性还具有额外的优势。由于稀疏水平固定为50%，将模型稀疏化问题分解为两个独立的子问题变得更加容易："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Accuracy - How can we find a set of 2:4 sparse weights that minimize the "
"accuracy degradation of our model?"
msgstr "准确性 - 我们如何找到一组2:4稀疏权重以将模型的准确性降级降至最小？"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Performance - How can we accelerate our 2:4 sparse weights for inference and"
" reduced memory overhead?"
msgstr "性能 - 我们如何加速推理的2:4稀疏权重并减少内存开销？"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"\\begin{bmatrix}\n"
"1 & 1 & 0 & 0 \\\\\n"
"0 & 0 & 1 & 1 \\\\\n"
"1 & 0 & 0 & 0 \\\\\n"
"0 & 0 & 1 & 1 \\\\\n"
"\\end{bmatrix}"
msgstr ""
"\\begin{bmatrix}\n"
"1 & 1 & 0 & 0 \\\\\n"
"0 & 0 & 1 & 1 \\\\\n"
"1 & 0 & 0 & 0 \\\\\n"
"0 & 0 & 1 & 1 \\\\\n"
"\\end{bmatrix}\n"
"\n"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The natural handoff point between these two problems are zeroed-out dense "
"tensors. Our inference solution is designed to compress and accelerate "
"tensors in this format. We anticipate many users coming up with custom "
"masking solution, as this is an active area of research."
msgstr ""
"这两个问题的自然交接点是零化的稠密张量。我们的推理解决方案旨在压缩和加速此格式中的张量。我们预计许多人会提出自定义屏蔽解决方案，因为这是一个活跃的研究领域。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now that we've learned a little more about semi-structured sparsity, let's "
"apply it to a BERT model trained on a question answering task, SQuAD."
msgstr "现在我们已经更多地了解了半结构化稀疏性，让我们将其应用于一个以问答任务训练过的BERT模型，即SQuAD。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Intro & Setup"
msgstr "介绍和设置"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Let's start by importing all the packages we need."
msgstr "让我们开始导入所有需要的包。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We'll also need to define some helper functions that are specific to the "
"dataset / task at hand. These were adapted from `this "
"<https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt>`_ huggingface "
"course as a reference."
msgstr ""
"我们还需要定义一些特定于数据集/任务的辅助函数。这些函数是参考`这个 <https://huggingface.co/learn/nlp-"
"course/chapter7/7?fw=pt>`_ huggingface课程进行改编的。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now that those are defined, we just need one additional helper function, "
"which will help us benchmark our model."
msgstr "现在这些都已定义，我们只需要一个额外的辅助函数，它将帮助我们对我们的模型进行基准测试。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We will get started by loading our model and tokenizer, and then setting up "
"our dataset."
msgstr "我们将从加载我们的模型和分词器开始，然后设置我们的数据集。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Next, we'll train a quick baseline of our model on SQuAD. This task asks our"
" model to identify spans, or segments of text, in a given context (Wikipedia"
" articles) that answer a given question. Running the following code gives me"
" an F1 score of 86.9. This is quite close to the reported NVIDIA score and "
"the difference is likely due to BERT-base vs. BERT-large or fine-tuning "
"hyperparams."
msgstr ""
"接下来，我们将在SQuAD上快速训练一个模型基线。此任务要求我们的模型在给定上下文（Wikipedia文章）中识别出回答给定问题的文本段或片段。运行以下代码让我得到了86.9的F1分数。这与NVIDIA报告的分数相当接近，差异可能是由于使用了BERT-"
"base与BERT-large或微调超参数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Pruning BERT to be 2:4 sparse"
msgstr "将BERT裁剪为2:4稀疏"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Now that we have our baseline, it's time we prune BERT. There are many "
"different pruning strategies, but one of the most common is **magnitude "
"pruning**, which seeks to remove the weights with the lowest L1 norm. "
"Magnitude pruning was used by NVIDIA in all their results and is a common "
"baseline."
msgstr ""
"现在我们有了基线，是时候裁剪BERT了。有许多不同的裁剪策略，但最常见的一种是**幅值裁剪**，其目标是移除L1范数最低的权重。NVIDIA在所有结果中都使用了幅值裁剪，这也是一种常见的基准。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To do this, we will use the ``torch.ao.pruning`` package, which contains a "
"weight-norm (magnitude) sparsifier. These sparsifiers work by applying mask "
"parameterizations to the weight tensors in a model. This lets them simulate "
"sparsity by masking out the pruned weights."
msgstr ""
"为此，我们将使用``torch.ao.pruning``包，该包包含一个权重范数（幅值）稀疏器。这些稀疏器通过对模型中的权重张量应用掩码参数化来工作。这使得它们可以通过掩码掉被裁剪的权重来模拟稀疏性。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We'll also have to decide what layers of the model to apply sparsity to, "
"which in this case is all of the `nn.Linear` layers, except for the task-"
"specific head outputs. That's because semi-structured sparsity has `shape "
"constraints <https://pytorch.org/docs/2.1/sparse.html#constructing-sparse-"
"semi-structured-tensors>`_, and the task-specific nn.Linear layers do not "
"satisfy them."
msgstr ""
"我们还必须决定将稀疏性应用于模型的哪些层，这种情况下是所有的`nn.Linear`层，除了任务特定的输出头部。这是因为半结构化稀疏性有`形状约束 "
"<https://pytorch.org/docs/2.1/sparse.html#constructing-sparse-semi-"
"structured-tensors>`_，而任务特定的nn.Linear层不满足这些约束。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The first step for pruning the model is to insert paramterizations for "
"masking the weights of the model. This is done by the prepare step. Anytime "
"we try to access the ``.weight`` we will get ``mask * weight`` instead."
msgstr ""
"裁剪模型的第一步是插入参数化以屏蔽模型的权重。这是通过准备步骤完成的。每当我们尝试访问``.weight``时，我们将得到``mask * "
"weight``。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Then, we'll take a single pruning step. All pruners implement a "
"``update_mask()`` method that updates the mask with the logic being "
"determined by the pruner implementation. The step method calls this "
"``update_mask`` functions for the weights specified in the sparse config."
msgstr ""
"然后，我们将进行一步裁剪。所有裁剪器都实现了一个``update_mask()``方法，该方法根据裁剪器实现的逻辑更新掩码。步骤方法为稀疏配置中指定的权重调用此``update_mask``函数。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"We will also evaluate the model to show the accuracy degradation of zero-"
"shot pruning, or pruning without fine-tuning / retraining."
msgstr "我们还将评估模型以显示零次裁剪，即未经微调/重新训练的裁剪的准确性降级。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this state, we can start fine-tuning the model, updating the elements "
"that wouldn't be pruned to better account for the accuracy loss. Once we've "
"reached a satisfied state, we can call ``squash_mask`` to fuse the mask and "
"the weight together. This will remove the parameterizations and we are left "
"with a zeroed-out 2:4 dense model."
msgstr ""
"在这个阶段，我们可以开始微调模型，更新那些不会被剪枝的元素，以更好地弥补准确度损失。一旦达到满意的状态，我们可以调用``squash_mask``来将掩码和权重融合在一起。这将移除参数化，我们最终将得到一个填充零的2:4稠密模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Accelerating 2:4 sparse models for inference "
"--------i------------------------------------ Now that we have a model in "
"this format, we can accelerate it for inference just like in the QuickStart "
"Guide."
msgstr ""
"加速2:4稀疏模型的推理 --------i------------------------------------ "
"现在我们已经拥有这种格式的模型，可以像快速入门指南中一样加速其推理。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Retraining our model after magnitude pruning has recovered nearly all of the"
" F1 that has been lost when the model was pruned. At the same time we have "
"achieved a 1.28x speedup for bs=16. Note that not all shapes are amenable to"
" performance improvements. When batch sizes are small and limited time is "
"spent in compute sparse kernels may be slower than their dense counterparts."
msgstr ""
"重量剪枝后的模型重新训练几乎恢复了所有在剪枝时丢失的F1。同时我们在bs=16的情况下，达到了1.28倍的速度提升。请注意，并不是所有形状都适合性能提升。当批量大小较小且计算稀疏内核所用时间有限时，稀疏模型的性能可能比稠密模型更慢。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "results"
msgstr "结果"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Metrics"
msgstr "指标"

#: ../../prototype/vulkan_workflow.rst:250
msgid "fp16"
msgstr "fp16"

#: ../../prototype/vulkan_workflow.rst:250
msgid "2:4 sparse"
msgstr "2:4稀疏"

#: ../../prototype/vulkan_workflow.rst:250
msgid "delta / speedup"
msgstr "差异 / 提速"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Exact Match (%)"
msgstr "精确匹配率 (%)"

#: ../../prototype/vulkan_workflow.rst:250
msgid "78.53"
msgstr "78.53"

#: ../../prototype/vulkan_workflow.rst:250
msgid "78.44"
msgstr "78.44"

#: ../../prototype/vulkan_workflow.rst:250
msgid "-0.09"
msgstr "-0.09"

#: ../../prototype/vulkan_workflow.rst:250
msgid "F1 (%)"
msgstr "F1率 (%)"

#: ../../prototype/vulkan_workflow.rst:250
msgid "86.93"
msgstr "86.93"

#: ../../prototype/vulkan_workflow.rst:250
msgid "86.49"
msgstr "86.49"

#: ../../prototype/vulkan_workflow.rst:250
msgid "-0.44"
msgstr "-0.44"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Time (bs=4)"
msgstr "时间 (bs=4)"

#: ../../prototype/vulkan_workflow.rst:250
msgid "10.93"
msgstr "10.93"

#: ../../prototype/vulkan_workflow.rst:250
msgid "12.62"
msgstr "12.62"

#: ../../prototype/vulkan_workflow.rst:250
msgid "0.87x"
msgstr "0.87倍"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Time (bs=16)"
msgstr "时间 (bs=16)"

#: ../../prototype/vulkan_workflow.rst:250
msgid "19.61"
msgstr "19.61"

#: ../../prototype/vulkan_workflow.rst:250
msgid "15.37"
msgstr "15.37"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.28x"
msgstr "1.28倍"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Time (bs=64)"
msgstr "时间 (bs=64)"

#: ../../prototype/vulkan_workflow.rst:250
msgid "73.19"
msgstr "73.19"

#: ../../prototype/vulkan_workflow.rst:250
msgid "58.70"
msgstr "58.70"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.25x"
msgstr "1.25倍"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Time (bs=256)"
msgstr "时间 (bs=256)"

#: ../../prototype/vulkan_workflow.rst:250
msgid "286.91"
msgstr "286.91"

#: ../../prototype/vulkan_workflow.rst:250
msgid "244.19"
msgstr "244.19"

#: ../../prototype/vulkan_workflow.rst:250
msgid "1.18x"
msgstr "1.18倍"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we have shown how to prune BERT to be 2:4 sparse and how "
"to accelerate a 2:4 sparse model for inference. By taking advantage of our "
"SparseSemiStructuredTensor subclass, we were able to achieve a 1.3x speedup "
"over the fp16 baseline. We also demonstrated the benefits of 2:4 sparsity by"
" fine-tuning BERT to recover any lost F1 (86.92 dense vs 86.48 sparse)."
msgstr ""
"在本教程中，我们展示了如何对BERT进行2:4稀疏剪枝以及如何加速一个2:4稀疏模型的推理。通过利用我们的SparseSemiStructuredTensor子类，我们实现了比fp16基线快1.3倍的速度提升。我们还通过微调BERT恢复了因2:4稀疏性导致的任何F1损失（86.92稠密"
" vs 86.48稀疏）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Skipping Module Parameter Initialization"
msgstr "跳过模块参数初始化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"When a module is created, its learnable parameters are initialized according"
" to a default initialization scheme associated with the module type. For "
"example, the `weight` parameter for a :class:`torch.nn.Linear` module is "
"initialized from a `uniform(-1/sqrt(in_features), 1/sqrt(in_features))` "
"distribution. If some other initialization scheme is desired, this has "
"traditionally required re-initializing the parameters after module "
"instantiation:"
msgstr ""
"当创建模块时，其可学习参数会根据与模块类型相关的默认初始化方案进行初始化。例如，:class:`torch.nn.Linear`模块的`weight`参数会从`uniform(-1/sqrt(in_features),"
" 1/sqrt(in_features))`分布初始化。如果希望使用其他初始化方案，这通常需要在模块实例化后重新初始化参数："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this case, the initialization done during construction is wasted "
"computation, and it may be non-trivial if the `weight` parameter is large."
msgstr "在这种情况下，构造时完成的初始化是浪费的计算，并且如果`weight`参数很大，可能会复杂而耗时。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Skipping Initialization"
msgstr "跳过初始化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"It is now possible to skip parameter initialization during module "
"construction, avoiding wasted computation. This is easily accomplished using"
" the :func:`torch.nn.utils.skip_init` function:"
msgstr ""
"现在可在模块构造过程中跳过参数初始化，从而避免浪费计算。这可以轻松实现，使用 :func:`torch.nn.utils.skip_init` 函数："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This can be applied to any module that satisfies the conditions described in"
" the :ref:`Updating` section below. Note that all modules provided by "
"`torch.nn` satisfy these conditions and thus support skipping init."
msgstr ""
"这可以应用于满足以下条件的任何模块（详见:ref:`更新`部分）。请注意，`torch.nn`提供的所有模块均满足这些条件，因此支持跳过初始化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Updating Modules to Support Skipping Initialization"
msgstr "更新模块以支持跳过初始化"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Due to the way :func:`torch.nn.utils.skip_init` is implemented (see "
":ref:`Details`), there are two requirements that a module must meet to be "
"compatible with the function. You can opt in to the parameter initialization"
" skipping functionality for your custom module simply by adhering to these "
"requirements:"
msgstr ""
"由于 :func:`torch.nn.utils.skip_init` "
"的实现方式（详见:ref:`细节`），模块必须满足两个要求才能与该函数兼容。您可以通过遵守这些要求，为您的自定义模块选择跳过参数初始化功能："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"1. The module must accept a `device` kwarg in its constructor that is passed"
" to any parameters or buffers created during construction."
msgstr "1. 模块的构造函数中必须接受一个`device`关键字参数，并将其传递给在构造期间创建的任何参数或缓冲区。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"2. The module must not perform any computation on parameters or buffers in "
"its constructor except initialization (i.e. functions from `torch.nn.init`)."
msgstr "2. 模块的构造函数中不能对参数或缓冲区进行除了初始化以外的任何计算（即，`torch.nn.init`中的函数）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The following example demonstrates a module updated to support the `device` "
"kwarg by passing it along to any created parameters, buffers, or submodules:"
msgstr "以下示例展示了一个支持 `device`关键字参数并将其传递给所创建的参数、缓冲区或子模块的模块："

#: ../../prototype/vulkan_workflow.rst:250
msgid "Implementation Details"
msgstr "实现细节"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Behind the scenes, the :func:`torch.nn.utils.skip_init` function is "
"implemented in terms of a two-step pattern:"
msgstr "在幕后，:func:`torch.nn.utils.skip_init` 函数是通过两步模式实现的："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"It works by instantiating the module onto a \"meta\" device, which has "
"tensor shape information but does not allocate any storage. The "
"`torch.nn.init` ops are specially implemented for this meta device so that "
"they have no-op behavior. This results in the parameter intialization logic "
"being essentially skipped."
msgstr ""
"它通过将模块实例化到一个\"meta\"设备上来工作，该设备具有张量形状信息但不分配任何存储空间。`torch.nn.init` "
"操作特别为这个meta设备实现，因此它们的行为是无操作的。这使得参数初始化逻辑实际上被跳过了。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note that this pattern only works for modules that properly support a "
"`device` kwarg during construction, as described in :ref:`Updating`."
msgstr "请注意，仅对构造中正确支持`device`关键字参数的模块，这种模式才能正常工作，如:ref:`更新`中所述。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_torchscript_freezing.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_prototype_torchscript_freezing.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Model Freezing in TorchScript"
msgstr "TorchScript中的模型冻结"

#: ../../prototype/vulkan_workflow.rst:250
msgid "TorchScript is no longer in active development."
msgstr "TorchScript不再处于活跃开发阶段。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we introduce the syntax for *model freezing* in "
"TorchScript. Freezing is the process of inlining Pytorch module parameters "
"and attributes values into the TorchScript internal representation. "
"Parameter and attribute values are treated as final values and they cannot "
"be modified in the resulting Frozen module."
msgstr ""
"在本教程中，我们介绍了TorchScript中的*模型冻结*语法。冻结是将Pytorch模块参数和属性值内联到TorchScript内部表示的过程。参数和属性值被视为最终值，它们不能在生成的冻结模块中修改。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Basic Syntax"
msgstr "基础语法"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Model freezing can be invoked using API below:"
msgstr "模型冻结可以通过以下API调用:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``torch.jit.freeze(mod : ScriptModule, names : str[]) -> ScriptModule``"
msgstr ""
"``torch.jit.freeze(mod : ScriptModule, names : str[]) -> ScriptModule``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Note the input module can either be the result of scripting or tracing. See "
"https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html"
msgstr ""
"注意，输入模块可以是通过脚本化或跟踪生成的结果。参阅 "
"https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Next, we demonstrate how freezing works using an example:"
msgstr "接下来，我们通过示例展示冻结如何工作："

#: ../../prototype/vulkan_workflow.rst:250
msgid "On my machine, I measured the time:"
msgstr "在我的机器上，我测量了时间:"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Scripted - Warm up time:  0.0107"
msgstr "脚本化模型 - 热身时间:  0.0107"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Frozen   - Warm up time:  0.0048"
msgstr "冻结模型   - 热身时间:  0.0048"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Scripted - Inference:  1.35"
msgstr "脚本化模型 - 推理时间:  1.35"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Frozen   - Inference time:  1.17"
msgstr "冻结模型   - 推理时间:  1.17"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In our example, warm up time measures the first two runs. The frozen model "
"is 50% faster than the scripted model. On some more complex models, we "
"observed even higher speed up of warm up time. freezing achieves this speed "
"up because it is doing some the work TorchScript has to do when the first "
"couple runs are initiated."
msgstr ""
"在我们的示例中，热身时间测量前两次运行。冻结模型比脚本化模型快了50%。在一些更复杂的模型上，我们观察到热身时间的更高提速。冻结实现这种提速是因为它完成了一些TorchScript在首次运行时必须做的工作。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Inference time measures inference execution time after the model is warmed "
"up. Although we observed significant variation in execution time, the frozen"
" model is often about 15% faster than the scripted model. When input is "
"larger, we observe a smaller speed up because the execution is dominated by "
"tensor operations."
msgstr ""
"推理时间测量在模型热身后推理执行时间。虽然我们观察到执行时间的显著变化，但冻结模型通常比脚本化模型快约15%。当输入较大时，我们观察到较小的提速，因为执行被"
" Tensor 操作主导。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this tutorial, we learned about model freezing. Freezing is a useful "
"technique to optimize models for inference and it also can significantly "
"reduce TorchScript warmup time."
msgstr "在本教程中，我们学习了模型冻结。冻结是一种优化推理模型的有用技术，同时还可以显著减少TorchScript的热身时间。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: torchscript_freezing.py "
"<torchscript_freezing.py>`"
msgstr ""
":download:`下载Python源代码: torchscript_freezing.py <torchscript_freezing.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: torchscript_freezing.ipynb "
"<torchscript_freezing.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: torchscript_freezing.ipynb "
"<torchscript_freezing.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"(prototype) Tracing-based Selective Build Mobile Interpreter in Android and "
"iOS"
msgstr "（原型）基于跟踪的选择性构建移动解释器（Android和iOS）"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial has been replaced with a newer tutorial on this topic: "
"https://pytorch.org/executorch/stable/kernel-library-selective-build.html"
msgstr ""
"本教程已被关于此主题的更详细教程所取代：https://pytorch.org/executorch/stable/kernel-library-"
"selective-build.html"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Click :ref:`here <sphx_glr_download_prototype_vmap_recipe.py>` to download "
"the full example code"
msgstr "点击 :ref:`此处 <sphx_glr_download_prototype_vmap_recipe.py>` 下载完整示例代码"

#: ../../prototype/vulkan_workflow.rst:250
msgid "torch.vmap"
msgstr "torch.vmap"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"This tutorial introduces torch.vmap, an autovectorizer for PyTorch "
"operations. torch.vmap is a prototype feature and cannot handle a number of "
"use cases; however, we would like to gather use cases for it to inform the "
"design. If you are considering using torch.vmap or think it would be really "
"cool for something, please contact us at "
"https://github.com/pytorch/pytorch/issues/42368."
msgstr ""
"本教程介绍torch.vmap，PyTorch操作的自动向量化工具。torch.vmap是一个原型功能，无法处理许多用例；然而，我们希望收集相关用例以指导其设计。如果您正在考虑使用torch.vmap或者认为它在某些方面会非常有用，请通过"
" https://github.com/pytorch/pytorch/issues/42368 联系我们。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "So, what is vmap?"
msgstr "那么，什么是vmap？"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"vmap is a higher-order function. It accepts a function `func` and returns a "
"new function that maps `func` over some dimension of the inputs. It is "
"highly inspired by JAX's vmap."
msgstr ""
"vmap是一个高阶函数。它接受一个函数`func`并返回一个新的函数，将`func`映射到输入的某个维度上。这受到JAX的vmap强烈启发。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Semantically, vmap pushes the \"map\" into PyTorch operations called by "
"`func`, effectively vectorizing those operations."
msgstr "从语义上讲，vmap将\"map\"应用到`func`调用的PyTorch操作中，有效地向量化这些操作。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The first use case for vmap is making it easier to handle batch dimensions "
"in your code. One can write a function `func` that runs on examples and then"
" lift it to a function that can take batches of examples with `vmap(func)`. "
"`func` however is subject to many restrictions:"
msgstr ""
"vmap的第一个用例是使处理代码中的批量维度更容易。您可以编写一个`func`函数，该函数在单个样本上运行，然后使用`vmap(func)`提升到支持处理样本批量的函数。然而，`func`受许多限制："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"it must be functional (one cannot mutate a Python data structure inside of "
"it), with the exception of in-place PyTorch operations."
msgstr "它必须是功能性的（不能在其中修改Python数据结构），除非是PyTorch的就地操作。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"batches of examples must be provided as Tensors. This means that vmap "
"doesn't handle variable-length sequences out of the box."
msgstr "样本批量必须以张量形式提供。这意味着vmap不能直接处理变长序列。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"One example of using `vmap` is to compute batched dot products. PyTorch "
"doesn't provide a batched `torch.dot` API; instead of unsuccessfully "
"rummaging through docs, use `vmap` to construct a new function:"
msgstr ""
"使用`vmap`的一个例子是计算批量点积。PyTorch没有提供批量`torch.dot` "
"API；与其在文档中找不到相关API，不如使用`vmap`构造一个新函数："

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`vmap` can be helpful in hiding batch dimensions, leading to a simpler model"
" authoring experience."
msgstr "`vmap`可以帮助隐藏批量维度，从而简化模型设计体验。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"`vmap` can also help vectorize computations that were previously difficult "
"or impossible to batch. This bring us to our second use case: batched "
"gradient computation."
msgstr "`vmap`还可以帮助向量化以前难以或几乎不可能批量化的计算。这将我们引导到第二个用例：批量梯度计算。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "https://github.com/pytorch/pytorch/issues/8304"
msgstr "https://github.com/pytorch/pytorch/issues/8304"

#: ../../prototype/vulkan_workflow.rst:250
msgid "https://github.com/pytorch/pytorch/issues/23475"
msgstr "https://github.com/pytorch/pytorch/issues/23475"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The PyTorch autograd engine computes vjps (vector-Jacobian products). Using "
"vmap, we can compute (batched vector) - jacobian products."
msgstr "PyTorch自动梯度引擎计算vjps（向量-雅可比积）。使用vmap，我们可以计算（批量向量）-雅可比积。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"One example of this is computing a full Jacobian matrix (this can also be "
"applied to computing a full Hessian matrix). Computing a full Jacobian "
"matrix for some function f: R^N -> R^N usually requires N calls to "
"`autograd.grad`, one per Jacobian row."
msgstr ""
"一个例子是计算一个完整的雅可比矩阵（这也可以应用于计算完整的海森矩阵）。计算某个函数f: R^N -> "
"R^N的一个完整雅可比矩阵通常需要N次调用`autograd.grad`，每次调用计算一个雅可比行。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The third main use case for vmap is computing per-sample-gradients. This is "
"something that the vmap prototype cannot handle performantly right now. "
"We're not sure what the API for computing per-sample-gradients should be, "
"but if you have ideas, please comment in "
"https://github.com/pytorch/pytorch/issues/7786."
msgstr ""
"vmap的第三个主要用例是计算每样本梯度。这是vmap原型目前无法高效处理的事情。我们还不确定计算每样本梯度的API应该是什么样，但如果您有任何想法，请在"
" https://github.com/pytorch/pytorch/issues/7786 上评论。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Python source code: vmap_recipe.py <vmap_recipe.py>`"
msgstr ":download:`下载Python源代码: vmap_recipe.py <vmap_recipe.py>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
":download:`Download Jupyter notebook: vmap_recipe.ipynb <vmap_recipe.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: vmap_recipe.ipynb <vmap_recipe.ipynb>`"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"PyTorch Vulkan Backend is no longer maintained. Please review the "
"`ExecuTorch Vulkan Delegate <https://pytorch.org/executorch/stable/native-"
"delegates-executorch-vulkan-delegate.html>`_ implementation instead."
msgstr ""
"PyTorch Vulkan后端不再维护。请查看`ExecuTorch Vulkan Delegate "
"<https://pytorch.org/executorch/stable/native-delegates-executorch-vulkan-"
"delegate.html>`_ 的实现。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "PyTorch Vulkan Backend User Workflow"
msgstr "PyTorch Vulkan后端用户工作流程"

#: ../../prototype/vulkan_workflow.rst:250
msgid "**Author**: `Ivan Kobzarev <https://github.com/IvanKobzarev>`_"
msgstr "**作者**: `Ivan Kobzarev <https://github.com/IvanKobzarev>`_"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"PyTorch 1.7 supports the ability to run model inference on GPUs that support"
" the Vulkan graphics and compute API. The primary target devices are mobile "
"GPUs on Android devices. The Vulkan backend can also be used on Linux, Mac, "
"and Windows desktop builds to use Vulkan devices like Intel integrated GPUs."
" This feature is in the prototype stage and is subject to change."
msgstr ""
"PyTorch "
"1.7支持在支持Vulkan图形和计算API的GPU上运行模型推理。主要目标设备是安卓设备上的移动GPU。Vulkan后端也可用于Linux、Mac和Windows桌面版本，以使用诸如英特尔集成GPU这样的Vulkan设备。该功能处于原型阶段，可能会发生变化。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Building PyTorch with Vulkan backend"
msgstr "构建带Vulkan后端的PyTorch"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Vulkan backend is not included by default. The main switch to include Vulkan"
" backend is cmake option ``USE_VULKAN``, that can be set by environment "
"variable ``USE_VULKAN``."
msgstr ""
"Vulkan后端默认不包括在内。包括Vulkan后端的主开关是cmake选项``USE_VULKAN``，可以通过环境变量``USE_VULKAN``设置。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To use PyTorch with Vulkan backend, we need to build it from source with "
"additional settings. Checkout the PyTorch source code from GitHub master "
"branch."
msgstr "要使用带Vulkan后端的PyTorch，需要从源码构建并设置额外配置。从GitHub主分支检出PyTorch源码。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Optional usage of vulkan wrapper"
msgstr "Vulkan包装器的可选使用"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"By default, Vulkan library will be loaded at runtime using the "
"vulkan_wrapper library. If you specify the environment variable "
"``USE_VULKAN_WRAPPER=0`` libvulkan will be linked directly."
msgstr ""
"默认情况下，Vulkan库将在运行时通过vulkan_wrapper库加载。如果您指定环境变量``USE_VULKAN_WRAPPER=0``，libvulkan将直接链接。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Desktop build"
msgstr "桌面构建"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Vulkan SDK"
msgstr "Vulkan SDK"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Download VulkanSDK from https://vulkan.lunarg.com/sdk/home and set "
"environment variable ``VULKAN_SDK``"
msgstr "从https://vulkan.lunarg.com/sdk/home下载VulkanSDK并设置环境变量``VULKAN_SDK``"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Unpack VulkanSDK to ``VULKAN_SDK_ROOT`` folder, install VulkanSDK following "
"VulkanSDK instructions for your system."
msgstr ""
"解压 VulkanSDK 到 ``VULKAN_SDK_ROOT`` 文件夹，根据您系统的 VulkanSDK 指南安装 VulkanSDK。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "For Mac:"
msgstr "针对 Mac:"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Building PyTorch:"
msgstr "构建 PyTorch:"

#: ../../prototype/vulkan_workflow.rst:250
msgid "For Linux:"
msgstr "针对 Linux:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After successful build, open another terminal and verify the version of "
"installed PyTorch."
msgstr "成功构建后，打开另一个终端并验证已安装的 PyTorch 版本。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"At the time of writing of this recipe, the version is 1.8.0a0+41237a4. You "
"might be seeing different numbers depending on when you check out the code "
"from master, but it should be greater than 1.7.0."
msgstr ""
"在撰写此教程时，版本为 1.8.0a0+41237a4。根据您从 master 分支检出的代码时间可能会看到不同的版本号，但应该大于 1.7.0。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Android build"
msgstr "Android 构建"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"To build LibTorch for android with Vulkan backend for specified "
"``ANDROID_ABI``."
msgstr "使用 Vulkan 后端为指定的 ``ANDROID_ABI`` 构建 LibTorch 以用于安卓。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "To prepare pytorch_android aars that you can use directly in your app:"
msgstr "准备 pytorch_android aars，可直接在您的应用中使用:"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Model preparation"
msgstr "模型准备"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Install torchvision, get the default pretrained float model."
msgstr "安装 torchvision，获取默认的预训练浮点模型。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Python script to save pretrained mobilenet_v2 to a file:"
msgstr "保存预训练的 mobilenet_v2 到文件的 Python 脚本如下:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"PyTorch 1.7 Vulkan backend supports only float 32bit operators. The default "
"model needs additional step that will optimize operators fusing"
msgstr "PyTorch 1.7 的 Vulkan 后端仅支持 32 位浮点运算符。默认模型需要额外的优化步骤以进行算子融合。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The result model can be used only on Vulkan backend as it contains specific "
"to the Vulkan backend operators."
msgstr "结果模型只能用于 Vulkan 后端，因为它包含专门针对 Vulkan 后端的算子。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"By default, ``optimize_for_mobile`` with ``backend='vulkan'`` rewrites the "
"graph so  that inputs are transferred to the Vulkan backend, and outputs are"
" transferred to the CPU backend, therefore, the model can be run on CPU "
"inputs and produce CPU outputs. To disable this, add the argument "
"``optimization_blocklist={MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER}``"
" to ``optimize_for_mobile``. (``MobileOptimizerType`` can be imported from "
"``torch.utils.mobile_optimizer``)"
msgstr ""
"默认情况下，``optimize_for_mobile`` 的 ``backend=&apos;vulkan&apos;`` 会重写图形，使得输入转到 "
"Vulkan 后端，输出转到 CPU 后端，因此模型可以在 CPU 输入上运行并生成 CPU 输出。若关闭此功能，请在 "
"``optimize_for_mobile`` 中添加参数 "
"``optimization_blocklist={MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER}``。(``MobileOptimizerType``"
" 可以从 ``torch.utils.mobile_optimizer`` 导入)"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For more information, see the `torch.utils.mobile_optimizer` `API "
"documentation <https://pytorch.org/docs/stable/mobile_optimizer.html>`_."
msgstr ""
"查看更多信息，请参阅 `torch.utils.mobile_optimizer` 的 `API 文档 "
"<https://pytorch.org/docs/stable/mobile_optimizer.html>`。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Using Vulkan backend in code"
msgstr "代码中使用 Vulkan 后端"

#: ../../prototype/vulkan_workflow.rst:250
msgid "C++ API"
msgstr "C++ API"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``at::is_vulkan_available()`` function tries to initialize Vulkan backend "
"and if Vulkan device is successfully found and context is created - it will "
"return true, false otherwise."
msgstr ""
"``at::is_vulkan_available()`` 函数尝试初始化 Vulkan 后端，如果成功找到 Vulkan 设备并创建上下文，它将返回 "
"true，否则为 false。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``.vulkan()`` function called on Tensor will copy tensor to Vulkan device, "
"and for operators called with this tensor as input - the operator will run "
"on Vulkan device, and its output will be on the Vulkan device."
msgstr ""
"在张量上调用 ``.vulkan()`` 函数时，张量将被复制到 Vulkan 设备，对于以此张量为输入调用的运算符，它将在 Vulkan "
"设备上运行，其输出也将位于 Vulkan 设备上。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``.cpu()`` function called on Vulkan tensor will copy its data to CPU tensor"
" (default)"
msgstr "在 Vulkan 张量上调用 ``.cpu()`` 函数时，张量的数据将被复制到 CPU 张量（默认设置）。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Operators called with a tensor on a Vulkan device as an input will be "
"executed on a Vulkan device. If an operator is not supported for the Vulkan "
"backend the exception will be thrown."
msgstr "以 Vulkan 设备上的张量为输入调用的运算符将会在 Vulkan 设备上执行。如果某个运算符不支持 Vulkan 后端，将会抛出异常。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "List of supported operators:"
msgstr "支持运算符列表:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Those operators allow to use torchvision models for image classification on "
"Vulkan backend."
msgstr "这些运算符允许在 Vulkan 后端使用 torchvision 模型进行图像分类。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Python API"
msgstr "Python API"

#: ../../prototype/vulkan_workflow.rst:250
msgid "``torch.is_vulkan_available()`` is exposed to Python API."
msgstr "``torch.is_vulkan_available()`` 已在 Python API 中公开。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``tensor.to(device='vulkan')`` works as ``.vulkan()`` moving tensor to the "
"Vulkan device."
msgstr ""
"``tensor.to(device=&apos;vulkan&apos;)`` 的效果类似于 ``.vulkan()``，将张量移至 Vulkan "
"设备。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"``.vulkan()`` at the moment of writing of this tutorial is not exposed to "
"Python API, but it is planned to be there."
msgstr "截至撰写此教程时，``.vulkan()`` 尚未公开给 Python API，但计划添加。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Android Java API"
msgstr "Android Java API"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"For Android API to run model on Vulkan backend we have to specify this "
"during model loading:"
msgstr "在加载模型时，我们需要指定模型将在 Vulkan 后端运行:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"In this case, all inputs will be transparently copied from CPU to the Vulkan"
" device, and model will be run on Vulkan device, the output will be copied "
"transparently to CPU."
msgstr "在这种情况下，所有输入会透明地从 CPU 复制到 Vulkan 设备，模型将在 Vulkan 设备上运行，输出也会透明地复制到 CPU。"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"The example of using Vulkan backend can be found in test application within "
"the PyTorch repository: "
"https://github.com/pytorch/pytorch/blob/master/android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java#L133"
msgstr ""
"有关使用 Vulkan 后端的示例可以在 PyTorch 仓库中的测试应用程序找到: "
"https://github.com/pytorch/pytorch/blob/master/android/test_app/app/src/main/java/org/pytorch/testapp/MainActivity.java#L133"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Building android test app with Vulkan"
msgstr "构建带 Vulkan 的安卓测试应用"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Or if you need only specific abi you can set it as an argument:"
msgstr "或者如果您只需要指定的 abi，可以将其作为参数设置:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Add prepared model ``mobilenet2-vulkan.pt`` to test applocation assets:"
msgstr "将准备好的模型 ``mobilenet2-vulkan.pt`` 添加到测试应用资产中:"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"After successful installation, the application with the name 'MBQ' can be "
"launched on the device."
msgstr "成功安装后，可以在设备上启动名为 &apos;MBQ&apos; 的应用程序。"

#: ../../prototype/vulkan_workflow.rst:250
msgid "Testing models without uploading to android device"
msgstr "无需上传到安卓设备即可测试模型"

#: ../../prototype/vulkan_workflow.rst:250
msgid ""
"Software implementations of Vulkan (e.g. "
"https://swiftshader.googlesource.com/SwiftShader ) can be used to test if a "
"model can be run using PyTorch Vulkan Backend (e.g. check if all model "
"operators are supported)."
msgstr ""
"Vulkan 软件实现（例如 https://swiftshader.googlesource.com/SwiftShader "
"）可用于测试模型是否可以使用 PyTorch Vulkan 后端运行（例如检查是否支持所有模型运算符）。"
