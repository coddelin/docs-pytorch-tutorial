#
msgid ""
msgstr ""

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Getting Started with Fully Sharded Data Parallel(FSDP)"
msgstr "入门全分片数据并行（FSDP）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Author**: `Hamid Shojanazeri <https://github.com/HamidShojanazeri>`__, "
"`Yanli Zhao <https://github.com/zhaojuanmao>`__, `Shen Li "
"<https://mrshenli.github.io/>`__"
msgstr ""
"**作者**：`Hamid Shojanazeri <https://github.com/HamidShojanazeri>`__，`Yanli "
"Zhao <https://github.com/zhaojuanmao>`__，`Shen Li "
"<https://mrshenli.github.io/>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FSDP1 is deprecated. Please check out `FSDP2 tutorial "
"<https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`_."
msgstr ""
"FSDP1 已被弃用。请查看 `FSDP2 教程 "
"<https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Training AI models at a large scale is a challenging task that requires a "
"lot of compute power and resources. It also comes with considerable "
"engineering complexity to handle the training of these very large models. "
"`PyTorch FSDP <https://pytorch.org/blog/introducing-pytorch-fully-sharded-"
"data-parallel-api/>`__, released in PyTorch 1.11 makes this easier."
msgstr ""
"在大规模训练 AI 模型是一项具有挑战性的任务，需要大量的计算能力和资源。此外，还需要处理这些超大型模型的训练所需的显著工程复杂性。`PyTorch "
"FSDP <https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-"
"parallel-api/>`__ 在 PyTorch 1.11 中发布，使这一过程变得更加简单。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we show how to use `FSDP APIs "
"<https://pytorch.org/docs/stable/fsdp.html>`__, for simple MNIST models that"
" can be extended to other larger models such as `HuggingFace BERT models "
"<https://huggingface.co/blog/zero-deepspeed-fairscale>`__, `GPT 3 models up "
"to 1T parameters <https://pytorch.medium.com/training-a-1-trillion-"
"parameter-model-with-pytorch-fully-sharded-data-parallel-on-"
"aws-3ac13aa96cff>`__ . The sample DDP MNIST code courtesy of `Patrick Hu "
"<https://github.com/yqhu/>`_."
msgstr ""
"在本教程中，我们展示了如何使用 `FSDP API <https://pytorch.org/docs/stable/fsdp.html>`__ "
"来训练简单的 MNIST 模型，该方法也可以扩展到其他大型模型，例如 `HuggingFace BERT 模型 "
"<https://huggingface.co/blog/zero-deepspeed-fairscale>`__，`支持高达 1T 参数的 GPT-3"
" 模型 <https://pytorch.medium.com/training-a-1-trillion-parameter-model-with-"
"pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff>`__ 。MNIST 的 DDP "
"示例代码由 `Patrick Hu <https://github.com/yqhu/>`__ 提供。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How FSDP works"
msgstr "FSDP 的工作原理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__,"
" (DDP) training, each process/ worker owns a replica of the model and "
"processes a batch of data, finally it uses all-reduce to sum up gradients "
"over different workers. In DDP the model weights and optimizer states are "
"replicated across all workers. FSDP is a type of data parallelism that "
"shards model parameters, optimizer states and gradients across DDP ranks."
msgstr ""
"在 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__（DDP）训练中，每个进程/工作器拥有模型的副本并处理一批数据，最终通过"
" all-reduce 在不同工作器上汇总梯度。在 DDP 中，模型权重和优化器状态会在所有工作器之间复制。FSDP "
"是一种数据并行性，它将模型参数、优化器状态和梯度分片到 DDP 排名中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When training with FSDP, the GPU memory footprint is smaller than when "
"training with DDP across all workers. This makes the training of some very "
"large models feasible by allowing larger models or batch sizes to fit on "
"device. This comes with the cost of increased communication volume. The "
"communication overhead is reduced by internal optimizations like overlapping"
" communication and computation."
msgstr ""
"使用 FSDP 进行训练时，GPU 的内存占用比使用 DDP "
"低。这使得一些超大型模型的训练变得可行，因为允许更大的模型或批量尺寸适合于设备。但这会带来更大通信开销的成本。通信开销通过如通信与计算重叠等内部优化降低。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP workflow"
msgstr "FSDP 工作流程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP Workflow"
msgstr "FSDP 工作流程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "At a high level FSDP works as follow:"
msgstr "从高层次来看，FSDP 工作如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*In constructor*"
msgstr "*在构造函数中*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Shard model parameters and each rank only keeps its own shard"
msgstr "分片模型参数，每个排名仅保留自己的分片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*In forward path*"
msgstr "*在前向路径中*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Run all_gather to collect all shards from all ranks to recover the full "
"parameter in this FSDP unit"
msgstr "运行 all_gather，从所有排名中收集所有分片以恢复这个 FSDP 单元的完整参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Run forward computation"
msgstr "运行前向计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Discard parameter shards it has just collected"
msgstr "丢弃刚刚收集的参数分片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*In backward path*"
msgstr "*在反向路径中*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Run backward computation"
msgstr "运行反向计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Run reduce_scatter to sync gradients"
msgstr "运行 reduce_scatter 同步梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Discard parameters."
msgstr "丢弃参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One way to view FSDP's sharding is to decompose the DDP gradient all-reduce "
"into reduce-scatter and all-gather. Specifically, during the backward pass, "
"FSDP reduces and scatters gradients, ensuring that each rank possesses a "
"shard of the gradients. Then it updates the corresponding shard of the "
"parameters in the optimizer step. Finally, in the subsequent forward pass, "
"it performs an all-gather operation to collect and combine the updated "
"parameter shards."
msgstr ""
"一种查看 FSDP 分片的方法是将 DDP 梯度 all-reduce 分解为 reduce-scatter 和 all-"
"gather。具体来说，反向过程时，FSDP "
"归约并分发梯度，确保每个排名拥有梯度的一个分片。然后在优化器步骤中更新与对应参数分片相关的内容。最后，在随后的前向过程中，它执行 all-gather "
"运算来收集和组合更新后的参数分片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP allreduce"
msgstr "FSDP allreduce"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP Allreduce"
msgstr "FSDP Allreduce"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to use FSDP"
msgstr "如何使用 FSDP"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here we use a toy model to run training on the MNIST dataset for "
"demonstration purposes. The APIs and logic can be applied to training larger"
" models as well."
msgstr "这里我们使用一个示例模型在 MNIST 数据集上运行训练，以作为演示用途。这些 API 和逻辑也可以应用于训练更大的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*Setup*"
msgstr "*设置*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.1 Install PyTorch along with Torchvision"
msgstr "1.1 安装 PyTorch 和 Torchvision"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"See the `Get Started guide <https://pytorch.org/get-started/locally/>`__ for"
" information on installation."
msgstr "有关安装信息，请参考 `入门指南 <https://pytorch.org/get-started/locally/>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We add the following code snippets to a python script “FSDP_mnist.py”."
msgstr "我们将在 Python 脚本“FSDP_mnist.py”中添加以下代码片段。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.2  Import necessary packages"
msgstr "1.2 导入必要的包"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial is intended for PyTorch versions 1.12 and later. If you are "
"using an earlier version, replace all instances of "
"`size_based_auto_wrap_policy` with `default_auto_wrap_policy` and "
"`fsdp_auto_wrap_policy` with `auto_wrap_policy`."
msgstr ""
"本教程适用于 PyTorch 1.12 及以上版本。如果您使用的是更早版本，请将所有的 `size_based_auto_wrap_policy` "
"替换为 `default_auto_wrap_policy`，并将 `fsdp_auto_wrap_policy` 替换为 "
"`auto_wrap_policy`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"1.3 Distributed training setup. As we mentioned FSDP is a type of data "
"parallelism which requires a distributed training environment, so here we "
"use two helper functions to initialize the processes for distributed "
"training and clean up."
msgstr ""
"1.3 分布式训练设置。如上提到的，FSDP 是一种需要分布式训练环境的数据并行方式，因此我们在这里使用两个辅助函数来初始化分布式训练的进程并清理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.1  Define our toy model for handwritten digit classification."
msgstr "2.1 定义用于手写数字分类的示例模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.2 Define a train function"
msgstr "2.2 定义训练函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.3 Define a validation function"
msgstr "2.3 定义验证函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.4 Define a distributed train function that wraps the model in FSDP"
msgstr "2.4 定义一个分布式训练函数，用于在 FSDP 中包装模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Note: to save the FSDP model, we need to call the state_dict on each rank "
"then on Rank 0 save the overall states.**"
msgstr "**注意：要保存 FSDP 模型，我们需要在每个排名调用 state_dict，然后在排名 0 保存整体状态。**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.5 Finally, parse the arguments and set the main function"
msgstr "2.5 最后，解析参数并设置主函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have recorded cuda events to measure the time of FSDP model specifics. "
"The CUDA event time was 110.85 seconds."
msgstr "我们记录了 CUDA 事件以测量 FSDP 模型的特定时间。CUDA 事件时间为 110.85 秒。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Wrapping the model with FSDP, the model will look as follows, we can see the"
" model has been wrapped in one FSDP unit. Alternatively, we will look at "
"adding the auto_wrap_policy next and will discuss the differences."
msgstr ""
"使用 FSDP 包裹模型后，模型结构如下，我们可以看到模型已被包装为一个 FSDP 单元。或者，我们会在之后添加 auto_wrap_policy "
"并讨论区别。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following is the peak memory usage from FSDP MNIST training on "
"g4dn.12.xlarge AWS EC2 instance with 4 GPUs captured from PyTorch Profiler."
msgstr ""
"以下是在 g4dn.12.xlarge AWS EC2 实例上的 4 GPUs 训练 FSDP MNIST 时通过 PyTorch Profiler "
"捕获的峰值内存使用情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP peak memory"
msgstr "FSDP 峰值内存"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP Peak Memory Usage"
msgstr "FSDP 峰值内存使用情况"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Applying *auto_wrap_policy* in FSDP otherwise, FSDP will put the entire "
"model in one FSDP unit, which will reduce computation efficiency and memory "
"efficiency. The way it works is that, suppose your model contains 100 Linear"
" layers. If you do FSDP(model), there will only be one FSDP unit which wraps"
" the entire model. In that case, the allgather would collect the full "
"parameters for all 100 linear layers, and hence won't save CUDA memory for "
"parameter sharding. Also, there is only one blocking allgather call for the "
"all 100 linear layers, there will not be communication and computation "
"overlapping between layers."
msgstr ""
"在 FSDP 中应用 *auto_wrap_policy* 否则 FSDP 会将整个模型放入一个 FSDP "
"单元，这会降低计算效率和内存效率。其工作方式是，假设模型包含 100 个线性层。如果直接调用 FSDP(model)，将只有一个 FSDP "
"单元包装整个模型。在这种情况下，allgather 会收集所有 100 个线性层的完整参数，因此不会节省用于参数分片的 CUDA 内存。此外，所有 "
"100 个线性层只有一个阻塞的 allgather 调用，因此层之间的通信与计算不会发生重叠。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To avoid that, you can pass in an auto_wrap_policy, which will seal the "
"current FSDP unit and start a new one automatically when the specified "
"condition is met (e.g., size limit). In that way you will have multiple FSDP"
" units, and only one FSDP unit needs to collect full parameters at a time. "
"E.g., suppose you have 5 FSDP units, and each wraps 20 linear layers. Then, "
"in the forward, the 1st FSDP unit will allgather parameters for the first 20"
" linear layers, do computation, discard the parameters and then move on to "
"the next 20 linear layers. So, at any point in time, each rank only "
"materializes parameters/grads for 20 linear layers instead of 100."
msgstr ""
"为了避免这种情况，可以传递 auto_wrap_policy，当满足指定条件（例如大小限制）时会自动封闭当前 FSDP 单元并启动新单元。这样就会有多个"
" FSDP 单元，每次只有一个 FSDP 单元需要收集完整参数。例如，假设有 5 个 FSDP 单元，每个单元包裹 20 个线性层。在前向过程时，第一个"
" FSDP 单元会 allgather 20 个线性层的参数，进行计算，丢弃参数然后继续处理接下来的 20 个线性层。因此，在任何时刻，每个排名只实际化"
" 20 个线性层的参数/梯度，而不是 100 个。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To do so in 2.4 we define the auto_wrap_policy and pass it to FSDP wrapper, "
"in the following example, my_auto_wrap_policy defines that a layer could be "
"wrapped or sharded by FSDP if the number of parameters in this layer is "
"larger than 100. If the number of parameters in this layer is smaller than "
"100, it will be wrapped with other small layers together by FSDP. Finding an"
" optimal auto wrap policy is challenging, PyTorch will add auto tuning for "
"this config in the future. Without an auto tuning tool, it is good to "
"profile your workflow using different auto wrap policies experimentally and "
"find the optimal one."
msgstr ""
"为了在 2.4 中做到这一点，我们定义 auto_wrap_policy 并将其传递给 FSDP "
"包裹器，在下列示例中，my_auto_wrap_policy 定义一个层可以通过 FSDP 包裹或分片，如果该层中的参数数量大于 "
"100。若该层中的参数数量小于 100，将与其他小层一起通过 FSDP 包裹。找到优化的自动包裹策略是一项挑战，PyTorch "
"将来会为这个配置提供自动调优支持。在目前没有自动调优工具的情况下，使用实验性方式针对不同的自动包裹策略进行性能分析并找到最优策略是个好方法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Applying the auto_wrap_policy, the model would be as follows:"
msgstr "应用 auto_wrap_policy 后，模型将如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following is the peak memory usage from FSDP with auto_wrap policy of "
"MNIST training on a g4dn.12.xlarge AWS EC2 instance with 4 GPUs captured "
"from PyTorch Profiler. It can be observed that the peak memory usage on each"
" device is smaller compared to FSDP without auto wrap policy applied, from "
"~75 MB to 66 MB."
msgstr ""
"以下是在 g4dn.12.xlarge AWS EC2 实例上的 4 GPUs 训练应用了 auto_wrap_policy 的 FSDP MNIST "
"时从 PyTorch Profiler 捕获的峰值内存使用情况。可以看到与未应用自动包裹策略的 FSDP 相比，每个设备上的峰值内存使用更小，从约 75"
" MB 降至 66 MB。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP Peak Memory Usage using Auto_wrap policy"
msgstr "使用 Auto_wrap policy 的 FSDP 峰值内存使用情况"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"*CPU Off-loading*: In case the model is very large that even with FSDP "
"wouldn't fit into GPUs, then CPU offload can be helpful here."
msgstr "*CPU 卸载*：如果模型非常大，即使使用 FSDP 也无法适配于 GPU，那么 CPU 卸载会在此处有帮助。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Currently, only parameter and gradient CPU offload is supported. It can be "
"enabled via passing in cpu_offload=CPUOffload(offload_params=True)."
msgstr ""
"当前，仅支持参数和梯度的 CPU 卸载。可以通过传递 cpu_offload=CPUOffload(offload_params=True) 来启用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that this currently implicitly enables gradient offloading to CPU in "
"order for params and grads to be on the same device to work with the "
"optimizer. This API is subject to change. The default is None in which case "
"there will be no offloading."
msgstr ""
"请注意，这目前已隐式启用了梯度卸载到 CPU，以便参数和梯度可在相同设备上与优化器配合工作。此 API 可能会更改。默认值为 "
"None，这种情况下不会进行卸载。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using this feature may slow down the training considerably, due to frequent "
"copying of tensors from host to device, but it could help improve memory "
"efficiency and train larger scale models."
msgstr "使用此功能可能会显著减慢训练速度，因为需要将张量频繁从主机复制到设备，但它可以帮助提高内存效率并训练更大规模的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In 2.4 we just add it to the FSDP wrapper"
msgstr "在 2.4 中我们只需将它添加到 FSDP 包裹器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compare it with DDP, if in 2.4 we just normally wrap the model in DPP, "
"saving the changes in “DDP_mnist.py”."
msgstr "与 DDP 比较，如果在 2.4 中仅正常地将模型包装到 DDP，则将更改保存到“DDP_mnist.py”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following is the peak memory usage from DDP MNIST training on "
"g4dn.12.xlarge AWS EC2 instance with 4 GPUs captured from PyTorch profiler."
msgstr ""
"以下是在 g4dn.12.xlarge AWS EC2 实例上的 4 GPUs 训练 DDP MNIST 时通过 PyTorch Profiler "
"捕获的峰值内存使用情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "DDP Peak Memory Usage using Auto_wrap policy"
msgstr "使用 Auto_wrap policy 的 DDP 峰值内存使用情况"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Considering the toy example and tiny MNIST model we defined here, we can "
"observe the difference between peak memory usage of DDP and FSDP. In DDP "
"each process holds a replica of the model, so the memory footprint is higher"
" compared to FSDP which shards the model parameters, optimizer states and "
"gradients over DDP ranks. The peak memory usage using FSDP with auto_wrap "
"policy is the lowest followed by FSDP and DDP."
msgstr ""
"考虑到我们在这里定义的示例和小的 MNIST 模型，我们可以观察到 DDP 和 FSDP 的峰值内存使用差异。在 DDP "
"中，每个进程持有一个模型的副本，因此与 FSDP 分片模型参数、优化器状态和梯度的情况相比，内存占用更高。使用 auto_wrap policy 的 "
"FSDP 峰值内存使用是最低的，其次是 FSDP 和 DDP。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Also, looking at timings, considering the small model and running the "
"training on a single machine, FSDP with and without auto_wrap policy "
"performed almost as fast as DDP. This example does not represent most of the"
" real applications, for detailed analysis and comparison between DDP and "
"FSDP please refer to this `blog post  "
"<https://pytorch.medium.com/6c8da2be180d>`__ ."
msgstr ""
"此外，从时间来看，考虑模型较小并在单机上运行训练时，启用和未启用 auto_wrap policy 的 FSDP 表现几乎与 DDP "
"一样快。这个示例不代表大多数真实应用。有关 DDP 和 FSDP 的详细分析和比较，请参考该 `博客文章  "
"<https://pytorch.medium.com/6c8da2be180d>`__ 。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "edit"
msgstr "编辑"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Advanced Model Training with Fully Sharded Data Parallel (FSDP)"
msgstr "基于全分片数据并行 (FSDP) 的高级模型训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Author**: `Hamid Shojanazeri <https://github.com/HamidShojanazeri>`__, "
"`Less Wright <https://github.com/lessw2020>`__, `Rohan Varma "
"<https://github.com/rohan-varma/>`__, `Yanli Zhao "
"<https://github.com/zhaojuanmao>`__"
msgstr ""
"**作者**：`Hamid Shojanazeri <https://github.com/HamidShojanazeri>`__，`Less "
"Wright <https://github.com/lessw2020>`__，`Rohan Varma "
"<https://github.com/rohan-varma/>`__，`Yanli Zhao "
"<https://github.com/zhaojuanmao>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"What you will learn"
msgstr ""
"您将学习的内容"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch's Fully Sharded Data Parallel Module: A wrapper for sharding module "
"parameters across"
msgstr "PyTorch 的完全分片数据并行模块：一个用于在"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "data parallel workers."
msgstr "数据并行工作者之间分片模块参数的封装。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Prerequisites"
msgstr ""
"前置条件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch 1.12 or later"
msgstr "PyTorch 1.12 或更高版本"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Read about the `FSDP API <https://pytorch.org/docs/main/fsdp.html>`__."
msgstr "阅读 `FSDP API <https://pytorch.org/docs/main/fsdp.html>`__ 的相关内容。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial introduces more advanced features of Fully Sharded Data "
"Parallel (FSDP) as part of the PyTorch 1.12 release. To get familiar with "
"FSDP, please refer to the `FSDP getting started tutorial "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__."
msgstr ""
"本教程介绍了 PyTorch 1.12 版本中完全分片数据并行(FSDP)的一些高级功能。要了解 FSDP 的基础知识，请参阅 `FSDP 入门教程 "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we fine-tune a HuggingFace (HF) T5 model with FSDP for "
"text summarization as a working example."
msgstr "在本教程中，我们以实际示例，通过使用 FSDP 对 HuggingFace (HF) T5 模型进行文本摘要微调。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The example uses Wikihow and for simplicity, we will showcase the training "
"on a single node, P4dn instance with 8 A100 GPUs. We now have several blog "
"posts ( `(link1), <https://pytorch.org/blog/introducing-pytorch-fully-"
"sharded-data-parallel-api/>`__ `(link2) "
"<https://engineering.fb.com/2021/07/15/open-source/fsdp/>`__) and a `paper "
"<https://arxiv.org/abs/2304.11277>`__ on large scale FSDP training on a "
"multi-node cluster."
msgstr ""
"示例使用了 Wikihow 数据集，为了简单起见，我们将在一台节点上展示训练过程，该节点是具有 8 个 A100 GPU 的 P4dn "
"实例。目前，我们已有多个关于多节点集群上大规模 FSDP 训练的博客文章 (`(链接1), "
"<https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-"
"api/>`__ `(链接2) <https://engineering.fb.com/2021/07/15/open-"
"source/fsdp/>`__) 和一篇 `论文 <https://arxiv.org/abs/2304.11277>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FSDP is a production ready package with focus on ease of use, performance, "
"and long-term support.  One of the main benefits of FSDP is reducing the "
"memory footprint on each GPU. This enables training of larger models with "
"lower total memory vs DDP, and leverages the overlap of computation and "
"communication to train models efficiently. This reduced memory pressure can "
"be leveraged to either train larger models or increase batch size, "
"potentially helping overall training throughput.  You can read more about "
"PyTorch FSDP `here <https://pytorch.org/blog/introducing-pytorch-fully-"
"sharded-data-parallel-api/>`__."
msgstr ""
"FSDP 是一个面向生产环境的软件包，着重于易用性、性能和长期支持。FSDP 的主要优势之一是减少每个 GPU "
"的内存占用。这使得可以用较少的总内存训练更大的模型，并利用计算和通信的重叠来高效训练模型。减少的内存压力可以用于训练更大的模型或增加批量大小，从而可能提高整体训练吞吐量。您可以在"
" `此处 <https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-"
"parallel-api/>`__ 阅读有关 PyTorch FSDP 的更多信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP Features in This Tutorial"
msgstr "本教程中的 FSDP 功能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Transformer Auto Wrap Policy"
msgstr "Transformer 自动封装策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Mixed Precision"
msgstr "混合精度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Initializing FSDP Model on Device"
msgstr "在设备上初始化 FSDP 模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sharding Strategy"
msgstr "分片策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Backward Prefetch"
msgstr "反向预取"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model Checkpoint Saving via Streaming to CPU"
msgstr "通过流式传输到 CPU 保存模型检查点"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Recap on How FSDP Works"
msgstr "FSDP 的工作原理回顾"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "At a high level FDSP works as follow:"
msgstr "从整体来看，FDSP 的工作流程如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*In the constructor*"
msgstr "*在构造函数中*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*In the forward pass*"
msgstr "*在前向传递中*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Run `all_gather` to collect all shards from all ranks to recover the full "
"parameter for this FSDP unit and run the forward computation"
msgstr "运行 `all_gather` 从所有排名中收集所有分片以恢复完整参数，用于该 FSDP 单元并运行前向计算"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Discard the non-owned parameter shards it has just collected to free memory"
msgstr "丢弃其刚刚收集的非拥有分片以释放内存"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*In the backward pass*"
msgstr "*在反向传递中*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Run `all_gather` to collect all shards from all ranks to recover the full "
"parameter in this FSDP unit and run backward computation"
msgstr "运行 `all_gather` 从所有排名中收集所有分片以恢复该 FSDP 单元中的完整参数并运行反向计算"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Discard non-owned parameters to free memory."
msgstr "丢弃非拥有参数以释放内存。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fine-tuning HF T5"
msgstr "HF T5 微调"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"HF T5 pre-trained models are available in four different sizes, ranging from"
" small with 60 Million parameters to XXL with 11 Billion parameters. In this"
" tutorial, we demonstrate the fine-tuning of a T5 3B with FSDP for text "
"summarization using WikiHow dataset.  The main focus of this tutorial is to "
"highlight different available features in FSDP that are helpful for training"
" large scale model above 3B parameters. Also, we cover specific features for"
" Transformer based models. The code for this tutorial is available in  "
"`Pytorch examples "
"<https://github.com/pytorch/examples/tree/main/distributed/FSDP/>`__."
msgstr ""
"HF T5 预训练模型有四种不同的大小，从带有 6000 万参数的小型模型到带有 110 亿参数的 XXL 模型。在本教程中，我们利用 FSDP 对 "
"WikiHow 数据集中的 T5 3B 模型进行文本摘要微调。本教程的主要重点是突出 FSDP 中可用的不同功能，这些功能对训练规模超过 3B "
"参数的大型模型有帮助。此外，我们还涵盖了基于 Transformer 模型的特定功能。本教程的代码可在 `Pytorch 示例 "
"<https://github.com/pytorch/examples/tree/main/distributed/FSDP/>`__ 中找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.1 Install the latest PyTorch"
msgstr "1.1 安装最新的 PyTorch"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.2 Dataset Setup"
msgstr "1.2 数据集设置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Please create a `data` folder, download the WikiHow dataset from "
"`wikihowAll.csv "
"<https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358>`__  and "
"`wikihowSep.cs "
"<https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag>`__, and place "
"them in the `data` folder.  We will use the wikihow dataset from "
"`summarization_dataset "
"<https://github.com/pytorch/examples/blob/main/distributed/FSDP/summarization_dataset.py>`__."
msgstr ""
"请创建一个 `data` 文件夹，从 `wikihowAll.csv "
"<https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358>`__ 和 "
"`wikihowSep.cs "
"<https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag>`__ 下载 WikiHow "
"数据集，并将其放置在 `data` 文件夹中。我们将使用 `summarization_dataset "
"<https://github.com/pytorch/examples/blob/main/distributed/FSDP/summarization_dataset.py>`__"
" 中的 wikihow 数据集。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we add the following code snippets to a Python script "
"“T5_training.py”."
msgstr "接下来，我们将以下代码片段添加到 Python 脚本“T5_training.py”中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The full source code for this tutorial is available in `PyTorch examples "
"<https://github.com/pytorch/examples/tree/main/distributed/FSDP/>`__."
msgstr ""
"本教程的完整源码可在 `PyTorch 示例 "
"<https://github.com/pytorch/examples/tree/main/distributed/FSDP/>`__ 中找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.3  Import necessary packages:"
msgstr "1.3 导入必要的软件包："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"1.4 Distributed training setup. Here we use two helper functions to "
"initialize the processes for distributed training,  and then to clean up "
"after training completion.  In this tutorial, we are going to use torch "
"elastic, using `torchrun "
"<https://pytorch.org/docs/stable/elastic/run.html>`__ , which will set the "
"worker `RANK` and `WORLD_SIZE` automatically."
msgstr ""
"1.4 分布式训练设置。这里我们使用两个辅助函数分别用于初始化分布式训练的进程，以及在训练完成后进行清理。在本教程中，我们将使用 torch "
"elatic，通过 `torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__ "
"来自动设置 worker 的 `RANK` 和 `WORLD_SIZE`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.1  Set up the HuggingFace T5 model:"
msgstr "2.1 设置 HuggingFace T5 模型："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We also, add couple of helper functions here for date and formatting memory "
"metrics."
msgstr "同时我们还添加了一些用于日期和格式化内存指标的辅助函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.2 Define a train function:"
msgstr "2.2 定义一个训练函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.3 Define a validation function:"
msgstr "2.3 定义一个验证函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.4 Define a distributed train function that wraps the model in FSDP:"
msgstr "2.4 定义一个分布式训练函数，包装 FSDP 中的模型："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.5 Parse the arguments and set the main function:"
msgstr "2.5 解析参数并设置主函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "To run the the training using torchrun:"
msgstr "使用 torchrun 运行训练："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Transformer Wrapping Policy"
msgstr "Transformer 封装策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As discussed in the `previous tutorial "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__, "
"auto_wrap_policy is one of the FSDP features that make it easy to "
"automatically shard a given model and put the model, optimizer and gradient "
"shards into distinct FSDP units."
msgstr ""
"正如 `先前教程 <https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__ "
"所讨论的，auto_wrap_policy 是 FSDP 的一项功能，它使得自动分片给定模型并将模型、优化器和梯度分片放入不同的 FSDP "
"单元变得容易。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For some architectures such as Transformer encoder-decoders, some parts of "
"the model such as embedding table is being shared with both encoder and "
"decoder.  In this case, we need to place the embedding table in the outer "
"FSDP unit so that it could be accessed from both encoder and decoder.  In "
"addition, by registering the layer class for a transformer, the sharding "
"plan can be made much more communication efficient.  In PyTorch 1.12, FSDP "
"added this support and now we have a wrapping policy for transfomers."
msgstr ""
"对于一些架构，如 Transformer 编码器-解码器，模型的某些部分（如嵌入表）同时与编码器和解码器共享。在这种情况下，我们需要将嵌入表放置在外部 "
"FSDP 单元中，以便从编码器和解码器访问它。此外，通过注册 Transformer 的层类，可以使分片计划更具通信效率。在 PyTorch 1.12 "
"中，FSDP 增加了对此功能的支持，现在我们有了 Transformer 的封装策略。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It can be created as follows, where the T5Block represents the T5 "
"transformer layer class (holding MHSA and FFN)."
msgstr "可以按如下方式创建，其中 T5Block 表示 T5 Transformer 层类（包含 MHSA 和 FFN）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To see the wrapped model, you can easily print the model and visually "
"inspect the sharding and FSDP units as well."
msgstr "要查看包装的模型，您可以轻松打印模型并可视化检查分片和 FSDP 单元。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FSDP supports flexible mixed precision training allowing for arbitrary "
"reduced precision types (such as fp16 or bfloat16). Currently BFloat16 is "
"only available on Ampere GPUs, so you need to confirm native support before "
"you use it. On V100s for example, BFloat16 can still be run but because it "
"runs non-natively, it can result in significant slowdowns."
msgstr ""
"FSDP 支持灵活的混合精度训练，允许使用任意减少精度类型（如 fp16 或 bfloat16）。目前，BFloat16 仅适用于 Ampere "
"GPU，因此在使用前需确认其原生支持。例如，在 V100 GPU 上，尽管 BFloat16 仍可运行，但由于非原生运行，可能导致显著的性能下降。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To check if BFloat16 is natively supported, you can use the following :"
msgstr "要检查 BFloat16 是否原生支持，您可使用以下代码："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One of the advantages of mixed precision in FSDP is providing granular "
"control over different precision levels for parameters, gradients, and "
"buffers as follows:"
msgstr "混合精度的一个优势是可以对参数、梯度和缓冲区的不同精度级别进行细粒度控制，具体如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that if a certain type (parameter, reduce, buffer) is not specified, "
"they will not be casted at all."
msgstr "注意，如果未指定某种类型（参数、梯度归约、缓冲区），它们将不会进行精度转换。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This flexibility allows users fine grained control, such as only setting "
"gradient communication to happen in reduced precision, and all parameters / "
"buffer computation to be done in full precision. This is potentially useful "
"in cases where intra-node communication is the main bottleneck and "
"parameters / buffers must be in full precision to avoid accuracy issues. "
"This can be done with the following policy:"
msgstr ""
"此灵活性允许用户进行精细控制，例如仅设置梯度通信以较低精度运行，而所有参数/缓冲区计算以全精度完成。这在节点间通信是主要瓶颈而参数/缓冲区必须全精度以避免准确性问题的情况下可能很有用。这可以通过以下策略完成："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In 2.4 we just add the relevant mixed precision policy to the FSDP wrapper:"
msgstr "在步骤 2.4 中，我们仅仅将相关的混合精度策略添加到 FSDP 包装器中："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In our experiments, we have observed up to 4x speed up by using BFloat16 for"
" training and memory reduction of approximately 30% in some experiments that"
" can be used for batch size increases."
msgstr "在我们的实验中，使用 BFloat16 进行了训练，观察到速度提高最多达 4 倍，并且内存减少大约 30%，这些内存可用于增加批量大小。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Intializing FSDP Model on Device"
msgstr "在设备上初始化 FSDP 模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In 1.12, FSDP supports a `device_id` argument meant to initialize input CPU "
"module on the device given by `device_id`. This is useful when the entire "
"model does not fit on a single GPU, but fits in a host's CPU memory. When "
"`device_id` is specified, FSDP will move the model to the specified device "
"on a per-FSDP unit basis, avoiding GPU OOM issues while initializing several"
" times faster than CPU-based initialization:"
msgstr ""
"在 1.12 中，FSDP 支持 `device_id` 参数，用于在 `device_id` 指定的设备上初始化输入的 CPU "
"模块。当整个模型无法放入单个 GPU，但能装入主机的 CPU 内存时，此功能很有用。当指定 `device_id` 时，FSDP 将逐个 FSDP "
"单元将模型移至指定设备，避免 GPU 内存不足问题，同时初始化速度比基于 CPU 的方式快几倍："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FSDP sharding strategy by default is set to fully shard the model "
"parameters, gradients and optimizer states get sharded across all ranks. "
"(also termed Zero3 sharding). In case you are interested to have the Zero2 "
"sharding strategy, where only optimizer states and gradients are sharded, "
"FSDP support this feature by passing the Sharding strategy by using  "
"\"ShardingStrategy.SHARD_GRAD_OP\", instead of "
"\"ShardingStrategy.FULL_SHARD\" to the FSDP initialization  as follows:"
msgstr ""
"默认情况下，FSDP 分片策略设置为完全分片模型参数、梯度和优化器状态（也称为 Zero3 分片）。如果您有兴趣使用 Zero2 "
"分片策略（仅分片优化器状态和梯度），FSDP "
"可以通过将分片策略从\"ShardingStrategy.FULL_SHARD\"改为\"ShardingStrategy.SHARD_GRAD_OP\"传递给"
" FSDP 初始化来支持该功能，如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This will reduce the communication overhead in FSDP, in this case, it holds "
"full parameters after forward and through the backwards pass."
msgstr "这将减少 FSDP 的通信开销，在此情况下，它会在前向和反向传递中保持完整参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This saves an all_gather during backwards so there is less communication at "
"the cost of a higher memory footprint. Note that full model params are freed"
" at the end of backwards and all_gather will happen on the next forward "
"pass."
msgstr ""
"这可以在反向传递期间节省一次 "
"all_gather，因此通信更少，但会以较高的内存占用为代价。注意，在反向传递的末尾会释放所有模型参数，而在下一个前向传递中会再次发生 "
"all_gather。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The backward prefetch setting controls the timing of when the next FSDP "
"unit's parameters should be requested.  By setting it to `BACKWARD_PRE`, the"
" next FSDP's unit params can begin to be requested and arrive sooner before "
"the computation of the current unit starts. This overlaps the `all_gather` "
"communication and gradient computation which can increase the training speed"
" in exchange for slightly higher memory consumption. It can be utilized in "
"the FSDP wrapper in 2.4 as follows:"
msgstr ""
"反向预取设置控制下一个 FSDP 单元的参数何时开始请求。通过将其设置为 `BACKWARD_PRE`，可以在当前单元的计算开始前提前请求并接收下一个 "
"FSDP 单元的参数。这种通信和梯度计算的重叠可以略微提高训练速度，但以稍高的内存消耗为代价。在 2.4 中可以通过以下方式在 FSDP "
"包装器中利用它："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`backward_prefetch` has two modes, `BACKWARD_PRE` and `BACKWARD_POST`. "
"`BACKWARD_POST` means that the next FSDP unit's params will not be requested"
" until the current FSDP unit processing is complete, thus minimizing memory "
"overhead.  In some cases, using `BACKWARD_PRE` can increase model training "
"speed up to 2-10%, with even higher speed improvements noted for larger "
"models."
msgstr ""
"`backward_prefetch` 有两种模式，`BACKWARD_PRE` 和 `BACKWARD_POST`。`BACKWARD_POST` "
"表示在完成当前 FSDP 单元处理之前不会请求下一个 FSDP 单元的参数，从而将内存开销最小化。在某些情况下，使用 `BACKWARD_PRE` "
"可以将模型训练速度提高 2% 到 10%，对于更大的模型甚至能实现更高的速度提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model Checkpoint Saving, by streaming to the Rank0 CPU"
msgstr "通过流式传输到 Rank0 CPU 保存模型检查点"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To save model checkpoints using FULL_STATE_DICT saving which saves model in "
"the same fashion as a local model, PyTorch 1.12 offers a few utilities to "
"support the saving of larger models."
msgstr ""
"要使用 FULL_STATE_DICT 进行保存（以与本地模型相同的方式保存模型），PyTorch 1.12 提供了一些实用工具来支持保存更大的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, a FullStateDictConfig can be specified, allowing the state_dict to be"
" populated on rank 0 only and offloaded to the CPU."
msgstr "首先，可以指定 FullStateDictConfig，以便将 `state_dict` 仅填充到 0 号排名并卸载到 CPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When using this configuration, FSDP will allgather model parameters, "
"offloading them to the CPU one by one, only on rank 0. When the state_dict "
"is finally saved, it will only be populated on rank 0 and contain CPU "
"tensors. This avoids potential OOM for models that are larger than a single "
"GPU memory and allows users to checkpoint models whose size is roughly the "
"available CPU RAM on the user's machine."
msgstr ""
"使用此配置时，FSDP将收集模型参数，仅在0号任务中将其逐一卸载到CPU。当最终保存state_dict时，它将仅在0号任务中填充，并包含CPU张量。这可以避免对于比单个GPU内存更大的模型可能出现的内存不足问题，并允许用户对模型大小接近用户机器上可用的CPU"
" RAM的模型进行检查点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This feature can be run as follows:"
msgstr "可以如下运行此功能："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Summary"
msgstr "摘要"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we have introduced many new features for FSDP available in"
" Pytorch 1.12 and used HF T5 as the running example.  Using the proper "
"wrapping policy especially for transformer models, along with mixed "
"precision and backward prefetch should speed up your training runs. Also, "
"features such as initializing the model on device, and checkpoint saving via"
" streaming to CPU should help to avoid OOM error in dealing with large "
"models."
msgstr ""
"在本教程中，我们介绍了PyTorch 1.12中FSDP的许多新功能，并使用HF "
"T5作为运行示例。对于Transformer模型，使用合适的包装策略，结合混合精度和后向预取，可以加速训练。此外，诸如在设备上初始化模型以及通过流式传输到CPU来保存检查点等功能可以帮助避免处理大模型时出现内存不足错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We are actively working to add new features to FSDP for the next release. If"
" you have feedback, feature requests, questions or are encountering issues "
"using FSDP, please feel free to contact us by opening an issue in the "
"`PyTorch Github repository <https://github.com/pytorch/pytorch>`__."
msgstr ""
"我们正在积极工作，为下一版本的FSDP添加新功能。如果您有反馈、功能请求、问题或在使用FSDP时遇到问题，请随时通过在`PyTorch "
"GitHub存储库 <https://github.com/pytorch/pytorch>`__中打开问题与我们联系。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Getting Started with Fully Sharded Data Parallel (FSDP2)"
msgstr "全面分片数据并行（FSDP2）入门"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Author**: `Wei Feng <https://github.com/weifengpy>`__, `Will Constable "
"<https://github.com/wconstab>`__, `Yifan Mao <https://github.com/mori360>`__"
msgstr ""
"**作者**: `Wei Feng <https://github.com/weifengpy>`__, `Will Constable "
"<https://github.com/wconstab>`__, `Yifan Mao <https://github.com/mori360>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| Check out the code in this tutorial from `pytorch/examples "
"<https://github.com/pytorch/examples/tree/main/distributed/FSDP2>`_. FSDP1 "
"will be deprecated. The old tutorial can be found `here "
"<https://docs.pytorch.org/tutorials/intermediate/FSDP1_tutorial.html>`_."
msgstr ""
"|edit| 从`PyTorch示例代码仓库 "
"<https://github.com/pytorch/examples/tree/main/distributed/FSDP2>`_中查看本教程代码。FSDP1将被弃用，旧教程可以在`这里"
" <https://docs.pytorch.org/tutorials/intermediate/FSDP1_tutorial.html>`_找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How FSDP2 works"
msgstr "FSDP2的工作原理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
" (DDP) training, each rank owns a model replica and processes a batch of "
"data, finally it uses all-reduce to sync gradients across ranks."
msgstr ""
"在`分布式数据并行 "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
" (DDP)训练中，每个任务拥有一个模型副本并处理一个批次的数据，最后使用全规约在任务之间同步梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Comparing with DDP, FSDP reduces GPU memory footprint by sharding model "
"parameters, gradients, and optimizer states. It makes it feasible to train "
"models that cannot fit on a single GPU. As shown below in the picture,"
msgstr ""
"与DDP相比，FSDP通过对模型参数、梯度和优化器状态进行分片来减少GPU内存占用。这使得训练无法放入单个GPU中的模型变得可行。如下图所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Outside of forward and backward computation, parameters are fully sharded"
msgstr "在前向和后向计算之外，参数是完全分片的"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before forward and backward, sharded parameters are all-gathered into "
"unsharded parameters"
msgstr "在前向和后向计算之前，分片的参数被收集为未分片的参数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Inside backward, local unsharded gradients are reduce-scatterred into "
"sharded gradients"
msgstr "在后向计算中，本地未分片梯度被减少分散为分片的梯度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimizer updates sharded parameters with sharded gradients, resulting in "
"sharded optimizer states"
msgstr "优化器使用分片的梯度更新分片的参数，从而生成分片的优化器状态"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FSDP can be considered a decomposition of DDP's all-reduce into reduce-"
"scatter and all-gather operations"
msgstr "FSDP可以被认为是DDP的全规约的分解为减少分散和全收集操作"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP all-gather and reduce-scatter"
msgstr "FSDP的全收集和减少分散"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Comparing with `FSDP1 <https://docs.pytorch.org/docs/stable/fsdp.html>`__, "
"FSDP2 has following advantages:"
msgstr ""
"与`FSDP1 <https://docs.pytorch.org/docs/stable/fsdp.html>`_相比，FSDP2具有以下优点："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Representing sharded parameters as `DTensor "
"<https://docs.pytorch.org/docs/stable/distributed.tensor.html>`_ sharded on "
"dim-i, allowing for easy manipulation of individual parameters, "
"communication-free sharded state dicts, and a simpler meta-device "
"initialization flow."
msgstr ""
"将分片参数表示为`DTensor "
"<https://docs.pytorch.org/docs/stable/distributed.tensor.html>`_在维度i上分片，允许轻松操作各个参数，无需通信的分片状态字典，以及更简单的元设备初始化流程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Improving memory management system that achieves lower and deterministic GPU"
" memory by avoiding ``recordStream`` (`doc <https://dev-"
"discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-"
"perspective/1486>`_) and does so without any CPU synchronization."
msgstr ""
"改进内存管理系统，通过避免使用``recordStream`` (`文档 <https://dev-"
"discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-"
"perspective/1486>`_)实现更低和可预测的GPU内存，并且无需任何CPU同步。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Offering a tensor subclass extension point to customize the all-gather, e.g."
" for float8 all-gather for float8 linears (`doc <https://dev-"
"discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359>`_), and NF4 "
"for QLoRA (`doc "
"<https://github.com/pytorch/torchtune/blob/main/README.md>`_)"
msgstr ""
"提供一个张量子类扩展点，以自定义全收集操作，例如用于float8线性层的float8全收集(`文档 <https://dev-"
"discuss.pytorch.org/t/enabling-float8-all-gather-in-"
"fsdp2/2359>`_)以及用于QLoRA的NF4 (`文档 "
"<https://github.com/pytorch/torchtune/blob/main/README.md>`_)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Mixing frozen and non-frozen parameters can in the same communication group "
"without using extra memory."
msgstr "混合冻结和非冻结参数可以在同一个通信组中，而无需使用额外的内存。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to use FSDP2"
msgstr "如何使用FSDP2"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model Initialization"
msgstr "模型初始化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Applying fully_shard on submodules**: Different from DDP, we should apply "
"`fully_shard "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html>`_ on "
"submodules as well as the root model. In the transformer example below, we "
"applied ``fully_shard`` on each layer first, then the root model"
msgstr ""
"**对子模块应用fully_shard**：与DDP不同，我们应对子模块和根模型应用`fully_shard "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html>`_。在下面的Transformer示例中，我们首先对每个层应用``fully_shard``，然后是根模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"During forward computation of ``layers[i]``, the rest of the layers are "
"sharded to reduce memory footprint"
msgstr "在``layers[i]``的前向计算过程中，其余层被分片以减少内存占用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Inside ``fully_shard(model)``, FSDP2 excludes parameters from "
"``model.layers`` and classify remaining parameters into a parameter group "
"for performant all-gather and reduce-scatter"
msgstr ""
"在``fully_shard(model)``中，FSDP2排除``model.layers``的参数，并将剩余参数分类为一个参数组，以实现高效的全收集和减少分散"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``fully_shard`` moves sharded model to actual training device (eg ``cuda``)"
msgstr "``fully_shard``将分片后的模型移动到实际的训练设备（例如``cuda``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Command**: ``torchrun --nproc_per_node 2 train.py``"
msgstr "**命令**: ``torchrun --nproc_per_node 2 train.py``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can inspect the nested wrapping with ``print(model)``. "
"``FSDPTransformer`` is a joint class of `Transformer "
"<https://github.com/pytorch/examples/blob/70922969e70218458d2a945bf86fd8cc967fc6ea/distributed/FSDP2/model.py#L100>`_"
" and `FSDPModule "
"<​https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule>`_."
" The same thing happens to `FSDPTransformerBlock "
"<https://github.com/pytorch/examples/blob/70922969e70218458d2a945bf86fd8cc967fc6ea/distributed/FSDP2/model.py#L76C7-L76C18>`_."
" All FSDP2 public APIs are exposed through ``FSDPModule``. For example, "
"users can call ``model.unshard()`` to manually control all-gather schedules."
" See \"explicit prefetching\" below for details."
msgstr ""
"我们可以使用``print(model)``检查嵌套包装情况。``FSDPTransformer``是`Transformer "
"<https://github.com/pytorch/examples/blob/70922969e70218458d2a945bf86fd8cc967fc6ea/distributed/FSDP2/model.py#L100>`_和`FSDPModule"
" "
"<​https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule>`_的联合类。同样的事情发生在`FSDPTransformerBlock"
" "
"<https://github.com/pytorch/examples/blob/70922969e70218458d2a945bf86fd8cc967fc6ea/distributed/FSDP2/model.py#L76C7-L76C18>`_上。所有FSDP2公开API通过``FSDPModule``暴露。例如，用户可以调用``model.unshard()``手动控制全收集计划。详细信息参见下面的“显式预取”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**model.parameters() as DTensor**: ``fully_shard`` shards parameters across "
"ranks, and convert ``model.parameters()`` from plain ``torch.Tensor`` to "
"DTensor to represent sharded parameters. FSDP2 shards on dim-0 by default so"
" DTensor placements are `Shard(dim=0)`. Say we have N ranks and a parameter "
"with N rows before sharding. After sharding, each rank will have 1 row of "
"the parameter. We can inspect sharded parameters using ``param.to_local()``."
msgstr ""
"**model.parameters()作为DTensor**: "
"``fully_shard``将在任务之间分片参数，并将``model.parameters()``从普通的``torch.Tensor``转化为DTensor以表示分片参数。FSDP2默认在维度0上分片，因此DTensor的配置为`Shard(dim=0)`。假设有N个任务以及一个有N行的参数，则在分片后每个任务拥有该参数的一行。我们可以使用``param.to_local()``检查分片参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note the optimizer is constructed after applying ``fully_shard``. Both model"
" and optimizer state dicts are represented in DTensor."
msgstr "注意优化器是在应用``fully_shard``之后构建的。模型和优化器状态字典都以DTensor表示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "DTensor facilitates optimizer, gradient clipping and checkpointing"
msgstr "DTensor方便了优化器、梯度裁剪和检查点保存"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.optim.Adam`` and ``torch.nn.utils.clip_grad_norm_`` works out of the"
" box for DTensor parameters. It makes the code consistent between single-"
"device and distributed training"
msgstr ""
"``torch.optim.Adam``和``torch.nn.utils.clip_grad_norm_``能够直接作用于DTensor参数。这使得代码在单设备和分布式训练之间保持一致"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"we can use DTensor and DCP APIs to manipulate parameters to get full state "
"dict, see \"state dict\" section below for details. For distributed state "
"dicts, we can save/load checkpoints (`doc "
"<https://docs.pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html>`_)"
" without extra communication"
msgstr ""
"我们可以使用DTensor和DCP API操作参数以获得完整的状态字典，更多细节请参见“状态字典”部分。对于分布式状态字典，我们可以保存/加载检查点 "
"(`文档 "
"<https://docs.pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html>`_)"
" 而无需额外通信"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Forward/Backward with Prefetching"
msgstr "通过预取进行前向/后向计算"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**command**: ``torchrun --nproc_per_node 2 train.py``"
msgstr "**命令**: ``torchrun --nproc_per_node 2 train.py``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``fully_shard`` registers forward/backward hooks to all-gather parameters "
"before computation, and reshards parameters after computation. To overlap "
"all-gathers with computation, FSDP2 offers **implicit prefetching** that "
"works out of the box with the training loop above and **explicit "
"prefetching** for advanced users to control all-gather schedules manually."
msgstr ""
"``fully_shard``注册前向/后向钩子，以便在计算前全收集参数，并在计算后重新分片参数。为了与计算重叠全收集，FSDP2提供了**隐式预取**，可以直接与上述训练循环一起运行，以及面向高级用户的**显式预取**以手动控制全收集计划。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Implicit Prefetching**: CPU thread issues all-gather i before layer i. "
"All-gathers are queued into its own cuda stream while layer i computation "
"happens in the default stream. For non-cpu-bound workload (eg Transformer "
"with big batch size), all-gather i+1 can overlap with computation for layer "
"i. Implicit prefetching works similarly in the backward, except all-gathers "
"are issued in the reverse of post-forward order."
msgstr ""
"**隐式预取**: "
"CPU线程在第i层前发出第i层的全收集。全收集被排队到其自己的CUDA流，而第i层的计算发生在默认流中。对于非CPU受限的工作负载（例如大型批量的Transformer），第i+1层的全收集可以与第i层的计算重叠。隐式预取在后向计算中类似进行，只是全收集是在前向计算完成的反顺序中发出的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP Implicit"
msgstr "FSDP隐式预取"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We recommend users to start with implicit prefetching to understand the "
"performance out of the box."
msgstr "我们建议用户从隐式预取开始，以了解开箱即用的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Explicit Prefetching**: Users can specify forward ordering with "
"`set_modules_to_forward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch>`_,"
" and backward ordering with `set_modules_to_backward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch>`_."
" As shown in the code below, CPU thread issue all-gather i + 1 and i + 2 at "
"layer i"
msgstr ""
"**显式预取**: 用户可以使用`set_modules_to_forward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch>`_指定前向顺序，以及使用`set_modules_to_backward_prefetch"
" "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch>`_指定后向顺序。如下代码所示，CPU线程在第i层时发出第i+1层和第i+2层的全收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Explicit prefetching works well in following situation:"
msgstr "显式预取在以下情况效果良好："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**CPU-bound workload**: If using implicit prefetching, CPU thread will be "
"too slow to issue all-gather for layer i+1 when kernels from layer i get "
"executed. We have to explicitly issue all-gather i+1 before running forward "
"for layer i"
msgstr ""
"**CPU受限工作负载**: "
"如果使用隐式预取，CPU线程在第i层的内核执行时发出第i+1层的全收集过慢。我们必须显式在第i层运行前发出第i+1层的全收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Prefetching for 2+ layers**: Implicit prefetching only all-gathers next "
"one layer at a time to keep memory footprint minimum. With explicit "
"prefetching can all-gather multiple layers at a time to possibly for better "
"perf with increased memory. See ``layers_to_prefetch`` in the code"
msgstr ""
"**预取超过2层**: "
"隐式预取每次仅全收集下一层，以保持最低内存占用。显式预取可以同时全收集多层，可能通过增加内存提高性能。参见代码中的``layers_to_prefetch``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Issuing 1st all-gather earlier**: Implicit prefetching happens at the time"
" of calling ``model(x)``. The 1st all-gather gets exposed. We can call "
"`model.unshard() "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.unshard>`_"
" explicitly earlier to issue 1st all-gather earlier"
msgstr ""
"**更早发出第一个全收集**: 隐式预取发生在调用``model(x)``时，第一个全收集才开始暴露。我们可以显式调用`model.unshard() "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.unshard>`_更早发出第一个全收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**command**: ``torchrun --nproc_per_node 2 train.py --explicit-prefetching``"
msgstr ""
"**命令**: ``torchrun --nproc_per_node 2 train.py --explicit-prefetching``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Enabling Mixed Precision"
msgstr "启用混合精度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FSDP2 offers a flexible `mixed precision policy "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.MixedPrecisionPolicy>`_"
" to speed up training. One typical use case is"
msgstr ""
"FSDP2提供了灵活的`混合精度策略 "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.MixedPrecisionPolicy>`_以加速训练。一个典型的用例是："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Casting float32 parameters to bfloat16 for forward/backward computation, see"
" ``param_dtype=torch.bfloat16``"
msgstr "将float32参数转换为bfloat16以进行前向/后向计算，参见``param_dtype=torch.bfloat16``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Upcasting gradients to float32 for reduce-scatter to preserve accuracy, see "
"``reduce_dtype=torch.float32``"
msgstr "将梯度上升转换为float32以进行减少分散以提高精度，参见``reduce_dtype=torch.float32``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Comparing with `torch.amp <https://docs.pytorch.org/docs/stable/amp.html>`_,"
" FSDP2 mixed precision has following advantages"
msgstr ""
"与`torch.amp "
"<https://docs.pytorch.org/docs/stable/amp.html>`_相比，FSDP2混合精度具有以下优势"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Performant and flexible parameter casting**: All the parameters inside a "
"``FSDPModule`` are cast together at the module boundary (before and after "
"before/backward). We can set different mixed precision policies for each "
"layer. For example, the first few layers can be in float32 while remaining "
"layers can be in bfloat16."
msgstr ""
"**高效和灵活的参数转换**: "
"``FSDPModule``中的所有参数在模块边界处（前向和后向之前和之后）会一起转换。我们可以为每个层设置不同的混合精度策略。例如，前几层可以是float32，而其余的层可以是bfloat16。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**float32 gradient reduction (reduce-scatter)**: Gradients might vary a lot "
"from rank to rank. Reducing gradients in float32 can be critical for "
"numerics."
msgstr "**float32梯度减少（减少分散）**: 梯度可能从任务到任务变化很大。使用float32减少梯度对于数值计算可能是关键的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**command**: ``torchrun --nproc_per_node 2 train.py --mixed-precision``"
msgstr "**命令**: ``torchrun --nproc_per_node 2 train.py --mixed-precision``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Gradient Clipping and Optimizer with DTensor"
msgstr "使用DTensor进行梯度裁剪和优化器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimizer is initialized after applying ``fully_shard`` on the model, and "
"holds reference to DTensor ``model.parameters()``. For gradient clipping, "
"``torch.nn.utils.clip_grad_norm_`` works for DTensor parameters. Tensor ops "
"will be dispatched correctly inside DTensor to communicate partial tensors "
"across ranks to preserve the single device semantic."
msgstr ""
"优化器是在对模型应用``fully_shard``之后初始化的，并持有对DTensor``model.parameters()``的引用。对于梯度裁剪，``torch.nn.utils.clip_grad_norm_``适用于DTensor参数。张量操作将在DTensor内正确分派，以在任务之间通信部分张量以保留单设备语义。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "State Dicts with DTensor APIs"
msgstr "使用DTensor API的状态字典"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We showcase how to convert a full state dict into a DTensor state dict for "
"loading, and how to convert it back to full state dict for saving."
msgstr "我们展示了如何将完整的状态字典转换为 DTensor 状态字典进行加载，以及如何将其转换回完整状态字典进行保存。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For the 1st time, it creates checkpoints for the model and optimizer"
msgstr "首次，为模型和优化器创建检查点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the 2nd time, it loads from the previous checkpoint to resume training"
msgstr "第二次，从之前的检查点加载以继续训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Loading state dicts**: We initialize the model under meta device and call "
"``fully_shard`` to convert ``model.parameters()`` from plain "
"``torch.Tensor`` to DTensor. After reading the full state dict from "
"torch.load, we can call `distributed_tensor "
"<https://docs.pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.distribute_tensor>`_"
" to convert plain ``torch.Tensor`` into DTensor, using the same placements "
"and device mesh from ``model.state_dict()``. Finally we can call "
"`model.load_state_dict "
"<https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict>`_"
" to load DTensor state dicts into the model."
msgstr ""
"**加载状态字典**：我们在元设备下初始化模型，并调用 ``fully_shard`` 将 ``model.parameters()`` 从普通 "
"``torch.Tensor`` 转换为 DTensor。在使用 torch.load 读取完整状态字典后，我们可以调用 "
"`distributed_tensor "
"<https://docs.pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.distribute_tensor>`_"
" 将普通 ``torch.Tensor`` 转换为 DTensor，使用 ``model.state_dict()`` "
"中的相同放置方式和设备网格。最后，我们可以调用 `model.load_state_dict "
"<https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict>`_"
" 将 DTensor 状态字典加载到模型中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Saving state dicts**: ``model.state_dict()`` returns a DTensor state dict."
" We can convert a DTensor into a plain ``torch.Tensor`` by calling "
"`full_tensor() "
"<https://docs.pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.DTensor.full_tensor>`_."
" Internally it issues an all-gather across ranks to get unsharded parameters"
" in plain torch.Tensor. For rank 0, ``full_param.cpu()`` offloads the tensor"
" to cpu one by one to avoid peaking GPU memory with unsharded parameters."
msgstr ""
"**保存状态字典**：``model.state_dict()`` 返回一个 DTensor 状态字典。我们可以通过调用 `full_tensor() "
"<https://docs.pytorch.org/docs/stable/distributed.tensor.html#torch.distributed.tensor.DTensor.full_tensor>`_"
" 将 DTensor 转换为普通 ``torch.Tensor``。内部调用了跨等级的 all-gather 来获取未分片参数的普通 "
"torch.Tensor。在 0 号等级，``full_param.cpu()`` 将张量逐一卸载到 CPU，以避免 GPU 内存峰值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimizer state dict works similarly (`code "
"<https://github.com/pytorch/examples/blob/70922969e70218458d2a945bf86fd8cc967fc6ea/distributed/FSDP2/checkpoint.py#L156>`_)."
" Users can customize the above DTensor scripts to work with 3rd party "
"checkpoints."
msgstr ""
"优化器状态字典的工作原理类似（`代码 "
"<https://github.com/pytorch/examples/blob/70922969e70218458d2a945bf86fd8cc967fc6ea/distributed/FSDP2/checkpoint.py#L156>`_）。用户可以自定义上述"
" DTensor 脚本以支持第三方检查点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If there is no need for customization, we can use `DCP APIs "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html>`_ "
"directly to support both single-node and multi-node training."
msgstr ""
"如果不需要自定义，我们可以直接使用 `DCP APIs "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html>`_ "
"来支持单节点和多节点训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "State Dict with DCP APIs"
msgstr "使用 DCP APIs 的状态字典"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**command**: ``torchrun --nproc_per_node 2 train.py --dcp-api``"
msgstr "**命令**：``torchrun --nproc_per_node 2 train.py --dcp-api``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Loading state dicts**: We can load a full state dict into a FSDP2 model "
"with `set_model_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_model_state_dict>`_."
" With ``broadcast_from_rank0=True``, we can load the full state dict only on"
" rank 0 to avoid peaking CPU memory. DCP will shard tensors and broadcast "
"them to other ranks."
msgstr ""
"**加载状态字典**：我们可以通过 `set_model_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_model_state_dict>`_"
" 将完整状态字典加载到 FSDP2 模型中。通过设置 ``broadcast_from_rank0=True``，我们可以仅在 0 "
"号等级加载完整状态字典，以避免 CPU 内存过载。DCP 会将张量分片并广播到其他等级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Saving state dicts**: `get_model_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_model_state_dict>`_"
" with ``full_state_dict=True`` and ``cpu_offload=True`` all-gathers tensors "
"and offload them to CPU. It works similarly to DTensor APIs."
msgstr ""
"**保存状态字典**：`get_model_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_model_state_dict>`_"
" 加上 ``full_state_dict=True`` 和 ``cpu_offload=True`` 可以完成 all-gather 张量并将其卸载到"
" CPU。这与 DTensor APIs 的工作方式类似。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Refer to `pytorch/examples "
"<https://github.com/pytorch/examples/blob/main/distributed/FSDP2/checkpoint.py>`__"
" for loading and saving optimizer state dicts with `set_optimizer_state_dict"
" "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_optimizer_state_dict>`_"
" and `get_optimizer_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_optimizer_state_dict>`_."
msgstr ""
"有关使用 `set_optimizer_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_optimizer_state_dict>`_"
" 和 `get_optimizer_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_optimizer_state_dict>`_"
" 加载和保存优化器状态字典的更多信息，请参阅 `pytorch/examples "
"<https://github.com/pytorch/examples/blob/main/distributed/FSDP2/checkpoint.py>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FSDP1-to-FSDP2 migration guide"
msgstr "FSDP1 到 FSDP2 的迁移指南"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s look at an example of an `FSDP "
"<https://docs.pytorch.org/docs/stable/fsdp.html>`_ usage and an equivalent "
"`fully_shard "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html>`_ "
"usage.  We’ll highlight the key differences and suggest steps for migration."
msgstr ""
"让我们以 `FSDP <https://docs.pytorch.org/docs/stable/fsdp.html>`_ 的一个使用示例及其等效的 "
"`fully_shard "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html>`_ "
"使用示例为例。我们将突出主要差异，并建议迁移步骤。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Original FSDP() usage"
msgstr "原始 FSDP() 使用方式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "New fully_shard() usage"
msgstr "新的 fully_shard() 使用方式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Migration Steps"
msgstr "迁移步骤"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Replace the imports"
msgstr "替换导入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Implement your ‘policy’ directly (apply ``fully_shard`` to the desired "
"sublayers)"
msgstr "直接实现您的 '策略' （对目标子层应用 ``fully_shard``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Wrap your root model with ``fully_shard`` instead of ``FSDP``"
msgstr "用 ``fully_shard`` 包裹您的根模型，而不是使用 ``FSDP``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Get rid of ``param_init_fn`` and manually call ``model.reset_parameters()``"
msgstr "去掉 ``param_init_fn`` 并手动调用 ``model.reset_parameters()``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Replace other FSDP1 kwargs (see below)"
msgstr "替换其他 FSDP1 的关键参数（详情见下）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "sharding_strategy"
msgstr "分片策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FULL_SHARD: ``reshard_after_forward=True``"
msgstr "FULL_SHARD: ``reshard_after_forward=True``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "SHARD_GRAD_OP: ``reshard_after_forward=False``"
msgstr "SHARD_GRAD_OP: ``reshard_after_forward=False``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "HYBRID_SHARD: ``reshard_after_forward=True`` with a 2D device mesh"
msgstr "HYBRID_SHARD: ``reshard_after_forward=True``，使用二维设备网格"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"_HYBRID_SHARD_ZERO2: ``reshard_after_forward=False`` with a 2D device mesh"
msgstr "_HYBRID_SHARD_ZERO2: ``reshard_after_forward=False``，使用二维设备网格"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "cpu_offload"
msgstr "CPU 卸载"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "CPUOffload.offload_params=False: ``offload_policy=None``"
msgstr "CPUOffload.offload_params=False: ``offload_policy=None``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"CPUOffload.offload_params = True: ``offload_policy=CPUOffloadPolicy()``"
msgstr "CPUOffload.offload_params=True: ``offload_policy=CPUOffloadPolicy()``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "backward_prefetch"
msgstr "反向预取"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "BACKWARD_PRE: always used"
msgstr "BACKWARD_PRE: 总是使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "BACKWARD_POST: not supported"
msgstr "BACKWARD_POST: 不支持"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "mixed_precision"
msgstr "混合精度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``buffer_dtype`` is omitted because fully_shard does not shard buffers"
msgstr "因为 fully_shard 不分片缓冲区，所以省略了 ``buffer_dtype``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"fully_shard’s ``cast_forward_inputs`` maps to both ``cast_forward_inputs`` "
"and ``cast_root_forward_inputs`` in FSDP1"
msgstr ""
"fully_shard 的 ``cast_forward_inputs`` 映射到 FSDP1 中的 ``cast_forward_inputs`` 和"
" ``cast_root_forward_inputs``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``output_dtype`` is a new config for fully_shard"
msgstr "``output_dtype`` 是一个新的配置用于 fully_shard"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "device_id: Inferred from device_mesh’s device"
msgstr "设备 ID: 从 device_mesh 的设备推断"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"sync_module_states=True/False: Moved to DCP. User can broadcast state dicts "
"from rank0 using `set_model_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_model_state_dict>`_"
" with ``broadcast_from_rank0=True``"
msgstr ""
"sync_module_states=True/False: 已移至 DCP。用户可以使用 `set_model_state_dict "
"<https://docs.pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.set_model_state_dict>`_"
" 和 ``broadcast_from_rank0=True`` 从 0 号等级广播状态字典"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "forward_prefetch: Manual control over prefetching is possible with"
msgstr "forward_prefetch: 可以手动控制预取"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Manually call `fsdp_module.unshard() "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.unshard>`_"
msgstr ""
"手动调用 `fsdp_module.unshard() "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.unshard>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Use these APIs to control automatic prefetching, "
"`set_modules_to_forward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch>`_"
" and `set_modules_to_backward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch>`_"
msgstr ""
"使用以下 API 控制自动预取：`set_modules_to_forward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_forward_prefetch>`_"
" 和 `set_modules_to_backward_prefetch "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_modules_to_backward_prefetch>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"limit_all_gathers: No longer needed, because ``fully_shard`` removed cpu "
"synchronization"
msgstr "限制所有 all-gathers: 不再需要，因为 ``fully_shard`` 删除了 CPU 同步"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"use_orig_params: Original params are always used (no more flat parameter)"
msgstr "使用原始参数: 始终使用原始参数（不再有平坦的参数）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"no_sync(): `set_requires_gradient_sync "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_requires_gradient_sync>`_"
msgstr ""
"no_sync(): `set_requires_gradient_sync "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.FSDPModule.set_requires_gradient_sync>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"ignored_params and ignored_states: `ignored_params "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.fully_shard>`_"
msgstr ""
"忽略参数和忽略状态: `ignored_params "
"<https://docs.pytorch.org/docs/main/distributed.fsdp.fully_shard.html#torch.distributed.fsdp.fully_shard>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introduction to Libuv TCPStore Backend"
msgstr "Libuv TCPStore 后端介绍"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Authors**: `Xilun Wu <https://github.com/XilunWu>`_"
msgstr "**作者**：`Xilun Wu <https://github.com/XilunWu>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst>`__."
msgstr ""
"|编辑| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst>`__"
" 查看和编辑本教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "What is the new TCPStore backend"
msgstr "什么是新的 TCPStore 后端"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compare the new libuv backend against the legacy backend"
msgstr "比较新的 libuv 后端与旧版后端"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to enable to use the legacy backend"
msgstr "如何启用旧版后端"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch 2.4 or later"
msgstr "PyTorch 2.4 或更高版本"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Read about the `TCPStore API "
"<https://pytorch.org/docs/main/distributed.html#torch.distributed.TCPStore>`__."
msgstr ""
"阅读有关 `TCPStore API "
"<https://pytorch.org/docs/main/distributed.html#torch.distributed.TCPStore>`__"
" 的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introduction"
msgstr "介绍"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Recently, we have rolled out a new TCPStore server backend using `libuv "
"<https://github.com/libuv/libuv>`__, a third-party library for asynchronous "
"I/O. This new server backend aims to address scalability and robustness "
"challenges in large-scale distributed training jobs, such as those with more"
" than 1024 ranks. We ran a series of benchmarks to compare the libuv backend"
" against the old one, and the experiment results demonstrated significant "
"improvements in store initialization time and maintained a comparable "
"performance in store I/O operations."
msgstr ""
"最近，我们推出了一个使用 `libuv <https://github.com/libuv/libuv>`__ 的新 TCPStore "
"服务器后端，这是一个用于异步 I/O 的第三方库。这个新的服务器后端旨在解决大规模分布式训练作业（例如拥有超过 1024 "
"个等级）中的可扩展性和稳健性挑战。我们进行了系列基准测试，比较了 libuv 后端与旧版后端，实验结果显示状态初始化时间大幅改善，同时在状态 I/O "
"操作中保持了可比性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As a result of these findings, the libuv backend has been set as the default"
" TCPStore server backend in PyTorch 2.4. This change is expected to enhance "
"the performance and scalability of distributed training jobs."
msgstr ""
"基于这些发现，libuv 后端已在 PyTorch 2.4 中设置为默认 TCPStore 服务器后端。此更改预计将提高分布式训练作业的性能和可扩展性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This change introduces a slight incompatibility to store initialization. For"
" users who wish to continue using the legacy backend, the tutorial will "
"provide guidance on how to specify to use the previous TCPStore server "
"backend."
msgstr "此更改对状态初始化引入了轻微的不兼容性。对于希望继续使用旧版后端的用户，本教程将提供如何指定使用旧版 TCPStore 服务器后端的指南。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Performance Benchmark"
msgstr "性能基准"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To better demonstrate the benefit of our new libuv TCPStore backend, we set "
"up a benchmark over a wide range of job size, from 1024 (1K) to 98304 (96K) "
"ranks. We first measured the TCPStore initialization time using the code "
"snippet below:"
msgstr ""
"为了更好地说明我们的新 libuv TCPStore 后端的优势，我们设置了一个覆盖从 1024 (1K) 到 98304 (96K) "
"等级的广泛范围的基准。我们首先使用以下代码片段测量了 TCPStore 的初始化时间："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since the execution of the TCPStore server thread will be blocked until all "
"clients are successfully connected, we take the time measured on rank 0 as "
"the total TCPStore initialization runtime. The experiment numbers are "
"reported in the figure below:"
msgstr ""
"由于 TCPStore 服务器线程的执行将被阻塞，直到所有客户端成功连接，我们测量了 0 号等级上的时间并作为总的 TCPStore "
"初始化运行时间。实验数据如图所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TCPStore Initialization Runtime Benchmark Result"
msgstr "TCPStore 初始化运行时间基准结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 1. shows some significant evidence that the libuv backend is superior"
" to the legacy backend:"
msgstr "图1 显示了一些显著证据表明 libuv 后端优于旧版后端："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TCPStore with libuv backend always has a faster initialization than the "
"legacy backend, especially at super-large scale"
msgstr "使用 libuv 后端的 TCPStore 总是比旧版后端初始化更快，特别是在超大规模时。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The legacy backend would timeout at server-client connecting at 96K scale "
"(for example, over 30 minutes) while the libuv backend completed the "
"initialization in 100 seconds."
msgstr "旧版后端在 96K 规模时服务器与客户端连接可能会超时（例如，超过30分钟），而 libuv 后端在 100 秒内完成了初始化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The second benchmark we did is to measure the runtime of TCPStore "
"``store_based_barrier`` operation:"
msgstr "我们做的第二个基准测试是测量 TCPStore 的 ``store_based_barrier`` 操作运行时间："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We compute the average by dividing the runtime measured on rank 0 by "
"``number_runs`` and report it in the figure below:"
msgstr "我们通过将 0 号等级上的运行时间除以 ``number_runs`` 来计算平均值，并报告在图中："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TCPStore Barrier Runtime Benchmark Result"
msgstr "TCPStore Barrier 运行时间基准结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 2. shows that the I/O performance of libuv backend is comparable to "
"the legacy backend:"
msgstr "图2 显示了 libuv 后端的 I/O 性能与旧版后端可比："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The libuv backend has a comparable performance over the whole spectrum in "
"terms of the number of ranks"
msgstr "libuv 后端在等级数量方面的整体表现是可比的"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The libuv backend runtime is more stable than the legacy backend as the "
"number of ranks grows"
msgstr "随着等级数量的增加，libuv 后端的运行时间比旧版后端更稳定"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Impact"
msgstr "影响"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One incompatibility that users may need to pay attention is, TCPStore "
"currently does not support initialization with a ``listen_fd`` when using "
"libuv backend. If the user wants to keep using this initialization method, "
"the user can simply pass ``use_libuv=False`` to stay with the old TCPStore "
"backend."
msgstr ""
"用户需要注意的是，使用 libuv 后端时，TCPStore 当前不支持通过 ``listen_fd`` "
"初始化。如果用户想继续使用此初始化方法，可以简单地传递 ``use_libuv=False`` 来继续使用旧版 TCPStore 后端。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Exit Route 1: Pass ``use_libuv=False`` to TCPStore Initialization"
msgstr "解决方案1：向 TCPStore 初始化传递 ``use_libuv=False``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the above code snippet shows, if user calls TCPStore init method to "
"create a store, simply passing ``use_libuv=False`` allows user to remain "
"using the old TCPStore backend. This override has the highest priority over "
"other approaches determining which backend the TCPStore server should "
"choose."
msgstr ""
"如上代码片段所示，如果用户调用 TCPStore 初始化方法创建状态，只需传递 ``use_libuv=False`` 就可以让用户保持使用旧版 "
"TCPStore 后端。此覆盖方式的优先级最高。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Exit Route 2: Add ``use_libuv=0`` to ``init_method`` at ProcessGroup "
"Initialization"
msgstr "解决方案2：在 ProcessGroup 初始化时向 ``init_method`` 添加 ``use_libuv=0``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``ProcessGroup`` creates a TCPStore if user does not explicitly pass one to "
"its initialization. User can add the query option ``use_libuv=0`` to "
"``init_method`` when initializing the ``ProcessGroup``. This approach has "
"lower priority than Exit Route 1."
msgstr ""
"如果用户在初始化 ``ProcessGroup`` 时没有明确传递 TCPStore，``ProcessGroup`` 会创建一个 "
"TCPStore。用户可以在初始化 ``ProcessGroup`` 时向 ``init_method`` 添加查询选项 "
"``use_libuv=0``。此方法优先级低于解决方案1。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Exit Route 3: Set Environment Variable ``USE_LIBUV`` to ``0``"
msgstr "解决方案3：设置环境变量 ``USE_LIBUV`` 为 ``0``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When ProcessGroup creates a TCPStore, it also checks the environment "
"vairable ``USE_LIBUV`` to determine which TCPStore backend to use. User can "
"set the environment variable ``\"USE_LIBUV\"`` to ``\"0\"`` to specify the "
"use of old TCPStore backend. This approach has lower priority than Exit "
"Route 2, for example, if the user sets environment variable ``USE_LIBUV`` to"
" ``1`` and also passes ``use_libuv=0`` in ``init_method``, then the old "
"store backend will be chosen."
msgstr ""
"当 ProcessGroup 创建 TCPStore 时，它还会检查环境变量 ``USE_LIBUV`` 来决定使用哪个 TCPStore "
"后端。用户可以将环境变量 ``\"USE_LIBUV\"`` 设置为 ``\"0\"`` 以指定使用旧的 TCPStore "
"后端。此方法的优先级低于退出路径 2，例如，如果用户将环境变量 ``USE_LIBUV`` 设置为 ``1`` 同时在 ``init_method`` "
"中传递 ``use_libuv=0``，那么旧的存储后端将被选择。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Conclusion"
msgstr "结论"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In PyTorch 2.4, we made the new libuv TCPStore backend the default. Although"
" the new backend has incompatibility with initialization from a "
"``listen_fd``, it shows significant performance improvement on store "
"initialization at large-scale and compatible performance on store I/O at "
"small/medium/large scales, which brings a major benefit to Distributed "
"Training's control plane. This tutorial explains our motivation, goes "
"through the performance benchmark, notifies users of the potential impact, "
"and introduces three exit routes to remain using the legacy backend. In the "
"long term, we aim to eventually deprecate the legacy backend."
msgstr ""
"在 PyTorch 2.4 中，我们将新的 libuv TCPStore 后端设置为默认值。尽管新的后端无法从 ``listen_fd`` "
"初始化，但它在大规模存储初始化上表现出显著的性能提升，并在小/中/大规模存储 I/O "
"上具有兼容的性能，这为分布式训练的控制平面带来了重大益处。本教程解释了我们的动机，回顾了性能基准测试，通知用户潜在影响，并介绍了三种继续使用旧版后端的退出路径。从长远来看，我们的目标是最终弃用旧版后端。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Large Scale Transformer model training with Tensor Parallel (TP)"
msgstr "使用张量并行 (TP) 进行大规模 Transformer 模型训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Author**: `Wanchao Liang <https://github.com/wanchaol>`__, `Tianyu Liu "
"<https://github.com/tianyu-l>`__"
msgstr ""
"**作者**: `Wanchao Liang <https://github.com/wanchaol>`__, `Tianyu Liu "
"<https://github.com/tianyu-l>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst>`__"
" 中查看和编辑本教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to train a large Transformer-like model "
"across hundreds to thousands of GPUs using Tensor Parallel and Fully Sharded"
" Data Parallel."
msgstr "本教程演示了如何使用张量并行和完全分片的数据并行在数百到数千个 GPU 上训练大型 Transformer 类模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Prerequisites:"
msgstr "前提条件："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch 2.3.0 or later installed with CUDA/Linux"
msgstr "安装了 PyTorch 2.3.0 或更高版本，并启用了 CUDA/Linux"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Tensor Parallel APIs "
"<https://pytorch.org/docs/stable/distributed.tensor.parallel.html>`__"
msgstr ""
"`张量并行 API "
"<https://pytorch.org/docs/stable/distributed.tensor.parallel.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Getting Started with DeviceMesh "
"<https://pytorch.org/tutorials/recipes/distributed_device_mesh.html>`__"
msgstr ""
"`设备网格入门 "
"<https://pytorch.org/tutorials/recipes/distributed_device_mesh.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Getting Started with Fully Sharded Data Parallel "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__"
msgstr ""
"`完全分片的数据并行入门 "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How Tensor Parallel works?"
msgstr "张量并行如何工作？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Tensor Parallel (TP) was originally proposed in the `Megatron-LM "
"<https://arxiv.org/abs/1909.08053>`__ paper, and it is an efficient model "
"parallelism technique to train large scale Transformer models. `Sequence "
"Parallel <https://arxiv.org/abs/2205.05198>`__ (SP) we mention in this "
"tutorial is a variant of Tensor Parallel that shards on the sequence "
"dimension for ``nn.LayerNorm`` or ``RMSNorm`` to further save activation "
"memory during training. As the model becomes larger, the activation memory "
"becomes the bottleneck, so in Tensor Parallel training it usually applies "
"Sequence Parallel to ``LayerNorm`` or ``RMSNorm`` layers."
msgstr ""
"张量并行 (TP) 最初是在 `Megatron-LM <https://arxiv.org/abs/1909.08053>`__ "
"论文中提出的，它是一种高效的模型并行技术，用于训练大规模 Transformer 模型。本教程中提到的 `序列并行 "
"<https://arxiv.org/abs/2205.05198>`__ (SP) 是张量并行的一种变体，可以在 ``nn.LayerNorm`` 或"
" ``RMSNorm`` "
"的序列维度上进行分片，从而进一步节省训练期间的激活内存。随着模型变得更大，激活内存成为瓶颈，因此在张量并行训练中通常将序列并行应用于 "
"``LayerNorm`` 或 ``RMSNorm`` 层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Megatron-LM TP"
msgstr "Megatron-LM 张量并行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 1. represents the sharding in Tensor Parallel style on a Transformer "
"model’s MLP and Self-Attention layer, where the matrix multiplications in "
"both attention/MLP happens through sharded computations (`image source "
"<https://arxiv.org/abs/1909.08053>`__)"
msgstr ""
"图 1 表示在 Transformer 模型的 MLP 和自注意力层上以张量并行方式进行分片，其中注意力和 MLP "
"中的矩阵乘法通过分片计算完成（`图片来源 <https://arxiv.org/abs/1909.08053>`__）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "At a high level, PyTorch Tensor Parallel works as follows:"
msgstr "从高层次来看，PyTorch 张量并行的工作原理如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Sharding initialization**"
msgstr "**分片初始化**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Determine which ``ParallelStyle`` to apply to each layer and shard the "
"initialized module by calling ``parallelize_module``."
msgstr ""
"确定对每一层应用哪种 ``ParallelStyle``，并通过调用 ``parallelize_module`` 对初始化的模块进行分片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The parallelized modules would have their model parameters be swapped to "
"DTensors, and DTensor would be responsible to run the parallelized module "
"using sharded computation."
msgstr "并行化的模块将其模型参数交换为 DTensors，DTensor 将负责使用分片计算运行并行化模块。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Runtime foward/backward**"
msgstr "**运行时前向/后向**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Depending on the input/outputs DTensor layouts user specified for each "
"``ParallelStyle``, it would run proper communication operation to transform "
"the DTensor layouts for inputs/outputs (such as ``allreduce``, ``allgather``"
" and ``reduce_scatter``)."
msgstr ""
"根据用户为每个 ``ParallelStyle`` 指定的输入/输出 DTensor 布局，运行适当的通信操作以转换输入/输出的 DTensor "
"布局（例如 ``allreduce``、``allgather`` 和 ``reduce_scatter``）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Run sharded computation for the parallelized layers to save compute/memory "
"(for example, ``nn.Linear``, ``nn.Embedding``)."
msgstr "对并行化的层运行分片计算以节省计算和内存（例如 ``nn.Linear``、``nn.Embedding``）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "When and Why you should apply Tensor Parallel"
msgstr "何时以及为什么应用张量并行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The PyTorch Fully Sharded Data Parallel (FSDP) already has the capability to"
" scale model training to a specific number of GPUs. However, when it comes "
"to further scale the model training in terms of model size and GPU quantity,"
" many additional challenges arise that may require combining Tensor Parallel"
" with FSDP.:"
msgstr ""
"PyTorch 的完全分片的数据并行 (FSDP) 已经能够将模型训练扩展到特定数量的 GPU。然而，当进一步在模型规模和 GPU "
"数量方面扩展模型训练时，会出现许多额外的挑战，这可能需要结合张量并行与 FSDP："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the world size (number of GPUs) is becoming excessively large (exceeding "
"128/256 GPUs), the FSDP collectives (such as ``allgather``) are being "
"dominated by ring latency. By implementing TP/SP on top of FSDP, the FSDP "
"world size could be reduced by 8 by applying FSDP to be inter-host only, "
"consequently decreasing the latency costs by the same amount."
msgstr ""
"当世界大小（即 GPU 数量）变得过大（超过 128/256 个 GPU）时，FSDP 的集合通信（例如 "
"``allgather``）开始被环形延迟所支配。通过在 FSDP 上实现 TP/SP，可以将 FSDP 的世界大小减少 8 倍，通过仅在主机间应用 "
"FSDP，从而相应减少延迟成本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Hit data parallelism limit where you can not raise the global batch size to "
"be above the number of GPUs due to both convergence and GPU memory "
"limitations, Tensor/Sequence Parallel is the only known way to “ballpark” "
"the global batch size and continue scaling with more GPUs. This means both "
"model size and number of GPUs could continue to scale."
msgstr ""
"数据并行性达到限制，即由于收敛性和 GPU 内存限制，无法将全局批大小提高到 GPU "
"数量之上，张量/序列并行是唯一已知的方法，可以“大致估算”全局批大小并继续通过更多 GPU 扩展。这意味着模型规模和 GPU 数量都可以继续扩展。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For certain types of models, when local batch size becomes smaller, TP/SP "
"can yield matrix multiplication shapes that are more optimized for floating "
"point operations (FLOPS)."
msgstr "对于某些类型的模型，当本地批量大小变得较小时，TP/SP 可以生成更优化浮点操作（FLOPS）的矩阵乘法形状。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So, when pre-training, how easy is it to hit those limits? As of now, pre-"
"training a Large Language Model (LLM) with billions or trillions of tokens "
"could take months, even when using thousands of GPUs."
msgstr ""
"那么，在预训练时，达到这些限制有多容易？截至目前，即使使用数千个 GPU，预训练一个有数十亿或万亿个标记的大型语言模型 (LLM) 也可能需要数月时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It will always hit limitation 1 when training LLM on a large scale. For "
"example, Llama 2 70B trained with 2k GPUs for 35 days, multi-dimensional "
"parallelisms are needed at 2k scale."
msgstr ""
"在大规模训练 LLM 时，总是会达到限制 1。例如，Llama 2 70B 在 2000 个 GPU 上训练 35 天，需要在 2000 "
"规模上使用多维并行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When the Transformer model becomes larger (such as Llama2 70B), it will also"
" quickly hit the limitation 2. One could not use FSDP alone with even local "
"``batch_size=1`` due to memory and convergence constraints. For example, "
"Llama 2 global batch size is 1K, so data parallelism alone can not be used "
"at 2K GPUs."
msgstr ""
"当 Transformer 模型变得更大（例如 Llama2 70B）时，也会迅速达到限制 2。即使本地 "
"``batch_size=1``，也不能单独使用 FSDP，原因是内存和收敛性限制。例如，Llama 2 的全局批量大小为 "
"1K，因此仅数据并行性无法用于 2K GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to apply Tensor Parallel"
msgstr "如何应用张量并行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch Tensor Parallel APIs offers a set of module level primitives "
"(``ParallelStyle``) to configure the sharding for each individual layers of "
"the model, including:"
msgstr "PyTorch 张量并行 API 提供了一组模块级原语（``ParallelStyle``）来配置模型每个单层的分片，包括："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``ColwiseParallel`` and ``RowwiseParallel``: Shard the ``nn.Linear`` and "
"``nn.Embedding`` in the column or row fashion."
msgstr ""
"``ColwiseParallel`` 和 ``RowwiseParallel``：以列或行方式分片 ``nn.Linear`` 和 "
"``nn.Embedding``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``SequenceParallel``: Perform sharded computations on ``nn.LayerNorm``, "
"``nn.Dropout``, ``RMSNormPython``, etc."
msgstr ""
"``SequenceParallel``：对 ``nn.LayerNorm``、``nn.Dropout``、``RMSNormPython`` "
"等进行分片计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``PrepareModuleInput`` and ``PrepareModuleOutput``: Configure the module "
"inputs/outputs sharding layouts with proper communication operations."
msgstr ""
"``PrepareModuleInput`` 和 ``PrepareModuleOutput``：通过适当的通信操作配置模块输入/输出的分片布局。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To demonstrate how to use the PyTorch native Tensor Parallel APIs, let us "
"look at a common Transformer model. In this tutorial, we use the most recent"
" `Llama2 model "
"<https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py>`__"
" as a reference Transformer model implementation, as it is also widely used "
"in the community."
msgstr ""
"为了演示如何使用 PyTorch 原生张量并行 API，让我们来看看一个常见的 Transformer 模型。本教程中，我们使用最新的 `Llama2 "
"模型 "
"<https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py>`__"
" 作为参考 Transformer 模型实现，因为它也被社区广泛使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since Tensor Parallel shard individual tensors over a set of devices, we "
"would need to set up the distributed environment (such as NCCL "
"communicators) first. Tensor Parallelism is a Single-Program Multiple-Data "
"(SPMD) sharding algorithm similar to PyTorch DDP/FSDP, and it under the hood"
" leverages the PyTorch DTensor to perform sharding. It also utilizes the "
"DeviceMesh abstraction (which under the hood manages ProcessGroups) for "
"device management and sharding. To see how to utilize DeviceMesh to set up "
"multi-dimensional parallelisms, please refer to `this tutorial "
"<https://pytorch.org/tutorials/recipes/distributed_device_mesh.html>`__. "
"Tensor Parallel usually works within each host, so let us first initialize a"
" DeviceMesh that connects 8 GPUs within a host."
msgstr ""
"由于张量并行对单个张量在一组设备上进行分片，我们需要首先设置分布式环境（例如 NCCL 通信器）。张量并行是一种类似于 PyTorch DDP/FSDP"
" 的单程序多数据 (SPMD) 分片算法，底层利用 PyTorch DTensor 执行分片。它还利用 DeviceMesh 抽象（底层管理 "
"ProcessGroups）进行设备管理和分片。要了解如何利用 DeviceMesh 设置多维并行，请参考 `本教程 "
"<https://pytorch.org/tutorials/recipes/distributed_device_mesh.html>`__。张量并行通常在每个主机内工作，因此我们首先初始化一个连接主机内"
" 8 个 GPU 的 DeviceMesh。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have initialized DeviceMesh, let us take a detailed look at the "
"Llama 2 model architecture and see how we should perform the Tensor Parallel"
" sharding. Here we focus on the core ``TransformerBlock``, where the "
"Transformer model stacks the identical ``TransformerBlock`` s to scale up "
"the model."
msgstr ""
"现在我们已经初始化了 DeviceMesh，让我们详细查看 Llama 2 模型架构并了解如何执行张量并行分片。这里我们重点关注核心 "
"``TransformerBlock``，其中 Transformer 模型堆叠多个相同的 ``TransformerBlock`` 来增强模型规模。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The core ``TransformerBlock`` consists of an ``Attention`` layer and a "
"``FeedForward`` layer. Let us first look at the simpler ``FeedForward`` "
"layer. For the ``FeedForward`` Layer it consists of three Linear layers, "
"where it performs a SwiGLU style MLP, looking at its forward function:"
msgstr ""
"核心 ``TransformerBlock`` 包括一个 ``Attention`` 层和一个 ``FeedForward`` 层。我们先来看简化版 "
"``FeedForward`` 层。对于 ``FeedForward`` 层，它由三个线性层组成，其中执行了 SwiGLU 风格的 "
"MLP，看其前向函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It performs ``w1`` and ``w3`` matmuls concurrently and followed by a ``w2`` "
"matmul with the result of the combined w1/w3 linear projection results. This"
" means we could use the idea from the Tensor Parallelism paper to shard the "
"w1/w3 Linear layers in the colwise fashion and shard the ``w2`` Linear layer"
" in the rowwise fashion, so that there is only one ``allreduce`` "
"communication happening at the end of all the three layers. With the PyTorch"
" native Tensor Parallel, we can simply create a ``parallelize_plan`` for the"
" ``FeedForward`` layer like below:"
msgstr ""
"它并行执行 ``w1`` 和 ``w3`` 的矩阵乘法，然后结合 w1/w3 的线性投影结果进行 ``w2`` "
"的矩阵乘法。这意味着我们可以利用张量并行论文中的思路以列分片方式分片 w1/w3 线性层，并以行分片方式分片 ``w2`` "
"线性层，以便在所有三个层结束时仅发生一次 ``allreduce`` 通信。通过 PyTorch 原生张量并行，我们可以像下面这样为 "
"``FeedForward`` 层简单地创建一个 ``parallelize_plan``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That's simply how we configure the shardings for the ``FeedForward`` layer "
"using the PyTorch Tensor Parallel APIs. Note that users would only need to "
"specify how to shard the individual layers and the communications (for "
"example, ``allreduce``) will happen under the hood."
msgstr ""
"这就是我们如何使用 PyTorch 张量并行 API 配置 ``FeedForward`` "
"层分片的方式。请注意，用户只需指定如何分片单独的层，通信（例如 ``allreduce``）将在底层自动进行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Moving on to the ``Attention`` Layer. It consists of ``wq``, ``wk``, ``wv`` "
"Linear layers to project input to ``q``/ ``k`` / ``v``, and then it performs"
" attention and output projection with the ``wo`` Linear layer. Tensor "
"Parallelism here intends to perform column-wise sharding for the q/k/v "
"projection and row-wise sharding for the ``wo`` linear projection. So we can"
" add the Attention plan to the ``tp_plan`` that we just drafted up:"
msgstr ""
"接下来是 ``Attention`` 层。它包括将输入投射到 ``q``/ ``k``/ ``v`` 的 ``wq``、``wk``、``wv`` "
"线性层，然后使用 ``wo`` 线性层执行注意力和输出投影。这里张量并行意图对 q/k/v 投影进行列分片，对 ``wo`` "
"线性投影进行行分片。因此，我们可以将 Attention 计划添加到我们刚刚起草的 ``tp_plan`` 中："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is almost the ``layer_tp_plan`` we need to apply Tensor Parallelism to "
"the ``TransformerBlock``. However, one thing we should be aware is that when"
" sharding the linear layer column-wise, the output of the linear layers "
"would become sharded on the last tensor dimension, and the row-wise sharding"
" linear layer directly accepts an input that shards on the last dimension. "
"If there are any more tensor operations (such as view operations) between "
"the column-wise linear and the row-wise linear, we would need to adjust the "
"relevant shape related ops to sharded shape."
msgstr ""
"这几乎就是我们需要应用张量并行的 ``layer_tp_plan``，以用于 "
"``TransformerBlock``。然而，我们需要注意的是，在对线性层进行列分片时，线性层的输出会在张量的最后一个维度上分片，而行分片线性层直接接收在最后一个维度上分片的输入。如果在列分片线性层和行分片线性层之间还有其他张量操作（例如视图操作），我们需要调整相关的与形状有关的操作到分片形状。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the Llama model, in the attention layer there are couple of view "
"operations that are shape related. In particular, column-wise parallel for "
"``wq``/ ``wk``/ ``wv`` linear layers, the activation tensor is sharded on "
"the ``num_heads`` dimension, so we would need to adjust the ``num_heads`` to"
" local ``num_heads``."
msgstr ""
"对于 Llama 模型，注意力层中有一些与形状相关的视图操作。特别是对于 ``wq``、``wk`` 和 ``wv`` 线性层的列分片并行，激活张量在 "
"``num_heads`` 维度上分片，因此我们需要调整 ``num_heads`` 到本地 ``num_heads``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, we need to call ``parallelize_module`` API to make the plan for "
"each ``TransformerBlock`` effective. Under the hood, it distributes the "
"model parameters inside ``Attention`` and ``FeedForward`` layers to "
"DTensors, and registers communication hooks for model inputs and outputs "
"(before and after each module respectively), if necessary:"
msgstr ""
"最后，我们需要调用 ``parallelize_module`` API 使每个 ``TransformerBlock`` 的计划生效。底层会将 "
"``Attention`` 和 ``FeedForward`` 层中的模型参数分发到 "
"DTensors，并为模型输入和输出（分别在每个模块之前和之后）注册通信钩子（如有必要）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have elaborated the sharding plan for each ``TransformerBlock``,"
" there is usually a ``nn.Embedding`` in the first layer and a final "
"``nn.Linear`` projection layer, where user could choose row-wise or column-"
"wise sharding to the first ``nn.Embedding`` and column-wise sharding to the "
"last ``nn.Linear`` projection layer with proper input and output layouts "
"specified. Here is an example:"
msgstr ""
"现在我们已经详细制定了每个``TransformerBlock``的分片计划，通常在第一层会有一个``nn.Embedding``，最后会有一个``nn.Linear``投影层。在这些层中，用户可以选择对第一个``nn.Embedding``进行行切分或列切分，并通过指定适当的输入和输出布局，对最后的``nn.Linear``投影层进行列切分。以下是一个示例："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If the model to be partitioned is too large to fit into CPU memory, one "
"could either use ``meta`` device initialization (for example, initialize the"
" model on meta device first, shard the layers, and the materialize the "
"model), or parallelize the ``TransformerBlock`` layer by layer during the "
"Transformer model initialization."
msgstr ""
"如果要分割的模型太大，无法放入 CPU 内存，可以使用``meta``设备初始化（例如，首先在 meta "
"设备上初始化模型，然后分割层，再实现模型），或者在 Transformer 模型初始化期间逐层并行化``TransformerBlock``层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Apply Sequence Parallel to ``LayerNorm/RMSNorm`` layers"
msgstr "将序列并行应用于``LayerNorm/RMSNorm``层"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Sequence Parallel works on top of the Tensor Parallel illustrated above. "
"Compared with basic Tensor Parallel, which only shards tensors within the "
"``Attention`` modules and ``FeedForward`` modules and keep their module "
"inputs and outputs (namely activations in the forward pass and gradients in "
"the backward pass) replicated, Sequence Parallel keeps them sharded on the "
"sequence dimension."
msgstr ""
"序列并行基于上面介绍的张量并行构建。与基本的张量并行相比，它不仅分割``Attention``模块和``FeedForward``模块中的张量，还将它们的模块输入和输出（即在前向传递中的激活和在反向传递中的梯度）保持分割，而不是复制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In a typical ``TransformerBlock``, the forward function combines norm layers"
" (``LayerNorm`` or ``RMSNorm``), an attention layer, a feed forward layer, "
"and residual connections. For example:"
msgstr ""
"在典型的``TransformerBlock``中，前向函数结合了规范层（``LayerNorm``或``RMSNorm``）、注意力层、前馈层和残差连接。例如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In most use cases, the activations (and gradients) are of the shape ``[batch"
" size, sequence length, hidden dimension]`` outside the ``Attention`` and "
"``FeedForward`` modules. In the DTensor’s language, Sequence Parallel "
"performs activation computation using the ``Shard(1)`` layout for both "
"forward/backward of the module. Following the code example earlier, the code"
" below demonstrates how we apply Sequence Parallel to the norm layers within"
" a ``TransformerBlock``:"
msgstr ""
"在大多数使用场景中，激活（以及梯度）的形状通常是``[批量大小，序列长度，隐藏维度]``，位于``Attention``和``FeedForward``模块之外。在"
" DTensor "
"的语言中，序列并行使用``Shard(1)``布局对模块进行前向和后向激活计算。以下代码展示了如何将序列并行应用于``TransformerBlock``中的规范层："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "First let's import the required dependencies for Sequence Parallel:"
msgstr "首先我们导入序列并行所需的依赖项："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next let's adjust the ``layer_tp_plan`` to enable sequence parallel on the "
"``RMSNorm`` layers:"
msgstr "接下来我们调整``layer_tp_plan``以在``RMSNorm``层上启用序列并行："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One can see we now use ``PrepareModuleInput`` to modify the module input "
"layouts to the Attention and FeedForward layers from ``Shard(1)`` to "
"``Replicate()``, and mark their output layouts as ``Shard(1)``. Just like "
"what happens to Tensor Parallelism, one only needs to specify the tensor "
"sharding layouts of the inputs and outputs, and the communication between "
"layers will happen automatically."
msgstr ""
"可以看到，我们现在使用``PrepareModuleInput``将 Attention 和 FeedForward "
"层的模块输入布局从``Shard(1)``修改为``Replicate()``，并将它们的输出布局标记为``Shard(1)``。就像张量并行性一样，我们只需要指定输入和输出的张量分片布局，层之间的通信将自动发生。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that with Sequence Parallel, we assume the inputs and outputs of a "
"``TransformerBlock`` are always sharded on the sequence dimension, so that "
"multiple ``TransformerBlocks`` can be concatenated seamlessly. This can be "
"facilitated by explicitly specifying the output of the beginning "
"``nn.Embedding`` layer and the input of the final ``nn.Linear`` projection "
"layer to be ``Shard(1)``:"
msgstr ""
"请注意，使用序列并行时，我们假设``TransformerBlock``的输入和输出始终在序列维度上分片，以便可以无缝连接多个``TransformerBlocks``。这可以通过显式指定起始``nn.Embedding``层的输出和最终``nn.Linear``投影层的输入为``Shard(1)``来实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Apply Loss Parallel"
msgstr "应用损失并行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Loss Parallel is a related technique to save memory and communication when "
"the loss function is computed, as model outputs are usually very large. In "
"Loss Parallel, when the model outputs are sharded on the (often huge) "
"vocabulary dimension, the cross-entropy loss can be computed efficiently, "
"without gathering all the model outputs to every single GPU. This not only "
"significantly reduces the memory consumption, but also improves training "
"speed by reducing communication overhead and doing sharded computation in "
"parallel. The picture below briefly illustrates how Loss Parallel avoids "
"gathering all model outputs to every GPU by doing sharded computation."
msgstr ""
"损失并行是一种相关技术，用于在计算损失函数时节省内存和通信，因为模型的输出通常非常大。在损失并行中，当模型输出在（通常是巨大的）词汇维度上分片时，可以高效地计算交叉熵损失，而无需将所有模型输出收集到每个单独的"
" "
"GPU。这不仅显著减少了内存消耗，还通过减少通信开销和在并行中进行分片计算来提高训练速度。下图简要说明了损失并行如何通过分片计算避免将所有模型输出收集到每个"
" GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "loss parallel"
msgstr "损失并行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 2. Cross-entropy loss forward computation with loss parallel on one "
"GPU. Blue represents sharded tensors; green represents replicated tensors; "
"yellow represents tensors with partial values (to be all-reduced). Black "
"arrows are local computations; red arrows are functional collectives among "
"GPUs."
msgstr ""
"图 2. 使用损失并行在单个 GPU "
"上进行交叉熵损失的前向计算。蓝色表示分片张量；绿色表示复制张量；黄色表示具有部分值的张量（待全局缩减）。黑色箭头是本地计算；红色箭头是 GPU "
"之间的功能集合。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the PyTorch Tensor Parallel API, Loss Parallel can be enabled via a "
"context manager ``loss_parallel``, with which one can directly use "
"``torch.nn.functional.cross_entropy`` or ``torch.nn.CrossEntropyLoss`` "
"without modifying other parts of their code."
msgstr ""
"在 PyTorch 的张量并行 API "
"中，可以通过上下文管理器``loss_parallel``启用损失并行，使用此管理器，一可以直接使用``torch.nn.functional.cross_entropy``或``torch.nn.CrossEntropyLoss``，而无需修改代码的其他部分。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To apply Loss Parallel, the model predictions, usually of the shape ``[batch"
" size, sequence length, vocabulary size]``, should be sharded on the "
"vocabulary dimension. This can be easily done via marking the output layouts"
" of the last linear projection layer output:"
msgstr ""
"要应用损失并行，模型预测通常的形状``[批量大小，序列长度，词汇表大小]``应该在词汇表维度上分片。这可以通过标记最后线性投影层输出的布局轻松完成："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the code above, we also apply Sequence Parallel to the norm layer before "
"output. We apply ``use_local_output=False`` to let the output stay as a "
"DTensor, to work with the ``loss_parallel`` context manager. After that, one"
" can simply call the cross_entropy loss function as is shown below. Note "
"that the backward computation also needs to happen within the context."
msgstr ""
"在上面的代码中，我们还将序列并行应用于输出之前的规范层。我们应用``use_local_output=False``以使输出保持为 "
"DTensor，能够与``loss_parallel``上下文管理器一起工作。在此之后，可以简单地调用交叉熵损失函数，如下所示。请注意，反向计算也需要在上下文内进行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Combine Tensor Parallel with Fully Sharded Data Parallel together"
msgstr "将张量并行与完全分片数据并行结合使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have shown how to apply Tensor/Sequence Parallel to the model, "
"let us also take a look at how Tensor Parallel and Fully Sharded Data "
"Parallel could work together. Since Tensor Parallelism incurs communications"
" that block the computation, we want to make sure it runs within a fast "
"communication channel, such as NVLink. In practice, we usually apply Tensor "
"Parallel within each host, and apply Fully Sharded Data Parallel across the "
"hosts."
msgstr ""
"现在我们已经展示了如何将张量/序列并行应用于模型，让我们看一下张量并行和完全分片数据并行如何协同工作。由于张量并行性会导致阻止计算的通信，我们需要确保它在一个快速通信通道内运行，例如"
" NVLink。在实践中，我们通常在每个主机内应用张量并行，并在主机之间应用完全分片数据并行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "fsdp + tp"
msgstr "FSDP + TP"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 3. FSDP and TP work on separate device dimensions, FSDP communication"
" happens inter-host and TP communication happens intra-host."
msgstr "图 3. FSDP 和 TP 在单独的设备维度上工作，FSDP 通信发生在主机之间，TP 通信发生在主机内部。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This 2-D parallelism pattern can be easily expressed via a 2-D DeviceMesh, "
"and we just need pass each “sub” DeviceMesh to each individual parallelism "
"APIs:"
msgstr "这种二维并行模式可以通过二维设备网格轻松表达，我们只需将每个“子”设备网格传递给各自的并行 API："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This would allow us to easily apply Tensor Parallel within each host (intra-"
"host) and apply FSDP across hosts (inter-hosts), with **0-code changes** to "
"the Llama model. The Tensor(Model) Parallel and Data Parallel techniques "
"combined together provides the ability to continue increasing model size and"
" training efficiently using a large number of GPUs."
msgstr ""
"这使我们能够轻松地在每个主机内（主机内部）应用张量并行，并在主机间（主机之间）应用 FSDP，对 Llama "
"模型无代码更改。张量（模型）并行和数据并行技术结合在一起提供了继续增大模型规模的能力，并使用大量 GPU 进行高效训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to train a large Transformer-like model "
"across hundreds to thousands of GPUs using Tensor Parallel in combination "
"with Fully Sharded Data Parallel. It explains how to apply Tensor Parallel "
"to different parts of the model, with **no code changes** to the model "
"itself. Tensor Parallel is a efficient model parallelism technique for large"
" scale training."
msgstr ""
"本教程演示了如何使用张量并行结合完全分片数据并行在数百到数千个 GPU 上训练大型 Transformer "
"类模型。它解释了如何将张量并行应用于模型的不同部分，并且对模型本身没有代码更改。张量并行是一种高效的大规模训练模型并行技术。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To see the complete end-to-end code example explained in this tutorial, "
"please refer to the `Tensor Parallel examples "
"<https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/fsdp_tp_example.py>`__"
" in the pytorch/examples repository."
msgstr ""
"要查看本教程中解释的完整端到端代码示例，请参阅 PyTorch/examples 存储库中的`张量并行示例 "
"<https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/fsdp_tp_example.py>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_autograd_saved_tensors_hooks_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_intermediate_autograd_saved_tensors_hooks_tutorial.py>` "
"下载完整的示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Hooks for autograd saved tensors"
msgstr "用于自动求导保存张量的钩子"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch typically computes gradients using backpropagation. However, certain"
" operations require intermediary results to be saved in order to perform "
"backpropagation. This tutorial walks through how these tensors are "
"saved/retrieved and how you can define hooks to control the "
"packing/unpacking process."
msgstr ""
"PyTorch "
"通常使用反向传播计算梯度。然而，某些操作需要保存中间结果以进行反向传播。本教程将详细探讨如何保存/检索这些张量以及如何定义钩子以控制打包/解包过程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial assumes you are familiar with how backpropagation works in "
"theory. If not, read `this "
"<https://colab.research.google.com/drive/1aWNdmYt7RcHMbUk-"
"Xz2Cv5-cGFSWPXe0#scrollTo=AHcEJ6nXUb7W>`_ first."
msgstr ""
"本教程假设你已经熟悉反向传播的原理。如果不熟悉，请先阅读`这一部分 "
"<https://colab.research.google.com/drive/1aWNdmYt7RcHMbUk-"
"Xz2Cv5-cGFSWPXe0#scrollTo=AHcEJ6nXUb7W>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saved tensors"
msgstr "保存的张量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Training a model usually consumes more memory than running it for inference."
" Broadly speaking, one can say that it is because “PyTorch needs to save the"
" computation graph, which is needed to call ``backward``”, hence the "
"additional memory usage. One goal of this tutorial is to finetune this "
"understanding."
msgstr ""
"训练模型通常比推理运行模型消耗更多的内存。广义上可以说，这是因为“PyTorch "
"需要保存计算图，而这会调用``backward``”，因此增加了内存使用。本教程的目标之一是精确化这种理解。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In fact, the graph in itself sometimes does not consume much more memory as "
"it never copies any tensors. However, the graph can keep *references* to "
"tensors that would otherwise have gone out of scope: those are referred to "
"as **saved tensors**."
msgstr ""
"事实上，计算图本身有时并不大量占用内存，因为它从未复制任何张量。然而，计算图可以保留那些原本会超出范围的张量的*引用*：这些被称为**保存的张量**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Why does training a model (typically) requires more memory than evaluating "
"it?"
msgstr "为什么训练一个模型（通常）需要比评估它更多的内存？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We start with a simple example: :math:`y = a \\cdot b` , for which we know "
"the gradients of :math:`y` with respect to :math:`a` and :math:`b`:"
msgstr ""
"我们从一个简单的例子开始： :math:`y = a \\cdot b` ，对于该例子我们知道 :math:`y` 对 :math:`a` 和 "
":math:`b` 的梯度："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\frac{\\partial y}{\\partial a} = b"
msgstr ""
"\\frac{\\partial y}{\\partial a} = b\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\frac{\\partial y}{\\partial b} = a"
msgstr ""
"\\frac{\\partial y}{\\partial b} = a\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using a torchviz, we can visualize the computation graph"
msgstr "使用 torchviz，我们可以可视化计算图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example, PyTorch saves intermediary values :math:`a` and :math:`b` "
"in order to compute the gradient during the backward."
msgstr "在这个例子中，PyTorch 保存了中间值 :math:`a` 和 :math:`b`，以便在反向传递中计算梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Those intermediary values (in orange above) can be accessed (for debugging "
"purposes) by looking for attributes of the ``grad_fn`` of ``y`` which start "
"with the prefix ``_saved``:"
msgstr "这些中间值（上图橙色标记）可以通过查看``y``的``grad_fn``属性，并寻找前缀为``_saved``的属性进行调试访问："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the computation graph grows in depth, it will store more *saved tensors*."
" Meanwhile, those tensors would have gone out of scope if not for the graph."
msgstr "随着计算图深度的增加，它将存储更多*保存的张量*。同时，这些张量如果不是计算图存储，那么通常会超出范围。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example above, executing without grad would only have kept ``x`` and "
"``y`` in the scope, But the graph additionally stores ``f(x)`` and "
"``f(f(x))``. Hence, running a forward pass during training will be more "
"costly in memory usage than during evaluation (more precisely, when autograd"
" is not required)."
msgstr ""
"在上述例子中，如果执行不带渐变的操作，范围内将仅保留``x``和``y``。但是计算图额外存储了``f(x)``和``f(f(x))``。因此，训练期间运行一次前向传递在内存使用上比评估期间更昂贵（更准确地说，当不需要自动求导时）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The concept of packing / unpacking"
msgstr "打包/解包的概念"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Going back to the first example: ``y.grad_fn._saved_self`` and "
"``y.grad_fn._saved_other`` point to the original tensor object, respectively"
" ``a`` and ``b``."
msgstr ""
"回到第一个例子：``y.grad_fn._saved_self`` 和 ``y.grad_fn._saved_other`` "
"分别指向原始张量对象``a``和``b``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "However, that may not always be the case."
msgstr "然而，这种情况并不是总是如此。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Under the hood, PyTorch has **packed** and **unpacked** the tensor ``y`` to "
"prevent reference cycles."
msgstr "在幕后，PyTorch 已经对张量``y``进行了**打包**和**解包**操作以防止引用循环。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As a rule of thumb, you should *not* rely on the fact that accessing the "
"tensor saved for backward will yield the same tensor object as the original "
"tensor. They will however share the same *storage*."
msgstr "作为经验法则，你应该*不*依赖于访问保存的用于反向传播的张量会产生与原始张量对象相同的张量。它们将共享相同的*存储*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saved tensors hooks"
msgstr "保存张量的钩子"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch provides an API to control how saved tensors should be packed / "
"unpacked."
msgstr "PyTorch 提供了一个 API 来控制保存的张Tensor如何打包/解包。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``pack_hook`` function will be called every time an operation saves a "
"tensor for backward. The output of ``pack_hook`` is then stored in the "
"computation graph instead of the original tensor. The ``unpack_hook`` uses "
"that return value to compute a new tensor, which is the one actually used "
"during the backward pass. In general, you want ``unpack_hook(pack_hook(t))``"
" to be equal to ``t``."
msgstr ""
"每当操作保存用于反向传播的张量时，``pack_hook``函数都会被调用。``pack_hook``的输出随后将被存储在计算图中，而不是原始张量。``unpack_hook``使用该返回值计算新张量，这就是反向过程中实际使用的张量。通常，你希望``unpack_hook(pack_hook(t))``等于``t``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One thing to note is that the output of ``pack_hook`` can be *any Python "
"object*, as long as ``unpack_hook`` can derive a tensor with the correct "
"value from it."
msgstr ""
"需要注意的是，``pack_hook``的输出可以是*任何 Python "
"对象*，只要``unpack_hook``可以根据该对象派生出具有正确值的张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Some unconventional examples"
msgstr "一些非传统的例子"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, some silly examples to illustrate what is possible but you probably "
"don’t ever want to do it."
msgstr "首先用一些简单的例子来说明可能性，但你可能不会希望实际使用它们。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Returning an ``int``"
msgstr "返回一个``int``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Returning the index of a Python list Relatively harmless but with debatable "
"usefulness"
msgstr "返回一个 Python 列表的索引，虽然相对无害，但实用性值得商榷。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Returning a tuple"
msgstr "返回一个元组"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Returning some tensor and a function how to unpack it Quite unlikely to be "
"useful in its current form"
msgstr "返回某个张量以及解包它的方法，形式上极不可能实用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Returning a ``str``"
msgstr "返回一个``str``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Returning the ``__repr__ of`` the tensor Probably never do this"
msgstr "返回张量的``__repr__``，可能永远不会这么做"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Although those examples will not be useful in practice, they illustrate that"
" the output of ``pack_hook`` can really be any Python object as long as it "
"contains enough information to retrieve the content of the original tensor. "
"In the next sections, we focus on more useful applications."
msgstr ""
"虽然这些示例在实际中可能没用，但它们说明了``pack_hook``的输出可以是任何包含足够信息以检索原始张量内容的Python对象。在接下来的部分中，我们将重点关注更有用的应用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving tensors to CPU"
msgstr "将张量保存到CPU"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Very often, the tensors involved in the computation graph live on GPU. "
"Keeping a reference to those tensors in the graph is what causes most models"
" to run out of GPU memory during training while they would have done fine "
"during evaluation."
msgstr "在计算图中涉及的张量通常都存在于GPU上。在训练期间，保留对这些张量的引用是大多数模型GPU内存不足的原因，而在评估期间则能够正常运行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Hooks provide a very simple way to implement that."
msgstr "Hooks提供了一种非常简单的方法来实现这一点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In fact, PyTorch provides an API to conveniently use those hooks (as well as"
" the ability to use pinned memory)."
msgstr "实际上，PyTorch提供了一个API，方便使用这些hooks（以及使用固定内存的能力）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In practice, on a A100 GPU, for a ResNet-152 with batch size 256, this "
"corresponds to a GPU memory usage reduction from 48GB to 5GB, at the cost of"
" a 6x slowdown."
msgstr ""
"在实际应用中，在A100 "
"GPU上，对于一个批量大小为256的ResNet-152，这对应于GPU内存使用从48GB减少到5GB，但代价是6倍的速度减慢。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Of course, you can modulate the tradeoff by only saving to CPU certain parts"
" of the network."
msgstr "当然，您可以通过只将网络的某些部分保存到CPU来调整权衡。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For instance, you could define a special ``nn.Module`` that wraps any module"
" and saves its tensors to CPU."
msgstr "例如，您可以定义一个特殊的``nn.Module``，包装任意模块并将其张量保存到CPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving tensors to disk"
msgstr "将张量保存到磁盘"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Similarly, you may want to save those tensors to disk. Again, this is "
"achievable with those hooks."
msgstr "类似地，您可能希望将这些张量保存到磁盘。同样，这可以通过这些hooks实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "A naive version would look like this."
msgstr "一个简单的版本可能如下所示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The reason the above code is bad is that we are leaking files on the disk "
"and they are never cleared. Fixing this is not as trivial as it seems."
msgstr "上述代码的原因在于，我们在磁盘上泄漏了文件，而这些文件永远不会被清除。修复这个问题并不像看起来那么容易。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The reason the above code doesn’t work is that ``unpack_hook`` can be called"
" multiple times. If we delete the file during unpacking the first time, it "
"will not be available when the saved tensor is accessed a second time, which"
" will raise an error."
msgstr ""
"上述代码不起作用的原因是``unpack_hook``可以被多次调用。如果我们在第一次解包时删除文件，那么当保存的张量第二次被访问时，文件将不可用，从而引发错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To fix this, we can write a version of those hooks that takes advantage of "
"the fact that PyTorch automatically releases (deletes) the saved data when "
"it is no longer needed."
msgstr "为了解决这个问题，我们可以编写一个版本的这些hooks，利用PyTorch在数据不再需要时自动释放（删除）保存的数据的事实。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When we call ``backward``, the output of ``pack_hook`` will be deleted, "
"which causes the file to be removed, so we’re no longer leaking the files."
msgstr "当我们调用``backward``时，``pack_hook``的输出将被删除，这将导致文件被移除，从而不会再泄漏文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This can then be used in your model, in the following way:"
msgstr "然后可以像下面这样在您的模型中使用它："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this last example, we also demonstrate how to filter which tensors should"
" be saved (here, those whose number of elements is greater than 1000) and "
"how to combine this feature with ``nn.DataParallel``."
msgstr ""
"在最后一个示例中，我们还演示了如何筛选应保存的张量（在此示例中，是那些元素数大于1000的张量），以及如何将此功能与``nn.DataParallel``结合使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you’ve made it this far, congratulations! You now know how to use saved "
"tensor hooks and how they can be useful in a few scenarios to tradeoff "
"memory for compute."
msgstr "如果您阅读到这里，恭喜您！现在您已经知道如何使用保存张量hooks，并了解它们在一些场景中如何有助于在内存和计算之间进行权衡。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Total running time of the script:** ( 0 minutes  0.000 seconds)"
msgstr "**脚本的总运行时间：**（0分钟0.000秒）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: "
"autograd_saved_tensors_hooks_tutorial.py "
"<autograd_saved_tensors_hooks_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：autograd_saved_tensors_hooks_tutorial.py "
"<autograd_saved_tensors_hooks_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: "
"autograd_saved_tensors_hooks_tutorial.ipynb "
"<autograd_saved_tensors_hooks_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：autograd_saved_tensors_hooks_tutorial.ipynb "
"<autograd_saved_tensors_hooks_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`通过Sphinx-Gallery生成的图集 <https://sphinx-gallery.github.io>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_ax_multiobjective_nas_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`这里 "
"<sphx_glr_download_intermediate_ax_multiobjective_nas_tutorial.py>`以下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Multi-Objective NAS with Ax"
msgstr "使用Ax进行多目标NAS"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Authors:** `David Eriksson <https://github.com/dme65>`__, `Max Balandat "
"<https://github.com/Balandat>`__, and the Adaptive Experimentation team at "
"Meta."
msgstr ""
"**作者:** `David Eriksson <https://github.com/dme65>`__, `Max Balandat "
"<https://github.com/Balandat>`__, 和Meta的自适应实验团队。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we show how to use `Ax <https://ax.dev/>`__ to run multi-"
"objective neural architecture search (NAS) for a simple neural network model"
" on the popular MNIST dataset. While the underlying methodology would "
"typically be used for more complicated models and larger datasets, we opt "
"for a tutorial that is easily runnable end-to-end on a laptop in less than "
"20 minutes."
msgstr ""
"在本教程中，我们展示了如何使用`Ax "
"<https://ax.dev/>`__进行多目标神经架构搜索（NAS），以优化一个简单的神经网络模型在流行的MNIST数据集上的性能。虽然这种方法通常适用于更复杂的模型和更大的数据集，但我们选择了一个可以在笔记本电脑上少于20分钟内完整运行的教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In many NAS applications, there is a natural tradeoff between multiple "
"objectives of interest. For instance, when deploying models on-device we may"
" want to maximize model performance (for example, accuracy), while "
"simultaneously minimizing competing metrics like power consumption, "
"inference latency, or model size in order to satisfy deployment constraints."
" Often, we may be able to reduce computational requirements or latency of "
"predictions substantially by accepting minimally lower model performance. "
"Principled methods for exploring such tradeoffs efficiently are key enablers"
" of scalable and sustainable AI, and have many successful applications at "
"Meta - see for instance our `case study "
"<https://research.facebook.com/blog/2021/07/optimizing-model-accuracy-and-"
"latency-using-bayesian-multi-objective-neural-architecture-search/>`__ on a "
"Natural Language Understanding model."
msgstr ""
"在许多NAS应用中，多个感兴趣目标之间存在自然权衡。例如，在设备上部署模型时，我们可能希望最大化模型性能（如准确性），同时最小化诸如功耗、推理延迟或模型大小等竞争性指标，以满足部署约束条件。通常，我们可能通过接受稍微较低的模型性能，来显著减少计算需求或预测的延迟。高效探索这类权衡的系统方法是可扩展和可持续AI的重要推动因素，并在Meta有着许多成功应用"
" - 例如，请参阅我们关于自然语言理解模型的`案例研究 "
"<https://research.facebook.com/blog/2021/07/optimizing-model-accuracy-and-"
"latency-using-bayesian-multi-objective-neural-architecture-search/>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In our example here, we will tune the widths of two hidden layers, the "
"learning rate, the dropout probability, the batch size, and the number of "
"training epochs. The goal is to trade off performance (accuracy on the "
"validation set) and model size (the number of model parameters)."
msgstr ""
"在这个示例中，我们将调整两个隐藏层的宽度、学习率、丢弃概率、批量大小以及训练周期的数量。目标是在性能（验证集上的准确性）和模型大小（模型参数的数量）之间进行权衡。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This tutorial makes use of the following PyTorch libraries:"
msgstr "本教程使用了以下PyTorch库："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`PyTorch Lightning <https://github.com/PyTorchLightning/pytorch-"
"lightning>`__ (specifying the model and training loop)"
msgstr ""
"`PyTorch Lightning <https://github.com/PyTorchLightning/pytorch-"
"lightning>`__（指定模型和训练循环）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`TorchX <https://github.com/pytorch/torchx>`__ (for running training jobs "
"remotely / asynchronously)"
msgstr "`TorchX <https://github.com/pytorch/torchx>`__（用于远程/异步运行训练作业）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`BoTorch <https://github.com/pytorch/botorch>`__ (the Bayesian Optimization "
"library powering Ax's algorithms)"
msgstr "`BoTorch <https://github.com/pytorch/botorch>`__（支持Ax算法的贝叶斯优化库）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Defining the TorchX App"
msgstr "定义TorchX应用程序"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our goal is to optimize the PyTorch Lightning training job defined in "
"`mnist_train_nas.py "
"<https://github.com/pytorch/tutorials/tree/main/intermediate_source/mnist_train_nas.py>`__."
" To do this using TorchX, we write a helper function that takes in the "
"values of the architecture and hyperparameters of the training job and "
"creates a `TorchX AppDef <https://pytorch.org/torchx/latest/basics.html>`__ "
"with the appropriate settings."
msgstr ""
"我们的目标是优化在`mnist_train_nas.py "
"<https://github.com/pytorch/tutorials/tree/main/intermediate_source/mnist_train_nas.py>`__中定义的PyTorch"
" "
"Lightning训练作业。为此，在使用TorchX时，我们编写了一个辅助函数，该函数接收训练作业的架构和值超参数，并创建一个具有适当设置的`TorchX"
" AppDef <https://pytorch.org/torchx/latest/basics.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Setting up the Runner"
msgstr "设置运行器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Ax’s `Runner <https://ax.dev/api/core.html#ax.core.runner.Runner>`__ "
"abstraction allows writing interfaces to various backends. Ax already comes "
"with Runner for TorchX, and so we just need to configure it. For the purpose"
" of this tutorial we run jobs locally in a fully asynchronous fashion."
msgstr ""
"Ax的`Runner "
"<https://ax.dev/api/core.html#ax.core.runner.Runner>`__抽象允许为各种后端编写接口。Ax已包含适用于TorchX的Runner，因此我们只需要配置它。在本教程中，我们完全异步地本地运行作业。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to launch them on a cluster, you can instead specify a different "
"TorchX scheduler and adjust the configuration appropriately. For example, if"
" you have a Kubernetes cluster, you just need to change the scheduler from "
"``local_cwd`` to ``kubernetes``)."
msgstr ""
"为了在集群上启动作业，您可以指定一个不同的TorchX调度器并相应调整配置。例如，如果您有一个Kubernetes集群，您只需将调度器从``local_cwd``修改为``kubernetes``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Setting up the ``SearchSpace``"
msgstr "设置``SearchSpace``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, we define our search space. Ax supports both range parameters of type"
" integer and float as well as choice parameters which can have non-numerical"
" types such as strings. We will tune the hidden sizes, learning rate, "
"dropout, and number of epochs as range parameters and tune the batch size as"
" an ordered choice parameter to enforce it to be a power of 2."
msgstr ""
"首先，我们定义我们的搜索空间。Ax支持整数和浮点类型的范围参数以及具有非数值类型（如字符串）的选项参数。我们将隐藏大小、学习率、丢弃、训练周期设置为范围参数，并将批量大小作为有序选项参数进行调整，使其强制为2的幂。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Setting up Metrics"
msgstr "设置指标"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Ax has the concept of a `Metric <https://ax.dev/api/core.html#metric>`__ "
"that defines properties of outcomes and how observations are obtained for "
"these outcomes. This allows e.g. encoding how data is fetched from some "
"distributed execution backend and post-processed before being passed as "
"input to Ax."
msgstr ""
"Ax有一个`Metric "
"<https://ax.dev/api/core.html#metric>`__的概念，用于定义结果的属性以及如何为这些结果获取观测值。这允许例如编码如何从分布式执行后端获取数据并在传递给Ax之前对其进行后处理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial we will use `multi-objective optimization "
"<https://ax.dev/tutorials/multiobjective_optimization.html>`__ with the goal"
" of maximizing the validation accuracy and minimizing the number of model "
"parameters. The latter represents a simple proxy of model latency, which is "
"hard to estimate accurately for small ML models (in an actual application we"
" would benchmark the latency while running the model on-device)."
msgstr ""
"在本教程中，我们将使用`多目标优化 "
"<https://ax.dev/tutorials/multiobjective_optimization.html>`__，目标是最大化验证准确性并最小化模型参数的数量。后者代表模型延迟的简单代理，因为对于小型ML模型很难准确估算延迟（在实际应用中，我们会在设备上运行模型时测试延迟）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In our example TorchX will run the training jobs in a fully asynchronous "
"fashion locally and write the results to the ``log_dir`` based on the trial "
"index (see the ``trainer()`` function above). We will define a metric class "
"that is aware of that logging directory. By subclassing "
"`TensorboardCurveMetric "
"<https://ax.dev/api/metrics.html?highlight=tensorboardcurvemetric#ax.metrics.tensorboard.TensorboardCurveMetric>`__"
" we get the logic to read and parse the TensorBoard logs for free."
msgstr ""
"在我们的示例中，TorchX将在本地完全异步地运行训练作业，并将结果基于试验索引写入到``log_dir`` "
"(见上文的``trainer()``函数)。我们将定义一个能够识别该日志目录的指标类。通过继承`TensorboardCurveMetric "
"<https://ax.dev/api/metrics.html?highlight=tensorboardcurvemetric#ax.metrics.tensorboard.TensorboardCurveMetric>`__，我们可以直接使用解析TensorBoard日志的逻辑。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we can instantiate the metrics for accuracy and the number of model "
"parameters. Here `curve_name` is the name of the metric in the TensorBoard "
"logs, while `name` is the metric name used internally by Ax. We also specify"
" `lower_is_better` to indicate the favorable direction of the two metrics."
msgstr ""
"现在，我们可以为准确性和模型参数数量实例化指标。这里`curve_name`是TensorBoard日志中的指标名称，而`name`是Ax内部使用的指标名称。我们还指定了`lower_is_better`以指明两个指标的优选方向。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Setting up the ``OptimizationConfig``"
msgstr "设置``OptimizationConfig``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The way to tell Ax what it should optimize is by means of an "
"`OptimizationConfig <https://ax.dev/api/core.html#module-"
"ax.core.optimization_config>`__. Here we use a "
"``MultiObjectiveOptimizationConfig`` as we will be performing multi-"
"objective optimization."
msgstr ""
"告诉Ax应该优化的方式是通过一个`OptimizationConfig <https://ax.dev/api/core.html#module-"
"ax.core.optimization_config>`__来实现的。在这里，我们使用``MultiObjectiveOptimizationConfig``因为我们将进行多目标优化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Additionally, Ax supports placing constraints on the different metrics by "
"specifying objective thresholds, which bound the region of interest in the "
"outcome space that we want to explore. For this example, we will constrain "
"the validation accuracy to be at least 0.94 (94%) and the number of model "
"parameters to be at most 80,000."
msgstr ""
"此外，Ax通过指定目标阈值支持为不同的指标设置约束，这些阈值限定了我们希望探索的结果空间的兴趣区域。在本示例中，我们将验证准确性限制为至少0.94（94%），模型参数数量限制为最多80,000。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Creating the Ax Experiment"
msgstr "创建Ax实验"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In Ax, the `Experiment "
"<https://ax.dev/api/core.html#ax.core.experiment.Experiment>`__ object is "
"the object that stores all the information about the problem setup."
msgstr ""
"在Ax中，`Experiment "
"<https://ax.dev/api/core.html#ax.core.experiment.Experiment>`__对象是存储有关问题设置的所有信息的对象。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Choosing the Generation Strategy"
msgstr "选择生成策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A `GenerationStrategy "
"<https://ax.dev/api/modelbridge.html#ax.modelbridge.generation_strategy.GenerationStrategy>`__"
" is the abstract representation of how we would like to perform the "
"optimization. While this can be customized (if you’d like to do so, see "
"`this tutorial <https://ax.dev/tutorials/generation_strategy.html>`__), in "
"most cases Ax can automatically determine an appropriate strategy based on "
"the search space, optimization config, and the total number of trials we "
"want to run."
msgstr ""
"`GenerationStrategy "
"<https://ax.dev/api/modelbridge.html#ax.modelbridge.generation_strategy.GenerationStrategy>`__是我们希望如何进行优化的抽象表示。虽然它可以定制（如果您想这么做，请参阅`本教程"
" "
"<https://ax.dev/tutorials/generation_strategy.html>`__），但在大多数情况下，Ax可以根据搜索空间、优化配置和要运行的试验总数自动确定合适的策略。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Typically, Ax chooses to evaluate a number of random configurations before "
"starting a model-based Bayesian Optimization strategy."
msgstr "通常，Ax会在开始基于模型的贝叶斯优化策略之前评估一些随机配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Configuring the Scheduler"
msgstr "配置调度器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``Scheduler`` acts as the loop control for the optimization. It "
"communicates with the backend to launch trials, check their status, and "
"retrieve results. In the case of this tutorial, it is simply reading and "
"parsing the locally saved logs. In a remote execution setting, it would call"
" APIs. The following illustration from the Ax `Scheduler tutorial "
"<https://ax.dev/tutorials/scheduler.html>`__ summarizes how the Scheduler "
"interacts with external systems used to run trial evaluations:"
msgstr ""
"``调度器``作为优化的循环控制器。它与后端通信以启动试验，检查其状态，并检索结果。在本教程的情况下，它只是读取和解析本地保存的日志。在远程执行设置中，它将调用API。以下插图来自Ax的`调度器教程"
" <https://ax.dev/tutorials/scheduler.html>`__，总结了调度器如何与用于运行试验评估的外部系统交互："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``Scheduler`` requires the ``Experiment`` and the "
"``GenerationStrategy``. A set of options can be passed in via "
"``SchedulerOptions``. Here, we configure the number of total evaluations as "
"well as ``max_pending_trials``, the maximum number of trials that should run"
" concurrently. In our local setting, this is the number of training jobs "
"running as individual processes, while in a remote execution setting, this "
"would be the number of machines you want to use in parallel."
msgstr ""
"``Scheduler`` 需要 ``Experiment`` 和 ``GenerationStrategy``。可以通过 "
"``SchedulerOptions`` 提供一组选项。在这里，我们配置总评估次数以及 "
"``max_pending_trials``，也就是应并行运行的试验的最大数量。在本地设置中，这是作为单独进程运行的训练任务的数量，而在远程执行设置中，这将是您希望并行使用的机器数量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Running the optimization"
msgstr "运行优化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that everything is configured, we can let Ax run the optimization in a "
"fully automated fashion. The Scheduler will periodically check the logs for "
"the status of all currently running trials, and if a trial completes the "
"scheduler will update its status on the experiment and fetch the "
"observations needed for the Bayesian optimization algorithm."
msgstr ""
"现在，一切都已配置完毕，我们可以让 Ax 完全自动化地运行优化。Scheduler "
"将定期检查所有当前运行试验的状态日志，如果某个试验完成，调度器将更新实验中的状态并获取贝叶斯优化算法所需的观测值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Evaluating the results"
msgstr "评估结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can now inspect the result of the optimization using helper functions and"
" visualizations included with Ax."
msgstr "现在我们可以使用 Ax 包含的辅助函数和可视化工具来检查优化的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, we generate a dataframe with a summary of the results of the "
"experiment. Each row in this dataframe corresponds to a trial (that is, a "
"training job that was run), and contains information on the status of the "
"trial, the parameter configuration that was evaluated, and the metric values"
" that were observed. This provides an easy way to sanity check the "
"optimization."
msgstr ""
"首先，我们生成一个包含实验结果摘要的数据框。此数据框中的每一行对应一个试验（即运行的训练任务），并包含关于试验状态、评估的参数配置和观察到的度量值的信息。这提供了一种检查优化结果的便捷方式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can also visualize the Pareto frontier of tradeoffs between the "
"validation accuracy and the number of model parameters."
msgstr "我们还可以可视化验证精度和模型参数数量之间的折衷的帕累托前沿。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Ax uses Plotly to produce interactive plots, which allow you to do things "
"like zoom, crop, or hover in order to view details of components of the "
"plot. Try it out, and take a look at the `visualization tutorial "
"<https://ax.dev/tutorials/visualizations.html>`__ if you'd like to learn "
"more)."
msgstr ""
"Ax 使用 Plotly 生成交互式图表，允许您进行缩放、裁剪或悬停以查看图中各部分的详细信息。您可以试试看，如果想了解更多内容，请查看 `可视化教程 "
"<https://ax.dev/tutorials/visualizations.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The final optimization results are shown in the figure below where the color"
" corresponds to the iteration number for each trial. We see that our method "
"was able to successfully explore the trade-offs and found both large models "
"with high validation accuracy as well as small models with comparatively "
"lower validation accuracy."
msgstr ""
"最终优化结果如下图所示，其中颜色对应每次试验的迭代次数。我们看到，我们的方法成功地探索了折衷权衡，发现了既有高验证精度的大型模型，也有相对较低验证精度的小型模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To better understand what our surrogate models have learned about the black "
"box objectives, we can take a look at the leave-one-out cross validation "
"results. Since our models are Gaussian Processes, they not only provide "
"point predictions but also uncertainty estimates about these predictions. A "
"good model means that the predicted means (the points in the figure) are "
"close to the 45 degree line and that the confidence intervals cover the 45 "
"degree line with the expected frequency (here we use 95% confidence "
"intervals, so we would expect them to contain the true observation 95% of "
"the time)."
msgstr ""
"为了更好地理解我们的代理模型对黑箱目标的学习情况，我们可以查看留一交叉验证结果。由于我们的模型是高斯过程，它们不仅提供点预测，还提供了关于这些预测的不确定性估计。一个好的模型意味着预测均值（图中的点）接近"
" 45 度线，并且置信区间以预期的频率覆盖 45 度线（这里我们使用 95% 的置信区间，因此我们预计在 95% 的情况下包含真实观测值）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the figures below show, the model size (``num_params``) metric is much "
"easier to model than the validation accuracy (``val_acc``) metric."
msgstr "如下图所示，模型规模（``num_params``）度量比验证精度（``val_acc``）度量更容易建模。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can also make contour plots to better understand how the different "
"objectives depend on two of the input parameters. In the figure below, we "
"show the validation accuracy predicted by the model as a function of the two"
" hidden sizes. The validation accuracy clearly increases as the hidden sizes"
" increase."
msgstr ""
"我们还可以绘制等高线图，以更好地理解不同目标如何依赖于两个输入参数。在下图中，我们显示了模型预测的验证精度作为两个隐藏层大小的函数。验证精度明显随着隐藏层大小的增大而增加。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Similarly, we show the number of model parameters as a function of the "
"hidden sizes in the figure below and see that it also increases as a "
"function of the hidden sizes (the dependency on ``hidden_size_1`` is much "
"larger)."
msgstr ""
"类似地，我们在下图中显示了模型参数数量作为隐藏层大小的函数，发现其也随着隐藏层大小的增大而增加（对 ``hidden_size_1`` 的依赖性更大）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Acknowledgments"
msgstr "致谢"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We thank the TorchX team (in particular Kiuk Chung and Tristan Rice) for "
"their help with integrating TorchX with Ax."
msgstr "我们感谢 TorchX 团队（特别是 Kiuk Chung 和 Tristan Rice）在将 TorchX 与 Ax 集成方面的帮助。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: ax_multiobjective_nas_tutorial.py "
"<ax_multiobjective_nas_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: ax_multiobjective_nas_tutorial.py "
"<ax_multiobjective_nas_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: ax_multiobjective_nas_tutorial.ipynb "
"<ax_multiobjective_nas_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本: ax_multiobjective_nas_tutorial.ipynb "
"<ax_multiobjective_nas_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_char_rnn_classification_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_intermediate_char_rnn_classification_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "NLP From Scratch: Classifying Names with a Character-Level RNN"
msgstr "从头开始的 NLP：使用字符级 RNN 分类姓名"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Sean Robertson <https://github.com/spro>`_"
msgstr "**作者**: `Sean Robertson <https://github.com/spro>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This tutorials is part of a three-part series:"
msgstr "本教程是一个三部分系列教程的一部分："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`NLP From Scratch: Classifying Names with a Character-Level RNN "
"<https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html>`__"
msgstr ""
"`从头开始的 NLP：使用字符级 RNN 分类姓名 "
"<https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`NLP From Scratch: Generating Names with a Character-Level RNN "
"<https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html>`__"
msgstr ""
"`从头开始的 NLP：使用字符级 RNN 生成姓名 "
"<https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`NLP From Scratch: Translation with a Sequence to Sequence Network and "
"Attention "
"<https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__"
msgstr ""
"`从头开始的 NLP：使用序列对序列网络和注意力进行翻译 "
"<https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will be building and training a basic character-level Recurrent Neural "
"Network (RNN) to classify words. This tutorial, along with two other Natural"
" Language Processing (NLP) \"from scratch\" tutorials "
":doc:`/intermediate/char_rnn_generation_tutorial` and "
":doc:`/intermediate/seq2seq_translation_tutorial`, show how to preprocess "
"data to model NLP. In particular, these tutorials show how preprocessing to "
"model NLP works at a low level."
msgstr ""
"我们将构建并训练一个基本的字符级循环神经网络（RNN）来对单词进行分类。本教程，以及另外两个有关自然语言处理（NLP）的“从头开始”的教程 "
":doc:`/intermediate/char_rnn_generation_tutorial` 和 "
":doc:`/intermediate/seq2seq_translation_tutorial`，展示了如何为 NLP "
"对数据进行预处理建模。特别是，这些教程展示了 NLP 的低级别预处理建模过程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A character-level RNN reads words as a series of characters - outputting a "
"prediction and \"hidden state\" at each step, feeding its previous hidden "
"state into each next step. We take the final prediction to be the output, "
"i.e. which class the word belongs to."
msgstr ""
"一个字符级 RNN "
"将单词读取为一系列字符——在每一步输出一个预测和“隐藏状态”，并在每一步中将其前一个隐藏状态传递到下一个步骤。我们将最终的预测视为输出，即单词所属的类别。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Specifically, we'll train on a few thousand surnames from 18 languages of "
"origin, and predict which language a name is from based on the spelling."
msgstr "具体来说，我们将以来自 18 种语言的数千个姓氏作为训练数据，根据拼写预测姓名的来源语言。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Recommended Preparation"
msgstr "推荐的准备工作"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before starting this tutorial it is recommended that you have installed "
"PyTorch, and have a basic understanding of Python programming language and "
"Tensors:"
msgstr "在开始本教程之前，建议您安装 PyTorch，并对 Python 编程语言和张量有基本的了解："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "https://pytorch.org/ For installation instructions"
msgstr "有关安装说明，请访问 https://pytorch.org/"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in "
"general and learn the basics of Tensors"
msgstr ":doc:`/beginner/deep_learning_60min_blitz` 快速入门 PyTorch 并学习张量基础知识"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ":doc:`/beginner/pytorch_with_examples` for a wide and deep overview"
msgstr ":doc:`/beginner/pytorch_with_examples` 浏览广泛且深入的概述"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user"
msgstr ":doc:`/beginner/former_torchies_tutorial` 如果您是前 Lua Torch 用户"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "It would also be useful to know about RNNs and how they work:"
msgstr "了解 RNN 是如何工作的也会对学习有帮助："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`The Unreasonable Effectiveness of Recurrent Neural Networks "
"<https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__ shows a bunch "
"of real life examples"
msgstr ""
"`循环神经网络的惊人效果 <https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__ "
"中展示了大量实际案例"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Understanding LSTM Networks "
"<https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__ is about "
"LSTMs specifically but also informative about RNNs in general"
msgstr ""
"`理解 LSTM 网络 <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__ "
"专门介绍了 LSTM，但同时也对 RNN 有 general 方面的重要信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preparing Torch"
msgstr "准备 Torch"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Set up torch to default to the right device use GPU acceleration depending "
"on your hardware (CPU or CUDA)."
msgstr "设置 Torch 默认使用正确的设备，根据硬件（CPU 或 CUDA）利用 GPU 加速。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preparing the Data"
msgstr "准备数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Download the data from `here "
"<https://download.pytorch.org/tutorial/data.zip>`__ and extract it to the "
"current directory."
msgstr ""
"从 `此处 <https://download.pytorch.org/tutorial/data.zip>`__ 下载数据并将其解压到当前目录下。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Included in the ``data/names`` directory are 18 text files named as "
"``[Language].txt``. Each file contains a bunch of names, one name per line, "
"mostly romanized (but we still need to convert from Unicode to ASCII)."
msgstr ""
"``data/names`` 目录包含 18 个以 ``[语言].txt`` "
"命名的文本文件。每个文件包含一堆名字，每行一个名字，大部分已经罗马化了（但我们仍然需要将 Unicode 转换为 ASCII）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The first step is to define and clean our data. Initially, we need to "
"convert Unicode to plain ASCII to limit the RNN input layers. This is "
"accomplished by converting Unicode strings to ASCII and allowing only a "
"small set of allowed characters."
msgstr ""
"第一步是定义并清理数据。最初，我们需要将 Unicode 转换为普通 ASCII 来限制 RNN 输入层。这是通过将 Unicode 字符串转换为 "
"ASCII 并仅允许一小部分允许字符集来实现的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here's an example of converting a unicode alphabet name to plain ASCII. This"
" simplifies the input layer"
msgstr "以下是将 Unicode 字母表名转换为普通 ASCII 的一个示例。这简化了输入层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Turning Names into Tensors"
msgstr "将姓名转换为张量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have all the names organized, we need to turn them into Tensors "
"to make any use of them."
msgstr "现在我们已经将所有姓名进行了整理，我们需要将它们转换为张量以便使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To represent a single letter, we use a \"one-hot vector\" of size ``<1 x "
"n_letters>``. A one-hot vector is filled with 0s except for a 1 at index of "
"the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``."
msgstr ""
"为了表示单个字母，我们使用一个大小为 ``<1 x n_letters>`` 的“独热向量”。一个独热向量中除了当前字母索引处为 1，其他均为 "
"0，例如：``\"b\" = <0 1 0 0 0 ...>``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To make a word we join a bunch of those into a 2D matrix ``<line_length x 1 "
"x n_letters>``."
msgstr "要表示一个单词，我们将这些转化为一个二维矩阵 ``<line_length x 1 x n_letters>``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That extra 1 dimension is because PyTorch assumes everything is in batches -"
" we're just using a batch size of 1 here."
msgstr "额外的 1 维度是因为 PyTorch 假设一切都在批次中——在这里我们只是使用批次大小为 1。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here are some examples of how to use ``lineToTensor()`` for a single and "
"multiple character string."
msgstr "以下是如何使用 ``lineToTensor()`` 处理单个和多个字符字符串的一些示例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Congratulations, you have built the foundational tensor objects for this "
"learning task! You can use a similar approach for other RNN tasks with text."
msgstr "恭喜，您已经为此学习任务构建了基础张量对象！您可以为其他包含文本的 RNN 任务使用类似的方法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we need to combine all our examples into a dataset so we can train, "
"test and validate our models. For this, we will use the `Dataset and "
"DataLoader "
"<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>`__ "
"classes to hold our dataset. Each Dataset needs to implement three "
"functions: ``__init__``, ``__len__``, and ``__getitem__``."
msgstr ""
"接下来，我们需要将所有示例组合到一个数据集中，这样我们就可以训练、测试和验证我们的模型。为此，我们将使用 `Dataset 和 DataLoader "
"<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>`__ "
"类来保存我们的数据集。每个 Dataset 都需要实现三个函数：``__init__``、``__len__`` 和 ``__getitem__``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Here we can load our example data into the ``NamesDataset``"
msgstr "在这里我们将示例数据加载到 ``NamesDataset``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using the dataset object allows us to easily split the data into train and "
"test sets. Here we create a 80/20"
msgstr "使用数据集对象允许我们轻松将数据分为训练集和测试集。在此我们创建一个 80/20"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"split but the ``torch.utils.data`` has more useful utilities. Here we "
"specify a generator since we need to use the"
msgstr ""
"的分割，但 ``torch.utils.data`` 中有更多实用工具。这里我们指定一个生成器，因为我们需要与上方 PyTorch 默认设备一致。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "same device as PyTorch defaults to above."
msgstr ""
"现在我们有一个包含 **20074** "
"个示例的基本数据集，其中每个示例是一个标签和姓名的配对。同时，我们已经将数据集分为训练集和测试集，以便验证我们构建的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we have a basic dataset containing **20074** examples where each example"
" is a pairing of label and name. We have also split the dataset into "
"training and testing so we can validate the model that we build."
msgstr "创建网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Creating the Network"
msgstr ""
"在自动求导之前，用 Torch "
"创建循环神经网络需要在多个时间步克隆一个层的参数。层保留了隐藏状态和梯度，而现在这些完全由图所处理。这意味着您可以以一种非常“纯”的方式实现 "
"RNN，就像常规的前馈层一样。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before autograd, creating a recurrent neural network in Torch involved "
"cloning the parameters of a layer over several timesteps. The layers held "
"hidden state and gradients which are now entirely handled by the graph "
"itself. This means you can implement a RNN in a very \"pure\" way, as "
"regular feed-forward layers."
msgstr ""
"此 CharRNN 类实现了一个包含三个组件的 RNN。首先，我们使用 `nn.RNN 实现 "
"<https://pytorch.org/docs/stable/generated/torch.nn.RNN.html>`__。接下来，我们定义一个将"
" RNN 隐藏层映射到我们输出的层。最后，我们应用一个 ``softmax`` 函数。使用 ``nn.RNN`` 带来了显著的性能改进，例如基于 "
"cuDNN 的加速内核，而不是将每层实现为 ``nn.Linear``。此外，它还简化了 ``forward()`` 中的实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This CharRNN class implements an RNN with three components. First, we use "
"the `nn.RNN implementation "
"<https://pytorch.org/docs/stable/generated/torch.nn.RNN.html>`__. Next, we "
"define a layer that maps the RNN hidden layers to our output. And finally, "
"we apply a ``softmax`` function. Using ``nn.RNN`` leads to a significant "
"improvement in performance, such as cuDNN-accelerated kernels, versus "
"implementing each layer as a ``nn.Linear``. It also simplifies the "
"implementation in ``forward()``."
msgstr "然后我们可以创建一个 RNN，其中有 58 个输入节点、128 个隐藏节点和 18 个输出："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can then create an RNN with 58 input nodes, 128 hidden nodes, and 18 "
"outputs:"
msgstr ""
"之后我们可以将张量传递给 RNN 以获得预测输出。随后，我们使用帮助函数 ``label_from_output`` 来为类别得出文本标签。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After that we can pass our Tensor to the RNN to obtain a predicted output. "
"Subsequently, we use a helper function, ``label_from_output``, to derive a "
"text label for the class."
msgstr "训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training"
msgstr "训练网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training the Network"
msgstr "现在训练这个网络只需要向它展示大量示例，让它进行猜测，并告诉它结果是否错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now all it takes to train this network is show it a bunch of examples, have "
"it make guesses, and tell it if it's wrong."
msgstr ""
"我们通过定义一个 ``train()`` 函数，在给定数据集上使用小批量训练来训练模型。RNNs "
"的训练与其他网络相似。因此，为了完整性，我们在这里包含了一种分批训练方法。循环（``for i in "
"batch``）计算批中每个项目的损失，然后调整权重。该操作会一直重复，直到达到指定的训练次数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We do this by defining a ``train()`` function which trains the model on a "
"given dataset using minibatches. RNNs RNNs are trained similarly to other "
"networks; therefore, for completeness, we include a batched training method "
"here. The loop (``for i in batch``) computes the losses for each of the "
"items in the batch before adjusting the weights. This operation is repeated "
"until the number of epochs is reached."
msgstr "现在我们可以用小批量数据集训练一个指定的次数。在此示例中，降低了训练次数以加速构建。您可以通过调整参数获得更好的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Plotting the Results"
msgstr "绘制结果图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Plotting the historical loss from ``all_losses`` shows the network learning:"
msgstr "从 ``all_losses`` 绘制的历史损失显示了网络的学习过程："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Evaluating the Results"
msgstr "评估结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To see how well the network performs on different categories, we will create"
" a confusion matrix, indicating for every actual language (rows) which "
"language the network guesses (columns). To calculate the confusion matrix a "
"bunch of samples are run through the network with ``evaluate()``, which is "
"the same as ``train()`` minus the backprop."
msgstr ""
"为了查看网络对不同类别的表现，我们将创建一个混淆矩阵，表示每种实际语言（行）与网络猜测的语言（列）。为了计算混淆矩阵，使用 ``evaluate()``"
" 运行一组样本，这与 ``train()`` 类似，但不进行反向传播。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can pick out bright spots off the main axis that show which languages it"
" guesses incorrectly, e.g. Chinese for Korean, and Spanish for Italian. It "
"seems to do very well with Greek, and very poorly with English (perhaps "
"because of overlap with other languages)."
msgstr ""
"你可以挑选出偏离主轴的亮点，看看网络在哪些语言上猜错了，例如将韩语误认为是中文，将意大利语误认为是西班牙语。网络在希腊语上的表现非常好，而在英语上的表现则很差（可能是由于与其他语言的重叠）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Exercises"
msgstr "练习"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Get better results with a bigger and/or better shaped network"
msgstr "通过更大或形状更佳的网络获得更好的结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Adjust the hyperparameters to enhance performance, such as changing the "
"number of epochs, batch size, and learning rate"
msgstr "调整超参数以提升性能，例如改变训练轮数、批量大小和学习率"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Try the ``nn.LSTM`` and ``nn.GRU`` layers"
msgstr "尝试 ``nn.LSTM`` 和 ``nn.GRU`` 层"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Modify the size of the layers, such as increasing or decreasing the number "
"of hidden nodes or adding additional linear layers"
msgstr "修改层的大小，例如增加或减少隐藏节点数量或增加额外的线性层"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Combine multiple of these RNNs as a higher level network"
msgstr "将多个这些 RNN 组合为高级网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Try with a different dataset of line -> label, for example:"
msgstr "尝试使用不同的行 -> 标签数据集，例如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Any word -> language"
msgstr "任何单词 -> 语言"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "First name -> gender"
msgstr "名字 -> 性别"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Character name -> writer"
msgstr "角色名称 -> 作家"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Page title -> blog or subreddit"
msgstr "页面标题 -> 博客或子版块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: char_rnn_classification_tutorial.py "
"<char_rnn_classification_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: char_rnn_classification_tutorial.py "
"<char_rnn_classification_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: char_rnn_classification_tutorial.ipynb"
" <char_rnn_classification_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: char_rnn_classification_tutorial.ipynb "
"<char_rnn_classification_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_char_rnn_generation_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_intermediate_char_rnn_generation_tutorial.py>` 下载完整的示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "NLP From Scratch: Generating Names with a Character-Level RNN"
msgstr "从零开始的 NLP：使用字符级 RNN 生成名称"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is our second of three tutorials on \"NLP From Scratch\". In the `first"
" tutorial </tutorials/intermediate/char_rnn_classification_tutorial>`_ we "
"used a RNN to classify names into their language of origin. This time we'll "
"turn around and generate names from languages."
msgstr ""
"这是我们 \"从零开始的 NLP\" 的三个教程中的第二个。在 `第一个教程 "
"</tutorials/intermediate/char_rnn_classification_tutorial>`_ 中，我们使用 RNN "
"将名称分类到其原始语言。这次我们将反过来，从语言生成名称。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We are still hand-crafting a small RNN with a few linear layers. The big "
"difference is instead of predicting a category after reading in all the "
"letters of a name, we input a category and output one letter at a time. "
"Recurrently predicting characters to form language (this could also be done "
"with words or other higher order constructs) is often referred to as a "
"\"language model\"."
msgstr ""
"我们仍在手工打造一个小型 "
"RNN，包含几个线性层。这里的主要差异是，代替在读取名称的所有字母后预测一个类别，我们输入一个类别并逐个输出一个字母。递归预测字符以形成语言（这也可以用单词或其他高级构造来完成）通常被称为“语言模型”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Recommended Reading:**"
msgstr "**推荐阅读：**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"I assume you have at least installed PyTorch, know Python, and understand "
"Tensors:"
msgstr "我假设你至少安装了 PyTorch，熟悉 Python，理解张量："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in "
"general"
msgstr ":doc:`/beginner/deep_learning_60min_blitz` 开始学习 PyTorch"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"I also suggest the previous tutorial, "
":doc:`/intermediate/char_rnn_classification_tutorial`"
msgstr "我也建议阅读之前的教程：:doc:`/intermediate/char_rnn_classification_tutorial`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Download the data from `here "
"<https://download.pytorch.org/tutorial/data.zip>`_ and extract it to the "
"current directory."
msgstr ""
"从 `这里 <https://download.pytorch.org/tutorial/data.zip>`_ 下载数据并解压到当前目录。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"See the last tutorial for more detail of this process. In short, there are a"
" bunch of plain text files ``data/names/[Language].txt`` with a name per "
"line. We split lines into an array, convert Unicode to ASCII, and end up "
"with a dictionary ``{language: [names ...]}``."
msgstr ""
"请参阅上一个教程以了解此过程的更多细节。简而言之，有一堆纯文本文件 "
"``data/names/[Language].txt``，每行一个名称。我们将行拆分为数组，将 Unicode 转换为 ASCII，最终得到一个字典 "
"``{language: [names ...]}``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This network extends `the last tutorial's RNN <#Creating-the-Network>`__ "
"with an extra argument for the category tensor, which is concatenated along "
"with the others. The category tensor is a one-hot vector just like the "
"letter input."
msgstr ""
"此网络扩展了 `上一个教程的 RNN <#Creating-the-"
"Network>`__，增加了一个用于类别张量的额外参数，与其他参数一起连接。类别张量是一个和字母输入一样的独热向量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will interpret the output as the probability of the next letter. When "
"sampling, the most likely output letter is used as the next input letter."
msgstr "我们将输出解释为下一个字母的概率。在采样时，最可能的输出字母被用作下一个输入字母。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"I added a second linear layer ``o2o`` (after combining hidden and output) to"
" give it more muscle to work with. There's also a dropout layer, which "
"`randomly zeros parts of its input <https://arxiv.org/abs/1207.0580>`__ with"
" a given probability (here 0.1) and is usually used to fuzz inputs to "
"prevent overfitting. Here we're using it towards the end of the network to "
"purposely add some chaos and increase sampling variety."
msgstr ""
"我添加了一个第二线性层 ``o2o``（在合并隐藏和输出之后）以增强其能力。此外，还有一个 dropout 层，`随机将输入的部分置零 "
"<https://arxiv.org/abs/1207.0580>`__ "
"以给定概率（这里是0.1），通常用来扰乱输入以防止过拟合。在这里我们在网络的末尾使用它，故意添加一些混乱来增加采样的多样性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preparing for Training"
msgstr "训练准备"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First of all, helper functions to get random pairs of (category, line):"
msgstr "首先，创建一些获取随机 (类别, 行) 对的辅助函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For each timestep (that is, for each letter in a training word) the inputs "
"of the network will be ``(category, current letter, hidden state)`` and the "
"outputs will be ``(next letter, next hidden state)``. So for each training "
"set, we'll need the category, a set of input letters, and a set of "
"output/target letters."
msgstr ""
"对于每个时间步（即训练字词中的每个字母），网络的输入将是 ``(类别, 当前字母, 隐状态)``，输出是 ``(下一个字母, "
"下一隐状态)``。因此，对于每组训练集，我们需要类别、输入字母集和输出/目标字母集。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since we are predicting the next letter from the current letter for each "
"timestep, the letter pairs are groups of consecutive letters from the line -"
" e.g. for ``\"ABCD<EOS>\"`` we would create (\"A\", \"B\"), (\"B\", \"C\"), "
"(\"C\", \"D\"), (\"D\", \"EOS\")."
msgstr ""
"由于我们在每个时间步从当前字母预测下一个字母，字母对是行中成对的连续字母组 - 例如对于 ``\"ABCD<EOS>\"`` 将创建 (\"A\", "
"\"B\"), (\"B\", \"C\"), (\"C\", \"D\"), (\"D\", \"EOS\")。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The category tensor is a `one-hot tensor <https://en.wikipedia.org/wiki/One-"
"hot>`__ of size ``<1 x n_categories>``. When training we feed it to the "
"network at every timestep - this is a design choice, it could have been "
"included as part of initial hidden state or some other strategy."
msgstr ""
"类别张量是一个大小为 ``<1 x n_categories>`` 的 `独热张量 "
"<https://en.wikipedia.org/wiki/One-hot>`__。在训练时我们在每个时间步将其输入到网络 - "
"这是一个设计选择，它也可以包括在初始隐状态或其他策略中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For convenience during training we'll make a ``randomTrainingExample`` "
"function that fetches a random (category, line) pair and turns them into the"
" required (category, input, target) tensors."
msgstr ""
"为了方便训练，我们将创建一个 ``randomTrainingExample`` 函数来获取随机的 (类别, 行) 对并将其转换为所需的 (类别, "
"输入, 目标) 张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In contrast to classification, where only the last output is used, we are "
"making a prediction at every step, so we are calculating loss at every step."
msgstr "与分类不同的是，我们在每一步都在进行预测，因此需要计算每一步的损失。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The magic of autograd allows you to simply sum these losses at each step and"
" call backward at the end."
msgstr "自动求导的魔力允许你在每一步简单地将这些损失相加，并在最后调用 backward。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To keep track of how long training takes I am adding a "
"``timeSince(timestamp)`` function which returns a human readable string:"
msgstr "为了跟踪训练时间，我添加了一个 ``timeSince(timestamp)`` 函数，返回一个可读的时间字符串："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Training is business as usual - call train a bunch of times and wait a few "
"minutes, printing the current time and loss every ``print_every`` examples, "
"and keeping store of an average loss per ``plot_every`` examples in "
"``all_losses`` for plotting later."
msgstr ""
"训练的过程如往常一样 - 多次调用训练函数并等待几分钟，每隔 ``print_every`` 次打印当前时间和损失，并在 ``plot_every`` "
"次保存平均损失到 ``all_losses`` 中以便稍后绘制图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Plotting the Losses"
msgstr "绘制损失"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Plotting the historical loss from all\\_losses shows the network learning:"
msgstr "从 all\\_losses 绘制的历史损失显示了网络的学习过程："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sampling the Network"
msgstr "网络采样"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To sample we give the network a letter and ask what the next one is, feed "
"that in as the next letter, and repeat until the EOS token."
msgstr "为了采样，我们向网络提供一个字母并询问下一个字母是什么，将其作为下一个字母，再重复直到 EOS 符号。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Create tensors for input category, starting letter, and empty hidden state"
msgstr "为输入类别、起始字母和空的隐状态创建张量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Create a string ``output_name`` with the starting letter"
msgstr "使用起始字母创建一个字符串 ``output_name``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Up to a maximum output length,"
msgstr "直到达到最大输出长度，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Feed the current letter to the network"
msgstr "将当前字母输入到网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Get the next letter from highest output, and next hidden state"
msgstr "从最高输出中获取下一个字母，以及下一隐状态"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "If the letter is EOS, stop here"
msgstr "如果字母是 EOS，则停止"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "If a regular letter, add to ``output_name`` and continue"
msgstr "如果是普通字母，将其添加到 ``output_name`` 并继续"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Return the final name"
msgstr "返回最终生成的名称"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Rather than having to give it a starting letter, another strategy would have"
" been to include a \"start of string\" token in training and have the "
"network choose its own starting letter."
msgstr "另一种策略是训练时使用一个 \"字符串起始\" 符号，以便网络可以自己选择起始字母，而不需要提供。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Try with a different dataset of category -> line, for example:"
msgstr "尝试使用类别 -> 行的不同数据集，例如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fictional series -> Character name"
msgstr "虚构系列 -> 角色名称"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Part of speech -> Word"
msgstr "词性 -> 单词"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Country -> City"
msgstr "国家 -> 城市"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Use a \"start of sentence\" token so that sampling can be done without "
"choosing a start letter"
msgstr "使用 \"句子起始\" 符号，以便可以在不选择起始字母的情况下进行采样"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: char_rnn_generation_tutorial.py "
"<char_rnn_generation_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: char_rnn_generation_tutorial.py "
"<char_rnn_generation_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: char_rnn_generation_tutorial.ipynb "
"<char_rnn_generation_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: char_rnn_generation_tutorial.ipynb "
"<char_rnn_generation_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compiled Autograd: Capturing a larger backward graph for ``torch.compile``"
msgstr "编译自动求导：捕获 ``torch.compile`` 的更大反向图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author:** `Simon Fan <https://github.com/xmfan>`_"
msgstr "**作者：** `Simon Fan <https://github.com/xmfan>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How compiled autograd interacts with ``torch.compile``"
msgstr "编译自动求导如何与 ``torch.compile`` 交互"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to use the compiled autograd API"
msgstr "如何使用编译自动求导 API"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to inspect logs using ``TORCH_LOGS``"
msgstr "如何使用 ``TORCH_LOGS`` 检查日志"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch 2.4"
msgstr "PyTorch 2.4"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Complete the `Introduction to torch.compile "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"
msgstr ""
"完成 `torch.compile 入门 "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Read through the TorchDynamo and AOTAutograd sections of `Get Started with "
"PyTorch 2.x <https://pytorch.org/get-started/pytorch-2.0/>`_"
msgstr ""
"阅读 PyTorch 2.x 入门的 TorchDynamo 和 AOTAutograd 部分：`PyTorch 2.x 入门 "
"<https://pytorch.org/get-started/pytorch-2.0/>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Overview"
msgstr "概述"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compiled Autograd is a ``torch.compile`` extension introduced in PyTorch 2.4"
" that allows the capture of a larger backward graph."
msgstr "编译自动求导是 PyTorch 2.4 中引入的 ``torch.compile`` 扩展，允许捕获更大的反向图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"While ``torch.compile`` does capture the backward graph, it does so "
"**partially**. The AOTAutograd component captures the backward graph ahead-"
"of-time, with certain limitations:"
msgstr ""
"尽管 ``torch.compile`` 能捕获反向图，但它是 **部分** 捕获。AOTAutograd 组件提前捕获反向图，但存在一定限制："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Graph breaks in the forward lead to graph breaks in the backward"
msgstr "正向中的图断点会导致反向中的图断点"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Backward hooks "
"<https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-"
"execution>`_ are not captured"
msgstr ""
"`反向钩子 <https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-"
"execution>`_ 不被捕获"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compiled Autograd addresses these limitations by directly integrating with "
"the autograd engine, allowing it to capture the full backward graph at "
"runtime. Models with these two characteristics should try Compiled Autograd,"
" and potentially observe better performance."
msgstr ""
"编译自动求导通过直接与自动求导引擎集成解决了这些限制，使其能够在运行时捕获完整的反向图。具有以下两种特点的模型应尝试编译自动求导，并可能观察到更好的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "However, Compiled Autograd introduces its own limitations:"
msgstr "然而，编译自动求导也引入了自身的限制："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Added runtime overhead at the start of the backward for cache lookup"
msgstr "在反向开始时增加了查找缓存的运行时开销"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"More prone to recompiles and graph breaks in dynamo due to the larger "
"capture"
msgstr "由于较大的捕获，更容易在 Dynamo 中发生重新编译和图断点"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compiled Autograd is under active development and is not yet compatible with"
" all existing PyTorch features. For the latest status on a particular "
"feature, refer to `Compiled Autograd Landing Page "
"<https://docs.google.com/document/d/11VucFBEewzqgkABIjebZIzMvrXr3BtcY1aGKpX61pJY>`_."
msgstr ""
"编译自动求导仍在积极开发中，与现有 PyTorch 功能尚未完全兼容。有关某项功能的最新状态，请参阅 `编译自动求导主页 "
"<https://docs.google.com/document/d/11VucFBEewzqgkABIjebZIzMvrXr3BtcY1aGKpX61pJY>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Setup"
msgstr "设置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will base our examples on this simple neural network "
"model. It takes a 10-dimensional input vector, processes it through a single"
" linear layer, and outputs another 10-dimensional vector."
msgstr "在本教程中，我们将基于这个简单的神经网络模型进行示例。它接受一个10维输入向量，通过一个单线性层处理，并输出另一个10维向量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Basic usage"
msgstr "基础用法"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before calling the ``torch.compile`` API, make sure to set "
"``torch._dynamo.config.compiled_autograd`` to ``True``:"
msgstr ""
"在调用 ``torch.compile`` API 之前，请确保将 ``torch._dynamo.config.compiled_autograd``"
" 设置为 ``True``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the code above, we create an instance of the ``Model`` class and generate"
" a random 10-dimensional tensor ``x`` by using ``torch.randn(10)``. We "
"define the training loop function ``train`` and decorate it with "
"@torch.compile to optimize its execution. When ``train(model, x)`` is "
"called:"
msgstr ""
"在上面的代码中，我们创建了一个 ``Model`` 类的实例，并通过使用 ``torch.randn(10)`` 生成一个随机的10维张量 "
"``x``。我们定义了训练循环函数 ``train`` 并使用 @torch.compile 装饰以优化其执行。当调用 ``train(model, "
"x)`` 时："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Python Interpreter calls Dynamo, since this call was decorated with "
"``@torch.compile``."
msgstr "由于调用被 ``@torch.compile`` 装饰，Python 解释器调用 Dynamo。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Dynamo intercepts the Python bytecode, simulates their execution and records"
" the operations into a graph."
msgstr "Dynamo 拦截Python字节码，模拟其执行并将操作记录到图中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``AOTDispatcher`` disables hooks and calls the autograd engine to compute "
"gradients for ``model.linear.weight`` and ``model.linear.bias``, and records"
" the operations into a graph. Using ``torch.autograd.Function``, "
"AOTDispatcher rewrites the forward and backward implementation of ``train``."
msgstr ""
"``AOTDispatcher`` 禁用钩子并调用自动求导引擎计算 ``model.linear.weight`` 和 "
"``model.linear.bias`` 的梯度，并将操作记录到图中。使用 "
"``torch.autograd.Function``，AOTDispatcher 重写了 ``train`` 的正向和反向实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Inductor generates a function corresponding to an optimized implementation "
"of the AOTDispatcher forward and backward."
msgstr "Inductor 生成了一个对应于 AOTDispatcher 正向和反向优化实现的函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Dynamo sets the optimized function to be evaluated next by Python "
"Interpreter."
msgstr "Dynamo 设置优化后的函数，由Python解释器接下来进行评估。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Python Interpreter executes the optimized function, which executes ``loss = "
"model(x).sum()``."
msgstr "Python解释器执行优化后的函数，该函数执行 ``loss = model(x).sum()``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Python Interpreter executes ``loss.backward()``, calling into the autograd "
"engine, which routes to the Compiled Autograd engine since we set "
"``torch._dynamo.config.compiled_autograd = True``."
msgstr ""
"Python解释器执行``loss.backward()``，调用自动微分引擎，因为我们设置了``torch._dynamo.config.compiled_autograd"
" = True``，所以路由到编译自动微分引擎。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compiled Autograd computes the gradients for ``model.linear.weight`` and "
"``model.linear.bias``, and records the operations into a graph, including "
"any hooks it encounters. During this process, it will record the backward "
"previously rewritten by AOTDispatcher. Compiled Autograd then generates a "
"new function which corresponds to a fully-traced implementation of "
"``loss.backward()``, and executes it with ``torch.compile`` in inference "
"mode."
msgstr ""
"编译自动微分引擎计算``model.linear.weight``和``model.linear.bias``的梯度，并将操作记录到一个图中，包括遇到的任何钩子。在此过程中，它会记录之前通过AOTDispatcher重写的反向操作。然后，编译自动微分引擎生成一个新函数，该函数对应于``loss.backward()``的完全追踪实现，并在推理模式下用``torch.compile``执行它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The same steps recursively apply to the Compiled Autograd graph, but this "
"time AOTDispatcher will not need to partition the graph."
msgstr "相同的步骤递归地应用于编译自动微分图，但这次AOTDispatcher不需要对图进行划分。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inspecting the compiled autograd logs"
msgstr "检查编译自动微分日志"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Run the script with the ``TORCH_LOGS`` environment variables:"
msgstr "使用``TORCH_LOGS``环境变量运行脚本："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To only print the compiled autograd graph, use "
"``TORCH_LOGS=\"compiled_autograd\" python example.py``"
msgstr "如果只需打印编译的自动微分图，请使用``TORCH_LOGS=\"compiled_autograd\" python example.py``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To print the graph with more tensor metadata and recompile reasons, at the "
"cost of performance, use ``TORCH_LOGS=\"compiled_autograd_verbose\" python "
"example.py``"
msgstr ""
"如果要打印包含更多张量元数据的图以及重新编译的原因，请使用``TORCH_LOGS=\"compiled_autograd_verbose\" "
"python example.py``，但性能会有所下降。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Rerun the snippet above, the compiled autograd graph should now be logged to"
" ``stderr``. Certain graph nodes will have names that are prefixed by "
"``aot0_``, these correspond to the nodes previously compiled ahead of time "
"in AOTAutograd backward graph 0, for example, ``aot0_view_2`` corresponds to"
" ``view_2`` of the AOT backward graph with id=0."
msgstr ""
"重新运行上述代码片段，编译的自动微分图现在应记录到``stderr``中。某些图节点的名称会以``aot0_``为前缀，这些节点对应于之前在AOT自动微分反向图0中编译的节点，例如``aot0_view_2``对应于id=0的AOT反向图中的``view_2``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the image below, the red box encapsulates the AOT backward graph that is "
"captured by ``torch.compile`` without Compiled Autograd."
msgstr "下面的图像中，红框包裹的是在没有编译自动微分的情况下由``torch.compile``捕获的AOT反向图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is the graph on which we will call ``torch.compile``, **NOT** the "
"optimized graph. Compiled Autograd essentially generates some unoptimized "
"Python code to represent the entire C++ autograd execution."
msgstr ""
"这是我们将调用``torch.compile``的图，**不是**优化后的图。编译自动微分本质上生成了一些未优化的Python代码以表示整个C++自动微分执行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compiling the forward and backward pass using different flags"
msgstr "使用不同标志编译正向和反向传递"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can use different compiler configs for the two compilations, for "
"example, the backward may be a fullgraph even if there are graph breaks in "
"the forward."
msgstr "你可以为两种编译使用不同的编译器配置，例如，即使正向中有图中断，反向仍可以是完整图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Or you can use the context manager, which will apply to all autograd calls "
"within its scope."
msgstr "或者你可以使用上下文管理器，它会在其作用范围内应用于所有自动微分调用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compiled Autograd addresses certain limitations of AOTAutograd"
msgstr "编译自动微分解决了AOTAutograd的某些限制"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Graph breaks in the forward pass no longer necessarily lead to graph breaks "
"in the backward pass:"
msgstr "正向传递中的图中断不再必然导致反向传递中的图中断："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the first ``torch.compile`` case, we see that 3 backward graphs were "
"produced due to the 2 graph breaks in the compiled function ``fn``. Whereas "
"in the second ``torch.compile`` with compiled autograd case, we see that a "
"full backward graph was traced despite the graph breaks."
msgstr ""
"在第一个``torch.compile``案例中，由于在编译函数``fn``中有2个图中断，生成了3个反向图。而在第二个开启编译自动微分的``torch.compile``案例中，尽管有图中断，仍然追踪到了一个完整的反向图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It is still possible for the Dynamo to graph break when tracing backward "
"hooks captured by Compiled Autograd."
msgstr "在追踪由编译自动微分捕获的反向钩子时，Dynamo仍可能发生图中断。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Backward hooks can now be captured"
msgstr "现在可以捕获反向钩子"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There should be a ``call_hook`` node in the graph, which dynamo will later "
"inline into the following:"
msgstr "图中应有一个``call_hook``节点，随后dynamo会将其内联为以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Common recompilation reasons for Compiled Autograd"
msgstr "编译自动微分常见的重新编译原因"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Due to changes in the autograd structure of the loss value:"
msgstr "由于损失值的自动微分结构发生变化："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example above, we call a different operator on each iteration, "
"leading to ``loss`` tracking a different autograd history each time. You "
"should see some recompile messages: **Cache miss due to new autograd node**."
msgstr ""
"在上面的例子中，我们每次迭代调用不同的操作，导致``loss``每次都跟踪不同的自动微分历史。你应该会看到一些重新编译的消息：**由于新的自动微分节点导致缓存未命中**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Due to tensors changing shapes:"
msgstr "由于张量形状变化："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example above, ``x`` changes shapes, and compiled autograd will mark "
"``x`` as a dynamic shape tensor after the first change. You should see "
"recompiles messages: **Cache miss due to changed shapes**."
msgstr ""
"在上面的例子中，``x``的形状发生变化，编译自动微分将在第一个变化后将``x``标记为动态形状张量。你应该会看到重新编译的消息：**由于形状变化导致缓存未命中**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we went over the high-level ecosystem of ``torch.compile``"
" with compiled autograd, the basics of compiled autograd and a few common "
"recompilation reasons. Stay tuned for deep dives on `dev-discuss "
"<https://dev-discuss.pytorch.org/>`_."
msgstr ""
"在本教程中，我们介绍了``torch.compile``与编译自动微分相关的高层生态系统、编译自动微分的基本知识以及一些常见的重新编译原因。请关注`开发讨论"
" <https://dev-discuss.pytorch.org/>`_中的深入内容。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_custom_function_conv_bn_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`这里 "
"<sphx_glr_download_intermediate_custom_function_conv_bn_tutorial.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fusing Convolution and Batch Norm using Custom Function"
msgstr "使用自定义函数融合卷积和批量归一化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Fusing adjacent convolution and batch norm layers together is typically an "
"inference-time optimization to improve run-time. It is usually achieved by "
"eliminating the batch norm layer entirely and updating the weight and bias "
"of the preceding convolution [0]. However, this technique is not applicable "
"for training models."
msgstr ""
"将相邻的卷积层和批量归一化层融合通常是一种推理时优化，以提高运行时性能。这通常通过完全消除批量归一化层并更新前面卷积的权重和偏差来实现[0]。然而，这种技术不适用于训练模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will show a different technique to fuse the two layers "
"that can be applied during training. Rather than improved runtime, the "
"objective of this optimization is to reduce memory usage."
msgstr "在本教程中，我们将展示一种可以在训练期间应用的不同融合技术。这项优化的目标是减少内存使用，而不是改进运行时。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The idea behind this optimization is to see that both convolution and batch "
"norm (as well as many other ops) need to save a copy of their input during "
"forward for the backward pass. For large batch sizes, these saved inputs are"
" responsible for most of your memory usage, so being able to avoid "
"allocating another input tensor for every convolution batch norm pair can be"
" a significant reduction."
msgstr ""
"这项优化的理念是观察到卷积和批量归一化（以及许多其他操作）在正向传递时需要保存输入的副本，以便进行反向传递。对于大批量大小的情况下，这些保存的输入占用了大部分内存，因此能够避免为每个卷积和批量归一化对分配额外的输入张量，会显著减少内存使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we avoid this extra allocation by combining convolution "
"and batch norm into a single layer (as a custom function). In the forward of"
" this combined layer, we perform normal convolution and batch norm as-is, "
"with the only difference being that we will only save the inputs to the "
"convolution. To obtain the input of batch norm, which is necessary to "
"backward through it, we recompute convolution forward again during the "
"backward pass."
msgstr ""
"在本教程中，我们通过将卷积和批量归一化组合为一个单独的层（作为自定义函数），避免了额外分配。在这个组合层的正向传递中，我们按原样执行正常的卷积和批量归一化，唯一不同的是我们只保存卷积的输入。为了获得批量归一化的输入，这在反向传递中是必要的，我们将在反向传递期间重新计算卷积正向传递。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It is important to note that the usage of this optimization is situational. "
"Though (by avoiding one buffer saved) we always reduce the memory allocated "
"at the end of the forward pass, there are cases when the *peak* memory "
"allocated may not actually be reduced. See the final section for more "
"details."
msgstr ""
"需要注意的是，这种优化的使用是有情境性的。虽然通过避免一个保存的缓冲区，我们始终减少了正向传递结束时分配的内存，但在某些情况下，*峰值*分配的内存可能并未真正减少。详见最后一节了解更多细节。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For simplicity, in this tutorial we hardcode `bias=False`, `stride=1`, "
"`padding=0`, `dilation=1`, and `groups=1` for Conv2D. For BatchNorm2D, we "
"hardcode `eps=1e-3`, `momentum=0.1`, `affine=False`, and "
"`track_running_statistics=False`. Another small difference is that we add "
"epsilon in the denominator outside of the square root in the computation of "
"batch norm."
msgstr ""
"为简单起见，在本教程中我们为Conv2D硬编码了`bias=False`、`stride=1`、`padding=0`、`dilation=1`和`groups=1`。对于BatchNorm2D，我们硬编码了`eps=1e-3`、`momentum=0.1`、`affine=False`以及`track_running_statistics=False`。另一个小的区别是我们在批量归一化计算中在平方根之外添加了epsilon。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "[0] https://nenadmarkus.com/p/fusing-batchnorm-and-conv/"
msgstr "[0] https://nenadmarkus.com/p/fusing-batchnorm-and-conv/"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Backward Formula Implementation for Convolution"
msgstr "卷积的反向公式实现"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Implementing a custom function requires us to implement the backward "
"ourselves. In this case, we need both the backward formulas for Conv2D and "
"BatchNorm2D. Eventually we'd chain them together in our unified backward "
"function, but below we first implement them as their own custom functions so"
" we can validate their correctness individually"
msgstr ""
"实现一个自定义函数要求我们自己实现反向传递。在这种情况下，我们需要Conv2D和BatchNorm2D的反向公式。最终我们会将它们链式整合到一个统一的反向函数中，但在下面我们先将它们作为各自的自定义函数实现以分别验证其正确性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When testing with ``gradcheck``, it is important to use double precision"
msgstr "在使用``gradcheck``测试时，重要的是要使用双精度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Backward Formula Implementation for Batch Norm"
msgstr "批量归一化的反向公式实现"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Batch Norm has two modes: training and ``eval`` mode. In training mode the "
"sample statistics are a function of the inputs. In ``eval`` mode, we use the"
" saved running statistics, which are not a function of the inputs. This "
"makes non-training mode's backward significantly simpler. Below we implement"
" and test only the training mode case."
msgstr ""
"批量归一化有两种模式：训练模式和``eval``模式。在训练模式下，样本统计是输入的函数。在``eval``模式下，我们使用保存的运行统计，这不是输入的函数。这使得非训练模式的反向显著更简单。下面我们仅实现和测试训练模式的情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Testing with ``gradcheck``"
msgstr "使用``gradcheck``进行测试"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fusing Convolution and BatchNorm"
msgstr "融合卷积和批量归一化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that the bulk of the work has been done, we can combine them together. "
"Note that in (1) we only save a single buffer for backward, but this also "
"means we recompute convolution forward in (5). Also see that in (2), (3), "
"(4), and (6), it's the same exact code as the examples above."
msgstr ""
"现在大部分工作已完成，我们可以将它们组合在一起。注意，在(1)中我们仅保存了一个缓冲区用于反向传递，但这也意味着我们需要在(5)中重新计算卷积的正向传递。此外，请注意在(2)、(3)、(4)和(6)中的代码与上面的例子完全相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The next step is to wrap our functional variant in a stateful `nn.Module`"
msgstr "下一步是将我们的功能变体包装到有状态的`nn.Module`中"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Use ``gradcheck`` to validate the correctness of our backward formula"
msgstr "使用``gradcheck``验证我们反向公式的正确性"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Testing out our new Layer"
msgstr "测试我们的新层"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Use ``FusedConvBN`` to train a basic network The code below is after some "
"light modifications to the example here: "
"https://github.com/pytorch/examples/tree/master/mnist"
msgstr ""
"使用``FusedConvBN``训练一个基本网络 "
"下面的代码对示例代码进行了轻微修改：https://github.com/pytorch/examples/tree/master/mnist"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "A Comparison of Memory Usage"
msgstr "内存使用对比"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If CUDA is enabled, print out memory usage for both `fused=True` and "
"`fused=False` For an example run on NVIDIA GeForce RTX 3070, NVIDIA CUDA® "
"Deep Neural Network library (cuDNN) 8.0.5: fused peak memory: 1.56GB, "
"unfused peak memory: 2.68GB"
msgstr ""
"如果启用了CUDA，打印出`fused=True`和`fused=False`情况下的内存使用情况。在NVIDIA GeForce RTX "
"3070、NVIDIA CUDA深度神经网络库(cuDNN) 8.0.5上的示例运行中：融合后的峰值内存为1.56GB，未融合的峰值内存为2.68GB。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It is important to note that the *peak* memory usage for this model may vary"
" depending the specific cuDNN convolution algorithm used. For shallower "
"models, it may be possible for the peak memory allocated of the fused model "
"to exceed that of the unfused model! This is because the memory allocated to"
" compute certain cuDNN convolution algorithms can be high enough to \"hide\""
" the typical peak you would expect to be near the start of the backward "
"pass."
msgstr ""
"需要注意的是，该模型的*峰值*内存使用可能会因使用的具体cuDNN卷积算法而有所不同。对于较浅的模型，融合模型的峰值内存分配可能会超过未融合模型！这是因为某些cuDNN卷积算法的内存分配高到足以\"隐藏\"你期望在反向传递开始时附近的典型峰值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For this reason, we also record and display the memory allocated at the end "
"of the forward pass as an approximation, and to demonstrate that we indeed "
"allocate one fewer buffer per fused ``conv-bn`` pair."
msgstr "因此，我们还记录并显示了正向传递结束时分配的内存量作为近似值，以证明我们确实为每个融合的``conv-bn``对减少了一个缓冲区的分配。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: custom_function_conv_bn_tutorial.py "
"<custom_function_conv_bn_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：custom_function_conv_bn_tutorial.py "
"<custom_function_conv_bn_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: custom_function_conv_bn_tutorial.ipynb"
" <custom_function_conv_bn_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：custom_function_conv_bn_tutorial.ipynb "
"<custom_function_conv_bn_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Double Backward with Custom Functions"
msgstr "自定义函数的双重反向"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It is sometimes useful to run backwards twice through backward graph, for "
"example to compute higher-order gradients. It takes an understanding of "
"autograd and some care to support double backwards, however. Functions that "
"support performing backward a single time are not necessarily equipped to "
"support double backward. In this tutorial we show how to write a custom "
"autograd function that supports double backward, and point out some things "
"to look out for."
msgstr ""
"有时候通过反向图再次运行反向是有用的，例如计算高阶梯度。然而，支持双重反向需要对自动微分的理解以及一些谨慎。支持单次反向的函数不一定具备支持双重反向的能力。在本教程中，我们展示如何编写支持双重反向的自定义自动微分函数，并指出一些需要注意的事项。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When writing a custom autograd function to backward through twice, it is "
"important to know when operations performed in a custom function are "
"recorded by autograd, when they aren't, and most importantly, how "
"`save_for_backward` works with all of this."
msgstr ""
"在编写自定义自动微分函数以实现双重反向时，重要的是了解自定义函数中执行的操作何时会被自动微分记录，何时不会，最重要的是，`save_for_backward`如何与这一切交互。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Custom functions implicitly affects grad mode in two ways:"
msgstr "自定义函数对梯度模式有两个隐性影响："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"During forward, autograd does not record any the graph for any operations "
"performed within the forward function. When forward completes, the backward "
"function of the custom function becomes the `grad_fn` of each of the "
"forward's outputs"
msgstr "在正向中，自动微分不会记录在正向函数内部执行的任何操作的图。当正向完成时，自定义函数的反向函数成为每个正向输出的`grad_fn`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"During backward, autograd records the computation graph used to compute the "
"backward pass if create_graph is specified"
msgstr "在反向中，如果指定了create_graph，自动微分会记录用于计算反向传递的计算图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, to understand how `save_for_backward` interacts with the above, we can"
" explore a couple examples:"
msgstr "接下来，为了理解`save_for_backward`如何与上述内容交互，我们可以探索几个例子："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving the Inputs"
msgstr "保存输入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Consider this simple squaring function. It saves an input tensor for "
"backward. Double backward works automatically when autograd is able to "
"record operations in the backward pass, so there is usually nothing to worry"
" about when we save an input for backward as the input should have grad_fn "
"if it is a function of any tensor that requires grad. This allows the "
"gradients to be properly propagated."
msgstr ""
"考虑这个简单的平方函数。它保存了一个输入张量以供后向计算使用。当autograd能够记录后向过程中的操作时，双重后向会自动运行。因此在保存一个输入用于后向计算时通常不需要担心，因为如果该输入是任何需要梯度的张量的函数，它应该具有一个grad_fn。这允许梯度被正确传播。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We can use torchviz to visualize the graph to see why this works"
msgstr "我们可以使用torchviz来可视化计算图，从而理解为什么这会有效。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can see that the gradient wrt to x, is itself a function of x (dout/dx = "
"2x) And the graph of this function has been properly constructed"
msgstr "我们可以看到相对于变量x的梯度，实际上是x的函数 (dout/dx = 2x)，并且这个函数的计算图已经被正确构造。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving the Outputs"
msgstr "保存输出值"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A slight variation on the previous example is to save an output instead of "
"input. The mechanics are similar because outputs are also associated with a "
"grad_fn."
msgstr "前一个例子的一个小变化是保存输出值而不是输入值。机制类似，因为输出值也与一个grad_fn相关联。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Use torchviz to visualize the graph:"
msgstr "使用torchviz可视化计算图："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving Intermediate Results"
msgstr "保存中间结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A more tricky case is when we need to save an intermediate result. We "
"demonstrate this case by implementing:"
msgstr "一个更复杂的情况是需要保存中间结果。我们通过实现以下情况来演示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "sinh(x) := \\frac{e^x - e^{-x}}{2}"
msgstr ""
"sinh(x) := \\frac{e^x - e^{-x}}{2}\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since the derivative of sinh is cosh, it might be useful to reuse `exp(x)` "
"and `exp(-x)`, the two intermediate results in forward in the backward "
"computation."
msgstr "由于sinh的导数是cosh，在后向计算中重用`exp(x)`和`exp(-x)`这两个中间结果可能很有用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Intermediate results should not be directly saved and used in backward "
"though. Because forward is performed in no-grad mode, if an intermediate "
"result of the forward pass is used to compute gradients in the backward pass"
" the backward graph of the gradients would not include the operations that "
"computed the intermediate result. This leads to incorrect gradients."
msgstr ""
"然而，中间结果不应直接保存并在后向计算中使用。因为前向过程是在无梯度模式下执行的，如果前向过程的中间结果被用来在后向过程中计算梯度，那么梯度的后向图不会包括计算中间结果的操作。这会导致梯度计算错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving Intermediate Results: What not to do"
msgstr "保存中间结果：错误示例"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we show what happens when we don't also return our intermediate results "
"as outputs: `grad_x` would not even have a  backward graph because it is "
"purely a function `exp` and `expnegx`, which don't require grad."
msgstr ""
"现在我们展示当没有将中间结果作为输出返回时会发生什么情况：`grad_x`甚至不会有一个后向计算图，因为它是`exp`和`expnegx`的纯函数，而这两个函数不需要梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Use torchviz to visualize the graph. Notice that `grad_x` is not part of the"
" graph!"
msgstr "使用torchviz可视化计算图。注意`grad_x`并不在计算图的一部分！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "When Backward is not Tracked"
msgstr "当后向过程不能被追踪时"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, let's consider an example when it may not be possible for autograd "
"to track gradients for a functions backward at all. We can imagine "
"cube_backward to be a function that may require a non-PyTorch library like "
"SciPy or NumPy, or written as a C++ extension. The workaround demonstrated "
"here is to create another custom function CubeBackward where you also "
"manually specify the backward of cube_backward!"
msgstr ""
"最后，我们考虑一个示例，其中autograd可能完全无法追踪函数的后向梯度。可以设想`cube_backward`是一个可能需要非PyTorch库（如SciPy或NumPy）或以C++扩展编写的函数。这里展示的解决方法是创建另一个自定义函数`CubeBackward`，并手动指定`cube_backward`的后向过程！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To conclude, whether double backward works for your custom function simply "
"depends on whether the backward pass can be tracked by autograd. With the "
"first two examples we show situations where double backward works out of the"
" box. With the third and fourth examples, we demonstrate techniques that "
"enable a backward function to be tracked, when they otherwise would not be."
msgstr ""
"总结起来，是否双重后向适用于自定义函数仅仅取决于后向过程是否可以被autograd追踪。在前两个示例中，我们展示了双重后向自动适用的情况。在第三和第四个示例中，我们演示了使后向函数能够被追踪的技巧，否则就无法追踪。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Introduction <../beginner/ddp_series_intro.html>`__ \\|\\| `What is DDP "
"<../beginner/ddp_series_theory.html>`__ \\|\\| `Single-Node Multi-GPU "
"Training <../beginner/ddp_series_multigpu.html>`__ \\|\\| `Fault Tolerance "
"<../beginner/ddp_series_fault_tolerance.html>`__ \\|\\| `Multi-Node training"
" <ddp_series_multinode.html>`__ \\|\\| **minGPT Training**"
msgstr ""
"`教程介绍 <../beginner/ddp_series_intro.html>`__ \\|\\| `什么是DDP "
"<../beginner/ddp_series_theory.html>`__ \\|\\| `单节点多GPU训练 "
"<../beginner/ddp_series_multigpu.html>`__ \\|\\| `容错机制 "
"<../beginner/ddp_series_fault_tolerance.html>`__ \\|\\| `多节点训练 "
"<ddp_series_multinode.html>`__ \\|\\| **minGPT训练**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training “real-world” models with DDP"
msgstr "使用DDP进行“真实世界”模型训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Authors: `Suraj Subramanian <https://github.com/subramen>`__"
msgstr "作者：`Suraj Subramanian <https://github.com/subramen>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Best practices when writing a distributed training script"
msgstr "编写分布式训练脚本的最佳实践"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Increased flexibility with saving/loading artifacts in the cloud"
msgstr "保存/加载云端制品时的灵活性提升"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "When DDP is NOT suitable"
msgstr "当DDP不适用时"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":octicon:`code-square;1.0em;` View the code used in this tutorial on `GitHub"
" <https://github.com/pytorch/examples/tree/main/distributed/minGPT-ddp>`__"
msgstr ""
":octicon:`code-square;1.0em;` 查看此教程中使用的代码，请访问`GitHub "
"<https://github.com/pytorch/examples/tree/main/distributed/minGPT-ddp>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Familiarity with `multi-GPU training "
"<../beginner/ddp_series_multigpu.html>`__ and `torchrun "
"<../beginner/ddp_series_fault_tolerance.html>`__"
msgstr ""
"熟悉`多GPU训练 <../beginner/ddp_series_multigpu.html>`__和`torchrun "
"<../beginner/ddp_series_fault_tolerance.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"[Optional] Familiarity with `multinode training "
"<ddp_series_multinode.html>`__"
msgstr "[可选]熟悉`多节点训练 <ddp_series_multinode.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"2 or more TCP-reachable GPU machines (this tutorial uses AWS p3.2xlarge "
"instances)"
msgstr "2台或以上通过TCP连接的GPU机器（此教程使用了AWS p3.2xlarge实例）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch `installed <https://pytorch.org/get-started/locally/>`__ with CUDA "
"on all machines"
msgstr ""
"所有机器上安装了支持CUDA的PyTorch `安装教程 <https://pytorch.org/get-started/locally/>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch/XFsFDGKZHh4>`__."
msgstr "可以跟随以下视频，或者访问`YouTube <https://www.youtube.com/watch/XFsFDGKZHh4>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this video, we will review the process of training a GPT model in "
"multinode DDP. We first clone the `minGPT repo "
"<https://github.com/karpathy/minGPT>`__ and refactor the Trainer to resemble"
" the structure we have used in this series. Watch the video for details on "
"these changes."
msgstr ""
"在视频中，我们将回顾使用多节点DDP训练GPT模型的过程。我们首先克隆`minGPT代码库 "
"<https://github.com/karpathy/minGPT>`__并将Trainer类调整为与本系列教程类似的结构。观看视频以了解这些更改的详细信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We use `hydra <https://hydra.cc/>`__ to centrally manage all the "
"configurations for our training run. Once the code has been refactored, we "
"run it first on a single-node with 4 GPUs, and then on a slurm cluster."
msgstr ""
"我们使用`hydra "
"<https://hydra.cc/>`__集中管理训练运行的所有配置。一旦代码被调整，我们先在一台单节点的机器上用4个GPU运行，然后在slurm集群上运行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Files used for training"
msgstr "训练中使用的文件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`trainer.py "
"<https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/trainer.py>`__ includes the Trainer class that runs the "
"distributed training iterations on the model with the provided dataset."
msgstr ""
"`trainer.py "
"<https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/trainer.py>`__ 包含了运行分布式训练迭代的Trainer类。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`model.py <https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/model.py>`__ defines the model architecture."
msgstr ""
"`model.py <https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/model.py>`__ 定义了模型架构。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`char_dataset.py "
"<https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/char_dataset.py>`__ contains the ``Dataset`` class for a "
"character-level dataset."
msgstr ""
"`char_dataset.py "
"<https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/char_dataset.py>`__ 包含了字符级数据集的``Dataset``类。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`gpt2_train_cfg.yaml "
"<https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/gpt2_train_cfg.yaml>`__ contains the configurations for data, "
"model, optimizer, and training run."
msgstr ""
"`gpt2_train_cfg.yaml "
"<https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/gpt2_train_cfg.yaml>`__ 包含了数据、模型、优化器以及训练运行的配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`main.py <https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/main.py>`__ is the entry point to the training job. It sets up "
"the DDP process group, reads all the configurations and runs the training "
"job."
msgstr ""
"`main.py <https://github.com/pytorch/examples/blob/main/distributed/minGPT-"
"ddp/mingpt/main.py>`__ 是训练任务的入口点。它设置了DDP进程组，读取所有配置并运行训练任务。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saving and Loading from the cloud"
msgstr "从云端保存和加载"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the video above, we save training snapshots directly to the cloud. This "
"gives us the flexibility to continue training from any node that has access "
"to the cloud bucket."
msgstr "在以上视频中，我们将训练快照直接保存到云端。这使我们能够从任何具有云存储访问权限的节点继续训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using Mixed Precision"
msgstr "使用混合精度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To speed things up, you might be able to use `Mixed Precision "
"<https://pytorch.org/docs/stable/amp.html>`__ to train your models. In Mixed"
" Precision, some parts of the training process are carried out in reduced "
"precision, while other steps that are more sensitive to precision drops are "
"maintained in FP32 precision."
msgstr ""
"为了加快速度，可以尝试使用`混合精度训练 "
"<https://pytorch.org/docs/stable/amp.html>`__来训练模型。在混合精度模式中，某些训练过程将以降低的精度进行，而某些对精度更敏感的步骤则保持在FP32精度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "When is DDP not enough?"
msgstr "当DDP不足以处理时"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A typical training run's memory footprint consists of model weights, "
"activations, gradients, the input batch, and the optimizer state. Since DDP "
"replicates the model on each GPU, it only works when GPUs have sufficient "
"capacity to accomodate the full footprint. When models grow larger, more "
"aggressive techniques might be useful:"
msgstr ""
"典型训练任务的内存占用包括模型权重、激活值、梯度值、输入批次以及优化器状态。由于DDP在每个GPU上都复制模型，它仅在GPU有足够容量容纳完整占用时有效。当模型变得更大时，可以使用更为激进的技术："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`activation checkpointing "
"<https://pytorch.org/docs/stable/checkpoint.html>`__: Instead of saving "
"intermediate activations during the forward pass, the activations are "
"recomputed during the backward pass. In this approach, we run more compute "
"but save on memory footprint."
msgstr ""
"`激活检查点 "
"<https://pytorch.org/docs/stable/checkpoint.html>`__：在前向过程中不保存中间激活值，而是在后向过程中重新计算激活值。这种方法增加了计算量，但减少了内存占用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Fully-Sharded Data Parallel <https://pytorch.org/docs/stable/fsdp.html>`__:"
" Here the model is not replicated but \"sharded\" across all the GPUs, and "
"computation is overlapped with communication in the forward and backward "
"passes. Read our `blog <https://medium.com/pytorch/training-a-1-trillion-"
"parameter-model-with-pytorch-fully-sharded-data-parallel-on-"
"aws-3ac13aa96cff>`__ to learn how we trained a 1 Trillion parameter model "
"with FSDP."
msgstr ""
"`完全切片数据并行 "
"<https://pytorch.org/docs/stable/fsdp.html>`__：此方法不是复制模型，而是将模型“切片”分配到所有GPU中，并在前向和后向过程中通信与计算同时进行。阅读我们的`博客"
" <https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-"
"pytorch-fully-sharded-data-parallel-on-"
"aws-3ac13aa96cff>`__，了解如何使用FSDP训练一个拥有1万亿参数的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Further Reading"
msgstr "进一步阅读"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Multi-Node training with DDP <ddp_series_multinode.html>`__ (previous "
"tutorial in this series)"
msgstr "`使用DDP进行多节点训练 <ddp_series_multinode.html>`__ (本系列教程的上一个部分)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Mixed Precision training <https://pytorch.org/docs/stable/amp.html>`__"
msgstr "`混合精度训练 <https://pytorch.org/docs/stable/amp.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Fully-Sharded Data Parallel <https://pytorch.org/docs/stable/fsdp.html>`__"
msgstr "`完全切片数据并行 <https://pytorch.org/docs/stable/fsdp.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Training a 1T parameter model with FSDP "
"<https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-"
"pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff>`__"
msgstr ""
"`使用FSDP训练1万亿参数模型 <https://medium.com/pytorch/training-a-1-trillion-"
"parameter-model-with-pytorch-fully-sharded-data-parallel-on-"
"aws-3ac13aa96cff>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`FSDP Video Tutorial Series "
"<https://www.youtube.com/playlist?list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT>`__"
msgstr ""
"`FSDP视频教程系列 "
"<https://www.youtube.com/playlist?list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Introduction <../beginner/ddp_series_intro.html>`__ \\|\\| `What is DDP "
"<../beginner/ddp_series_theory.html>`__ \\|\\| `Single-Node Multi-GPU "
"Training <../beginner/ddp_series_multigpu.html>`__ \\|\\| `Fault Tolerance "
"<../beginner/ddp_series_fault_tolerance.html>`__ \\|\\| **Multi-Node "
"training** \\|\\| `minGPT Training <ddp_series_minGPT.html>`__"
msgstr ""
"`教程介绍 <../beginner/ddp_series_intro.html>`__ \\|\\| `什么是DDP "
"<../beginner/ddp_series_theory.html>`__ \\|\\| `单节点多GPU训练 "
"<../beginner/ddp_series_multigpu.html>`__ \\|\\| `容错机制 "
"<../beginner/ddp_series_fault_tolerance.html>`__ \\|\\| **多节点训练** \\|\\| "
"`minGPT训练 <ddp_series_minGPT.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Multinode Training"
msgstr "多节点训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Launching multinode training jobs with ``torchrun``"
msgstr "使用``torchrun``启动多节点训练任务"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Code changes (and things to keep in mind) when moving from single-node to "
"multinode training."
msgstr "从单节点到多节点训练时需要的代码更改（以及需要注意的地方）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":octicon:`code-square;1.0em;` View the code used in this tutorial on `GitHub"
" <https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multinode.py>`__"
msgstr ""
":octicon:`code-square;1.0em;` 查看此教程中使用的代码，请访问`GitHub "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multinode.py>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch/KaAJtI1T2x4>`__."
msgstr "可以跟随以下视频，或者访问`youtube <https://www.youtube.com/watch/KaAJtI1T2x4>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Multinode training involves deploying a training job across several "
"machines. There are two ways to do this:"
msgstr "多节点训练涉及将一个训练任务部署到多台机器上。有两种实现方式："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"running a ``torchrun`` command on each machine with identical rendezvous "
"arguments, or"
msgstr "在每台机器上运行带有相同集合参数的``torchrun``命令，或者"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"deploying it on a compute cluster using a workload manager (like SLURM)"
msgstr "使用工作负载管理器（例如SLURM）在计算集群上部署。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this video we will go over the (minimal) code changes required to move "
"from single-node multigpu to multinode training, and run our training script"
" in both of the above ways."
msgstr "在视频中，我们将讨论从单节点多GPU训练到多节点训练所需的代码更改，并以以上两种方式运行我们的训练脚本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that multinode training is bottlenecked by inter-node communication "
"latencies. Running a training job on 4 GPUs on a single node will be faster "
"than running it on 4 nodes with 1 GPU each."
msgstr "请注意，多节点训练的瓶颈在于节点间的通信延迟。在单台节点上的4个GPU上运行训练任务将比在4台节点上每台1个GPU速度更快。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Local and Global ranks"
msgstr "本地和全局等级"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In single-node settings, we were tracking the ``gpu_id`` of each device "
"running our training process. ``torchrun`` tracks this value in an "
"environment variable ``LOCAL_RANK`` which uniquely identifies each GPU-"
"process on a node. For a unique identifier across all the nodes, "
"``torchrun`` provides another variable ``RANK`` which refers to the global "
"rank of a process."
msgstr ""
"在单节点设置中，我们追踪每个运行训练过程的设备的``gpu_id``。``torchrun``通过环境变量``LOCAL_RANK``追踪每个节点上唯一标识的GPU进程。对于跨所有节点的唯一标识符，``torchrun``提供了另一个变量``RANK``，指代全局等级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Do not use ``RANK`` for critical logic in your training job. When "
"``torchrun`` restarts processes after a failure or membership changes, there"
" is no guarantee that the processes will hold the same ``LOCAL_RANK`` and "
"``RANKS``."
msgstr ""
"不要在训练任务中的关键逻辑中使用``RANK``。当``torchrun``在故障或成员变化后重新启动进程时，不能保证进程会保持相同的``LOCAL_RANK``和``RANK``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Heteregeneous Scaling"
msgstr "异构扩展"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Torchrun supports *heteregenous scaling* i.e. each of your multinode "
"machines can have different number of GPUs participating in the training "
"job. In the video, I deployed the code on 2 machines where one machine has 4"
" GPUs and the other used only 2 GPUs."
msgstr ""
"Torchrun支持*异构扩展*，即每台多节点机器在训练任务中可以有不同数量的参与GPU。在视频中，我将代码部署到两台机器上，其中一台有4个GPU，另一个只使用了2个GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Troubleshooting"
msgstr "故障排除"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Ensure that your nodes are able to communicate with each other over TCP."
msgstr "确保你的节点可以通过TCP互相通信。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Set env variable ``NCCL_DEBUG`` to ``INFO`` (using ``export "
"NCCL_DEBUG=INFO``) to print verbose logs that can help diagnose the issue."
msgstr ""
"将环境变量``NCCL_DEBUG``设置为``INFO``（使用``export "
"NCCL_DEBUG=INFO``），以打印能够帮助诊断问题的详细日志。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Sometimes you might need to explicitly set the network interface for the "
"distributed backend (``export NCCL_SOCKET_IFNAME=eth0``). Read more about "
"this `here <https://pytorch.org/docs/stable/distributed.html#choosing-the-"
"network-interface-to-use>`__."
msgstr ""
"有时需要显式设置分布式后端的网络接口（``export NCCL_SOCKET_IFNAME=eth0``）。更多信息请阅读`这里 "
"<https://pytorch.org/docs/stable/distributed.html#choosing-the-network-"
"interface-to-use>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Training a GPT model with DDP <ddp_series_minGPT.html>`__  (next tutorial "
"in this series)"
msgstr "`使用DDP训练GPT模型 <ddp_series_minGPT.html>`__  (本系列教程的下一个部分)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Fault Tolerant distributed training "
"<../beginner/ddp_series_fault_tolerance.html>`__ (previous tutorial in this "
"series)"
msgstr "`容错分布式训练 <../beginner/ddp_series_fault_tolerance.html>`__（本系列的上一个教程）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__"
msgstr "`torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Rendezvous arguments "
"<https://pytorch.org/docs/stable/elastic/run.html#note-on-rendezvous-"
"backend>`__"
msgstr ""
"`Rendezvous 参数 <https://pytorch.org/docs/stable/elastic/run.html#note-on-"
"rendezvous-backend>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Setting up a cluster on AWS "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/slurm/setup_pcluster_slurm.md>`__"
msgstr ""
"`在 AWS 上设置集群 <https://github.com/pytorch/examples/blob/main/distributed/ddp-"
"tutorial-series/slurm/setup_pcluster_slurm.md>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`Slurm docs <https://slurm.schedmd.com/>`__"
msgstr "`Slurm 文档 <https://slurm.schedmd.com/>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Getting Started with Distributed Data Parallel"
msgstr "分布式数据并行的入门"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Shen Li <https://mrshenli.github.io/>`_"
msgstr "**作者**: `Shen Li <https://mrshenli.github.io/>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Edited by**: `Joe Zhu <https://github.com/gunandrose4u>`_, `Chirag Pandya "
"<https://github.com/c-p-i-o>`__"
msgstr ""
"**编辑**: `Joe Zhu <https://github.com/gunandrose4u>`_, `Chirag Pandya "
"<https://github.com/c-p-i-o>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst>`__"
" 中查看和编辑此教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`PyTorch Distributed Overview <../beginner/dist_overview.html>`__"
msgstr "`PyTorch 分布式概述 <../beginner/dist_overview.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`DistributedDataParallel API documents "
"<https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
msgstr ""
"`分布式数据并行 API 文档 "
"<https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`DistributedDataParallel notes "
"<https://pytorch.org/docs/master/notes/ddp.html>`__"
msgstr "`分布式数据并行笔记 <https://pytorch.org/docs/master/notes/ddp.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`DistributedDataParallel <https://pytorch.org/docs/stable/nn.html#module-"
"torch.nn.parallel>`__ (DDP) is a powerful module in PyTorch that allows you "
"to parallelize your model across multiple machines, making it perfect for "
"large-scale deep learning applications. To use DDP, you'll need to spawn "
"multiple processes and create a single instance of DDP per process."
msgstr ""
"`分布式数据并行 <https://pytorch.org/docs/stable/nn.html#module-"
"torch.nn.parallel>`__ (DDP) 是 PyTorch "
"中一个强大的模块，它允许你在多台机器上并行化模型，使其适合大规模深度学习应用。要使用 DDP，你需要为每个进程生成多个进程，并为每个进程创建一个单独的 "
"DDP 实例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"But how does it work? DDP uses collective communications from the "
"`torch.distributed "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html>`__ package to "
"synchronize gradients and buffers across all processes. This means that each"
" process will have its own copy of the model, but they'll all work together "
"to train the model as if it were on a single machine."
msgstr ""
"但是它是如何工作的？DDP 使用 `torch.distributed "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html>`__ "
"包中的集合通信来同步所有进程之间的梯度和缓冲区。这意味着每个进程都拥有模型的一个副本，但它们会协同工作来训练模型，就像在单台机器上一样。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To make this happen, DDP registers an autograd hook for each parameter in "
"the model. When the backward pass is run, this hook fires and triggers "
"gradient synchronization across all processes. This ensures that each "
"process has the same gradients, which are then used to update the model."
msgstr ""
"为了实现这一点，DDP "
"为模型中的每个参数注册了一个自动梯度钩子。当反向传播运行时，这个钩子会触发并启动所有进程的梯度同步。这确保了每个进程拥有相同的梯度，然后用于更新模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For more information on how DDP works and how to use it effectively, be sure"
" to check out the `DDP design note "
"<https://pytorch.org/docs/master/notes/ddp.html>`__. With DDP, you can train"
" your models faster and more efficiently than ever before!"
msgstr ""
"要详细了解 DDP 的工作原理以及如何有效地使用它，请务必查看 `DDP 设计笔记 "
"<https://pytorch.org/docs/master/notes/ddp.html>`__。使用 "
"DDP，你可以比以往更快、更高效地训练模型！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The recommended way to use DDP is to spawn one process for each model "
"replica. The model replica can span multiple devices. DDP processes can be "
"placed on the same machine or across machines. Note that GPU devices cannot "
"be shared across DDP processes (i.e. one GPU for one DDP process)."
msgstr ""
"使用 DDP 的推荐方法是为每个模型副本生成一个进程。模型副本可以跨多个设备扩展。DDP 进程可以放在同一台机器上或跨机器分布。请注意，GPU "
"设备不能在 DDP 进程之间共享（即一个 GPU 对应一个 DDP 进程）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we'll start with a basic DDP use case and then demonstrate"
" more advanced use cases, including checkpointing models and combining DDP "
"with model parallel."
msgstr "在本教程中，我们将从一个基本的 DDP 用例开始，然后演示更高级的用例，包括模型检查点保存和将 DDP 与模型并行结合使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The code in this tutorial runs on an 8-GPU server, but it can be easily "
"generalized to other environments."
msgstr "本教程中的代码运行在一个 8 GPU 的服务器上，但可以轻松推广到其他环境。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Comparison between ``DataParallel`` and ``DistributedDataParallel``"
msgstr "``DataParallel`` 和 ``DistributedDataParallel`` 的比较"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before we dive in, let's clarify why you would consider using "
"``DistributedDataParallel`` over ``DataParallel``, despite its added "
"complexity:"
msgstr ""
"在深入探讨之前，让我们澄清一下，尽管增加了复杂性，为什么你会考虑使用 ``DistributedDataParallel`` 而不是 "
"``DataParallel``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, ``DataParallel`` is single-process, multi-threaded, but it only works"
" on a single machine. In contrast, ``DistributedDataParallel`` is multi-"
"process and supports both single- and multi- machine training. Due to GIL "
"contention across threads, per-iteration replicated model, and additional "
"overhead introduced by scattering inputs and gathering outputs, "
"``DataParallel`` is usually slower than ``DistributedDataParallel`` even on "
"a single machine."
msgstr ""
"首先，``DataParallel`` 是单进程、多线程的，但它仅适用于单台机器。相比之下，``DistributedDataParallel`` "
"是多进程的，并支持单机和多机训练。由于线程之间的 GIL "
"竞争、每次迭代都会复制模型，以及输入分散和输出聚合引入的额外开销，即使在单机上，``DataParallel`` 通常也比 "
"``DistributedDataParallel`` 慢。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Recall from the `prior tutorial "
"<https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html>`__"
" that if your model is too large to fit on a single GPU, you must use "
"**model parallel** to split it across multiple GPUs. "
"``DistributedDataParallel`` works with **model parallel**, while "
"``DataParallel`` does not at this time. When DDP is combined with model "
"parallel, each DDP process would use model parallel, and all processes "
"collectively would use data parallel."
msgstr ""
"回顾 `前面的教程 "
"<https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html>`__，如果你的模型太大而无法放在单个"
" GPU 上，你必须使用 **模型并行** 将其分割到多个 GPU 上。``DistributedDataParallel`` 可与 **模型并行** "
"一起工作，而目前 ``DataParallel`` 不能。当 DDP 与模型并行结合时，每个 DDP 进程将使用模型并行，所有进程将共同使用数据并行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Basic Use Case"
msgstr "基本用例"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To create a DDP module, you must first set up process groups properly. More "
"details can be found in `Writing Distributed Applications with PyTorch "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html>`__."
msgstr ""
"要创建一个 DDP 模块，你必须首先正确地设置进程组。详细信息可以在 `使用 PyTorch 编写分布式应用 "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html>`__ 中找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, let's create a toy module, wrap it with DDP, and feed it some dummy "
"input data. Please note, as DDP broadcasts model states from rank 0 process "
"to all other processes in the DDP constructor, you do not need to worry "
"about different DDP processes starting from different initial model "
"parameter values."
msgstr ""
"现在，让我们创建一个玩具模块，用 DDP 包装它，并为其提供一些虚拟的输入数据。请注意，由于 DDP 在构造函数中将模型状态从 rank 0 "
"进程广播到所有其他进程，你无需关心不同的 DDP 进程从不同的初始模型参数值开始。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As you can see, DDP wraps lower-level distributed communication details and "
"provides a clean API as if it were a local model. Gradient synchronization "
"communications take place during the backward pass and overlap with the "
"backward computation. When the ``backward()`` returns, ``param.grad`` "
"already contains the synchronized gradient tensor. For basic use cases, DDP "
"only requires a few more lines of code to set up the process group. When "
"applying DDP to more advanced use cases, some caveats require caution."
msgstr ""
"正如你所看到的，DDP 封装了底层的分布式通信细节，并提供了一个简单的 "
"API，就像它是一个本地模型一样。梯度同步通讯发生在反向传播过程中，并与反向计算重叠。当 ``backward()`` "
"返回时，``param.grad`` 已经包含了同步的梯度张量。对于基本用例，DDP 只需要多几行代码来设置进程组。在将 DDP "
"应用于更高级的用例时，需要注意一些注意事项。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Skewed Processing Speeds"
msgstr "处理速度不均衡"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In DDP, the constructor, the forward pass, and the backward pass are "
"distributed synchronization points. Different processes are expected to "
"launch the same number of synchronizations and reach these synchronization "
"points in the same order and enter each synchronization point at roughly the"
" same time. Otherwise, fast processes might arrive early and timeout while "
"waiting for stragglers. Hence, users are responsible for balancing workload "
"distributions across processes. Sometimes, skewed processing speeds are "
"inevitable due to, e.g., network delays, resource contentions, or "
"unpredictable workload spikes. To avoid timeouts in these situations, make "
"sure that you pass a sufficiently large ``timeout`` value when calling "
"`init_process_group "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group>`__."
msgstr ""
"在 DDP "
"中，构造函数、前向传播和反向传播是分布式同步点。期望不同的进程启动相同数量的同步，并按照相同的顺序到达这些同步点，并且大致同时进入每个同步点。否则，快速的进程可能会提前到达并在等待慢进程时超时。因此，用户有责任平衡进程之间的工作负载分布。有时，由于诸如网络延迟、资源竞争或不可预知的工作负荷波动等原因，处理速度不均衡是不可避免的。为避免这种情况下的超时，请确保在调用"
" `init_process_group "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group>`__"
" 时传递一个足够大的 ``timeout`` 值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Save and Load Checkpoints"
msgstr "保存和加载检查点"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It's common to use ``torch.save`` and ``torch.load`` to checkpoint modules "
"during training and recover from checkpoints. See `SAVING AND LOADING MODELS"
" <https://pytorch.org/tutorials/beginner/saving_loading_models.html>`__ for "
"more details. When using DDP, one optimization is to save the model in only "
"one process and then load it on all processes, reducing write overhead. This"
" works because all processes start from the same parameters and gradients "
"are synchronized in backward passes, and hence optimizers should keep "
"setting parameters to the same values. If you use this optimization (i.e. "
"save on one process but restore on all), make sure no process starts loading"
" before the saving is finished. Additionally, when loading the module, you "
"need to provide an appropriate ``map_location`` argument to prevent "
"processes from stepping into others' devices. If ``map_location`` is "
"missing, ``torch.load`` will first load the module to CPU and then copy each"
" parameter to where it was saved, which would result in all processes on the"
" same machine using the same set of devices. For more advanced failure "
"recovery and elasticity support, please refer to `TorchElastic "
"<https://pytorch.org/elastic>`__."
msgstr ""
"在训练过程中使用 ``torch.save`` 和 ``torch.load`` 来检查点模块并从检查点恢复是很常见的。详细信息请参见 `保存和加载模型"
" <https://pytorch.org/tutorials/beginner/saving_loading_models.html>`__。使用 "
"DDP "
"时，一种优化方法是仅在一个进程中保存模型，然后在所有进程中加载模型，从而减少写入开销。这可行是因为所有进程从相同的参数开始，并且在反向传播中梯度被同步，因此优化器应该继续设置相同的参数值。如果使用此优化（即在一个进程中保存但在所有进程中恢复），请确保在保存完成之前没有进程开始加载。此外，在加载模块时，你需要提供一个适当的"
" ``map_location`` 参数，以防止进程进入其他进程的设备。如果缺少 ``map_location``，``torch.load`` "
"将首先将模块加载到 CPU，然后将每个参数复制到保存它的地方，这会导致同一台机器上的所有进程使用同一组设备。有关更高级的故障恢复和弹性支持，请参考 "
"`TorchElastic <https://pytorch.org/elastic>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Combining DDP with Model Parallelism"
msgstr "将 DDP 与模型并行结合"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"DDP also works with multi-GPU models. DDP wrapping multi-GPU models is "
"especially helpful when training large models with a huge amount of data."
msgstr "DDP 还可用于多 GPU 模型。当训练具有大量数据的超大模型时，DDP 包裹多 GPU 模型尤其有帮助。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When passing a multi-GPU model to DDP, ``device_ids`` and ``output_device`` "
"must NOT be set. Input and output data will be placed in proper devices by "
"either the application or the model ``forward()`` method."
msgstr ""
"当将一个多 GPU 模型传递给 DDP 时，`device_ids` 和 `output_device` 必须未设置。输入和输出数据将由应用程序或模型的"
" ``forward()`` 方法正确地放置到设备上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Initialize DDP with torch.distributed.run/torchrun"
msgstr "使用 torch.distributed.run/torchrun 初始化 DDP"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can leverage PyTorch Elastic to simplify the DDP code and initialize the "
"job more easily. Let's still use the Toymodel example and create a file "
"named ``elastic_ddp.py``."
msgstr ""
"我们可以利用 PyTorch Elastic 简化 DDP 代码并更容易初始化工作。让我们仍然使用 Toymodel 示例并创建一个名为 "
"``elastic_ddp.py`` 的文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One can then run a `torch elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ command on all "
"nodes to initialize the DDP job created above:"
msgstr ""
"然后可以在所有节点上运行一个 `torch elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ 命令来初始化上述的 DDP "
"工作："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example above, we are running the DDP script on two hosts and we run "
"with 8 processes on each host. That is,  we are running this job on 16 GPUs."
" Note that ``$MASTER_ADDR`` must be the same across all nodes."
msgstr ""
"在上述示例中，我们在两个主机上运行 DDP 脚本，并且在每个主机上运行 8 个进程。也就是说，我们在 16 个 GPU "
"上运行此任务。请注意，``$MASTER_ADDR`` 必须在所有节点中相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here ``torchrun`` will launch 8 processes and invoke ``elastic_ddp.py`` on "
"each process on the node it is launched on, but user also needs to apply "
"cluster management tools like slurm to actually run this command on 2 nodes."
msgstr ""
"在这里，``torchrun`` 将启动 8 个进程，并在它启动的节点上的每个进程上调用 "
"``elastic_ddp.py``。但用户还需要应用集群管理工具（如 slurm）来实际在 2 个节点上运行此命令。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For example, on a SLURM enabled cluster, we can write a script to run the "
"command above and set ``MASTER_ADDR`` as:"
msgstr "例如，在启用了 SLURM 的集群上，我们可以编写一个脚本来运行上述命令并设置 ``MASTER_ADDR``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Then we can just run this script using the SLURM command: ``srun --nodes=2 "
"./torchrun_script.sh``."
msgstr "然后我们只需使用 SLURM 命令运行此脚本：``srun --nodes=2 ./torchrun_script.sh``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is just an example; you can choose your own cluster scheduling tools to"
" initiate the ``torchrun`` job."
msgstr "这只是一个例子；你可以选择自己的集群调度工具来启动 ``torchrun`` 作业。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For more information about Elastic run, please see the `quick start document"
" <https://pytorch.org/docs/stable/elastic/quickstart.html>`__."
msgstr ""
"有关 Elastic run 的更多信息，请参见 `快速入门文档 "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed Pipeline Parallelism Using RPC"
msgstr "使用 RPC 的分布式流水线并行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This tutorial has been deprecated."
msgstr "本教程已被弃用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Redirecting to a newer tutorial in 3 seconds..."
msgstr "3 秒钟后重定向到更新的教程..."

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Writing Distributed Applications with PyTorch"
msgstr "使用 PyTorch 编写分布式应用程序"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Séb Arnold <https://seba1511.com>`_"
msgstr "**作者**: `Séb Arnold <https://seba1511.com>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst>`__"
" 中查看和编辑此教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this short tutorial, we will be going over the distributed package of "
"PyTorch. We'll see how to set up the distributed setting, use the different "
"communication strategies, and go over some of the internals of the package."
msgstr "在本简短教程中，我们将介绍 PyTorch 的分布式包。我们将了解如何设置分布式环境，使用不同的通信策略，并深入了解一些包的内部工作原理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The distributed package included in PyTorch (i.e., ``torch.distributed``) "
"enables researchers and practitioners to easily parallelize their "
"computations across processes and clusters of machines. To do so, it "
"leverages message passing semantics allowing each process to communicate "
"data to any of the other processes. As opposed to the multiprocessing "
"(``torch.multiprocessing``) package, processes can use different "
"communication backends and are not restricted to being executed on the same "
"machine."
msgstr ""
"PyTorch 中包含的分布式包（即 "
"``torch.distributed``）使研究人员和从业者可以轻松地在进程和机器集群之间并行化计算。为此，它利用消息传递语义，允许每个进程将数据传递到其他任何进程。与"
" multiprocessing （``torch.multiprocessing``）包不同，进程可以使用不同的通信后端，并且不局限于同一机器上执行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to get started we need the ability to run multiple processes "
"simultaneously. If you have access to compute cluster you should check with "
"your local sysadmin or use your favorite coordination tool (e.g., `pdsh "
"<https://linux.die.net/man/1/pdsh>`__, `clustershell <https://cea-"
"hpc.github.io/clustershell/>`__, or `slurm <https://slurm.schedmd.com/>`__)."
" For the purpose of this tutorial, we will use a single machine and spawn "
"multiple processes using the following template."
msgstr ""
"为了开始，我们需要能够同时运行多个进程。如果您可以访问计算集群，应该咨询您所在的系统管理员，或者使用您喜欢的协调工具（例如 `pdsh "
"<https://linux.die.net/man/1/pdsh>`__、`clustershell <https://cea-"
"hpc.github.io/clustershell/>`__ 或 `slurm "
"<https://slurm.schedmd.com/>`__）。在本教程中，我们将使用单台机器，并使用以下模板生成多个进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The above script spawns two processes who will each setup the distributed "
"environment, initialize the process group (``dist.init_process_group``), and"
" finally execute the given ``run`` function."
msgstr ""
"以上脚本生成了两个进程，每个进程都会设置分布式环境，初始化进程组（``dist.init_process_group``），并最终执行给定的``run``函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's have a look at the ``init_process`` function. It ensures that every "
"process will be able to coordinate through a master, using the same ip "
"address and port. Note that we used the ``gloo`` backend but other backends "
"are available. (c.f. `Section 5.1 <#communication-backends>`__) We will go "
"over the magic happening in ``dist.init_process_group`` at the end of this "
"tutorial, but it essentially allows processes to communicate with each other"
" by sharing their locations."
msgstr ""
"让我们看看``init_process``函数。它确保每个进程能够通过主节点进行协调，使用相同的IP地址和端口。注意我们使用了``gloo``后端，但还有其他后端可用。（参见`第5.1节"
" <#communication-"
"backends>`__）在本教程的末尾我们将深入了解``dist.init_process_group``中的实现，但其基本作用是允许进程通过共享它们的位置进行通信。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Point-to-Point Communication"
msgstr "点对点通信"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Send and Recv"
msgstr "发送和接收"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A transfer of data from one process to another is called a point-to-point "
"communication. These are achieved through the ``send`` and ``recv`` "
"functions or their *immediate* counter-parts, ``isend`` and ``irecv``."
msgstr ""
"从一个进程向另一个进程传输数据称为点对点通信。可以通过``send``和``recv``函数或它们的非阻塞版本``isend``和``irecv``实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the above example, both processes start with a zero tensor, then process "
"0 increments the tensor and sends it to process 1 so that they both end up "
"with 1.0. Notice that process 1 needs to allocate memory in order to store "
"the data it will receive."
msgstr ""
"在上述示例中，两个进程都以一个零张量开始，然后进程0将张量加1并将其发送到进程1，以便两个进程最终拥有值1.0。注意进程1需要分配内存以存储接收到的数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Also notice that ``send/recv`` are **blocking**: both processes block until "
"the communication is completed. On the other hand immediates are **non-"
"blocking**; the script continues its execution and the methods return a "
"``Work`` object upon which we can choose to ``wait()``."
msgstr ""
"还要注意``send/recv``是**阻塞的**：两个进程都会阻塞，直到通信完成。而非阻塞版本方法是**非阻塞的**，脚本会继续执行，这些方法会返回一个``Work``对象，我们可以选择对其调用``wait()``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When using immediates we have to be careful about how we use the sent and "
"received tensors. Since we do not know when the data will be communicated to"
" the other process, we should not modify the sent tensor nor access the "
"received tensor before ``req.wait()`` has completed. In other words,"
msgstr ""
"使用非阻塞方法时，我们必须注意如何使用发送和接收的张量。由于我们无法确定数据何时会传输到另一个进程，因此在``req.wait()``完成之前，不应修改发送的张量或访问接收的张量。换句话说："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"writing to ``tensor`` after ``dist.isend()`` will result in undefined "
"behaviour."
msgstr "在``dist.isend()``后写入``tensor``将导致未定义行为。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"reading from ``tensor`` after ``dist.irecv()`` will result in undefined "
"behaviour, until ``req.wait()`` has been executed."
msgstr "在``dist.irecv()``后读取``tensor``将导致未定义行为，直到执行了``req.wait()``为止。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, after ``req.wait()`` has been executed we are guaranteed that the "
"communication took place, and that the value stored in ``tensor[0]`` is 1.0."
msgstr "然而，执行完``req.wait()``后，我们可以保证通信已经发生，并且``tensor[0]``中的值是1.0。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Point-to-point communication is useful when we want more fine-grained "
"control over the communication of our processes. They can be used to "
"implement fancy algorithms, such as the one used in `Baidu's DeepSpeech "
"<https://github.com/baidu-research/baidu-allreduce>`__ or `Facebook's large-"
"scale experiments "
"<https://research.fb.com/publications/imagenet1kin1h/>`__.(c.f. `Section 4.1"
" <#our-own-ring-allreduce>`__)"
msgstr ""
"当我们需要对进程间通信有更细粒度的控制时，点对点通信非常有用。它们可以用来实现例如`百度的DeepSpeech "
"<https://github.com/baidu-research/baidu-allreduce>`__或`Facebook的大规模实验 "
"<https://research.fb.com/publications/imagenet1kin1h/>`__中使用的复杂算法。（参见 `第4.1节"
" <#our-own-ring-allreduce>`__）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Collective Communication"
msgstr "集体通信"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Scatter"
msgstr "分散"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Gather"
msgstr "收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Reduce"
msgstr "归约"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "All-Reduce"
msgstr "全归约"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Broadcast"
msgstr "广播"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "All-Gather"
msgstr "全部收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As opposed to point-to-point communcation, collectives allow for "
"communication patterns across all processes in a **group**. A group is a "
"subset of all our processes. To create a group, we can pass a list of ranks "
"to ``dist.new_group(group)``. By default, collectives are executed on all "
"processes, also known as the **world**. For example, in order to obtain the "
"sum of all tensors on all processes, we can use the "
"``dist.all_reduce(tensor, op, group)`` collective."
msgstr ""
"与点对点通信相反，集体通信允许在一个**组**中的所有进程之间进行通信模式。组是所有进程的一个子集。我们可以通过向``dist.new_group(group)``传递一个排名列表来创建组。默认情况下，集体通信会在所有进程上执行，也称为**全局**。例如，为了获得所有进程上的张量的总和，我们可以使用``dist.all_reduce(tensor,"
" op, group)``集体通信。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since we want the sum of all tensors in the group, we use "
"``dist.ReduceOp.SUM`` as the reduce operator. Generally speaking, any "
"commutative mathematical operation can be used as an operator. Out-of-the-"
"box, PyTorch comes with many such operators, all working at the element-wise"
" level:"
msgstr ""
"由于我们希望获得组中所有张量的总和，我们使用``dist.ReduceOp.SUM``作为归约操作符。一般来说，任何交换律数学操作都可以作为操作符。PyTorch内置了许多这样的操作符，它们都在单个元素级别工作："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.SUM``,"
msgstr "``dist.ReduceOp.SUM``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.PRODUCT``,"
msgstr "``dist.ReduceOp.PRODUCT``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.MAX``,"
msgstr "``dist.ReduceOp.MAX``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.MIN``,"
msgstr "``dist.ReduceOp.MIN``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.BAND``,"
msgstr "``dist.ReduceOp.BAND``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.BOR``,"
msgstr "``dist.ReduceOp.BOR``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.BXOR``,"
msgstr "``dist.ReduceOp.BXOR``，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``dist.ReduceOp.PREMUL_SUM``."
msgstr "``dist.ReduceOp.PREMUL_SUM``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The full list of supported operators is `here "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp>`__."
msgstr ""
"支持的操作符的完整列表请参考`这里 "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.ReduceOp>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In addition to ``dist.all_reduce(tensor, op, group)``, there are many "
"additional collectives currently implemented in PyTorch. Here are a few "
"supported collectives."
msgstr ""
"除了``dist.all_reduce(tensor, op, "
"group)``，PyTorch目前还实现了许多其他集体通信操作。以下是一些支持的集体通信操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.broadcast(tensor, src, group)``: Copies ``tensor`` from ``src`` to "
"all other processes."
msgstr "``dist.broadcast(tensor, src, group)``：将``tensor``从``src``复制到所有其他进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.reduce(tensor, dst, op, group)``: Applies ``op`` to every ``tensor`` "
"and stores the result in ``dst``."
msgstr ""
"``dist.reduce(tensor, dst, op, "
"group)``：对每个``tensor``应用``op``并将结果存储在``dst``中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.all_reduce(tensor, op, group)``: Same as reduce, but the result is "
"stored in all processes."
msgstr "``dist.all_reduce(tensor, op, group)``：与reduce相同，但结果存储在所有进程中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.scatter(tensor, scatter_list, src, group)``: Copies the "
":math:`i^{\\text{th}}` tensor ``scatter_list[i]`` to the "
":math:`i^{\\text{th}}` process."
msgstr ""
"``dist.scatter(tensor, scatter_list, src, "
"group)``：将``scatter_list[i]``中的第i个张量复制到第i个进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.gather(tensor, gather_list, dst, group)``: Copies ``tensor`` from all"
" processes in ``dst``."
msgstr ""
"``dist.gather(tensor, gather_list, dst, "
"group)``：将所有进程中的``tensor``复制到``dst``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.all_gather(tensor_list, tensor, group)``: Copies ``tensor`` from all "
"processes to ``tensor_list``, on all processes."
msgstr ""
"``dist.all_gather(tensor_list, tensor, "
"group)``：将所有进程的``tensor``复制到``tensor_list``，在所有进程上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.barrier(group)``: Blocks all processes in `group` until each one has "
"entered this function."
msgstr "``dist.barrier(group)``：阻塞`group`中的所有进程直到每个进程都进入此函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``dist.all_to_all(output_tensor_list, input_tensor_list, group)``: Scatters "
"list of input tensors to all processes in a group and return gathered list "
"of tensors in output list."
msgstr ""
"``dist.all_to_all(output_tensor_list, input_tensor_list, "
"group)``：将输入张量列表分散到组中的所有进程，并将收集到的张量列表返回到输出列表中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The full list of supported collectives can be found by looking at the latest"
" documentation for PyTorch Distributed `(link) "
"<https://pytorch.org/docs/stable/distributed.html>`__."
msgstr ""
"支持的集体通信操作的完整列表可以通过查看PyTorch分布式的最新文档找到`(链接) "
"<https://pytorch.org/docs/stable/distributed.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed Training"
msgstr "分布式训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Note:** You can find the example script of this section in `this GitHub "
"repository <https://github.com/seba-1511/dist_tuto.pth/>`__."
msgstr ""
"**注意：**本节的示例脚本可以在`这个GitHub仓库中找到 "
"<https://github.com/seba-1511/dist_tuto.pth/>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we understand how the distributed module works, let us write "
"something useful with it. Our goal will be to replicate the functionality of"
" `DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__."
" Of course, this will be a didactic example and in a real-world situation "
"you should use the official, well-tested and well-optimized version linked "
"above."
msgstr ""
"现在我们已经理解了分布式模块的工作原理，让我们用它写点有用的东西。我们的目标是重现`DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__的功能。当然，这将是一个教学示例，在真实世界中，您应该使用上述链接中官方的、经过严格测试和优化的版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Quite simply we want to implement a distributed version of stochastic "
"gradient descent. Our script will let all processes compute the gradients of"
" their model on their batch of data and then average their gradients. In "
"order to ensure similar convergence results when changing the number of "
"processes, we will first have to partition our dataset. (You could also use "
"`torch.utils.data.random_split "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split>`__,"
" instead of the snippet below.)"
msgstr ""
"我们希望实现分布式版本的随机梯度下降。我们的脚本将让所有进程在它们的数据批次上计算模型的梯度，然后平均它们的梯度。为了确保在更改进程数量时具有类似的收敛结果，我们首先需要对数据集进行划分。（您也可以使用`torch.utils.data.random_split"
" "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split>`__，而不是下面的代码段。）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With the above snippet, we can now simply partition any dataset using the "
"following few lines:"
msgstr "通过上述代码段，我们现在可以使用以下几行代码简单地划分任何数据集："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Assuming we have 2 replicas, then each process will have a ``train_set`` of "
"60000 / 2 = 30000 samples. We also divide the batch size by the number of "
"replicas in order to maintain the *overall* batch size of 128."
msgstr ""
"假设我们有2个副本，那么每个进程将拥有60000 / 2 = "
"30000个样本的``train_set``。我们还将批量大小除以副本数，以保持总的批量大小为128。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can now write our usual forward-backward-optimize training code, and add "
"a function call to average the gradients of our models. (The following is "
"largely inspired by the official `PyTorch MNIST example "
"<https://github.com/pytorch/examples/blob/master/mnist/main.py>`__.)"
msgstr ""
"现在我们可以写我们通常的前向-反向优化训练代码，并添加一个函数调用以平均我们模型的梯度。（以下代码大部分是受官方`PyTorch MNIST示例 "
"<https://github.com/pytorch/examples/blob/master/mnist/main.py>`__启发的。）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It remains to implement the ``average_gradients(model)`` function, which "
"simply takes in a model and averages its gradients across the whole world."
msgstr "剩下的就是实现``average_gradients(model)``函数，该函数简单地接收一个模型并平均它的所有全局梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"*Et voilà*! We successfully implemented distributed synchronous SGD and "
"could train any model on a large computer cluster."
msgstr "*完成了*! 我们成功实现了分布式同步SGD，并可以在大型计算集群上训练任何模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Note:** While the last sentence is *technically* true, there are `a lot "
"more tricks <https://seba-1511.github.io/dist_blog>`__ required to implement"
" a production-level implementation of synchronous SGD. Again, use what `has "
"been tested and optimized "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`__."
msgstr ""
"**注意：**尽管最后一句话在技术上是正确的，要实现生产级的同步SGD实施还需要`很多技巧 "
"<https://seba-1511.github.io/dist_blog>`__。再次强调，请使用`经过测试和优化的版本 "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Our Own Ring-Allreduce"
msgstr "我们自己的环形全归约"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As an additional challenge, imagine that we wanted to implement DeepSpeech's"
" efficient ring allreduce. This is fairly easy to implement using point-to-"
"point collectives."
msgstr "作为一个额外的挑战，假设我们想实现DeepSpeech的高效环形全归约。这使用点对点通信操作相当容易实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the above script, the ``allreduce(send, recv)`` function has a slightly "
"different signature than the ones in PyTorch. It takes a ``recv`` tensor and"
" will store the sum of all ``send`` tensors in it. As an exercise left to "
"the reader, there is still one difference between our version and the one in"
" DeepSpeech: their implementation divides the gradient tensor into *chunks*,"
" so as to optimally utilize the communication bandwidth. (Hint: `torch.chunk"
" <https://pytorch.org/docs/stable/torch.html#torch.chunk>`__)"
msgstr ""
"在上述脚本中，``allreduce(send, "
"recv)``函数的签名与PyTorch中的稍有不同。它接收一个``recv``张量，并将所有``send``张量的总和存储在其中。作为留给读者的练习，我们的版本与DeepSpeech的版本之间还有一个不同之处：他们的实现将梯度张量分成*块*，以便最佳利用通信带宽。（提示：`torch.chunk"
" <https://pytorch.org/docs/stable/torch.html#torch.chunk>`__）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Advanced Topics"
msgstr "高级主题"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We are now ready to discover some of the more advanced functionalities of "
"``torch.distributed``. Since there is a lot to cover, this section is "
"divided into two subsections:"
msgstr "现在我们准备好了解``torch.distributed``的一些更高级的功能。由于涉及的内容很多，本节分为两个子部分："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Communication Backends: where we learn how to use MPI and Gloo for GPU-GPU "
"communication."
msgstr "通信后端：我们将在其中学习如何使用MPI和Gloo进行GPU-GPU通信。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Initialization Methods: where we understand how to best set up the initial "
"coordination phase in ``dist.init_process_group()``."
msgstr "初始化方法：我们将在其中理解如何在``dist.init_process_group()``中最好地设置初始协调阶段。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Communication Backends"
msgstr "通信后端"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One of the most elegant aspects of ``torch.distributed`` is its ability to "
"abstract and build on top of different backends. As mentioned before, there "
"are multiple backends implemented in PyTorch. Some of the most popular ones "
"are Gloo, NCCL, and MPI. They each have different specifications and "
"tradeoffs, depending on the desired use case. A comparative table of "
"supported functions can be found `here "
"<https://pytorch.org/docs/stable/distributed.html#module-"
"torch.distributed>`__."
msgstr ""
"``torch.distributed``最优雅的方面之一是它能够抽象并基于不同的后端构建。如前所述，PyTorch中实现了多个后端。一些最受欢迎的后端是Gloo、NCCL和MPI。它们根据所需的用例具有不同的规格和权衡。支持的功能的对比表可以参考`这里"
" <https://pytorch.org/docs/stable/distributed.html#module-"
"torch.distributed>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Gloo Backend**"
msgstr "**Gloo后端**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So far we have made extensive usage of the `Gloo backend "
"<https://github.com/facebookincubator/gloo>`__. It is quite handy as a "
"development platform, as it is included in the pre-compiled PyTorch binaries"
" and works on both Linux (since 0.2) and macOS (since 1.3). It supports all "
"point-to-point and collective operations on CPU, and all collective "
"operations on GPU. The implementation of the collective operations for CUDA "
"tensors is not as optimized as the ones provided by the NCCL backend."
msgstr ""
"到目前为止，我们已经广泛使用了`Gloo后端<https://github.com/facebookincubator/gloo>`__。它作为开发平台非常方便，因为它包括在预编译的PyTorch二进制文件中，并且可以在Linux（自0.2版本起）和macOS（自1.3版本起）上运行。它支持CPU上的所有点对点和集体操作，以及GPU上的所有集体操作。对于CUDA张量的集体操作实现并不像NCCL后端提供的那样优化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As you have surely noticed, our distributed SGD example does not work if you"
" put ``model`` on the GPU. In order to use multiple GPUs, let us also make "
"the following modifications:"
msgstr ""
"正如您肯定已经注意到的，如果将``model``放置到GPU上，我们的分布式SGD示例将无法工作。为了使用多个GPU，我们还需要进行以下修改："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Use ``device = torch.device(\"cuda:{}\".format(rank))``"
msgstr "使用``device = torch.device(\"cuda:{}\".format(rank))``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``model = Net()`` :math:`\\rightarrow` ``model = Net().to(device)``"
msgstr "``model = Net()`` :math:`\\rightarrow` ``model = Net().to(device)``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Use ``data, target = data.to(device), target.to(device)``"
msgstr "使用``data, target = data.to(device), target.to(device)``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With the above modifications, our model is now training on two GPUs and you "
"can monitor their utilization with ``watch nvidia-smi``."
msgstr "通过以上修改，我们的模型现在正在两个GPU上进行训练，您可以用``watch nvidia-smi``监控它们的利用率。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**MPI Backend**"
msgstr "**MPI后端**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The Message Passing Interface (MPI) is a standardized tool from the field of"
" high-performance computing. It allows to do point-to-point and collective "
"communications and was the main inspiration for the API of "
"``torch.distributed``. Several implementations of MPI exist (e.g. `Open-MPI "
"<https://www.open-mpi.org/>`__, `MVAPICH2 <http://mvapich.cse.ohio-"
"state.edu/>`__, `Intel MPI <https://software.intel.com/en-us/intel-mpi-"
"library>`__) each optimized for different purposes. The advantage of using "
"the MPI backend lies in MPI's wide availability - and high-level of "
"optimization - on large computer clusters. `Some "
"<https://developer.nvidia.com/mvapich>`__ `recent "
"<https://developer.nvidia.com/ibm-spectrum-mpi>`__ `implementations "
"<https://www.open-mpi.org/>`__ are also able to take advantage of CUDA IPC "
"and GPU Direct technologies in order to avoid memory copies through the CPU."
msgstr ""
"消息传递接口（MPI）是高性能计算领域的一种标准化工具。它允许进行点对点和集体通信，并且是``torch.distributed`` API "
"的主要灵感来源。存在若干种 MPI 实现（例如 `Open-MPI <https://www.open-mpi.org/>`__，`MVAPICH2 "
"<http://mvapich.cse.ohio-state.edu/>`__，`Intel MPI "
"<https://software.intel.com/en-us/intel-mpi-"
"library>`__），每种实现均针对不同的目的进行了优化。使用 MPI 后端的优势在于 MPI 在大型计算集群上的广泛可用性和高度优化。`一些 "
"<https://developer.nvidia.com/mvapich>`__ `近期的 "
"<https://developer.nvidia.com/ibm-spectrum-mpi>`__ `实现 <https://www.open-"
"mpi.org/>`__ 还能够利用 CUDA IPC 和 GPU 直连技术，从而避免通过 CPU 进行内存复制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Unfortunately, PyTorch's binaries cannot include an MPI implementation and "
"we'll have to recompile it by hand. Fortunately, this process is fairly "
"simple given that upon compilation, PyTorch will look *by itself* for an "
"available MPI implementation. The following steps install the MPI backend, "
"by installing PyTorch `from source <https://github.com/pytorch/pytorch#from-"
"source>`__."
msgstr ""
"不幸的是，PyTorch 的二进制文件不能包含 MPI 实现，因此我们需要手动重新编译它。幸运的是，这个过程相当简单，因为在编译期间 PyTorch "
"会自行查找可用的 MPI 实现。以下步骤通过从源码安装 PyTorch 来安装 MPI 后端。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Create and activate your Anaconda environment, install all the pre-"
"requisites following `the guide <https://github.com/pytorch/pytorch#from-"
"source>`__, but do **not** run ``python setup.py install`` yet."
msgstr ""
"创建并激活您的 Anaconda 环境，安装所有先决条件，按照`指南 <https://github.com/pytorch/pytorch#from-"
"source>`__操作，但请确保**不要**运行 ``python setup.py install``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Choose and install your favorite MPI implementation. Note that enabling "
"CUDA-aware MPI might require some additional steps. In our case, we'll stick"
" to Open-MPI *without* GPU support: ``conda install -c conda-forge openmpi``"
msgstr ""
"选择并安装您喜欢的 MPI 实现。请注意，启用 CUDA 感知 MPI 可能需要一些额外步骤。在本例中，我们将选择没有 GPU 支持的 Open-"
"MPI：``conda install -c conda-forge openmpi``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, go to your cloned PyTorch repo and execute ``python setup.py install``."
msgstr "现在进入您克隆的 PyTorch 仓库并执行 ``python setup.py install``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to test our newly installed backend, a few modifications are "
"required."
msgstr "为了测试我们新安装的后端，需要进行一些修改。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Replace the content under ``if __name__ == '__main__':`` with "
"``init_process(0, 0, run, backend='mpi')``."
msgstr ""
"将 ``if __name__ == &apos;__main__&apos;:`` 下的内容替换为 ``init_process(0, 0, run,"
" backend=&apos;mpi&apos;)``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Run ``mpirun -n 4 python myscript.py``."
msgstr "运行 ``mpirun -n 4 python myscript.py``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The reason for these changes is that MPI needs to create its own environment"
" before spawning the processes. MPI will also spawn its own processes and "
"perform the handshake described in `Initialization Methods <#initialization-"
"methods>`__, making the ``rank``\\ and ``size`` arguments of "
"``init_process_group`` superfluous. This is actually quite powerful as you "
"can pass additional arguments to ``mpirun`` in order to tailor computational"
" resources for each process. (Things like number of cores per process, hand-"
"assigning machines to specific ranks, and `some more <https://www.open-"
"mpi.org/faq/?category=running#mpirun-hostfile>`__) Doing so, you should "
"obtain the same familiar output as with the other communication backends."
msgstr ""
"进行这些更改的原因是 MPI 需要在生成进程之前创建自己的环境。MPI 还会生成自己的进程并执行在`初始化方法 <#initialization-"
"methods>`__中描述的握手操作，从而使得 ``init_process_group`` 的 ``rank`` 和 ``size`` "
"参数变得多余。这实际上相当强大，因为您可以向 ``mpirun`` "
"传递额外的参数，以针对每个进程定制计算资源（如每个进程的核心数量、手动将机器分配给特定的 rank，以及`其他 <https://www.open-"
"mpi.org/faq/?category=running#mpirun-"
"hostfile>`__内容）。通过这样做，您应该能够获得与其他通信后端相同的熟悉输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**NCCL Backend**"
msgstr "**NCCL 后端**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The `NCCL backend <https://github.com/nvidia/nccl>`__ provides an optimized "
"implementation of collective operations against CUDA tensors. If you only "
"use CUDA tensors for your collective operations, consider using this backend"
" for the best in class performance. The NCCL backend is included in the pre-"
"built binaries with CUDA support."
msgstr ""
"`NCCL 后端 <https://github.com/nvidia/nccl>`__ 提供了一种针对 CUDA "
"张量优化的集体操作实现。如果您的集体操作只使用 CUDA 张量，请考虑使用此后端以获得一流的性能。NCCL 后端包含在支持 CUDA "
"的预构建二进制文件中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Initialization Methods"
msgstr "初始化方法"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To conclude this tutorial, let's examine the initial function we invoked: "
"``dist.init_process_group(backend, init_method)``. Specifically, we will "
"discuss the various initialization methods responsible for the preliminary "
"coordination step between each process. These methods enable you to define "
"how this coordination is accomplished."
msgstr ""
"在本教程的最后，让我们检查我们调用的初始函数：``dist.init_process_group(backend, "
"init_method)``。具体来说，我们将讨论各种负责每个进程之间初步协调的初始化方法。这些方法使您能够定义如何完成这种协调。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The choice of initialization method depends on your hardware setup, and one "
"method may be more suitable than others. In addition to the following "
"sections, please refer to the `official documentation "
"<https://pytorch.org/docs/stable/distributed.html#initialization>`__ for "
"further information."
msgstr ""
"初始化方法的选择取决于您的硬件设置，一种方法可能比其他方法更适合。除了以下部分外，请参阅`官方文档 "
"<https://pytorch.org/docs/stable/distributed.html#initialization>`__以获取更多信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Environment Variable**"
msgstr "**环境变量**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have been using the environment variable initialization method throughout"
" this tutorial. By setting the following four environment variables on all "
"machines, all processes will be able to properly connect to the master, "
"obtain information about the other processes, and finally handshake with "
"them."
msgstr ""
"在本教程中，我们一直使用环境变量初始化方法。通过在所有机器上设置以下四个环境变量，所有进程将能够正确连接到主节点，获取其他进程的信息，并最终与它们握手。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``MASTER_PORT``: A free port on the machine that will host the process with "
"rank 0."
msgstr "``MASTER_PORT``：将托管 rank 为 0 的进程的机器上的一个空闲端口。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``MASTER_ADDR``: IP address of the machine that will host the process with "
"rank 0."
msgstr "``MASTER_ADDR``：将托管 rank 为 0 的进程的机器的 IP 地址。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``WORLD_SIZE``: The total number of processes, so that the master knows how "
"many workers to wait for."
msgstr "``WORLD_SIZE``：进程的总数，以便主节点知道需要等待多少个工作进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``RANK``: Rank of each process, so they will know whether it is the master "
"or a worker."
msgstr "``RANK``：每个进程的 rank，以便它们知道是主进程还是工作进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Shared File System**"
msgstr "**共享文件系统**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The shared filesystem requires all processes to have access to a shared file"
" system, and will coordinate them through a shared file. This means that "
"each process will open the file, write its information, and wait until "
"everybody did so. After that all required information will be readily "
"available to all processes. In order to avoid race conditions, the file "
"system must support locking through `fcntl <http://man7.org/linux/man-"
"pages/man2/fcntl.2.html>`__."
msgstr ""
"共享文件系统要求所有进程都能够访问一个共享文件系统，并通过共享文件对它们进行协调。这意味着每个进程都会打开文件，写入它的信息，并等待所有进程完成。之后，所有必要的信息将能为所有进程所用。为避免竞争条件，文件系统必须支持通过"
" `fcntl <http://man7.org/linux/man-pages/man2/fcntl.2.html>`__ 进行锁定。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**TCP**"
msgstr "**TCP**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Initializing via TCP can be achieved by providing the IP address of the "
"process with rank 0 and a reachable port number. Here, all workers will be "
"able to connect to the process with rank 0 and exchange information on how "
"to reach each other."
msgstr ""
"通过 TCP 初始化可以通过提供 rank 为 0 的进程的 IP 地址和一个可达的端口号来实现。在这里，所有工作进程都能够连接到 rank 为 0 "
"的进程并交换关于如何相互连接的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Acknowledgements**"
msgstr "**致谢**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"I'd like to thank the PyTorch developers for doing such a good job on their "
"implementation, documentation, and tests. When the code was unclear, I could"
" always count on the `docs "
"<https://pytorch.org/docs/stable/distributed.html>`__ or the `tests "
"<https://github.com/pytorch/pytorch/tree/master/test/distributed>`__ to find"
" an answer. In particular, I'd like to thank Soumith Chintala, Adam Paszke, "
"and Natalia Gimelshein for providing insightful comments and answering "
"questions on early drafts."
msgstr ""
"我要感谢 PyTorch 开发者在实现、文档和测试上出色的工作。当代码不清楚时，我总是可以依赖于`文档 "
"<https://pytorch.org/docs/stable/distributed.html>`__或`测试 "
"<https://github.com/pytorch/pytorch/tree/master/test/distributed>`__找到答案。特别感谢"
" Soumith Chintala、Adam Paszke 和 Natalia Gimelshein 对早期草稿提供的洞察性评论和回答问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_dqn_with_rnn_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_dqn_with_rnn_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Recurrent DQN: Training recurrent policies"
msgstr "循环 DQN：训练循环策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Vincent Moens <https://github.com/vmoens>`_"
msgstr "**作者**：`Vincent Moens <https://github.com/vmoens>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to incorporating an RNN in an actor in TorchRL"
msgstr "如何在 TorchRL 中将 RNN 集成到一个 actor 中"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to use that memory-based policy with a replay buffer and a loss module"
msgstr "如何将基于记忆的策略与回放缓冲区和损失模块结合使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch v2.0.0"
msgstr "PyTorch v2.0.0"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "gym[mujoco]"
msgstr "gym[mujoco]"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "tqdm"
msgstr "tqdm"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Memory-based policies are crucial not only when the observations are "
"partially observable but also when the time dimension must be taken into "
"account to make informed decisions."
msgstr "基于记忆的策略不仅在观察部分可观察时至关重要，而且在需要考虑时间维度以做出明智决策时也是如此。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Recurrent neural network have long been a popular tool for memory-based "
"policies. The idea is to keep a recurrent state in memory between two "
"consecutive steps, and use this as an input to the policy along with the "
"current observation."
msgstr ""
"循环神经网络长期以来一直是基于记忆的策略的热门工具。其核心思想是保留一个循环状态，在两个连续步骤之间存储在内存中，并将其作为策略的输入，与当前观察一起使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial shows how to incorporate an RNN in a policy using TorchRL."
msgstr "本教程展示了如何使用 TorchRL 将 RNN 集成到策略中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Key learnings:"
msgstr "关键学习点："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Incorporating an RNN in an actor in TorchRL;"
msgstr "在 TorchRL 中将 RNN 集成到一个 actor 中；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using that memory-based policy with a replay buffer and a loss module."
msgstr "将基于记忆的策略与回放缓冲区和损失模块结合使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The core idea of using RNNs in TorchRL is to use TensorDict as a data "
"carrier for the hidden states from one step to another. We'll build a policy"
" that reads the previous recurrent state from the current TensorDict, and "
"writes the current recurrent states in the TensorDict of the next state:"
msgstr ""
"在 TorchRL 中使用 RNN 的核心思想是使用 TensorDict 作为隐藏状态在步骤之间的载体。我们将构建一个策略，该策略从当前 "
"TensorDict 读取先前的循环状态，并将当前循环状态写入下一状态的 TensorDict 中："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Data collection with a recurrent policy"
msgstr "使用循环策略进行数据收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As this figure shows, our environment populates the TensorDict with zeroed "
"recurrent states which are read by the policy together with the observation "
"to produce an action, and recurrent states that will be used for the next "
"step. When the :func:`~torchrl.envs.utils.step_mdp` function is called, the "
"recurrent states from the next state are brought to the current TensorDict. "
"Let's see how this is implemented in practice."
msgstr ""
"如图所示，我们的环境用清零的循环状态填充 TensorDict，这些循环状态由策略与观察一起读取以生成操作，以及将用于下一步的循环状态。当调用 "
":func:`~torchrl.envs.utils.step_mdp` 函数时，下一状态的循环状态将被带入到当前 TensorDict "
"中。让我们看看如何在实际中实现这一点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are running this in Google Colab, make sure you install the following"
" dependencies:"
msgstr "如果您在 Google Colab 中运行此程序，请确保安装以下依赖项："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Environment"
msgstr "环境"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As usual, the first step is to build our environment: it helps us define the"
" problem and build the policy network accordingly. For this tutorial, we'll "
"be running a single pixel-based instance of the CartPole gym environment "
"with some custom transforms: turning to grayscale, resizing to 84x84, "
"scaling down the rewards and normalizing the observations."
msgstr ""
"通常，第一步是构建我们的环境：它帮助我们定义问题，并相应地构建策略网络。在本教程中，我们将运行一个基于像素的 CartPole gym "
"环境实例，并添加一些自定义变换：转换为灰度，调整大小为 84x84，缩减奖励并规范化观察。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The :class:`~torchrl.envs.transforms.StepCounter` transform is accessory. "
"Since the CartPole task goal is to make trajectories as long as possible, "
"counting the steps can help us track the performance of our policy."
msgstr ""
":class:`~torchrl.envs.transforms.StepCounter` 转换是附加的。由于 CartPole "
"任务的目标是使轨迹尽可能长，计数步骤可以帮助我们跟踪策略的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Two transforms are important for the purpose of this tutorial:"
msgstr "本教程中有两个重要的变换："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":class:`~torchrl.envs.transforms.InitTracker` will stamp the calls to "
":meth:`~torchrl.envs.EnvBase.reset` by adding a ``\"is_init\"`` boolean mask"
" in the TensorDict that will track which steps require a reset of the RNN "
"hidden states."
msgstr ""
":class:`~torchrl.envs.transforms.InitTracker` 将通过在 TensorDict 中添加一个 "
"``\"is_init\"`` 布尔掩码标记对 :meth:`~torchrl.envs.EnvBase.reset` "
"的调用，这个掩码将跟踪哪些步骤需要重置 RNN 隐藏状态。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The :class:`~torchrl.envs.transforms.TensorDictPrimer` transform is a bit "
"more technical. It is not required to use RNN policies. However, it "
"instructs the environment (and subsequently the collector) that some extra "
"keys are to be expected. Once added, a call to `env.reset()` will populate "
"the entries indicated in the primer with zeroed tensors. Knowing that these "
"tensors are expected by the policy, the collector will pass them on during "
"collection. Eventually, we'll be storing our hidden states in the replay "
"buffer, which will help us bootstrap the computation of the RNN operations "
"in the loss module (which would otherwise be initiated with 0s). In summary:"
" not including this transform will not impact hugely the training of our "
"policy, but it will make the recurrent keys disappear from the collected "
"data and the replay buffer, which will in turn lead to a slightly less "
"optimal training. Fortunately, the :class:`~torchrl.modules.LSTMModule` we "
"propose is equipped with a helper method to build just that transform for "
"us, so we can wait until we build it!"
msgstr ""
":class:`~torchrl.envs.transforms.TensorDictPrimer` 转换更为技术性。它不是使用 RNN "
"策略的必需条件。但是，它指示环境（以及随后是采集器）预期某些附加的关键值。一旦添加，调用 `env.reset()` 会用清零的张量填充 primer "
"中指示的条目。因为这些张量是策略所需的，采集器将在采集过程中传递它们。最终，我们将在回放缓冲区中存储我们的隐藏状态，这将有助于在损失模块中对 RNN "
"操作进行引导计算（否则将以 0s "
"开始）。总之：不包括此转换对我们的策略训练影响不大，但会使循环键从收集的数据和回放缓冲区中消失，从而导致训练效果略有下降。幸运的是，我们提供的 "
":class:`~torchrl.modules.LSTMModule` 配备了一个帮助方法，可以为我们构建这一变换，因此我们可以等到构建它为止！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "As always, we need to initialize manually our normalization constants:"
msgstr "像往常一样，我们需要手动初始化我们的归一化常数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Policy"
msgstr "策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our policy will have 3 components: a :class:`~torchrl.modules.ConvNet` "
"backbone, an :class:`~torchrl.modules.LSTMModule` memory layer and a shallow"
" :class:`~torchrl.modules.MLP` block that will map the LSTM output onto the "
"action values."
msgstr ""
"我们的策略将有三个组成部分：一个 :class:`~torchrl.modules.ConvNet` 主干，一个 "
":class:`~torchrl.modules.LSTMModule` 记忆层以及一个浅层 :class:`~torchrl.modules.MLP`"
" 块，将 LSTM 输出映射到动作值上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Convolutional network"
msgstr "卷积网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We build a convolutional network flanked with a "
":class:`torch.nn.AdaptiveAvgPool2d` that will squash the output in a vector "
"of size 64. The :class:`~torchrl.modules.ConvNet` can assist us with this:"
msgstr ""
"我们构建一个卷积网络，并在其两侧使用 :class:`torch.nn.AdaptiveAvgPool2d`，它会将输出压缩为大小为 64 "
"的向量。:class:`~torchrl.modules.ConvNet` 可以协助我们完成这个任务："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"we execute the first module on a batch of data to gather the size of the "
"output vector:"
msgstr "我们在一批数据上执行第一个模块以获取输出向量的大小："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "LSTM Module"
msgstr "LSTM 模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRL provides a specialized :class:`~torchrl.modules.LSTMModule` class to"
" incorporate LSTMs in your code-base. It is a "
":class:`~tensordict.nn.TensorDictModuleBase` subclass: as such, it has a set"
" of ``in_keys`` and ``out_keys`` that indicate what values should be "
"expected to be read and written/updated during the execution of the module. "
"The class comes with customizable predefined values for these attributes to "
"facilitate its construction."
msgstr ""
"TorchRL 提供了一个专用的 :class:`~torchrl.modules.LSTMModule` 类，用于将 LSTM "
"整合到您的代码库中。它是 :class:`~tensordict.nn.TensorDictModuleBase` 的子类：因此，它有一组 "
"``in_keys`` 和 ``out_keys``，指示在模块执行期间应读取和写入/更新哪些值。该类为这些属性提供了可自定义的预定义值，以便于其构建。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"*Usage limitations*: The class supports almost all LSTM features such as "
"dropout or multi-layered LSTMs. However, to respect TorchRL's conventions, "
"this LSTM must have the ``batch_first`` attribute set to ``True`` which is "
"**not** the default in PyTorch. However, our "
":class:`~torchrl.modules.LSTMModule` changes this default behavior, so we're"
" good with a native call."
msgstr ""
"*使用限制*：该类支持几乎所有 LSTM 的功能，例如 dropout 或多层 LSTM。然而，为了符合 TorchRL 的惯例，这个 LSTM 必须将"
" ``batch_first`` 属性设置为 ``True``，这在 PyTorch 中并非默认值。不过，我们的 "
":class:`~torchrl.modules.LSTMModule` 改变了这种默认行为，所以可以直接使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Also, the LSTM cannot have a ``bidirectional`` attribute set to ``True`` as "
"this wouldn't be usable in online settings. In this case, the default value "
"is the correct one."
msgstr ""
"此外，LSTM不能将``bidirectional``属性设置为``True``，因为这在在线设置中将不可用。在这种情况下，默认值是正确的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let us look at the LSTM Module class, specifically its in and out_keys:"
msgstr "让我们看看LSTM模块类，特别是它的in_keys和out_keys："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can see that these values contain the key we indicated as the in_key (and"
" out_key) as well as recurrent key names. The out_keys are preceded by a "
"\"next\" prefix that indicates that they will need to be written in the "
"\"next\" TensorDict. We use this convention (which can be overridden by "
"passing the in_keys/out_keys arguments) to make sure that a call to "
":func:`~torchrl.envs.utils.step_mdp` will move the recurrent state to the "
"root TensorDict, making it available to the RNN during the following call "
"(see figure in the intro)."
msgstr ""
"我们可以看到这些值包含了我们指定为in_key（和out_key）的键以及循环键名称。out_keys前缀为“next”，表明它们需要写入“next”TensorDict中。我们采用这种约定（可以通过传递in_keys/out_keys参数覆盖），以确保调用:func:`~torchrl.envs.utils.step_mdp`时，会将循环状态移动到根TensorDict，从而在后续调用中可供RNN使用（见介绍中的图）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As mentioned earlier, we have one more optional transform to add to our "
"environment to make sure that the recurrent states are passed to the buffer."
" The :meth:`~torchrl.modules.LSTMModule.make_tensordict_primer` method does "
"exactly that:"
msgstr ""
"如前所述，为了确保循环状态传递到缓冲区，我们还有一个可选的转化需要添加到我们的环境中。:meth:`~torchrl.modules.LSTMModule.make_tensordict_primer`方法正是这样做的："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"and that's it! We can print the environment to check that everything looks "
"good now that we have added the primer:"
msgstr "这样就完成了！我们可以打印环境以检查添加primer后的一切是否正常："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "MLP"
msgstr "MLP"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We use a single-layer MLP to represent the action values we'll be using for "
"our policy."
msgstr "我们使用一个单层MLP来表示我们用于策略的动作值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "and fill the bias with zeros:"
msgstr "并用零填充偏置："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using the Q-Values to select an action"
msgstr "使用Q值选择动作"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The last part of our policy is the Q-Value Module. The Q-Value module "
":class:`~torchrl.modules.tensordict_module.QValueModule` will read the "
"``\"action_values\"`` key that is produced by our MLP and from it, gather "
"the action that has the maximum value. The only thing we need to do is to "
"specify the action space, which can be done either by passing a string or an"
" action-spec. This allows us to use Categorical (sometimes called "
"\"sparse\") encoding or the one-hot version of it."
msgstr ""
"我们策略的最后一部分是Q值模块。Q值模块:class:`~torchrl.modules.tensordict_module.QValueModule`将读取由我们的MLP生成的``\"action_values\"``键，并从中选择值最大的动作。我们只需指定动作空间即可，这可以通过传递字符串或动作规格来完成。这样可以使用Categorical（有时称为\"稀疏\"）编码或其独热形式的版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRL also provides a wrapper class :class:`torchrl.modules.QValueActor` "
"that wraps a module in a Sequential together with a "
":class:`~torchrl.modules.tensordict_module.QValueModule` like we are doing "
"explicitly here. There is little advantage to do this and the process is "
"less transparent, but the end results will be similar to what we do here."
msgstr ""
"TorchRL还提供了一个包装类:class:`torchrl.modules.QValueActor`，它将一个模块与:class:`~torchrl.modules.tensordict_module.QValueModule`模块一起以Sequential的形式封装。这样做优势不大且过程不够透明，但结果与我们这里的实现相似。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can now put things together in a "
":class:`~tensordict.nn.TensorDictSequential`"
msgstr "我们现在可以使用:class:`~tensordict.nn.TensorDictSequential`将这些组件组合起来。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"DQN being a deterministic algorithm, exploration is a crucial part of it. "
"We'll be using an :math:`\\epsilon`-greedy policy with an epsilon of 0.2 "
"decaying progressively to 0. This decay is achieved via a call to "
":meth:`~torchrl.modules.EGreedyModule.step` (see training loop below)."
msgstr ""
"由于DQN是一个确定性算法，探索是其关键部分。我们将使用一个epsilon-"
"贪婪策略，其中epsilon从0.2逐步递减到0。这种衰减是通过调用:meth:`~torchrl.modules.EGreedyModule.step`实现的（见下方训练循环）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using the model for the loss"
msgstr "使用模型进行损失计算"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The model as we've built it is well equipped to be used in sequential "
"settings. However, the class :class:`torch.nn.LSTM` can use a cuDNN-"
"optimized backend to run the RNN sequence faster on GPU device. We would not"
" want to miss such an opportunity to speed up our training loop! To use it, "
"we just need to tell the LSTM module to run on \"recurrent-mode\" when used "
"by the loss. As we'll usually want to have two copies of the LSTM module, we"
" do this by calling a :meth:`~torchrl.modules.LSTMModule.set_recurrent_mode`"
" method that will return a new instance of the LSTM (with shared weights) "
"that will assume that the input data is sequential in nature."
msgstr ""
"我们构建的模型在顺序设置中可以很好地工作。然而，类:class:`torch.nn.LSTM`可以使用cuDNN优化后的后端在GPU设备上更快地运行RNN序列。我们绝不希望错过这样一个加速训练循环的机会！为了使用它，我们只需要告诉LSTM模块在通过损失使用时以“循环模式”运行。由于我们通常需要两个LSTM模块实例，我们通过调用:meth:`~torchrl.modules.LSTMModule.set_recurrent_mode`方法，该方法将返回一个新的LSTM实例（共享权重），并假设输入数据是顺序性质的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Because we still have a couple of uninitialized parameters we should "
"initialize them before creating an optimizer and such."
msgstr "因为我们仍然有一些未初始化的参数，所以在创建优化器等之前应该先初始化它们。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "DQN Loss"
msgstr "DQN损失"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Out DQN loss requires us to pass the policy and, again, the action-space. "
"While this may seem redundant, it is important as we want to make sure that "
"the :class:`~torchrl.objectives.DQNLoss` and the "
":class:`~torchrl.modules.tensordict_module.QValueModule` classes are "
"compatible, but aren't strongly dependent on each other."
msgstr ""
"我们的DQN损失需要我们传递策略和动作空间。虽然这看起来有些冗余，但很重要，因为我们必须确保:class:`~torchrl.objectives.DQNLoss`和:class:`~torchrl.modules.tensordict_module.QValueModule`类是兼容的，但不是强依赖。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To use the Double-DQN, we ask for a ``delay_value`` argument that will "
"create a non-differentiable copy of the network parameters to be used as a "
"target network."
msgstr "为了使用双DQN，我们提供``delay_value``参数，该参数将创建网络参数的非可微副本，用作目标网络。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since we are using a double DQN, we need to update the target parameters. "
"We'll use a  :class:`~torchrl.objectives.SoftUpdate` instance to carry out "
"this work."
msgstr ""
"由于我们使用双DQN，因此需要更新目标参数。我们将使用:class:`~torchrl.objectives.SoftUpdate`实例来完成这项工作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Collector and replay buffer"
msgstr "收集器和回放缓冲区"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We build the simplest data collector there is. We'll try to train our "
"algorithm with a million frames, extending the buffer with 50 frames at a "
"time. The buffer will be designed to store 20 thousands trajectories of 50 "
"steps each. At each optimization step (16 per data collection), we'll "
"collect 4 items from our buffer, for a total of 200 transitions. We'll use a"
" :class:`~torchrl.data.replay_buffers.LazyMemmapStorage` storage to keep the"
" data on disk."
msgstr ""
"我们构建了最简单的数据收集器。我们尝试用一百万帧训练算法，每次扩展缓冲区50帧。该缓冲区被设计为存储20000条50步的轨迹。在每次优化步骤（每次数据收集16次）中，我们从缓冲区中收集4项，总计200次转换。我们将使用:class:`~torchrl.data.replay_buffers.LazyMemmapStorage`存储器在磁盘上保存数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the sake of efficiency, we're only running a few thousands iterations "
"here. In a real setting, the total number of frames should be set to 1M."
msgstr "为了提高效率，这里我们只运行了一几千次迭代。在实际设置中，帧总数应设置为1M。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training loop"
msgstr "训练循环"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To keep track of the progress, we will run the policy in the environment "
"once every 50 data collection, and plot the results after training."
msgstr "为了跟踪进展，我们每进行50次数据收集就让策略运行一次，并在训练后绘制结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's plot our results:"
msgstr "让我们绘制结果："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have seen how an RNN can be incorporated in a policy in TorchRL. You "
"should now be able:"
msgstr "我们已经看到如何在TorchRL中将RNN融合到策略中。现在您应该能够："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Create an LSTM module that acts as a "
":class:`~tensordict.nn.TensorDictModule`"
msgstr "创建一个充当:class:`~tensordict.nn.TensorDictModule`的LSTM模块。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Indicate to the LSTM module that a reset is needed via an "
":class:`~torchrl.envs.transforms.InitTracker` transform"
msgstr "通过:class:`~torchrl.envs.transforms.InitTracker`转化向LSTM模块指示需要重置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Incorporate this module in a policy and in a loss module"
msgstr "将此模块集成到策略中以及损失模块中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Make sure that the collector is made aware of the recurrent state entries "
"such that they can be stored in the replay buffer along with the rest of the"
" data"
msgstr "确保收集器知道循环状态条目，使它们能够与数据的其余部分一起存储在回放缓冲区中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The TorchRL documentation can be found `here <https://pytorch.org/rl/>`_."
msgstr "可以在`此处 <https://pytorch.org/rl/>`_找到TorchRL文档。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: dqn_with_rnn_tutorial.py "
"<dqn_with_rnn_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：dqn_with_rnn_tutorial.py <dqn_with_rnn_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: dqn_with_rnn_tutorial.ipynb "
"<dqn_with_rnn_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：dqn_with_rnn_tutorial.ipynb "
"<dqn_with_rnn_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "(beta) Dynamic Quantization on BERT"
msgstr "(测试版) BERT动态量化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To get the most of this tutorial, we suggest using this `Colab Version "
"<https://colab.research.google.com/github/pytorch/tutorials/blob/gh-"
"pages/_downloads/dynamic_quantization_bert_tutorial.ipynb>`_. This will "
"allow you to experiment with the information presented below."
msgstr ""
"为了充分利用此教程，我们建议使用此`Colab版本 "
"<https://colab.research.google.com/github/pytorch/tutorials/blob/gh-"
"pages/_downloads/dynamic_quantization_bert_tutorial.ipynb>`_。这样可以让您试验下面所提供的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Jianyu Huang <https://github.com/jianyuh>`_"
msgstr "**作者**: `黄建宇 <https://github.com/jianyuh>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Reviewed by**: `Raghuraman Krishnamoorthi "
"<https://github.com/raghuramank100>`_"
msgstr ""
"**审阅**: `Raghuraman Krishnamoorthi <https://github.com/raghuramank100>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Edited by**: `Jessica Lin <https://github.com/jlin27>`_"
msgstr "**编辑**: `Jessica Lin <https://github.com/jlin27>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will apply the dynamic quantization on a BERT model, "
"closely following the BERT model from `the HuggingFace Transformers examples"
" <https://github.com/huggingface/transformers>`_. With this step-by-step "
"journey, we would like to demonstrate how to convert a well-known state-of-"
"the-art model like BERT into dynamic quantized model."
msgstr ""
"在本教程中，我们将对BERT模型应用动态量化，紧跟`HuggingFace Transformers示例 "
"<https://github.com/huggingface/transformers>`_中的BERT模型。通过这次循序渐进的旅程，我们希望展示如何将像BERT这样著名的顶尖模型转换为动态量化模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"BERT, or Bidirectional Embedding Representations from Transformers, is a new"
" method of pre-training language representations which achieves the state-"
"of-the-art accuracy results on many popular Natural Language Processing "
"(NLP) tasks, such as question answering, text classification, and others. "
"The original paper can be found `here "
"<https://arxiv.org/pdf/1810.04805.pdf>`_."
msgstr ""
"BERT，或双向嵌入表示的Transformer，是一种预训练语言表示的新方法，它在许多流行的自然语言处理（NLP）任务（如问答、文本分类等）上取得了最先进的准确性结果。原始论文可以在`此处"
" <https://arxiv.org/pdf/1810.04805.pdf>`_找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Dynamic quantization support in PyTorch converts a float model to a "
"quantized model with static int8 or float16 data types for the weights and "
"dynamic quantization for the activations. The activations are quantized "
"dynamically (per batch) to int8 when the weights are quantized to int8. In "
"PyTorch, we have `torch.quantization.quantize_dynamic API "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_,"
" which replaces specified modules with dynamic weight-only quantized "
"versions and output the quantized model."
msgstr ""
"PyTorch中的动态量化支持将浮点模型转换为具有静态int8或float16权重数据类型和动态激活量化的量化模型。激活是动态量化到int8的（按批次），而权重量化到int8。在PyTorch中，我们有`torch.quantization.quantize_dynamic"
" API "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_，其替换指定模块为动态权重量化版本并输出量化模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We demonstrate the accuracy and inference performance results on the "
"`Microsoft Research Paraphrase Corpus (MRPC) task "
"<https://www.microsoft.com/en-us/download/details.aspx?id=52398>`_ in the "
"General Language Understanding Evaluation benchmark `(GLUE) "
"<https://gluebenchmark.com/>`_. The MRPC (Dolan and Brockett, 2005) is a "
"corpus of sentence pairs automatically extracted from online news sources, "
"with human annotations of whether the sentences in the pair are semantically"
" equivalent. As the classes are imbalanced (68% positive, 32% negative), we "
"follow the common practice and report `F1 score <https://scikit-"
"learn.org/stable/modules/generated/sklearn.metrics.f1_score.html>`_. MRPC is"
" a common NLP task for language pair classification, as shown below."
msgstr ""
"我们展示了在通用语言理解评估基准`(GLUE) <https://gluebenchmark.com/>`_中的`微软研究复述语料库(MRPC)任务 "
"<https://www.microsoft.com/en-"
"us/download/details.aspx?id=52398>`_的准确性和推理性能结果。MRPC（Dolan和Brockett，2005）是一个自动从在线新闻来源中提取的句子对语料库，并由人工注释句子对是否语义等价。由于类别不平衡（68%为正，32%为负），我们遵循常见做法并报告`F1分数"
" <https://scikit-"
"learn.org/stable/modules/generated/sklearn.metrics.f1_score.html>`_。MRPC是语言对分类的常见NLP任务，见下方。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1. Setup"
msgstr "1. 设置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.1 Install PyTorch and HuggingFace Transformers"
msgstr "1.1 安装PyTorch和HuggingFace Transformers"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To start this tutorial, let’s first follow the installation instructions in "
"PyTorch `here <https://github.com/pytorch/pytorch/#installation>`_ and "
"HuggingFace Github Repo `here "
"<https://github.com/huggingface/transformers#installation>`_. In addition, "
"we also install `scikit-learn <https://github.com/scikit-learn/scikit-"
"learn>`_ package, as we will reuse its built-in F1 score calculation helper "
"function."
msgstr ""
"开始本教程之前，请首先遵循PyTorch `安装说明 "
"<https://github.com/pytorch/pytorch/#installation>`_和HuggingFace Github Repo"
" `安装说明 "
"<https://github.com/huggingface/transformers#installation>`_。此外，我们还安装`scikit-"
"learn <https://github.com/scikit-learn/scikit-"
"learn>`_包，因为我们将重用其内置的F1分数计算帮助函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Because we will be using the beta parts of the PyTorch, it is recommended to"
" install the latest version of torch and torchvision. You can find the most "
"recent instructions on local installation `here <https://pytorch.org/get-"
"started/locally/>`_. For example, to install on Mac:"
msgstr ""
"由于我们将使用PyTorch的测试版部分，建议安装最新版本的torch和torchvision。您可以在`此处 "
"<https://pytorch.org/get-started/locally/>`_找到本地安装的最新说明。例如，在Mac上安装："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.2 Import the necessary modules"
msgstr "1.2 导入必要的模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In this step we import the necessary Python modules for the tutorial."
msgstr "在这一步，我们导入本教程所需的Python模块。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We set the number of threads to compare the single thread performance "
"between FP32 and INT8 performance. In the end of the tutorial, the user can "
"set other number of threads by building PyTorch with right parallel backend."
msgstr ""
"我们设置线程数以比较FP32和INT8性能之间的单线程性能。在本教程末尾，用户可以通过使用合适的并行后端构建PyTorch来设置其他线程数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.3 Learn about helper functions"
msgstr "1.3 学习关于帮助函数的信息"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The helper functions are built-in in transformers library. We mainly use the"
" following helper functions: one for converting the text examples into the "
"feature vectors; The other one for measuring the F1 score of the predicted "
"result."
msgstr "transformers库中内置了帮助函数。我们主要使用以下帮助函数：一个用于将文本示例转换为特征向量；另一个用于测量预测结果的F1分数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The `glue_convert_examples_to_features "
"<https://github.com/huggingface/transformers/blob/main/src/transformers/data/datasets/glue.py>`_"
" function converts the texts into input features:"
msgstr ""
"`glue_convert_examples_to_features "
"<https://github.com/huggingface/transformers/blob/main/src/transformers/data/datasets/glue.py>`_函数将文本转换为输入特征："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Tokenize the input sequences;"
msgstr "对输入序列进行标记化；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Insert [CLS] in the beginning;"
msgstr "在开头插入[CLS]；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Insert [SEP] between the first sentence and the second sentence, and in the "
"end;"
msgstr "在第一句和第二句之间插入[SEP]，并在末尾插入；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Generate token type ids to indicate whether a token belongs to the first "
"sequence or the second sequence."
msgstr "生成token type ids以指示标记属于第一序列还是第二序列。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The `glue_compute_metrics "
"<https://github.com/huggingface/transformers/blob/main/src/transformers/data/metrics/__init__.py#L60>`_"
"  function has the compute metrics with the `F1 score <https://scikit-"
"learn.org/stable/modules/generated/sklearn.metrics.f1_score.html>`_, which "
"can be interpreted as a weighted average of the precision and recall, where "
"an F1 score reaches its best value at 1 and worst score at 0. The relative "
"contribution of precision and recall to the F1 score are equal."
msgstr ""
"`glue_compute_metrics "
"<https://github.com/huggingface/transformers/blob/main/src/transformers/data/metrics/__init__.py#L60>`_"
" 函数包含了计算指标的功能，其中包括 `F1分数 <https://scikit-"
"learn.org/stable/modules/generated/sklearn.metrics.f1_score.html>`_。F1分数可以被解释为精确率和召回率的加权平均值，当F1分数达到1时效果最好，达到0时效果最差。精确率和召回率对F1分数的相对贡献是相等的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The equation for the F1 score is:"
msgstr "F1分数的计算公式为："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"F1 = 2 * (\\text{precision} * \\text{recall}) / (\\text{precision} + "
"\\text{recall})"
msgstr ""
"F1 = 2 * (\\text{精确率} * \\text{召回率}) / (\\text{精确率} + \\text{召回率})\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.4 Download the dataset"
msgstr "1.4 下载数据集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before running MRPC tasks we download the `GLUE data "
"<https://gluebenchmark.com/tasks>`_ by running `this script "
"<https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e>`_ and "
"unpack it to a directory ``glue_data``."
msgstr ""
"在运行MRPC任务之前，我们通过运行 `该脚本 "
"<https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e>`_ 下载 "
"`GLUE数据集 <https://gluebenchmark.com/tasks>`_ 并将其解压到目录 ``glue_data`` 下。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2. Fine-tune the BERT model"
msgstr "2. 微调BERT模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The spirit of BERT is to pre-train the language representations and then to "
"fine-tune the deep bi-directional representations on a wide range of tasks "
"with minimal task-dependent parameters, and achieves state-of-the-art "
"results. In this tutorial, we will focus on fine-tuning with the pre-trained"
" BERT model to classify semantically equivalent sentence pairs on MRPC task."
msgstr ""
"BERT的核心理念是预训练语言表示，然后在广泛的任务上使用最少的任务相关参数对深度双向表示进行微调，从而实现最先进的成果。在本教程中，我们将专注于使用预训练的BERT模型进行微调，以分类MRPC任务中语义等同的句子对。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To fine-tune the pre-trained BERT model (``bert-base-uncased`` model in "
"HuggingFace transformers) for the MRPC task, you can follow the command in "
"`examples "
"<https://github.com/huggingface/transformers/tree/master/examples#mrpc>`_:"
msgstr ""
"为MRPC任务微调预训练的BERT模型（HuggingFace transformers中的``bert-base-uncased``模型），可以参考 "
"`examples "
"<https://github.com/huggingface/transformers/tree/master/examples#mrpc>`_ "
"中的指令："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We provide the fine-tuned BERT model for MRPC task `here "
"<https://download.pytorch.org/tutorial/MRPC.zip>`_. To save time, you can "
"download the model file (~400 MB) directly into your local folder "
"``$OUT_DIR``."
msgstr ""
"我们提供了为MRPC任务微调的BERT模型 `下载链接 "
"<https://download.pytorch.org/tutorial/MRPC.zip>`_。为了节省时间，您可以直接将该模型文件（约400 "
"MB）下载到本地文件夹 ``$OUT_DIR``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.1 Set global configurations"
msgstr "2.1 设置全局配置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here we set the global configurations for evaluating the fine-tuned BERT "
"model before and after the dynamic quantization."
msgstr "在动态量化前后评估微调的BERT模型时，我们设置全局配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.2 Load the fine-tuned BERT model"
msgstr "2.2 加载微调的BERT模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We load the tokenizer and fine-tuned BERT sequence classifier model (FP32) "
"from the ``configs.output_dir``."
msgstr "我们从 ``configs.output_dir`` 中加载分词器和微调的BERT序列分类模型（FP32）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2.3 Define the tokenize and evaluation function"
msgstr "2.3 定义分词和评估函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We reuse the tokenize and evaluation function from `HuggingFace "
"<https://github.com/huggingface/transformers/blob/main/examples/legacy/pytorch-"
"lightning/run_glue.py>`_."
msgstr ""
"我们从 `HuggingFace "
"<https://github.com/huggingface/transformers/blob/main/examples/legacy/pytorch-"
"lightning/run_glue.py>`_ 中复用了分词和评估函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3. Apply the dynamic quantization"
msgstr "3. 应用动态量化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We call ``torch.quantization.quantize_dynamic`` on the model to apply the "
"dynamic quantization on the HuggingFace BERT model. Specifically,"
msgstr ""
"我们调用 ``torch.quantization.quantize_dynamic`` 对HuggingFace BERT模型应用动态量化。具体来说，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We specify that we want the torch.nn.Linear modules in our model to be "
"quantized;"
msgstr "我们指定希望对模型中的torch.nn.Linear模块进行量化；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We specify that we want weights to be converted to quantized int8 values."
msgstr "我们指定希望将权重转换为量化的int8值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3.1 Check the model size"
msgstr "3.1 检查模型大小"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s first check the model size. We can observe a significant reduction in "
"model size (FP32 total size: 438 MB; INT8 total size: 181 MB):"
msgstr "首先检查模型大小。我们可以观察到模型大小显著减少（FP32总大小：438 MB；INT8总大小：181 MB）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The BERT model used in this tutorial (``bert-base-uncased``) has a "
"vocabulary size V of 30522. With the embedding size of 768, the total size "
"of the word embedding table is ~ 4 (Bytes/FP32) \\* 30522 \\* 768 = 90 MB. "
"So with the help of quantization, the model size of the non-embedding table "
"part is reduced from 350 MB (FP32 model) to 90 MB (INT8 model)."
msgstr ""
"本教程中使用的BERT模型（``bert-base-uncased``）的词汇表大小为30522。嵌入大小为768，总词嵌入表大小约为 ~ "
"4（字节/FP32） \\* 30522 \\* 768 = 90 MB。因此，借助量化，非嵌入表部分的模型大小从350 MB（FP32模型）减少到90"
" MB（INT8模型）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3.2 Evaluate the inference accuracy and time"
msgstr "3.2 评估推理准确性和速度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, let’s compare the inference time as well as the evaluation accuracy "
"between the original FP32 model and the INT8 model after the dynamic "
"quantization."
msgstr "接下来，我们比较原始FP32模型与动态量化后INT8模型之间的推理时间以及评估准确性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Running this locally on a MacBook Pro, without quantization, inference (for "
"all 408 examples in MRPC dataset) takes about 160 seconds, and with "
"quantization it takes just about 90 seconds. We summarize the results for "
"running the quantized BERT model inference on a Macbook Pro as the follows:"
msgstr ""
"在MacBook "
"Pro本地运行测试时，无量化情况下，运行MRPC数据集中所有408个样本的推理大约需要160秒，而进行量化后仅需约90秒。我们总结了在MacBook "
"Pro上运行量化BERT模型推理的结果如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have 0.6% lower F1 score accuracy after applying the post-training "
"dynamic quantization on the fine-tuned BERT model on the MRPC task. As a "
"comparison, in a `recent paper <https://arxiv.org/pdf/1910.06188.pdf>`_ "
"(Table 1), it achieved 0.8788 by applying the post-training dynamic "
"quantization and 0.8956 by applying the quantization-aware training. The "
"main difference is that we support the asymmetric quantization in PyTorch "
"while that paper supports the symmetric quantization only."
msgstr ""
"在MRPC任务中，对微调BERT模型应用后训练动态量化后，我们的F1分数准确率下降了0.6%。作为比较，在 `最近的一篇论文 "
"<https://arxiv.org/pdf/1910.06188.pdf>`_ "
"（表1）中，通过应用后训练动态量化实现了0.8788，通过应用量化感知训练实现了0.8956。主要区别在于我们在PyTorch中支持非对称量化，而论文中仅支持对称量化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that we set the number of threads to 1 for the single-thread comparison"
" in this tutorial. We also support the intra-op parallelization for these "
"quantized INT8 operators. The users can now set multi-thread by "
"``torch.set_num_threads(N)`` (``N`` is the number of intra-op "
"parallelization threads). One preliminary requirement to enable the intra-op"
" parallelization support is to build PyTorch with the right `backend "
"<https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-"
"options>`_ such as OpenMP, Native or TBB. You can use "
"``torch.__config__.parallel_info()`` to check the parallelization settings. "
"On the same MacBook Pro using PyTorch with Native backend for "
"parallelization, we can get about 46 seconds for processing the evaluation "
"of MRPC dataset."
msgstr ""
"注意，在本教程中，我们将线程数量设置为1，以进行单线程比较。我们还支持这些量化INT8操作符的内操作并行化。用户现在可以通过 "
"``torch.set_num_threads(N)`` （``N``是内操作并行化线程数）设置多线程。启用内操作并行化支持的一个前提要求，是使用正确的"
" `后端 "
"<https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-"
"options>`_ 构建PyTorch，例如OpenMP、Native或TBB。您可以使用 "
"``torch.__config__.parallel_info()`` 检查并行化设置。在同一台MacBook "
"Pro上使用PyTorch和用于并行化的Native后端，我们可以得到大约46秒的MRPC数据集评估处理时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3.3 Serialize the quantized model"
msgstr "3.3 序列化量化模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can serialize and save the quantized model for the future use using "
"`torch.jit.save` after tracing the model."
msgstr "我们可以在跟踪模型后使用 `torch.jit.save` 序列化并保存量化模型以供未来使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "To load the quantized model, we can use `torch.jit.load`"
msgstr "加载量化模型时，我们可以使用 `torch.jit.load`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we demonstrated how to convert a well-known state-of-the-"
"art NLP model like BERT into dynamic quantized model. Dynamic quantization "
"can reduce the size of the model while only having a limited implication on "
"accuracy."
msgstr "在本教程中，我们展示了如何将一个知名的最先进NLP模型（如BERT）转换为动态量化模型。动态量化可以减少模型大小，同时对准确性的影响有限。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Thanks for reading! As always, we welcome any feedback, so please create an "
"issue `here <https://github.com/pytorch/pytorch/issues>`_ if you have any."
msgstr ""
"感谢阅读！如有任何反馈，欢迎在 `这里 <https://github.com/pytorch/pytorch/issues>`_ 提交问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "References"
msgstr "参考文献"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, `BERT: Pre-training of Deep"
" Bidirectional Transformers for Language Understanding (2018) "
"<https://arxiv.org/pdf/1810.04805.pdf>`_."
msgstr ""
"[1] J.Devlin, M. Chang, K. Lee和K. Toutanova，`BERT: Pre-training of Deep "
"Bidirectional Transformers for Language Understanding (2018) "
"<https://arxiv.org/pdf/1810.04805.pdf>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"[2] `HuggingFace Transformers "
"<https://github.com/huggingface/transformers>`_."
msgstr ""
"[2] `HuggingFace Transformers "
"<https://github.com/huggingface/transformers>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"[3] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat (2019). `Q8BERT: "
"Quantized 8bit BERT <https://arxiv.org/pdf/1910.06188.pdf>`_."
msgstr ""
"[3] O. Zafrir, G. Boudoukh, P. Izsak和M. Wasserblat (2019)。`Q8BERT: Quantized"
" 8bit BERT <https://arxiv.org/pdf/1910.06188.pdf>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_ensembling.py>` to download"
" the full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_intermediate_ensembling.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model ensembling"
msgstr "模型集成"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial illustrates how to vectorize model ensembling using "
"``torch.vmap``."
msgstr "本教程说明如何使用 ``torch.vmap`` 对模型集成进行向量化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "What is model ensembling?"
msgstr "什么是模型集成？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Model ensembling combines the predictions from multiple models together. "
"Traditionally this is done by running each model on some inputs separately "
"and then combining the predictions. However, if you're running models with "
"the same architecture, then it may be possible to combine them together "
"using ``torch.vmap``. ``vmap`` is a function transform that maps functions "
"across dimensions of the input tensors. One of its use cases is eliminating "
"for-loops and speeding them up through vectorization."
msgstr ""
"模型集成将多个模型的预测结果组合在一起。传统上，这是通过分别运行每个模型处理某些输入，然后合并预测结果来实现的。然而，如果运行的模型具有相同的架构，则可能利用"
" ``torch.vmap`` 将它们组合。``vmap`` "
"是一个函数转换工具，可将函数映射到输入张量的维度之一。其用途之一是消除循环并通过向量化加速处理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's demonstrate how to do this using an ensemble of simple MLPs."
msgstr "让我们演示如何使用简单MLP进行集成。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This tutorial requires PyTorch 2.0.0 or later."
msgstr "本教程需要PyTorch 2.0.0或更高版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s generate a batch of dummy data and pretend that we’re working with an "
"MNIST dataset. Thus, the dummy images are 28 by 28, and we have a minibatch "
"of size 64. Furthermore, lets say we want to combine the predictions from 10"
" different models."
msgstr ""
"我们生成一批虚拟数据，并假设正在处理MNIST数据集。因此，虚拟图像尺寸为28乘28，我们有一个大小为64的小批量。此外，假设我们希望组合来自10个不同模型的预测结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have a couple of options for generating predictions. Maybe we want to "
"give each model a different randomized minibatch of data. Alternatively, "
"maybe we want to run the same minibatch of data through each model (e.g. if "
"we were testing the effect of different model initializations)."
msgstr ""
"我们有几种生成预测的选择。可能我们希望为每个模型提供一个不同的随机化数据小批量。或者，也可能希望为每个模型运行相同的数据小批量（例如，如果我们想测试不同模型初始化的效果）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Option 1: different minibatch for each model"
msgstr "选项1：为每个模型使用不同的小批量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Option 2: Same minibatch"
msgstr "选项2：使用相同的小批量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using ``vmap`` to vectorize the ensemble"
msgstr "使用 ``vmap`` 对集成进行向量化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's use ``vmap`` to speed up the for-loop. We must first prepare the "
"models for use with ``vmap``."
msgstr "让我们使用 ``vmap`` 加速循环。首先需要为 ``vmap`` 准备模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, let’s combine the states of the model together by stacking each "
"parameter. For example, ``model[i].fc1.weight`` has shape ``[784, 128]``; we"
" are going to stack the ``.fc1.weight`` of each of the 10 models to produce "
"a big weight of shape ``[10, 784, 128]``."
msgstr ""
"首先，通过堆叠每个参数来组合模型状态。例如，``model[i].fc1.weight`` 的形状为 ``[784, "
"128]``；我们将每个模型的`.fc1.weight`堆叠起来以生成一个形状为 ``[10, 784, 128]``的大权重。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch offers the ``torch.func.stack_module_state`` convenience function to"
" do this."
msgstr "PyTorch提供了便捷函数 ``torch.func.stack_module_state`` 来完成这一操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we need to define a function to ``vmap`` over. The function should, "
"given parameters and buffers and inputs, run the model using those "
"parameters, buffers, and inputs. We'll use ``torch.func.functional_call`` to"
" help out:"
msgstr ""
"接下来，我们需要定义一个用于 ``vmap`` 的函数。该函数应该接收参数、缓冲区和输入，根据这些参数、缓冲区和输入运行模型。我们将使用 "
"``torch.func.functional_call`` 来帮助实现："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Option 1: get predictions using a different minibatch for each model."
msgstr "选项1：通过为每个模型使用不同的小批量获得预测。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By default, ``vmap`` maps a function across the first dimension of all "
"inputs to the passed-in function. After using ``stack_module_state``, each "
"of the ``params`` and buffers have an additional dimension of size "
"'num_models' at the front, and minibatches has a dimension of size "
"'num_models'."
msgstr ""
"默认情况下，``vmap`` 会将输入函数映射到传递进来函数输入的第一个维度。在使用 ``stack_module_state`` "
"后，每个参数和缓冲区的前方都有一个附加维度大小为 'num_models'，同时小批量也有一个 'num_models' 的维度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Option 2: get predictions using the same minibatch of data."
msgstr "选项2：通过为所有10个模型使用相同的小批量数据获得预测。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``vmap`` has an ``in_dims`` argument that specifies which dimensions to map "
"over. By using ``None``, we tell ``vmap`` we want the same minibatch to "
"apply for all of the 10 models."
msgstr ""
"``vmap`` 有一个 ``in_dims`` 参数，用于指定要映射的维度。通过使用 ``None``，我们告诉 ``vmap`` "
"我们希望同一个小批量应用于所有10个模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A quick note: there are limitations around what types of functions can be "
"transformed by ``vmap``. The best functions to transform are ones that are "
"pure functions: a function where the outputs are only determined by the "
"inputs that have no side effects (e.g. mutation). ``vmap`` is unable to "
"handle mutation of arbitrary Python data structures, but it is able to "
"handle many in-place PyTorch operations."
msgstr ""
"简短说明：使用 ``vmap`` "
"转换函数时，对某些类型函数有一定限制。最适合转换的函数是纯函数：一种输出只依赖于输入且没有副作用（例如，改变）的函数。``vmap`` "
"无法处理任意Python数据结构的变异操作，但能够处理许多PyTorch的原位操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Performance"
msgstr "性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Curious about performance numbers? Here's how the numbers look."
msgstr "想知道性能数据吗？下面是性能表现的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "There's a large speedup using ``vmap``!"
msgstr "使用 ``vmap`` 有显著的速度提升！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In general, vectorization with ``vmap`` should be faster than running a "
"function in a for-loop and competitive with manual batching. There are some "
"exceptions though, like if we haven’t implemented the ``vmap`` rule for a "
"particular operation or if the underlying kernels weren’t optimized for "
"older hardware (GPUs). If you see any of these cases, please let us know by "
"opening an issue on GitHub."
msgstr ""
"通常使用 ``vmap`` 的向量化应比在循环中运行函数更快，并且与手动批处理具有竞争力。但仍有一些例外情况，比如某种操作可能没有实现 ``vmap``"
" 规则，或者底层内核没有针对较老硬件（如GPU）进行优化。如果发现这些情况，请通过在GitHub上打开问题让我们知道。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Total running time of the script:** ( 0 minutes  1.504 seconds)"
msgstr "**脚本总运行时间：** ( 0分钟 1.504秒)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ":download:`Download Python source code: ensembling.py <ensembling.py>`"
msgstr ":download:`下载Python源码：ensembling.py <ensembling.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: ensembling.ipynb <ensembling.ipynb>`"
msgstr ":download:`下载Jupyter notebook：ensembling.ipynb <ensembling.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_flask_rest_api_tutorial.py>` to download the"
" full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_flask_rest_api_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Deploying PyTorch in Python via a REST API with Flask"
msgstr "通过Flask以REST API的形式部署PyTorch模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Avinash Sajjanshetty <https://avi.im>`_"
msgstr "**作者**：`Avinash Sajjanshetty <https://avi.im>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will deploy a PyTorch model using Flask and expose a "
"REST API for model inference. In particular, we will deploy a pretrained "
"DenseNet 121 model which detects the image."
msgstr ""
"在本教程中，我们将使用Flask部署一个PyTorch模型，并为模型推理公开一个REST API。具体来说，我们将部署一个预训练的DenseNet "
"121模型，该模型可用于检测图像。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"All the code used here is released under MIT license and is available on "
"`Github <https://github.com/avinassh/pytorch-flask-api>`_."
msgstr ""
"此处使用的所有代码均根据MIT许可证发布，并可在 `Github <https://github.com/avinassh/pytorch-flask-"
"api>`_ 上找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This represents the first in a series of tutorials on deploying PyTorch "
"models in production. Using Flask in this way is by far the easiest way to "
"start serving your PyTorch models, but it will not work for a use case with "
"high performance requirements. For that:"
msgstr ""
"这是有关在生产环境中部署PyTorch模型系列的第一个教程。以这种方式使用Flask是开始服务您的PyTorch模型的最简单方式，但对于有高性能需求的用例，此方法可能不适合。针对这类需求："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you're already familiar with TorchScript, you can jump straight into our "
"`Loading a TorchScript Model in C++ "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_ tutorial."
msgstr ""
"如果您已经熟悉 TorchScript，可以直接进入我们的 C++ 教程《加载 TorchScript "
"模型》(<https://pytorch.org/tutorials/advanced/cpp_export.html>)。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you first need a refresher on TorchScript, check out our `Intro a "
"TorchScript "
"<https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>`_"
" tutorial."
msgstr ""
"如果您需要先复习一下 TorchScript，可以查看我们的入门教程《TorchScript "
"简介》(<https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>)。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "API Definition"
msgstr "API 定义"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will first define our API endpoints, the request and response types. Our "
"API endpoint will be at ``/predict`` which takes HTTP POST requests with a "
"``file`` parameter which contains the image. The response will be of JSON "
"response containing the prediction:"
msgstr ""
"我们将首先定义 API 端点以及请求和响应类型。我们的 API 端点将位于 ``/predict``，它接收带有包含图像的 ``file`` 参数的 "
"HTTP POST 请求。响应将是包含预测结果的 JSON 响应："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Dependencies"
msgstr "依赖"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Install the required dependencies by running the following command:"
msgstr "通过运行以下命令安装所需的依赖项："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Simple Web Server"
msgstr "简单的 Web 服务器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Following is a simple web server, taken from Flask's documentation"
msgstr "以下是从 Flask 官方文档中截取的一个简单 Web 服务器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will also change the response type, so that it returns a JSON response "
"containing ImageNet class id and name. The updated ``app.py`` file will be "
"now:"
msgstr "我们还将更改响应类型，使其返回包含 ImageNet 类 ID 和名称的 JSON 响应。更新后的 ``app.py`` 文件如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inference"
msgstr "推断"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the next sections we will focus on writing the inference code. This will "
"involve two parts, one where we prepare the image so that it can be fed to "
"DenseNet and next, we will write the code to get the actual prediction from "
"the model."
msgstr "在接下来的部分中，我们将重点编写推断代码。这将包括两个步骤：准备图像以便可以馈送到 DenseNet，然后编写代码从模型中获取实际预测。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preparing the image"
msgstr "准备图像"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"DenseNet model requires the image to be of 3 channel RGB image of size 224 x"
" 224. We will also normalize the image tensor with the required mean and "
"standard deviation values. You can read more about it `here "
"<https://pytorch.org/vision/stable/models.html>`_."
msgstr ""
"DenseNet 模型要求图像是大小为 224 x 224 的 3 通道 RGB 图像。我们还将使用所需的均值和标准差值对图像张量进行归一化。可以在 "
"`这里 <https://pytorch.org/vision/stable/models.html>`_ 阅读更多相关信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will use ``transforms`` from ``torchvision`` library and build a "
"transform pipeline, which transforms our images as required. You can read "
"more about transforms `here "
"<https://pytorch.org/vision/stable/transforms.html>`_."
msgstr ""
"我们将使用来自 ``torchvision`` 库的 ``transforms``，并构建一个转换管道，以按需转换我们的图像。您可以在 `这里 "
"<https://pytorch.org/vision/stable/transforms.html>`_ 阅读更多关于转换的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The above method takes image data in bytes, applies the series of transforms"
" and returns a tensor. To test the above method, read an image file in bytes"
" mode (first replacing `../_static/img/sample_file.jpeg` with the actual "
"path to the file on your computer) and see if you get a tensor back:"
msgstr ""
"上述方法接收字节形式的图像数据，应用一系列转换并返回一个张量。为了测试上述方法，以字节模式读取图像文件（首先将 "
"`../_static/img/sample_file.jpeg` 替换为您计算机上的实际文件路径），并查看是否可以成功返回张量："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Prediction"
msgstr "预测"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now will use a pretrained DenseNet 121 model to predict the image class. We "
"will use one from ``torchvision`` library, load the model and get an "
"inference. While we'll be using a pretrained model in this example, you can "
"use this same approach for your own models. See more about loading your "
"models in this :doc:`tutorial </beginner/saving_loading_models>`."
msgstr ""
"现在我们将使用一个预训练的 DenseNet 121 模型来预测图像类别。我们将使用来自 ``torchvision`` "
"库的模型，加载模型并进行推断。尽管本示例中我们使用的是预训练的模型，但您可以对自己的模型使用相同的方式。有关加载您自己的模型的更多信息，请参阅此 "
":doc:`教程 </beginner/saving_loading_models>`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The tensor ``y_hat`` will contain the index of the predicted class id. "
"However, we need a human readable class name. For that we need a class id to"
" name mapping. Download `this file <https://s3.amazonaws.com/deep-learning-"
"models/image-models/imagenet_class_index.json>`_ as "
"``imagenet_class_index.json`` and remember where you saved it (or, if you "
"are following the exact steps in this tutorial, save it in "
"`tutorials/_static`). This file contains the mapping of ImageNet class id to"
" ImageNet class name. We will load this JSON file and get the class name of "
"the predicted index."
msgstr ""
"张量 ``y_hat`` 将包含预测类别 ID 的索引。然而，我们需要一个人类可读的类别名称。为此，我们需要从类别 ID 到名称的映射。下载 `此文件 "
"<https://s3.amazonaws.com/deep-learning-models/image-"
"models/imagenet_class_index.json>`_ 并保存为 "
"``imagenet_class_index.json``，记住保存位置（如果您按照本教程的具体步骤操作，请将其保存到 "
"`tutorials/_static` 文件夹中）。此文件包含 ImageNet 类别 ID 与名称的映射。我们将加载此 JSON "
"文件，并通过预测索引获取类别名称。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before using ``imagenet_class_index`` dictionary, first we will convert "
"tensor value to a string value, since the keys in the "
"``imagenet_class_index`` dictionary are strings. We will test our above "
"method:"
msgstr ""
"在使用 ``imagenet_class_index`` 字典之前，我们首先需要将张量值转换为字符串值，因为 "
"``imagenet_class_index`` 字典中的键是字符串。我们将测试上述方法："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You should get a response like this:"
msgstr "您应该会得到如下响应："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The first item in array is ImageNet class id and second item is the human "
"readable name."
msgstr "数组中的第一个元素是 ImageNet 类别 ID，第二个元素是人类可读的名称。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Integrating the model in our API Server"
msgstr "将模型集成到我们的 API 服务器中"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this final part we will add our model to our Flask API server. Since our "
"API server is supposed to take an image file, we will update our ``predict``"
" method to read files from the requests:"
msgstr ""
"在最后一部分中，我们将把模型添加到我们的 Flask API 服务器中。由于我们的 API 服务器需要接收图像文件，我们将更新 ``predict`` "
"方法以从请求中读取文件："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "library to send a POST request to our app:"
msgstr "使用库发送一个 POST 请求到我们的应用程序："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Printing `resp.json()` will now show the following:"
msgstr "打印 `resp.json()` 现在会显示以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The server we wrote is quite trivial and may not do everything you need for "
"your production application. So, here are some things you can do to make it "
"better:"
msgstr "我们编写的服务器非常简单，可能无法满足您的生产应用的所有需求。因此，以下是一些可以改进的地方："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The endpoint ``/predict`` assumes that always there will be a image file in "
"the request. This may not hold true for all requests. Our user may send "
"image with a different parameter or send no images at all."
msgstr ""
"端点 ``/predict`` 假设请求中总是会包含一个图像文件。这种情况可能并不适用于所有请求。用户可能会发送包含不同参数的图像，或根本不发送图像。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The user may send non-image type files too. Since we are not handling "
"errors, this will break our server. Adding an explicit error handing path "
"that will throw an exception would allow us to better handle the bad inputs"
msgstr ""
"用户也可能会发送非图像类型的文件。由于我们没有处理错误，这可能会导致服务器崩溃。添加一个显式的错误处理路径，可以在出现错误输入时抛出异常，更好地进行处理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Even though the model can recognize a large number of classes of images, it "
"may not be able to recognize all images. Enhance the implementation to "
"handle cases when the model does not recognize anything in the image."
msgstr "即使模型能够识别大量类别的图像，它也可能无法识别所有图像。可以增强实现，以处理模型无法从图像中识别任何内容的情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We run the Flask server in the development mode, which is not suitable for "
"deploying in production. You can check out `this tutorial "
"<https://flask.palletsprojects.com/en/1.1.x/tutorial/deploy/>`_ for "
"deploying a Flask server in production."
msgstr ""
"我们以开发模式运行 Flask 服务器，这并不适合生产环境中的部署。您可以查看 `此教程 "
"<https://flask.palletsprojects.com/en/1.1.x/tutorial/deploy/>`_，了解如何在生产环境中部署"
" Flask 服务器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can also add a UI by creating a page with a form which takes the image "
"and displays the prediction."
msgstr "您还可以通过创建一个带有表单的页面为服务器添加用户界面，该表单接收图像并显示预测结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we only showed how to build a service that could return "
"predictions for a single image at a time. We could modify our service to be "
"able to return predictions for multiple images at once. In addition, the "
"`service-streamer <https://github.com/ShannonAI/service-streamer>`_ library "
"automatically queues requests to your service and samples them into mini-"
"batches that can be fed into your model. You can check out `this tutorial "
"<https://github.com/ShannonAI/service-streamer/wiki/Vision-Recognition-"
"Service-with-Flask-and-service-streamer>`_."
msgstr ""
"在本教程中，我们仅展示如何构建一个服务，以一次返回单张图像的预测结果。您可以修改此服务，以一次返回多张图像的预测结果。此外，`service-"
"streamer <https://github.com/ShannonAI/service-streamer>`_ "
"库会自动将请求排队并采样为小批量进行处理，以供模型使用。您可以查看 `此教程 "
"<https://github.com/ShannonAI/service-streamer/wiki/Vision-Recognition-"
"Service-with-Flask-and-service-streamer>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, we encourage you to check out our other tutorials on deploying "
"PyTorch models linked-to at the top of the page."
msgstr "最后，我们鼓励您查看页面顶部提供的其他 PyTorch 模型部署教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: flask_rest_api_tutorial.py "
"<flask_rest_api_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: flask_rest_api_tutorial.py "
"<flask_rest_api_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: flask_rest_api_tutorial.ipynb "
"<flask_rest_api_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本: flask_rest_api_tutorial.ipynb "
"<flask_rest_api_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Forced Alignment with Wav2Vec2"
msgstr "Wav2Vec2 强制对齐"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html"
msgstr ""
"本教程已移至 "
"https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "It will redirect in 3 seconds."
msgstr "3 秒后将自动重定向。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_forward_ad_usage.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_forward_ad_usage.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Forward-mode Automatic Differentiation (Beta)"
msgstr "自动微分的正向模式 (Beta)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to use forward-mode AD to compute directional"
" derivatives (or equivalently, Jacobian-vector products)."
msgstr "本教程演示如何使用正向模式 AD 计算方向导数（或同义的雅可比矢量积）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The tutorial below uses some APIs only available in versions >= 1.11 (or "
"nightly builds)."
msgstr "下面的教程使用的部分 API 仅适用于版本 >= 1.11（或 nightly 版本）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Also note that forward-mode AD is currently in beta. The API is subject to "
"change and operator coverage is still incomplete."
msgstr "请注意，正向模式 AD 目前仍处于测试阶段。API 可能会发生变化，并且操作覆盖范围尚不完整。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Basic Usage"
msgstr "基本用法"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Unlike reverse-mode AD, forward-mode AD computes gradients eagerly alongside"
" the forward pass. We can use forward-mode AD to compute a directional "
"derivative by performing the forward pass as before, except we first "
"associate our input with another tensor representing the direction of the "
"directional derivative (or equivalently, the ``v`` in a Jacobian-vector "
"product). When an input, which we call \"primal\", is associated with a "
"\"direction\" tensor, which we call \"tangent\", the resultant new tensor "
"object is called a \"dual tensor\" for its connection to dual numbers[0]."
msgstr ""
"与反向模式 AD 不同，正向模式 AD 在前向传播过程中即刻计算梯度。我们可以通过执行前向传播并将输入与另一个表示方向导数方向（或雅可比矢量积中的 "
"``v``）的张量相关联，使用正向模式 AD "
"计算方向导数。当输入（称为“主”或“primal”）与表示方向的张量（称为“切线”或“tangent”）相关联时，生成的新张量对象称为“对偶张量”，它与对偶数有关[0]。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the forward pass is performed, if any input tensors are dual tensors, "
"extra computation is performed to propagate this \"sensitivity\" of the "
"function."
msgstr "在执行前向传播时，如果任何输入张量是对偶张量，则会进行额外计算以传播函数的这种“敏感性”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Usage with Modules"
msgstr "与模块一起使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To use ``nn.Module`` with forward AD, replace the parameters of your model "
"with dual tensors before performing the forward pass. At the time of "
"writing, it is not possible to create dual tensor `nn.Parameter`s. As a "
"workaround, one must register the dual tensor as a non-parameter attribute "
"of the module."
msgstr ""
"要将正向 AD 与 ``nn.Module`` 一起使用，需要在前向传播之前将模型的参数替换为对偶张量。目前无法创建对偶张量的 "
"`nn.Parameter`。作为一种解决方法，可以将对偶张量注册为模块的非参数属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using the functional Module API (beta)"
msgstr "使用函数式模块 API（测试阶段）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Another way to use ``nn.Module`` with forward AD is to utilize the "
"functional Module API (also known as the stateless Module API)."
msgstr "使用正向 AD 的另一个方法是利用函数式模块 API（也称为无状态模块 API）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Custom autograd Function"
msgstr "自定义自动微分函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Custom Functions also support forward-mode AD. To create custom Function "
"supporting forward-mode AD, register the ``jvp()`` static method. It is "
"possible, but not mandatory for custom Functions to support both forward and"
" backward AD. See the `documentation "
"<https://pytorch.org/docs/master/notes/extending.html#forward-mode-ad>`_ for"
" more information."
msgstr ""
"自定义函数也支持正向模式 AD。要创建支持正向模式 AD 的自定义函数，需要注册 ``jvp()`` 静态方法。自定义函数可以同时支持正向和反向 "
"AD，但这不是强制要求。有关更多信息，请参阅 `文档 "
"<https://pytorch.org/docs/master/notes/extending.html#forward-mode-ad>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Functional API (beta)"
msgstr "函数式 API（测试阶段）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We also offer a higher-level functional API in functorch for computing "
"Jacobian-vector products that you may find simpler to use depending on your "
"use case."
msgstr "我们还在 functorch 中提供了更高级别的函数式 API，您可能会发现根据使用场景，这种方法更简单。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The benefit of the functional API is that there isn't a need to understand "
"or use the lower-level dual tensor API and that you can compose it with "
"other `functorch transforms (like vmap) "
"<https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html>`_; "
"the downside is that it offers you less control."
msgstr ""
"函数式 API 的优点是无需了解或使用底层的对偶张量 API，并且可以将其与其他 `functorch 转换（例如 vmap） "
"<https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html>`_ "
"组合使用；其缺点是提供的控制较少。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that the remainder of this tutorial will require functorch "
"(https://github.com/pytorch/functorch) to run. Please find installation "
"instructions at the specified link."
msgstr ""
"请注意，本文余下部分需要 functorch "
"才能运行（https://github.com/pytorch/functorch）。安装说明请参阅指定链接。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using the functional API with Modules"
msgstr "结合模块使用函数式 API"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To use ``nn.Module`` with ``functorch.jvp`` to compute Jacobian-vector "
"products with respect to the model parameters, we need to reformulate the "
"``nn.Module`` as a function that accepts both the model parameters and "
"inputs to the module."
msgstr ""
"要通过 ``functorch.jvp`` 使用 ``nn.Module`` 计算关于模型参数的雅可比矢量积，我们需要将 ``nn.Module`` "
"表述为一个接受模型参数和模块输入的函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "[0] https://en.wikipedia.org/wiki/Dual_number"
msgstr "[0] https://en.wikipedia.org/wiki/Dual_number"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: forward_ad_usage.py "
"<forward_ad_usage.py>`"
msgstr ":download:`下载 Python 源代码: forward_ad_usage.py <forward_ad_usage.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: forward_ad_usage.ipynb "
"<forward_ad_usage.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本: forward_ad_usage.ipynb <forward_ad_usage.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_fx_conv_bn_fuser.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_fx_conv_bn_fuser.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "(beta) Building a Convolution/Batch Norm fuser in FX"
msgstr "（测试阶段）在 FX 中构建卷积/批量归一化融合器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Horace He <https://github.com/chillee>`_"
msgstr "**作者**: `Horace He <https://github.com/chillee>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we are going to use FX, a toolkit for composable function "
"transformations of PyTorch, to do the following:"
msgstr "在本教程中，我们将使用 FX（一种用于 PyTorch 的可组合函数变换工具包）完成以下任务："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Find patterns of conv/batch norm in the data dependencies."
msgstr "在数据依赖关系中查找卷积/批量归一化模式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the patterns found in 1), fold the batch norm statistics into the "
"convolution weights."
msgstr "针对步骤 1) 中找到的模式，将批量归一化统计数据融合到卷积权重中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that this optimization only works for models in inference mode (i.e. "
"`mode.eval()`)"
msgstr "请注意，此优化仅适用于推断模式下的模型（即 `model.eval()`）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will be building the fuser that exists here: "
"https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/fx/experimental/fuser.py"
msgstr ""
"我们将构建位于以下地址的融合器：https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/fx/experimental/fuser.py"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, let's get some imports out of the way (we will be using all of these "
"later in the code)."
msgstr "首先，让我们处理一些导入操作（稍后代码中将会用到所有这些内容）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For this tutorial, we are going to create a model consisting of convolutions"
" and batch norms. Note that this model has some tricky components - some of "
"the conv/batch norm patterns are hidden within Sequentials and one of the "
"``BatchNorms`` is wrapped in another Module."
msgstr ""
"在本教程中，我们将创建一个由卷积和批量归一化组成的模型。请注意，该模型包含一些复杂组件—它的一些卷积/批量归一化模式隐藏在 Sequentials "
"中，且其中一个 ``BatchNorms`` 被封装在另一个模块中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fusing Convolution with Batch Norm"
msgstr "将卷积与批量归一化结合"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One of the primary challenges with trying to automatically fuse convolution "
"and batch norm in PyTorch is that PyTorch does not provide an easy way of "
"accessing the computational graph. FX resolves this problem by symbolically "
"tracing the actual operations called, so that we can track the computations "
"through the `forward` call, nested within Sequential modules, or wrapped in "
"an user-defined module."
msgstr ""
"试图在PyTorch中自动融合卷积和批归一化的主要挑战之一是，PyTorch没有提供一种简单的方法来访问计算图。FX通过符号跟踪实际调用的操作来解决这个问题，因此我们可以通过`forward`调用、嵌套在Sequential模块中或封装在自定义模块中的计算进行跟踪。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This gives us a graph representation of our model. Note that both the "
"modules hidden within the sequential as well as the wrapped Module have been"
" inlined into the graph. This is the default level of abstraction, but it "
"can be configured by the pass writer. More information can be found at the "
"FX overview https://pytorch.org/docs/master/fx.html#module-torch.fx"
msgstr ""
"这为我们提供了模型的图结构表示。注意，无论是Sequential中的隐藏模块还是封装的Module都已内联到图中。这是默认的抽象级别，但可以由传递编写者进行配置。有关更多信息，请访问FX概述：https://pytorch.org/docs/master/fx.html#module-"
"torch.fx"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Unlike some other fusions, fusion of convolution with batch norm does not "
"require any new operators. Instead, as batch norm during inference consists "
"of a pointwise add and multiply, these operations can be \"baked\" into the "
"preceding convolution's weights. This allows us to remove the batch norm "
"entirely from our model! Read https://nenadmarkus.com/p/fusing-batchnorm-"
"and-conv/ for further details. The code here is copied from "
"https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/nn/utils/fusion.py"
" clarity purposes."
msgstr ""
"与某些其他融合不同，卷积与批归一化的融合不需要任何新的操作符。相反，由于推断中的批归一化由逐点求和和乘法组成，这些操作可以“烘焙”到前面的卷积的权重中。这使我们能够从模型中完全移除批归一化！请阅读https://nenadmarkus.com/p/fusing-"
"batchnorm-and-"
"conv/了解更多细节。这里的代码借鉴于https://github.com/pytorch/pytorch/blob/orig/release/1.8/torch/nn/utils/fusion.py，以提高清晰度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FX Fusion Pass"
msgstr "FX融合步骤"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have our computational graph as well as a method for fusing "
"convolution and batch norm, all that remains is to iterate over the FX graph"
" and apply the desired fusions."
msgstr "现在我们有了计算图以及融合卷积和批归一化的方法，仅剩下逐一遍历FX图并应用所需融合。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We make some simplifications here for demonstration purposes, such as only "
"matching 2D convolutions. View "
"https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/fuser.py"
" for a more usable pass."
msgstr ""
"这里我们为演示目的做了一些简化，例如仅匹配2D卷积。查看https://github.com/pytorch/pytorch/blob/master/torch/fx/experimental/fuser.py以获得更实用的步骤。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Testing out our Fusion Pass"
msgstr "测试我们的融合步骤"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can now run this fusion pass on our initial toy model and verify that our"
" results are identical. In addition, we can print out the code for our fused"
" model and verify that there are no more batch norms."
msgstr "现在我们可以在初始模型上运行此融合步骤，并验证我们的结果是相同的。此外，我们还可以打印出融合模型的代码并验证是否已无批归一化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Benchmarking our Fusion on ResNet18"
msgstr "ResNet18上的融合基准测试"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can test our fusion pass on a larger model like ResNet18 and see how much"
" this pass improves inference performance."
msgstr "我们可以在像ResNet18这样的较大模型上测试我们的融合步骤，查看该步骤对推断性能提升了多少。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As we previously saw, the output of our FX transformation is "
"(\"torchscriptable\") PyTorch code, we can easily ``jit.script`` the output "
"to try and increase our performance even more. In this way, our FX model "
"transformation composes with TorchScript with no issues."
msgstr ""
"正如我们之前看到的，我们的FX转换的输出是（“可脚本化的”）PyTorch代码，我们可以轻松地使用``jit.script``对其进行处理，进一步提高性能。通过这种方式，我们的FX模型转换与TorchScript无缝结合。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: fx_conv_bn_fuser.py "
"<fx_conv_bn_fuser.py>`"
msgstr ":download:`下载Python源代码：fx_conv_bn_fuser.py <fx_conv_bn_fuser.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: fx_conv_bn_fuser.ipynb "
"<fx_conv_bn_fuser.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：fx_conv_bn_fuser.ipynb "
"<fx_conv_bn_fuser.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_fx_profiling_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击:ref:`这里 "
"<sphx_glr_download_intermediate_fx_profiling_tutorial.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "(beta) Building a Simple CPU Performance Profiler with FX"
msgstr "(Beta) 使用FX构建简单的CPU性能分析工具"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `James Reed <https://github.com/jamesr66a>`_"
msgstr "**作者**: `James Reed <https://github.com/jamesr66a>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In this tutorial, we are going to use FX to do the following:"
msgstr "在本教程中，我们将使用FX完成以下任务："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Capture PyTorch Python code in a way that we can inspect and gather "
"statistics about the structure and execution of the code"
msgstr "以一种我们可以检查和收集代码结构和执行统计数据的方式捕获PyTorch Python代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Build out a small class that will serve as a simple performance "
"\"profiler\", collecting runtime statistics about each part of the model "
"from actual runs."
msgstr "构建一个小型类，用作简单性能“分析器”，收集模型每一部分在实际运行中的运行时统计数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For this tutorial, we are going to use the torchvision ResNet18 model for "
"demonstration purposes."
msgstr "在本教程中，我们将使用torchvision的ResNet18模型作为演示案例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have our model, we want to inspect deeper into its performance. "
"That is, for the following invocation, which parts of the model are taking "
"the longest?"
msgstr "现在我们得到了模型，我们希望深入研究其性能。也就是说，在下面的调用中，模型的哪些部分花费的时间最长？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A common way of answering that question is to go through the program source,"
" add code that collects timestamps at various points in the program, and "
"compare the difference between those timestamps to see how long the regions "
"between the timestamps take."
msgstr "回答这个问题的一种常见方法是遍历程序源代码，在程序中不同点添加收集时间戳的代码，并比较这些时间戳之间的差异以查看这些区域消耗的时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That technique is certainly applicable to PyTorch code, however it would be "
"nicer if we didn't have to copy over model code and edit it, especially code"
" we haven't written (like this torchvision model). Instead, we are going to "
"use FX to automate this \"instrumentation\" process without needing to "
"modify any source."
msgstr ""
"这种技术当然适用于PyTorch代码，但如果我们不必复制模型代码并进行编辑，尤其是对于我们没有编写的代码（比如这个torchvision模型），这将会更好。相反，我们将使用FX自动完成此“插装”过程，而无需修改任何源码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``tabulate`` is an external library that is not a dependency of PyTorch. We "
"will be using it to more easily visualize performance data. Please make sure"
" you've installed it from your favorite Python package source."
msgstr ""
"``tabulate``是一个外部库，不是PyTorch的必需依赖项。我们将使用它更轻松地可视化性能数据。请确保您已从最喜欢的Python软件包源安装它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Capturing the Model with Symbolic Tracing"
msgstr "使用符号跟踪捕获模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we are going to use FX's symbolic tracing mechanism to capture the "
"definition of our model in a data structure we can manipulate and examine."
msgstr "接下来，我们将使用FX的符号跟踪机制，以便在可以操控和检查的数据结构中捕获模型的定义。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This gives us a Graph representation of the ResNet18 model. A Graph consists"
" of a series of Nodes connected to each other. Each Node represents a call-"
"site in the Python code (whether to a function, a module, or a method) and "
"the edges (represented as ``args`` and ``kwargs`` on each node) represent "
"the values passed between these call-sites. More information about the Graph"
" representation and the rest of FX's APIs ca be found at the FX "
"documentation https://pytorch.org/docs/master/fx.html."
msgstr ""
"这为我们提供了ResNet18模型的图结构表示。图由一系列连接的节点组成。每个节点表示Python代码中的调用位置（无论是函数、模块还是方法），边（在每个节点上的``args``和``kwargs``表示）则表示这些调用位置之间传递的值。关于图结构表示及FX的其他API的更多信息，可以在FX文档中找到：https://pytorch.org/docs/master/fx.html。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Creating a Profiling Interpreter"
msgstr "创建性能分析解释器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we are going to create a class that inherits from "
"``torch.fx.Interpreter``. Though the ``GraphModule`` that ``symbolic_trace``"
" produces compiles Python code that is run when you call a ``GraphModule``, "
"an alternative way to run a ``GraphModule`` is by executing each ``Node`` in"
" the ``Graph`` one by one. That is the functionality that ``Interpreter`` "
"provides: It interprets the graph node- by-node."
msgstr ""
"接下来，我们将创建一个继承自``torch.fx.Interpreter``的类。虽然``symbolic_trace``生产的`GraphModule`编译了调用`GraphModule`时运行的Python代码，另一种运行`GraphModule`的方法是逐一执行图中的每个``Node``。``Interpreter``提供了这种功能：它逐节点解释图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By inheriting from ``Interpreter``, we can override various functionality "
"and install the profiling behavior we want. The goal is to have an object to"
" which we can pass a model, invoke the model 1 or more times, then get "
"statistics about how long the model and each part of the model took during "
"those runs."
msgstr ""
"通过继承``Interpreter``，我们可以覆盖各种功能并安装性能分析所需的行为。目标是提供一个对象，将模型传递给该对象后，可以调用模型1次或多次，然后获取关于模型及其每部分在这些运行中耗时的统计数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's define our ``ProfilingInterpreter`` class:"
msgstr "定义我们的``ProfilingInterpreter``类："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We use Python's ``time.time`` function to pull wall clock timestamps and "
"compare them. This is not the most accurate way to measure performance, and "
"will only give us a first- order approximation. We use this simple technique"
" only for the purpose of demonstration in this tutorial."
msgstr ""
"我们使用Python的``time.time``函数获取墙时钟时间戳并进行比较。这不是测量性能最准确的方法，仅会提供一个一级近似值。我们仅为本教程的演示目的使用此简单技术。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Investigating the Performance of ResNet18"
msgstr "研究ResNet18的性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can now use ``ProfilingInterpreter`` to inspect the performance "
"characteristics of our ResNet18 model;"
msgstr "我们现在可以使用``ProfilingInterpreter``检查ResNet18模型的性能特性；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "There are two things we should call out here:"
msgstr "这里有两件事值得注意："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``MaxPool2d`` takes up the most time. This is a known issue: "
"https://github.com/pytorch/pytorch/issues/51393"
msgstr ""
"``MaxPool2d``占用了最多的时间。这是一个已知问题：https://github.com/pytorch/pytorch/issues/51393"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"BatchNorm2d also takes up significant time. We can continue this line of "
"thinking and optimize this in the Conv-BN Fusion with FX `tutorial "
"<https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html>`_."
msgstr ""
"BatchNorm2d也占用了大量时间。我们可以继续这一思路，并通过FX教程优化卷积-"
"BN融合：https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As we can see, using FX we can easily capture PyTorch programs (even ones we"
" don't have the source code for!) in a machine-interpretable format and use "
"that for analysis, such as the performance analysis we've done here. FX "
"opens up an exciting world of possibilities for working with PyTorch "
"programs."
msgstr ""
"正如我们所见，使用FX我们可以轻松以机器可解读的格式捕获PyTorch程序（甚至是没有源码的程序！），并用于分析，例如我们这里进行的性能分析。FX为处理PyTorch程序开启了一个令人兴奋的可能性世界。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, since FX is still in beta, we would be happy to hear any feedback "
"you have about using it. Please feel free to use the PyTorch Forums "
"(https://discuss.pytorch.org/) and the issue tracker "
"(https://github.com/pytorch/pytorch/issues) to provide any feedback you "
"might have."
msgstr ""
"最后，由于FX仍处于beta阶段，我们很乐意收到有关使用它的任何反馈。请随时使用PyTorch论坛（https://discuss.pytorch.org/）和问题追踪器（https://github.com/pytorch/pytorch/issues）提供任何反馈。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: fx_profiling_tutorial.py "
"<fx_profiling_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：fx_profiling_tutorial.py <fx_profiling_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: fx_profiling_tutorial.ipynb "
"<fx_profiling_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：fx_profiling_tutorial.ipynb "
"<fx_profiling_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_inductor_debug_cpu.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`这里 <sphx_glr_download_intermediate_inductor_debug_cpu.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inductor CPU backend debugging and profiling"
msgstr "Inductor CPU后端调试和性能分析"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Authors**: `Xuan Liao <https://github.com/Valentine233>`_, `Haozhe Zhu "
"<https://github.com/zhuhaozhe>`_, `Jiong Gong <https://github.com/jgong5>`_,"
" `Weihan Wang <https://github.com/EikanWang>`_"
msgstr ""
"**作者**: `Xuan Liao <https://github.com/Valentine233>`_, `Haozhe Zhu "
"<https://github.com/zhuhaozhe>`_, `Jiong Gong <https://github.com/jgong5>`_,"
" `Weihan Wang <https://github.com/EikanWang>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch 2.0 introduced the compilation API called ``torch.compile``. This "
"new feature offers a significant speedup over eager mode execution through "
"graph-level optimization powered by the default Inductor backend."
msgstr ""
"PyTorch "
"2.0引入了名为``torch.compile``的编译API。此新功能通过默认的Inductor后端提供图级优化，在急切模式执行方面显著加快了速度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial is intended to provide an in-depth introduction on the "
"debugging and performance profiling on Inductor CPU backend by delving into "
"the intricacies of ``torch.compile``."
msgstr "本教程旨在通过深入探讨``torch.compile``的细节，对Inductor CPU后端进行调试和性能分析进行深入介绍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Meanwhile, you may also find related tutorials about ``torch.compile`` "
"around `basic usage "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_, "
"comprehensive `troubleshooting "
"<https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html>`_ and "
"GPU-specific knowledge like `GPU performance profiling "
"<https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html>`_."
msgstr ""
"同时，您还可以找到有关``torch.compile``的相关教程，包括`基本用法 "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_、全面的`故障排查"
" "
"<https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html>`_以及GPU相关知识，如`GPU性能分析"
" <https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will start debugging with a motivating example that triggers compilation "
"issues and accuracy problems by demonstrating the process of debugging to "
"pinpoint the problems."
msgstr "我们将从一个激发学习兴趣的例子开始调试，该例子会触发编译问题和精确性问题，并展示排查问题的过程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By enabling logging and exploring the underlying generated code, you can "
"learn how to narrow down the failure step by step and finally figure out the"
" route cause."
msgstr "通过启用日志和研究生成的底层代码，您可以学习如何逐步缩小故障范围，直到最终找出根本原因。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Following that, we will proceed to discuss how to profile the compiled code "
"and, through a performance comparison with eager mode, elaborate on the "
"reasons why ``torch.compile`` can provide an additional performance boost "
"compared to its eager counterpart."
msgstr "接下来，我们将探讨如何对编译代码进行分析，并通过与急切模式的性能比较，详细阐述``torch.compile``能提供额外性能提升的原因。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Debugging"
msgstr "调试"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here is a simple example to run the ``torch.compile`` using Inductor and "
"compare its result with eager mode:"
msgstr "这是一个简单运行``torch.compile``使用Inductor并将其结果与急切模式比较的例子："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The correct implementation of ``neg`` in the ``cpp`` codegen is as follows:"
msgstr "在``cpp``代码生成中的``neg``的正确实现如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to demonstrate the debugging, we will modify the function to a "
"wrong one later."
msgstr "为了演示调试，我们稍后将此函数修改为错误版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Get more logging information"
msgstr "获取更多日志信息"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"No debugging information would be provided if you run this simple example by"
" default. In order to get more useful debugging and logging information, we "
"usually add a ``TORCH_COMPILE_DEBUG`` environment variable like below:"
msgstr ""
"如果默认情况下运行此简单例子，将不会提供任何调试信息。为了获得更多有用的调试和日志信息，我们通常会添加一个``TORCH_COMPILE_DEBUG``环境变量，如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This would print more debug information in the output logs and also dump the"
" intermediate IRs generated during the codegen process. You can find the "
"dumped file paths in the log like below:"
msgstr "这将在输出日志中打印更多调试信息，并且还会转储代码生成过程中生成的中间IR。您可以在日志中找到转储文件路径，如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this directory, the following files are saved for debugging purposes:"
msgstr "在这个目录中，保存了以下文件以供调试使用："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "File"
msgstr "文件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Description"
msgstr "描述"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``fx_graph_runnable.py``"
msgstr "``fx_graph_runnable.py``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Executable FX graph, after decomposition, before pattern match"
msgstr "可执行FX图，分解后，模式匹配前"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``fx_graph_transformed.py``"
msgstr "``fx_graph_transformed.py``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Transformed FX graph, after pattern match"
msgstr "转化的FX图，模式匹配后"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``ir_pre_fusion.txt``"
msgstr "``ir_pre_fusion.txt``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inductor IR before fusion"
msgstr "融合前的Inductor IR"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``ir_post_fusion.txt``"
msgstr "``ir_post_fusion.txt``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inductor IR after fusion"
msgstr "融合后的Inductor IR"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``output_code.py``"
msgstr "``output_code.py``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Generated Python code for graph, with C++/Triton kernels"
msgstr "生成的带有C++/Triton内核的图Python代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that ``fx_graph_runnable.py`` and ``output_code.py`` are both runnable "
"and editable in order to make debugging easier. Here are the main parts of "
"code extracted from the files and we correlate the C++ generated line with "
"the FX code line."
msgstr ""
"注意，为了方便调试，``fx_graph_runnable.py``和``output_code.py``可以运行和编辑。以下是从文件中提取的主要代码部分，我们将C++生成的行与FX代码行关联。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``fx_graph_runnable``:"
msgstr "``fx_graph_runnable``:"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "C++ kernel in ``output_code``:"
msgstr "``output_code``中的C++内核："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Determine component of error"
msgstr "确定错误的组成部分"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When encountering errors or accuracy problems, a straightforward solution to"
" find the bug is to narrow down the problem. The first thing to do is to "
"determine the component where the error occurs. Luckily, it can be simply "
"achieved by changing the backend of ``torch.compile``."
msgstr ""
"在遇到错误或准确性问题时，找到问题的一个直接解决方法是缩小问题范围。第一步是确定错误发生的组件。幸运的是，通过更改``torch.compile``的后端可以简单实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Code"
msgstr "代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torch.compile(fn, backend=\"eager\")``"
msgstr "``torch.compile(fn, backend=\"eager\")``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Enable Dynamo"
msgstr "启用Dynamo"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torch.compile(fn, backend=\"aot_eager\")``"
msgstr "``torch.compile(fn, backend=\"aot_eager\")``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Enable Dynamo + AOT Autograd"
msgstr "启用Dynamo + AOT Autograd"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torch.compile(fn, backend=\"inductor\")``"
msgstr "``torch.compile(fn, backend=\"inductor\")``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Enable Dynamo + AOT Autograd + Inductor"
msgstr "启用Dynamo + AOT Autograd + Inductor"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If the model can successfully run when the backend is set to ``eager`` or "
"``aot_eager`` while it fails with ``inductor``, we can narrow down the "
"failure to Inductor."
msgstr ""
"如果将后端设置为``eager``或``aot_eager``时模型能够成功运行，而设置为``inductor``时失败，我们可以将问题范围缩小到Inductor。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compilation error"
msgstr "编译错误"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "As we know, the evolved chain of graph-level optimization is like:"
msgstr "众所周知，图级优化的演化链如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you encounter a compilation error, there is something wrong when "
"compiling C++ kernels in the output code. This type of error indicates that "
"bugs are introduced when lowering IR nodes to output code. The root cause of"
" compilation error is usually shown in the traceback log."
msgstr ""
"如果遇到编译错误，则表示在输出代码中编译C++内核时出现问题。这种错误表明在将IR节点降低到输出代码时引入了问题。编译错误的根本原因通常会显示在回溯日志中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For example, the ``neg`` function is modified like this:"
msgstr "例如，``neg``函数被修改如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The logging gives the following compile error with a rather clear reason."
msgstr "日志记录给出了如下编译错误，并列出了相当清晰的原因。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let us also see the corresponding C++ kernel in output code and IR node."
msgstr "让我们也看看对应输出代码中的C++内核和IR节点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "C++ kernel:"
msgstr "C++内核："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "IR node:"
msgstr "IR节点："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"According to the traceback logging, the compilation error is caused by the "
"data type inconsistency of ``max_propagate_nan``'s inputs. By checking the "
"C++ kernel, we know that ``tmp2`` is no longer ``long`` after doing ``-`` as"
" ``tmp0`` is ``long``. We can easily match ``-`` and ``max_propagate_nan`` "
"in C++ kernel with ``ops.neg`` and ``ops.maximum`` in IR node respectively."
msgstr ""
"根据回溯日志记录，编译错误是由于``max_propagate_nan``的输入数据类型不一致导致的。通过检查C++内核，我们知道在``tmp0``为``long``类型时进行``-``操作后，``tmp2``不再是``long``类型。我们可以轻松在C++内核中匹配``-``和``max_propagate_nan``，在IR节点中分别对应``ops.neg``和``ops.maximum``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we successfully find that the root cause is the implementation of "
"``ops.neg`` in ``cpp`` codegen, which silently changes the data type when "
"doing ``neg``."
msgstr "现在我们成功找到了问题的根本原因，是``ops.neg``在``cpp``代码生成中的实现，它在执行``neg``时静默改变了数据类型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Accuracy debugging"
msgstr "准确性调试"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Otherwise, if the model runs with other errors or accuracy problem, you can "
"use the PyTorch debugging tool called `Minifier "
"<https://pytorch.org/functorch/stable/notebooks/minifier.html>`_."
msgstr ""
"否则，如果模型运行时出现其他错误或准确性问题，可以使用PyTorch的调试工具`Minifier "
"<https://pytorch.org/functorch/stable/notebooks/minifier.html>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The core idea of ``Minifier`` is to keep removing the nodes and inputs of "
"graph until finding the minimal graph with problem. It helps to "
"automatically generate a minified problematic graph through 4 strategies: "
"truncating suffix, delta debugging, eliminating dead code and removing "
"unused inputs."
msgstr ""
"``Minifier``的核心思想是不断移除图的节点和输入直到找到最小的具有问题的图。它通过四种策略帮助自动生成精简的有问题的图：截断后缀、差分调试、消除死代码以及移除未使用输入。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will now show the debugging process for the accuracy problem with the "
"help of ``Minifer``. The accuracy problem refers to the case where the "
"outputs of backends eager and inductor are different."
msgstr ""
"现在我们将展示如何在``Minifier``的帮助下调试准确性问题。准确性问题指后端``eager``和``inductor``的输出不同的情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For instance, we modify the example like this:"
msgstr "例如，我们将示例修改如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "And also modify the ``neg`` function:"
msgstr "同时修改``neg``函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "An accuracy problem would be raised as follows:"
msgstr "准确性问题将会如下引发："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To debug an accuracy problem with Minifier, two environment variables are "
"needed:"
msgstr "要使用Minifier调试准确性问题，需要设置两个环境变量："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Which gives us logging information that demonstrates the steps of minifying:"
msgstr "由此我们可以获得记录信息，显示精简的步骤："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After running, we get the final minified graph with the target node ``neg``:"
msgstr "运行后，我们得到包含目标节点``neg``的最终精简图："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For more usage details about Minifier, please refer to `Troubleshooting "
"<https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html>`_."
msgstr ""
"有关Minifier的更多使用详情，请参考`故障排除指南 "
"<https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Performance profiling"
msgstr "性能分析"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Within this section, we will demonstrate the process of conducting "
"performance analysis for a model that has been compiled using the Inductor "
"CPU backend. In the example below, we benchmark a Hugging Face Transformer "
"model ``MobileBertForQuestionAnswering`` with both the eager mode and the "
"Inductor graph mode. The execution time and the speedup ratio of Inductor "
"are printed after the benchmark. We use Intel(R) Xeon(R) Platinum 8358 CPU @"
" 2.60GHz and run benchmark on the first socket to demonstrate the "
"optimization within this section. We set following environment variable as a"
" best practice to benchmark on Intel(R) CPU."
msgstr ""
"在本节中，我们将演示对使用Inductor CPU后端编译的模型进行性能分析的过程。在下面的示例中，我们比较了Hugging Face "
"Transformer模型``MobileBertForQuestionAnswering``在``eager``模式和Inductor图模式下的基准测试。基准测试完成后打印出Inductor的执行时间和加速比。我们使用Intel(R)"
" Xeon(R) Platinum 8358 CPU @ "
"2.60GHz并在第一插槽上运行基准测试来演示针对本节的优化。我们设置以下环境变量作为在Intel(R) CPU上进行基准测试的最佳实践。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Output:"
msgstr "输出："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In our own testing, we find the Inductor CPU backend speed up the model by "
"around 2.355x."
msgstr "在我们的测试中，我们发现Inductor CPU后端使模型加速了约2.355倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, let's dive deep into the performance at the operation level to "
"understand where the speed-up comes from. `Pytorch Profiler "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`_ is a "
"good tool to help us. Inductor CPU backend has the support to report the "
"time of the fusion kernels to the profiler with the "
"``enable_kernel_profile`` configuration option:"
msgstr ""
"接下来，让我们深入到操作级别性能分析以了解加速来源。`PyTorch Profiler "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`_是一个很好的工具，可以帮助我们完成此任务。Inductor"
" CPU后端支持通过启用``enable_kernel_profile``配置选项向Profiler报告融合内核的时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Following the steps in `Pytorch Profiler "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`_ We "
"are able to get the profiling table and trace files."
msgstr ""
"按照`PyTorch Profiler "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`_中的步骤，我们能够生成性能剖析表和跟踪文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We get the following performance profiling table for the eager-mode model "
"(omitting some columns):"
msgstr "我们得到以下基于``eager``模式的模型性能分析表（省略部分列）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Similarly, we also get the table for the compiled model with Inductor "
"(omitting some columns):"
msgstr "类似地，我们还为使用Inductor编译的模型生成了表（省略部分列）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"From the profiling table of the eager model, we can see the most time "
"consumption ops are [``aten::addmm``, ``aten::add``, ``aten::copy_``, "
"``aten::mul``, ``aten::clamp_min``, ``aten::bmm``]. Comparing with the "
"inductor model profiling table, we notice an ``mkl::_mkl_linear`` entry and "
"multiple fused kernels in the form ``graph_0_cpp_fused_*``. They are the "
"major optimizations that the inductor model is doing. Let us discuss them "
"separately."
msgstr ""
"从基于``eager``模式模型的性能剖析表中，我们可以看到最耗时的操作是[``aten::addmm``、``aten::add``、``aten::copy_``、``aten::mul``、``aten::clamp_min``、``aten::bmm``]。与Inductor模型剖析表相比，我们注意到有一个``mkl::_mkl_linear``条目以及多个以``graph_0_cpp_fused_*``形式存在的融合内核。这些是Inductor模型进行的主要优化。让我们分别讨论它们。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"(1) Regarding ``mkl::_mkl_linear``: You may notice the number of calls to "
"this kernel is 362, which is exactly the same as ``aten::linear`` in the "
"eager model profiling table. The CPU total of ``aten::linear`` is 376.888ms,"
" while it is 231.573ms for ``mkl::_mkl_linear``. This suggests a ~1.63x for "
"the \"linear\" part. The speedup mainly comes from `packing the weight "
"tensor to block memory format "
"<https://www.intel.com/content/www/us/en/docs/onemkl/developer-"
"reference-c/2023-1/cblas-gemm-pack-002.html>`_ and invoking "
"`cblas_sgemm_compute "
"<https://www.intel.com/content/www/us/en/docs/onemkl/developer-"
"reference-c/2023-1/cblas-gemm-compute-002.html>`_ within the Inductor CPU "
"backend to have a better cache behavior during GEMM computation."
msgstr ""
"(1) "
"关于``mkl::_mkl_linear``：你可能注意到对此内核的调用次数是362，正好与``eager``模型剖析表中的``aten::linear``相同。``aten::linear``的CPU总时间是376.888ms，而``mkl::_mkl_linear``为231.573ms。这表明“linear”部分有约1.63倍的加速。加速主要来自于`将权重张量打包为块内存格式"
" <https://www.intel.com/content/www/us/en/docs/onemkl/developer-"
"reference-c/2023-1/cblas-gemm-pack-002.html>`_并在Inductor "
"CPU后端中调用`cblas_sgemm_compute "
"<https://www.intel.com/content/www/us/en/docs/onemkl/developer-"
"reference-c/2023-1/cblas-gemm-compute-002.html>`_以在GEMM计算时获得更好的缓存行为。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"(2) Regarding other memory-intensive ops: The end-to-end latency for the "
"eager/inductor model is 802/339ms in our testing. So we can roughly infer "
"that the speed up for the other memory-intensive ops is around 3.94x. Let's "
"read the generated code to understand how the inductor achieves this "
"impressive optimization. You can find the generated code by searching "
"``cpp_fused__mkl_linear_add_mul_relu_151`` in ``output_code.py``"
msgstr ""
"(2) "
"关于其他内存密集型操作：在我们的测试中，``eager``/``inductor``模型的端到端延迟分别为802ms和339ms。因此可以大致推断出其他内存密集型操作的加速约为3.94倍。让我们阅读生成的代码以了解Inductor如何实现这一令人印象深刻的优化。你可以通过搜索``cpp_fused__mkl_linear_add_mul_relu_151``在``output_code.py``中找到生成的代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"From the generated code above, we can see this kernel has done a typical "
"`Loop Fusion <https://en.wikipedia.org/wiki/Loop_fission_and_fusion>`_ on "
"``[add, add, mul, add]``. This is a memory-bound bottle neck preventing good"
" performance. To get a more intuitive feeling about this optimization, we "
"can infer the sizes and stride of the inputs and further benchmark this "
"``[add, add, mul, add]`` pattern."
msgstr ""
"从上面的生成代码中，我们可以看到该内核对``[add, add, mul, add]``进行了典型的`循环融合 "
"<https://en.wikipedia.org/wiki/Loop_fission_and_fusion>`_。这是一个内存受限的瓶颈，限制了性能的提升。为了更直观地理解此优化，我们可以推断输入的大小和步幅，并进一步对模式``[add,"
" add, mul, add]``进行基准测试。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is just an example. The profiling table shows all element-wise op are "
"fused within the inductor automatically in this model. You can read more "
"kernels in `output_code.py`"
msgstr ""
"这只是一个示例。性能分析表显示模型中的所有元素级操作都被Inductor自动融合。你可以阅读更多的内核内容在`output_code.py`中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The document gives an in-depth tutorial for the Inductor CPU backend."
msgstr "本文档提供了针对Inductor CPU后端的深入教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With motivating examples, we walk through the process of debugging and "
"profiling. The main idea is to narrow down the problem."
msgstr "通过激发示例，我们走过了调试和性能分析的全过程。主要思想是缩小问题范围。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We demonstrate step by step the way to delve deeper the issue and find the "
"root cause of failures, with the help of debugging logging and the tool "
"Minifier. Firstly determine which component the failure occurs in and then "
"try to generate the smallest snippet of code that can reproduce the failure."
msgstr ""
"我们一步步演示了深入分析问题并找到失败根本原因的方法，借助调试日志和工具Minifier。首先确定失败发生在哪个组件，然后尝试生成可以复制失败的最小代码片段。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When the performance with Inductor is better than that of eager mode, we "
"provide a solid analytical method for performance profiling. We show how to "
"find the time-consuming hotspot with PyTorch Profiler and figure out the "
"operator-level or kernel-level reason to explain the phenomenon."
msgstr ""
"当Inductor性能优于``eager``模式时，我们提供了进行性能分析的可靠的分析方法。我们展示了如何使用PyTorch "
"Profiler找到耗时的热点，并从操作级或内核级解释现象的原因。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: inductor_debug_cpu.py "
"<inductor_debug_cpu.py>`"
msgstr ":download:`下载Python源代码：inductor_debug_cpu.py <inductor_debug_cpu.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: inductor_debug_cpu.ipynb "
"<inductor_debug_cpu.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：inductor_debug_cpu.ipynb "
"<inductor_debug_cpu.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_jacobians_hessians.py>` to "
"download the full example code"
msgstr ""
"单击:ref:`这里<sphx_glr_download_intermediate_jacobians_hessians.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Jacobians, Hessians, hvp, vhp, and more: composing function transforms"
msgstr "雅可比矩阵、赫赛矩阵、hvp、vhp等：组合函数变换"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Computing jacobians or hessians are useful in a number of non-traditional "
"deep learning models. It is difficult (or annoying) to compute these "
"quantities efficiently using PyTorch's regular autodiff APIs "
"(``Tensor.backward()``, ``torch.autograd.grad``). PyTorch's `JAX-inspired "
"<https://github.com/google/jax>`_ `function transforms API "
"<https://pytorch.org/docs/master/func.html>`_ provides ways of computing "
"various higher-order autodiff quantities efficiently."
msgstr ""
"计算雅可比矩阵或赫赛矩阵对一些非传统深度学习模型很有用。使用PyTorch的常规自动微分API（``Tensor.backward()``、``torch.autograd.grad``）高效计算这些量是很困难（或者令人烦恼）的。PyTorch的`JAX启发式"
" <https://github.com/google/jax>`_ 的`函数变换API "
"<https://pytorch.org/docs/master/func.html>`_提供了高效计算各种高阶自动微分量的方法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Computing the Jacobian"
msgstr "计算雅可比矩阵"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's start with a function that we'd like to compute the jacobian of. This "
"is a simple linear function with non-linear activation."
msgstr "让我们首先定义一个函数，我们希望计算其雅可比矩阵。这是一个带有非线性激活的简单线性函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's add some dummy data: a weight, a bias, and a feature vector x."
msgstr "添加一些虚拟数据：一个权重，一个偏置，以及一个特征向量x。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's think of ``predict`` as a function that maps the input ``x`` from "
":math:`R^D \\to R^D`. PyTorch Autograd computes vector-Jacobian products. In"
" order to compute the full Jacobian of this :math:`R^D \\to R^D` function, "
"we would have to compute it row-by-row by using a different unit vector each"
" time."
msgstr ""
"我们可以将``predict``视为一个函数，该函数将输入``x``从:math:`R^D \\to R^D`进行映射。PyTorch "
"Autograd计算向量-雅可比矩阵积。为了计算此:math:`R^D \\to R^D`函数的完整雅可比矩阵，我们必须使用不同的单元向量逐行计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Instead of computing the jacobian row-by-row, we can use PyTorch's "
"``torch.vmap`` function transform to get rid of the for-loop and vectorize "
"the computation. We can’t directly apply ``vmap`` to "
"``torch.autograd.grad``; instead, PyTorch provides a ``torch.func.vjp`` "
"transform that composes with ``torch.vmap``:"
msgstr ""
"与逐行计算雅可比矩阵相比，我们可以使用PyTorch的``torch.vmap``函数变换来摆脱for循环并对计算进行矢量化。我们不能直接对``torch.autograd.grad``应用``vmap``；相反，PyTorch提供了一个``torch.func.vjp``变换，可以与``torch.vmap``组合使用："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In a later tutorial a composition of reverse-mode AD and ``vmap`` will give "
"us per-sample-gradients. In this tutorial, composing reverse-mode AD and "
"``vmap`` gives us Jacobian computation! Various compositions of ``vmap`` and"
" autodiff transforms can give us different interesting quantities."
msgstr ""
"在后续教程中，反向模式AD和``vmap``的组合将为我们提供每样本的梯度。在本教程中，反向模式AD和``vmap``的组合为我们提供雅可比矩阵计算！各种``vmap``和自动微分变换的组合可以为我们提供不同有趣的量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch provides ``torch.func.jacrev`` as a convenience function that "
"performs the ``vmap-vjp`` composition to compute jacobians. ``jacrev`` "
"accepts an ``argnums`` argument that says which argument we would like to "
"compute Jacobians with respect to."
msgstr ""
"PyTorch提供了``torch.func.jacrev``作为一种便利功能，执行``vmap-"
"vjp``组合来计算雅可比矩阵。``jacrev``接收一个``argnums``参数，用来指定我们希望计算雅可比矩阵的参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's compare the performance of the two ways to compute the jacobian. The "
"function transform version is much faster (and becomes even faster the more "
"outputs there are)."
msgstr "让我们比较两种计算雅可比矩阵方式的性能。函数变换版本更快（随着输出增加效果更为显著）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In general, we expect that vectorization via ``vmap`` can help eliminate "
"overhead and give better utilization of your hardware."
msgstr "一般来说，我们预计通过``vmap``进行矢量化可以帮助消除开销并更好地利用硬件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``vmap`` does this magic by pushing the outer loop down into the function's "
"primitive operations in order to obtain better performance."
msgstr "``vmap``通过将外循环下推到函数的原始操作中实现“魔法”，以获得更优性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's make a quick function to evaluate performance and deal with "
"microseconds and milliseconds measurements:"
msgstr "让我们快速编写一个函数来评估性能并处理微秒和毫秒测量："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "And then run the performance comparison:"
msgstr "然后运行性能比较："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's do a relative performance comparison of the above with our "
"``get_perf`` function:"
msgstr "让我们用``get_perf``函数对上述内容进行相对性能比较："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Furthermore, it’s pretty easy to flip the problem around and say we want to "
"compute Jacobians of the parameters to our model (weight, bias) instead of "
"the input"
msgstr "此外，问题可以很容易地反过来，假如我们想计算模型参数的雅可比矩阵（权重、偏差），而不是输入。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Reverse-mode Jacobian (``jacrev``) vs forward-mode Jacobian (``jacfwd``)"
msgstr "反向模式雅可比矩阵（``jacrev``）与前向模式雅可比矩阵（``jacfwd``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We offer two APIs to compute jacobians: ``jacrev`` and ``jacfwd``:"
msgstr "我们提供了两个用于计算雅可比矩阵的API：``jacrev``和``jacfwd``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``jacrev`` uses reverse-mode AD. As you saw above it is a composition of our"
" ``vjp`` and ``vmap`` transforms."
msgstr "``jacrev``使用反向模式自动微分（AD）。如您所见，它是``vjp``和``vmap``变换的组合。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``jacfwd`` uses forward-mode AD. It is implemented as a composition of our "
"``jvp`` and ``vmap`` transforms."
msgstr "``jacfwd``使用前向模式自动微分（AD）。它是``jvp``和``vmap``变换的组合。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``jacfwd`` and ``jacrev`` can be substituted for each other but they have "
"different performance characteristics."
msgstr "``jacfwd``和``jacrev``可以互相替换，但它们的性能特性不同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As a general rule of thumb, if you’re computing the jacobian of an "
":math:`R^N \\to R^M` function, and there are many more outputs than inputs "
"(for example, :math:`M > N`) then ``jacfwd`` is preferred, otherwise use "
"``jacrev``. There are exceptions to this rule, but a non-rigorous argument "
"for this follows:"
msgstr ""
"通常的经验法则是，如果您计算一个:math:`R^N \\to R^M`函数的雅可比矩阵，并且输出比输入多得多（例如，:math:`M > "
"N`），那么建议使用``jacfwd``，否则请使用``jacrev``。虽然这条规则有例外，但以下是一个非正式的解释："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In reverse-mode AD, we are computing the jacobian row-by-row, while in "
"forward-mode AD (which computes Jacobian-vector products), we are computing "
"it column-by-column. The Jacobian matrix has M rows and N columns, so if it "
"is taller or wider one way we may prefer the method that deals with fewer "
"rows or columns."
msgstr ""
"在反向模式自动微分中，我们逐行计算雅可比矩阵；而在前向模式自动微分中（计算雅可比向量积），我们逐列计算。雅可比矩阵有M行和N列，因此如果矩阵较高或较宽，我们可能更倾向于处理行或列较少的方式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "First, let's benchmark with more inputs than outputs:"
msgstr "首先，让我们在输入多于输出的情况下进行基准测试："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "and then do a relative benchmark:"
msgstr "然后执行一次相对基准测试："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "and now the reverse - more outputs (M) than inputs (N):"
msgstr "现在反过来——输出(M)多于输入(N)："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "and a relative performance comparison:"
msgstr "以及一次相对性能比较："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Hessian computation with functorch.hessian"
msgstr "使用functorch.hessian计算海森矩阵"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We offer a convenience API to compute hessians: ``torch.func.hessiani``. "
"Hessians are the jacobian of the jacobian (or the partial derivative of the "
"partial derivative, aka second order)."
msgstr ""
"我们提供了一个便捷的API来计算海森矩阵：``torch.func.hessiani``。海森矩阵是雅可比矩阵的雅可比矩阵（或者称为偏导的偏导，也就是二阶导数）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This suggests that one can just compose functorch jacobian transforms to "
"compute the Hessian. Indeed, under the hood, ``hessian(f)`` is simply "
"``jacfwd(jacrev(f))``."
msgstr ""
"这表明可以仅通过组合functorch的雅可比变换来计算海森矩阵。实际上，“``hessian(f)``”底层实现就是“``jacfwd(jacrev(f))``”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note: to boost performance: depending on your model, you may also want to "
"use ``jacfwd(jacfwd(f))`` or ``jacrev(jacrev(f))`` instead to compute "
"hessians leveraging the rule of thumb above regarding wider vs taller "
"matrices."
msgstr ""
"注意：根据模型情况，为了提高性能，您也可以使用``jacfwd(jacfwd(f))``或``jacrev(jacrev(f))``来计算海森矩阵，参考上述关于宽矩阵与高矩阵的经验法则。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's verify we have the same result regardless of using hessian API or "
"using ``jacfwd(jacfwd())``."
msgstr "让我们验证无论使用hessian API还是使用``jacfwd(jacfwd())``，结果是否一致。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Batch Jacobian and Batch Hessian"
msgstr "批处理雅可比矩阵和批处理海森矩阵"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the above examples we’ve been operating with a single feature vector. In "
"some cases you might want to take the Jacobian of a batch of outputs with "
"respect to a batch of inputs. That is, given a batch of inputs of shape "
"``(B, N)`` and a function that goes from :math:`R^N \\to R^M`, we would like"
" a Jacobian of shape ``(B, M, N)``."
msgstr ""
"在上述示例中，我们操作的是单个特征向量。在某些情况下，您可能希望针对一批输入计算一批输出的雅可比矩阵。即，给定形状为``(B, "
"N)``的一批输入和一个从:math:`R^N \\to R^M`的函数，我们希望得到形状为``(B, M, N)``的雅可比矩阵。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The easiest way to do this is to use ``vmap``:"
msgstr "最简单的方法是使用``vmap``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you have a function that goes from (B, N) -> (B, M) instead and are "
"certain that each input produces an independent output, then it's also "
"sometimes possible to do this without using ``vmap`` by summing the outputs "
"and then computing the Jacobian of that function:"
msgstr ""
"如果您有一个函数从(B, N) -> (B, "
"M)，并且确信每个输入都生成独立的输出，那么有时还可以通过汇总输出，然后计算该函数的雅可比矩阵，而不使用``vmap``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you instead have a function that goes from :math:`R^N \\to R^M` but "
"inputs that are batched, you compose ``vmap`` with ``jacrev`` to compute "
"batched jacobians:"
msgstr ""
"如果您有一个从:math:`R^N \\to "
"R^M`的函数，但是输入是批处理的，则需要使用``vmap``组合``jacrev``来计算批处理雅可比矩阵："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, batch hessians can be computed similarly. It's easiest to think "
"about them by using ``vmap`` to batch over hessian computation, but in some "
"cases the sum trick also works."
msgstr "最后，批处理海森矩阵可以类似地计算。可以通过使用``vmap``批处理海森矩阵计算来使其容易理解，但在某些情况下，累加技巧同样有效。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Computing Hessian-vector products"
msgstr "计算海森向量积"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The naive way to compute a Hessian-vector product (hvp) is to materialize "
"the full Hessian and perform a dot-product with a vector. We can do better: "
"it turns out we don't need to materialize the full Hessian to do this. We'll"
" go through two (of many) different strategies to compute Hessian-vector "
"products: - composing reverse-mode AD with reverse-mode AD - composing "
"reverse-mode AD with forward-mode AD"
msgstr ""
"计算海森向量积（hvp）最简单的方法是生成完整的海森矩阵并与向量执行点积。我们可以做得更好：事实证明我们不需要生成完整的海森矩阵即可实现。接下来我们将介绍两种（还有很多）计算海森向量积的策略：-"
" 组合反向模式自动微分与反向模式自动微分 - 组合反向模式自动微分与前向模式自动微分"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Composing reverse-mode AD with forward-mode AD (as opposed to reverse-mode "
"with reverse-mode) is generally the more memory efficient way to compute a "
"hvp because forward-mode AD doesn't need to construct an Autograd graph and "
"save intermediates for backward:"
msgstr ""
"组合反向模式自动微分与前向模式自动微分（而不是反向模式与反向模式）通常是更高效的内存计算海森向量积的方法，因为前向模式自动微分不需要构造自动微分图并保存中间结果以进行反向计算："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Here's some sample usage."
msgstr "这是一些示例用法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If PyTorch forward-AD does not have coverage for your operations, then we "
"can instead compose reverse-mode AD with reverse-mode AD:"
msgstr "如果PyTorch前向自动微分未涵盖您的操作，那么我们可以改为组合反向模式自动微分与反向模式自动微分："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: jacobians_hessians.py "
"<jacobians_hessians.py>`"
msgstr ":download:`下载Python源码: jacobians_hessians.py <jacobians_hessians.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: jacobians_hessians.ipynb "
"<jacobians_hessians.ipynb>`"
msgstr ""
":download:`下载 Jupyter笔记本: jacobians_hessians.ipynb "
"<jacobians_hessians.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_mario_rl_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_mario_rl_tutorial.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Train a Mario-playing RL Agent"
msgstr "训练一个马里奥玩游戏的强化学习代理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Authors:** `Yuansong Feng <https://github.com/YuansongFeng>`__, `Suraj "
"Subramanian <https://github.com/suraj813>`__, `Howard Wang "
"<https://github.com/hw26>`__, `Steven Guo "
"<https://github.com/GuoYuzhang>`__."
msgstr ""
"**作者:** `Yuansong Feng <https://github.com/YuansongFeng>`__, `Suraj "
"Subramanian <https://github.com/suraj813>`__, `Howard Wang "
"<https://github.com/hw26>`__, `Steven Guo "
"<https://github.com/GuoYuzhang>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial walks you through the fundamentals of Deep Reinforcement "
"Learning. At the end, you will implement an AI-powered Mario (using `Double "
"Deep Q-Networks <https://arxiv.org/pdf/1509.06461.pdf>`__) that can play the"
" game by itself."
msgstr ""
"本教程带您了解深度强化学习的基础知识。在结束时，您将实现一个AI驱动的马里奥（使用`双重深度Q网络 "
"<https://arxiv.org/pdf/1509.06461.pdf>`__)，它可以自行玩游戏。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Although no prior knowledge of RL is necessary for this tutorial, you can "
"familiarize yourself with these RL `concepts "
"<https://spinningup.openai.com/en/latest/spinningup/rl_intro.html>`__, and "
"have this handy `cheatsheet "
"<https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-"
"IYCDTFU7N>`__ as your companion. The full code is available `here "
"<https://github.com/yuansongFeng/MadMario/>`__."
msgstr ""
"尽管学习本教程不需要事先了解强化学习，您可以预先熟悉这些强化学习`概念 "
"<https://spinningup.openai.com/en/latest/spinningup/rl_intro.html>`__，并使用这个便利的`备忘表"
" <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-"
"IYCDTFU7N>`__作为伴侣。完整代码位于`此处 <https://github.com/yuansongFeng/MadMario/>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "mario"
msgstr "马里奥"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "RL Definitions"
msgstr "强化学习定义"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Environment** The world that an agent interacts with and learns from."
msgstr "**环境** 指代理与其交互并从中学习的世界。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Action** :math:`a` : How the Agent responds to the Environment. The set of"
" all possible Actions is called *action-space*."
msgstr "**动作** :math:`a` : 代理如何回应环境。所有可能动作的集合称为*动作空间*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**State** :math:`s` : The current characteristic of the Environment. The set"
" of all possible States the Environment can be in is called *state-space*."
msgstr "**状态** :math:`s` : 环境当前的特性。环境可能处于的所有状态的集合称为*状态空间*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Reward** :math:`r` : Reward is the key feedback from Environment to Agent."
" It is what drives the Agent to learn and to change its future action. An "
"aggregation of rewards over multiple time steps is called **Return**."
msgstr ""
"**奖励** :math:`r` : 奖励是环境对代理的关键反馈。它驱使代理学习并改变未来的动作。在多个时间步中累积的奖励称为**回报**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Optimal Action-Value function** :math:`Q^*(s,a)` : Gives the expected "
"return if you start in state :math:`s`, take an arbitrary action :math:`a`, "
"and then for each future time step take the action that maximizes returns. "
":math:`Q` can be said to stand for the “quality” of the action in a state. "
"We try to approximate this function."
msgstr ""
"**最优动作值函数** :math:`Q^*(s,a)` : "
"给出如果您开始于状态:math:`s`，执行一个任意动作:math:`a`，然后在每个未来的时间步都选择最大化回报的动作的期望回报。:math:`Q` "
"可以认为是动作在一个状态中的“质量”。我们尝试对此函数进行近似。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Initialize Environment"
msgstr "初始化环境"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In Mario, the environment consists of tubes, mushrooms and other components."
msgstr "在马里奥中，环境包括管道、蘑菇和其他组件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When Mario makes an action, the environment responds with the changed (next)"
" state, reward and other info."
msgstr "当马里奥执行一个动作时，环境返回改变后的（下一步）状态、奖励和其他信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preprocess Environment"
msgstr "预处理环境"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Environment data is returned to the agent in ``next_state``. As you saw "
"above, each state is represented by a ``[3, 240, 256]`` size array. Often "
"that is more information than our agent needs; for instance, Mario’s actions"
" do not depend on the color of the pipes or the sky!"
msgstr ""
"环境数据在``next_state``中返回给代理。如您上面看到的，每个状态由一个``[3, 240, "
"256]``大小的数组表示。通常对代理来说这包含的信息过多；例如，马里奥的动作并不依赖于管道或天空的颜色。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We use **Wrappers** to preprocess environment data before sending it to the "
"agent."
msgstr "我们使用**包装器**来在将环境数据发送给代理之前进行预处理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``GrayScaleObservation`` is a common wrapper to transform an RGB image to "
"grayscale; doing so reduces the size of the state representation without "
"losing useful information. Now the size of each state: ``[1, 240, 256]``"
msgstr ""
"``GrayScaleObservation``是一个常见的包装器，用于将RGB图像转换为灰度图像；这样可以在不丢失有用信息的情况下减小状态表示的大小。现在每个状态的大小为：``[1,"
" 240, 256]``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``ResizeObservation`` downsamples each observation into a square image. New "
"size: ``[1, 84, 84]``"
msgstr "``ResizeObservation``将每个观测值缩小为方形图像。新的大小：``[1, 84, 84]``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and "
"implements the ``step()`` function. Because consecutive frames don’t vary "
"much, we can skip n-intermediate frames without losing much information. The"
" n-th frame aggregates rewards accumulated over each skipped frame."
msgstr ""
"``SkipFrame``是继承自``gym.Wrapper``的自定义包装器，并实现了``step()``函数。因为连续帧变化不大，我们可以跳过n个中间帧而不丢失太多信息。第n帧累积了每个跳过帧的奖励。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``FrameStack`` is a wrapper that allows us to squash consecutive frames of "
"the environment into a single observation point to feed to our learning "
"model. This way, we can identify if Mario was landing or jumping based on "
"the direction of his movement in the previous several frames."
msgstr ""
"``FrameStack``是一个包装器，允许我们将环境的连续帧压缩为一个单独的观测点，以馈送给我们的学习模型。通过这种方式，我们可以根据马里奥过去几帧中的运动方向确定他是在降落还是在跳跃。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After applying the above wrappers to the environment, the final wrapped "
"state consists of 4 gray-scaled consecutive frames stacked together, as "
"shown above in the image on the left. Each time Mario makes an action, the "
"environment responds with a state of this structure. The structure is "
"represented by a 3-D array of size ``[4, 84, 84]``."
msgstr ""
"在对环境应用上述包装器后，最终包装的状态包括4个灰度化的连续帧堆叠在一起，如上图左所示。每次马里奥执行一个动作时，环境以这种结构的状态作出响应。该结构由大小为``[4,"
" 84, 84]``的三维数组表示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "picture"
msgstr "图片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Agent"
msgstr "代理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We create a class ``Mario`` to represent our agent in the game. Mario should"
" be able to:"
msgstr "我们创建一个``Mario``类来表示游戏中的代理。马里奥应该能够："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Act** according to the optimal action policy based on the current state "
"(of the environment)."
msgstr "**行动**：根据当前状态（环境）制定最优动作策略。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Remember** experiences. Experience = (current state, current action, "
"reward, next state). Mario *caches* and later *recalls* his experiences to "
"update his action policy."
msgstr "**记忆**经验。经验=（当前状态、当前动作、奖励、下一状态）。马里奥*缓存*并稍后*回忆*他的经验以更新动作策略。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Learn** a better action policy over time"
msgstr "**学习**随着时间推移更好的动作策略"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the following sections, we will populate Mario’s parameters and define "
"his functions."
msgstr "在以下部分中，我们将填充马里奥的参数并定义他的功能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Act"
msgstr "行动"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For any given state, an agent can choose to do the most optimal action "
"(**exploit**) or a random action (**explore**)."
msgstr "对于任何给定状态，代理可以选择执行最优动作（**利用**）或随机动作（**探索**）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Mario randomly explores with a chance of ``self.exploration_rate``; when he "
"chooses to exploit, he relies on ``MarioNet`` (implemented in ``Learn`` "
"section) to provide the most optimal action."
msgstr ""
"马里奥以``self.exploration_rate``的概率随机探索；当他选择利用时，他依赖于``MarioNet``（在``Learn``部分中实现）提供最优动作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Cache and Recall"
msgstr "缓存和回忆"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "These two functions serve as Mario’s “memory” process."
msgstr "这两个函数是马里奥的“记忆”过程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``cache()``: Each time Mario performs an action, he stores the "
"``experience`` to his memory. His experience includes the current *state*, "
"*action* performed, *reward* from the action, the *next state*, and whether "
"the game is *done*."
msgstr ""
"``cache()``：每次马里奥执行一个动作时，他都会将``经验``存储到他的记忆中。他的经验包括当前*状态*、执行的*动作*、来自动作的*奖励*、*下一状态*以及游戏是否结束的状态*done*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``recall()``: Mario randomly samples a batch of experiences from his memory,"
" and uses that to learn the game."
msgstr "``recall()``：马里奥从记忆中随机抽取一批经验样本，并用这些经验来学习游戏。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Learn"
msgstr "学习"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Mario uses the `DDQN algorithm <https://arxiv.org/pdf/1509.06461>`__ under "
"the hood. DDQN uses two ConvNets - :math:`Q_{online}` and :math:`Q_{target}`"
" - that independently approximate the optimal action-value function."
msgstr ""
"马里奥使用`DDQN算法 <https://arxiv.org/pdf/1509.06461>`__进行学习。DDQN使用两个卷积网络 - "
":math:`Q_{online}` 和 :math:`Q_{target}` - 独立地逼近最优动作值函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In our implementation, we share feature generator ``features`` across "
":math:`Q_{online}` and :math:`Q_{target}`, but maintain separate FC "
"classifiers for each. :math:`\\theta_{target}` (the parameters of "
":math:`Q_{target}`) is frozen to prevent updating by backprop. Instead, it "
"is periodically synced with :math:`\\theta_{online}` (more on this later)."
msgstr ""
"在我们的实现中，我们共享特征生成器``features``用于:math:`Q_{online}`和:math:`Q_{target}`，但为每个保持单独的全连接分类器。:math:`\\theta_{target}`"
" "
"(:math:`Q_{target}`的参数)被冻结以防止通过回传更新。相反，它会定期与:math:`\\theta_{online}`同步（稍后会详细介绍）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Neural Network"
msgstr "神经网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TD Estimate & TD Target"
msgstr "TD估计与TD目标"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Two values are involved in learning:"
msgstr "学习中涉及两个值："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**TD Estimate** - the predicted optimal :math:`Q^*` for a given state "
":math:`s`"
msgstr "**TD估计**：对于一个给定状态:math:`s`，预测的最优:math:`Q^*`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "{TD}_e = Q_{online}^*(s,a)"
msgstr "{TD}_e = Q_{online}^*(s,a)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**TD Target** - aggregation of current reward and the estimated :math:`Q^*` "
"in the next state :math:`s'`"
msgstr "**TD目标**：当前奖励与下一状态:math:`s&apos;`中估计的:math:`Q^*`的汇总"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "a' = argmax_{a} Q_{online}(s', a)"
msgstr "a' = argmax_{a} Q_{online}(s', a)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "{TD}_t = r + \\gamma Q_{target}^*(s',a')"
msgstr "{TD}_t = r + \\gamma Q_{target}^*(s',a')"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Because we don’t know what next action :math:`a'` will be, we use the action"
" :math:`a'` maximizes :math:`Q_{online}` in the next state :math:`s'`."
msgstr ""
"由于我们不知道下一步动作 :math:`a'` 将会是什么，我们选择在下一状态 :math:`s'` 中使 :math:`Q_{online}` "
"最大化的动作 :math:`a'`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Notice we use the `@torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__ "
"decorator on ``td_target()`` to disable gradient calculations here (because "
"we don’t need to backpropagate on :math:`\\theta_{target}`)."
msgstr ""
"注意我们在 ``td_target()`` 上使用了 `@torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__ "
"装饰器，通过此装饰器禁用梯度计算（因为我们不需要对 :math:`\\theta_{target}` 进行回传）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Updating the model"
msgstr "更新模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As Mario samples inputs from his replay buffer, we compute :math:`TD_t` and "
":math:`TD_e` and backpropagate this loss down :math:`Q_{online}` to update "
"its parameters :math:`\\theta_{online}` (:math:`\\alpha` is the learning "
"rate ``lr`` passed to the ``optimizer``)"
msgstr ""
"在马里奥从回放缓冲中采样输入时，我们计算 :math:`TD_t` 和 :math:`TD_e`，并将此损失进行回传以更新 "
":math:`Q_{online}` 的参数 :math:`\\theta_{online}`（:math:`\\alpha` 为传递给 "
"``optimizer`` 的学习率 ``lr``）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)"
msgstr "\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":math:`\\theta_{target}` does not update through backpropagation. Instead, "
"we periodically copy :math:`\\theta_{online}` to :math:`\\theta_{target}`"
msgstr ""
":math:`\\theta_{target}` 不通过回传更新。相反，我们周期性地将 :math:`\\theta_{online}` 复制给 "
":math:`\\theta_{target}`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\theta_{target} \\leftarrow \\theta_{online}"
msgstr "\\theta_{target} \\leftarrow \\theta_{online}"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Save checkpoint"
msgstr "保存检查点"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Putting it all together"
msgstr "把所有部分结合起来"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Logging"
msgstr "记录日志"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let’s play!"
msgstr "一起玩吧！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example we run the training loop for 40 episodes, but for Mario to "
"truly learn the ways of his world, we suggest running the loop for at least "
"40,000 episodes!"
msgstr "在此示例中，我们运行训练循环40次，但为了让马里奥真正掌握他的世界，我们建议至少运行训练循环40000次！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we saw how we can use PyTorch to train a game-playing AI. "
"You can use the same methods to train an AI to play any of the games at the "
"`OpenAI gym <https://gym.openai.com/>`__. Hope you enjoyed this tutorial, "
"feel free to reach us at `our github "
"<https://github.com/yuansongFeng/MadMario/>`__!"
msgstr ""
"在本教程中，我们展示了如何使用PyTorch训练一个游戏AI。您可以使用相同的方法训练AI玩 `OpenAI gym "
"<https://gym.openai.com/>`__ 中的任何游戏！希望您享受这个教程，欢迎通过 `我们的Github "
"<https://github.com/yuansongFeng/MadMario/>`__ 联系我们！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: mario_rl_tutorial.py "
"<mario_rl_tutorial.py>`"
msgstr ":download:`下载Python源代码: mario_rl_tutorial.py <mario_rl_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: mario_rl_tutorial.ipynb "
"<mario_rl_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: mario_rl_tutorial.ipynb <mario_rl_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_memory_format_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_memory_format_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "(beta) Channels Last Memory Format in PyTorch"
msgstr "(beta) PyTorch中的Channels Last内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Vitaly Fedyunin <https://github.com/VitalyFedyunin>`_"
msgstr "**作者**: `Vitaly Fedyunin <https://github.com/VitalyFedyunin>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "What is Channels Last"
msgstr "什么是Channels Last"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Channels last memory format is an alternative way of ordering NCHW tensors "
"in memory preserving dimensions ordering. Channels last tensors ordered in "
"such a way that channels become the densest dimension (aka storing images "
"pixel-per-pixel)."
msgstr ""
"Channels Last内存格式是一种对NCHW张量进行存储的新方式，保留维度顺序。Channels "
"Last张量以使通道成为最密集维度的方式排列（即逐像素存储图像）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For example, classic (contiguous) storage of NCHW tensor (in our case it is "
"two 4x4 images with 3 color channels) look like this:"
msgstr "例如，经典（连续）存储NCHW张量（在此例中为两个具有3个颜色通道的4x4图像）如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "classic_memory_format"
msgstr "经典内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Channels last memory format orders data differently:"
msgstr "Channels Last内存格式以不同方式排列数据："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "channels_last_memory_format"
msgstr "Channels Last内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Pytorch supports memory formats (and provides back compatibility with "
"existing models including eager, JIT, and TorchScript) by utilizing  "
"existing strides structure. For example, 10x3x16x16 batch in Channels last "
"format will have strides equal to (768, 1, 48, 3)."
msgstr ""
"Pytorch通过使用现存的strides结构支持内存格式（并提供对现有模型的向后兼容，包括eager, "
"JIT和TorchScript）。例如，Channels Last格式下的10x3x16x16批量的stride为(768, 1, 48, 3)。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Channels last memory format is implemented for 4D NCHW Tensors only."
msgstr "Channels Last内存格式仅适用于4D NCHW张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Memory Format API"
msgstr "内存格式API"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here is how to convert tensors between contiguous and channels last memory "
"formats."
msgstr "以下是如何在连续和Channels Last内存格式之间转换张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Classic PyTorch contiguous tensor"
msgstr "经典PyTorch连续张量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Conversion operator"
msgstr "转换操作符"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Back to contiguous"
msgstr "返回连续格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Alternative option"
msgstr "备选选项"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Format checks"
msgstr "格式检查"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are minor difference between the two APIs ``to`` and ``contiguous``. "
"We suggest to stick with ``to`` when explicitly converting memory format of "
"tensor."
msgstr "两个API ``to`` 和 ``contiguous`` 之间略有不同。我们建议在显式转换张量的内存格式时使用 ``to``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For general cases the two APIs behave the same. However in special cases for"
" a 4D tensor with size ``NCHW`` when either: ``C==1`` or ``H==1 && W==1``, "
"only ``to`` would generate a proper stride to represent channels last memory"
" format."
msgstr ""
"对于一般情况，两个API的行为是相同的。然而，对于4D张量，当大小为 ``NCHW`` 且满足以下条件时：``C==1`` 或 ``H==1 && "
"W==1``，仅 ``to`` 会生成适当stride来表示Channels Last内存格式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is because in either of the two cases above, the memory format of a "
"tensor is ambiguous, i.e. a contiguous tensor with size ``N1HW`` is both "
"``contiguous`` and channels last in memory storage. Therefore, they are "
"already considered as ``is_contiguous`` for the given memory format and "
"hence ``contiguous`` call becomes a no-op and would not update the stride. "
"On the contrary, ``to`` would restride tensor with a meaningful stride on "
"dimensions whose sizes are 1 in order to properly represent the intended "
"memory format"
msgstr ""
"这是因为以上任何一种情况下，张量的内存格式是模糊的。例如，即使大小为 ``N1HW`` 的连续张量，其存储格式也是连续的同时也是Channels "
"Last。因此，它们已经被认为是给定内存格式的 ``is_contiguous``，因此调用 ``contiguous`` "
"不再会操作stride更新。相反，``to`` 会通过有意义的stride重新设置张量，以正确表示目标内存格式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Same thing applies to explicit permutation API ``permute``. In special case "
"where ambiguity could occur, ``permute`` does not guarantee to produce a "
"stride that properly carry the intended memory format. We suggest to use "
"``to`` with explicit memory format to avoid unintended behavior."
msgstr ""
"显式API ``permute`` 的情况相同。在发生模糊情况时，``permute`` "
"不保证生成完全反映目标内存格式的适当stride。我们建议使用带显式内存格式的 ``to`` 以避免意外行为。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And a side note that in the extreme case, where three non-batch dimensions "
"are all equal to ``1`` (``C==1 && H==1 && W==1``), current implementation "
"cannot mark a tensor as channels last memory format."
msgstr ""
"需要注意的是，在极端情况下，当三个非批量维度都等于 ``1``（即 ``C==1 && H==1 && "
"W==1``），当前实现无法标记张量为Channels Last内存格式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Create as channels last"
msgstr "创建Channels Last"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``clone`` preserves memory format"
msgstr "``clone`` 保留内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``to``, ``cuda``, ``float`` ... preserves memory format"
msgstr "``to``, ``cuda``, ``float`` ... 保留内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``empty_like``, ``*_like`` operators preserves memory format"
msgstr "``empty_like``, ``*_like`` 操作符保留内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Pointwise operators preserves memory format"
msgstr "逐元素操作符保留内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``Conv``, ``Batchnorm`` modules using ``cudnn`` backends support channels "
"last (only works for cuDNN >= 7.6). Convolution modules, unlike binary "
"p-wise operator, have channels last as the dominating memory format. If all "
"inputs are in contiguous memory format, the operator produces output in "
"contiguous memory format. Otherwise, output will be in channels last memory "
"format."
msgstr ""
"``Conv``, ``Batchnorm`` 模块使用 ``cudnn`` 后端支持Channels Last（仅适用于cuDNN >= "
"7.6）。卷积模块与二元逐元素操作符不同，Channels "
"Last为主导内存格式。如果所有输入为连续内存格式，操作符将生成连续内存格式的输出；否则，输出将为Channels Last格式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When input tensor reaches a operator without channels last support, a "
"permutation should automatically apply in the kernel to restore contiguous "
"on input tensor. This introduces overhead and stops the channels last memory"
" format propagation. Nevertheless, it guarantees correct output."
msgstr ""
"当输入张量到达一个不支持Channels Last的操作符时，内核中将会自动应用排列以恢复输入张量的连续格式。这会引入开销，并停止Channels "
"Last内存格式的传播。尽管如此，它保证了正确的输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Performance Gains"
msgstr "性能提升"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Channels last memory format optimizations are available on both GPU and CPU."
" On GPU, the most significant performance gains are observed on NVIDIA's "
"hardware with Tensor Cores support running on reduced precision "
"(``torch.float16``). We were able to archive over 22% performance gains with"
" channels last comparing to contiguous format, both while utilizing 'AMP "
"(Automated Mixed Precision)' training scripts. Our scripts uses AMP supplied"
" by NVIDIA https://github.com/NVIDIA/apex."
msgstr ""
"Channels "
"Last内存格式优化适用于GPU和CPU。在GPU上，在运行降低精度（``torch.float16``）任务时，NVIDIA的支持张量核硬件上观察到最显著性能提升。通过与连续格式对比，我们获得了超过22%的性能提升，同时还使用了'AMP（自动混合精度）'训练脚本。我们的脚本使用了NVIDIA提供的AMP"
" https://github.com/NVIDIA/apex。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2  "
"./data``"
msgstr ""
"``python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 "
"./data``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Passing ``--channels-last true`` allows running a model in Channels last "
"format with observed 22% performance gain."
msgstr "添加 ``--channels-last true`` 可使模型在Channels Last格式下运行，并观察到22%的性能提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 "
"--channels-last true ./data``"
msgstr ""
"``python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 "
"--channels-last true ./data``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following list of models has the full support of Channels last and "
"showing 8%-35% performance gains on Volta devices: ``alexnet``, "
"``mnasnet0_5``, ``mnasnet0_75``, ``mnasnet1_0``, ``mnasnet1_3``, "
"``mobilenet_v2``, ``resnet101``, ``resnet152``, ``resnet18``, ``resnet34``, "
"``resnet50``, ``resnext50_32x4d``, ``shufflenet_v2_x0_5``, "
"``shufflenet_v2_x1_0``, ``shufflenet_v2_x1_5``, ``shufflenet_v2_x2_0``, "
"``squeezenet1_0``, ``squeezenet1_1``, ``vgg11``, ``vgg11_bn``, ``vgg13``, "
"``vgg13_bn``, ``vgg16``, ``vgg16_bn``, ``vgg19``, ``vgg19_bn``, "
"``wide_resnet101_2``, ``wide_resnet50_2``"
msgstr ""
"以下所有模型完全支持Channels Last，并且在Volta设备上表现出8%-35%的性能增益：``alexnet``, "
"``mnasnet0_5``, ``mnasnet0_75``, ``mnasnet1_0``, ``mnasnet1_3``, "
"``mobilenet_v2``, ``resnet101``, ``resnet152``, ``resnet18``, ``resnet34``, "
"``resnet50``, ``resnext50_32x4d``, ``shufflenet_v2_x0_5``, "
"``shufflenet_v2_x1_0``, ``shufflenet_v2_x1_5``, ``shufflenet_v2_x2_0``, "
"``squeezenet1_0``, ``squeezenet1_1``, ``vgg11``, ``vgg11_bn``, ``vgg13``, "
"``vgg13_bn``, ``vgg16``, ``vgg16_bn``, ``vgg19``, ``vgg19_bn``, "
"``wide_resnet101_2``, ``wide_resnet50_2``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following list of models has the full support of Channels last and "
"showing 26%-76% performance gains on Intel(R) Xeon(R) Ice Lake (or newer) "
"CPUs: ``alexnet``, ``densenet121``, ``densenet161``, ``densenet169``, "
"``googlenet``, ``inception_v3``, ``mnasnet0_5``, ``mnasnet1_0``, "
"``resnet101``, ``resnet152``, ``resnet18``, ``resnet34``, ``resnet50``, "
"``resnext101_32x8d``, ``resnext50_32x4d``, ``shufflenet_v2_x0_5``, "
"``shufflenet_v2_x1_0``, ``squeezenet1_0``, ``squeezenet1_1``, ``vgg11``, "
"``vgg11_bn``, ``vgg13``, ``vgg13_bn``, ``vgg16``, ``vgg16_bn``, ``vgg19``, "
"``vgg19_bn``, ``wide_resnet101_2``, ``wide_resnet50_2``"
msgstr ""
"以下所有模型完全支持Channels Last，并且在Intel(R) Xeon(R) Ice "
"Lake（或更新）CPU上表现出26%-76%的性能增益：``alexnet``, ``densenet121``, ``densenet161``, "
"``densenet169``, ``googlenet``, ``inception_v3``, ``mnasnet0_5``, "
"``mnasnet1_0``, ``resnet101``, ``resnet152``, ``resnet18``, ``resnet34``, "
"``resnet50``, ``resnext101_32x8d``, ``resnext50_32x4d``, "
"``shufflenet_v2_x0_5``, ``shufflenet_v2_x1_0``, ``squeezenet1_0``, "
"``squeezenet1_1``, ``vgg11``, ``vgg11_bn``, ``vgg13``, ``vgg13_bn``, "
"``vgg16``, ``vgg16_bn``, ``vgg19``, ``vgg19_bn``, ``wide_resnet101_2``, "
"``wide_resnet50_2``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Converting existing models"
msgstr "转换现有模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Channels last support is not limited by existing models, as any model can be"
" converted to channels last and propagate format through the graph as soon "
"as input (or certain weight) is formatted correctly."
msgstr ""
"Channels Last支持不仅限于现有模型，任何模型都可以转换为Channels Last并通过计算图传播格式，只要输入（或某些权重）已正确格式化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, not all operators fully converted to support channels last (usually"
" returning contiguous output instead). In the example posted above, layers "
"that does not support channels last will stop the memory format propagation."
" In spite of that, as we have converted the model to channels last format, "
"that means each convolution layer, which has its 4 dimensional weight in "
"channels last memory format, will restore channels last memory format and "
"benefit from faster kernels."
msgstr ""
"然而，并非所有操作符都完全转换为支持Channels Last（通常返回连续输出）。在上述示例中，不支持Channels "
"Last的层将停止内存格式的传播。尽管如此，我们已将模型转换为Channels Last格式，这意味着每个卷积层，其4维权重处于Channels "
"Last内存格式，将恢复Channels Last内存格式并从更快的内核中受益。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"But operators that does not support channels last does introduce overhead by"
" permutation. Optionally, you can investigate and identify operators in your"
" model that does not support channels last, if you want to improve the "
"performance of converted model."
msgstr ""
"但不支持Channels Last的操作符确实会通过排列引入开销。可选地，如果您希望优化已转换模型的性能，您可以调查并识别模型中不支持Channels "
"Last的操作符。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That means you need to verify the list of used operators against supported "
"operators list https://github.com/pytorch/pytorch/wiki/Operators-with-"
"Channels-Last-support, or introduce memory format checks into eager "
"execution mode and run your model."
msgstr ""
"这意味着您需要将所使用操作符的列表与支持操作符列表进行验证 "
"https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-"
"support，或者在Eager执行模式下引入内存格式检查并运行您的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After running the code below, operators will raise an exception if the "
"output of the operator doesn't match the memory format of the input."
msgstr "运行以下代码后，如果操作符的输出与输入的内存格式不匹配，操作符将抛出异常。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you found an operator that doesn't support channels last tensors and you "
"want to contribute, feel free to use following developers guide "
"https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-"
"operators."
msgstr ""
"如果您发现一个不支持Channels Last张量的操作符并希望贡献，请使用以下开发者指南 "
"https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-"
"operators。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Code below is to recover the attributes of torch."
msgstr "以下代码用于恢复torch的属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Work to do"
msgstr "待完成的工作"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "There are still many things to do, such as:"
msgstr "仍有许多事情需要完成，例如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Resolving ambiguity of ``N1HW`` and ``NC11`` Tensors;"
msgstr "解决 ``N1HW`` 和 ``NC11`` 张量的模糊性；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Testing of Distributed Training support;"
msgstr "测试分布式训练支持；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Improving operators coverage."
msgstr "提高操作符的覆盖范围。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you have feedback and/or suggestions for improvement, please let us know "
"by creating `an issue <https://github.com/pytorch/pytorch/issues>`_."
msgstr ""
"如果您有反馈或改进建议，请通过创建 `一个问题 <https://github.com/pytorch/pytorch/issues>`_ 告诉我们。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: memory_format_tutorial.py "
"<memory_format_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: memory_format_tutorial.py "
"<memory_format_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: memory_format_tutorial.ipynb "
"<memory_format_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: memory_format_tutorial.ipynb "
"<memory_format_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_mnist_train_nas.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_mnist_train_nas.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Example training code for ``ax_multiobjective_nas_tutorial.py``"
msgstr "``ax_multiobjective_nas_tutorial.py`` 的示例训练代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: mnist_train_nas.py "
"<mnist_train_nas.py>`"
msgstr ":download:`下载Python源代码: mnist_train_nas.py <mnist_train_nas.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: mnist_train_nas.ipynb "
"<mnist_train_nas.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: mnist_train_nas.ipynb <mnist_train_nas.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Single-Machine Model Parallel Best Practices"
msgstr "单机模型并行最佳实践"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Redirecting to latest parallelism APIs in 3 seconds..."
msgstr "3秒后重定向到最新的并行API..."

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_neural_tangent_kernels.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_neural_tangent_kernels.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Neural Tangent Kernels"
msgstr "神经切面核"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The neural tangent kernel (NTK) is a kernel that describes `how a neural "
"network evolves during training "
"<https://en.wikipedia.org/wiki/Neural_tangent_kernel>`_. There has been a "
"lot of research around it `in recent years "
"<https://arxiv.org/abs/1806.07572>`_. This tutorial, inspired by the "
"implementation of `NTKs in JAX <https://github.com/google/neural-tangents>`_"
" (see `Fast Finite Width Neural Tangent Kernel "
"<https://arxiv.org/abs/2206.08720>`_ for details), demonstrates how to "
"easily compute this quantity using ``torch.func``, composable function "
"transforms for PyTorch."
msgstr ""
"神经切面核 (NTK) 是一种描述神经网络在训练过程中如何演化的核函数 "
"`<https://en.wikipedia.org/wiki/Neural_tangent_kernel>`_。近年来围绕它进行了大量研究 "
"`<https://arxiv.org/abs/1806.07572>`_。本教程受到 `JAX 中 NTKs 实现 "
"<https://github.com/google/neural-tangents>`_（详见 `快速有限宽度神经切面核 "
"<https://arxiv.org/abs/2206.08720>`_）的启发，演示了如何使用 ``torch.func``（可组合的 PyTorch"
" 函数转换）轻松计算该值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, some setup. Let's define a simple CNN that we wish to compute the NTK"
" of."
msgstr "首先，进行一些设置。让我们定义一个简单的 CNN 来计算其 NTK。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "And let's generate some random data"
msgstr "然后生成一些随机数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Create a function version of the model"
msgstr "创建模型的函数版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.func`` transforms operate on functions. In particular, to compute "
"the NTK, we will need a function that accepts the parameters of the model "
"and a single input (as opposed to a batch of inputs!) and returns a single "
"output."
msgstr ""
"``torch.func`` 转换在函数上运行。特别是，为了计算 "
"NTK，我们需要一个函数，该函数接受模型的参数和单个输入（而不是一批输入！）并返回单个输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll use ``torch.func.functional_call``, which allows us to call an "
"``nn.Module`` using different parameters/buffers, to help accomplish the "
"first step."
msgstr ""
"我们将使用 ``torch.func.functional_call``，它允许我们使用不同的参数/缓冲区调用 "
"``nn.Module``，以帮助完成第一步。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Keep in mind that the model was originally written to accept a batch of "
"input data points. In our CNN example, there are no inter-batch operations. "
"That is, each data point in the batch is independent of other data points. "
"With this assumption in mind, we can easily generate a function that "
"evaluates the model on a single data point:"
msgstr ""
"请记住，模型最初是编写为接受一批输入数据点。在我们的 CNN "
"示例中，没有跨批次的操作。即，批次中的每个数据点彼此独立。基于这个假设，我们可以轻松生成一个函数来评估模型对单个数据点的表现："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compute the NTK: method 1 (Jacobian contraction)"
msgstr "计算 NTK：方法1（雅可比收缩法）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We're ready to compute the empirical NTK. The empirical NTK for two data "
"points :math:`x_1` and :math:`x_2` is defined as the matrix product between "
"the Jacobian of the model evaluated at :math:`x_1` and the Jacobian of the "
"model evaluated at :math:`x_2`:"
msgstr ""
"我们准备好计算经验 NTK。两个数据点 :math:`x_1` 和 :math:`x_2` 的经验 NTK 被定义为模型在 :math:`x_1` "
"和模型在 :math:`x_2` 的雅可比矩阵之间的矩阵乘积："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "J_{net}(x_1) J_{net}^T(x_2)"
msgstr "J_{net}(x_1) J_{net}^T(x_2)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the batched case where :math:`x_1` is a batch of data points and "
":math:`x_2` is a batch of data points, then we want the matrix product "
"between the Jacobians of all combinations of data points from :math:`x_1` "
"and :math:`x_2`."
msgstr ""
"在批处理情况下，其中 :math:`x_1` 是一批数据点，:math:`x_2` 是一批数据点，则我们需要所有组合数据点的雅可比矩阵之间的矩阵乘积。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The first method consists of doing just that - computing the two Jacobians, "
"and contracting them. Here's how to compute the NTK in the batched case:"
msgstr "第一种方法就是这样做 - 计算两个雅可比矩阵并收缩它们。以下是在批处理情况下计算 NTK 的方法："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In some cases, you may only want the diagonal or the trace of this quantity,"
" especially if you know beforehand that the network architecture results in "
"an NTK where the non-diagonal elements can be approximated by zero. It's "
"easy to adjust the above function to do that:"
msgstr ""
"在某些情况下，您可能只需要该值的对角线或迹，特别是如果您提前知道网络架构导致 NTK 的非对角线元素可以近似为零。调整上述函数来实现这一点很容易："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The asymptotic time complexity of this method is :math:`N O [FP]` (time to "
"compute the Jacobians) + :math:`N^2 O^2 P` (time to contract the Jacobians),"
" where :math:`N` is the batch size of :math:`x_1` and :math:`x_2`, :math:`O`"
" is the model's output size, :math:`P` is the total number of parameters, "
"and :math:`[FP]` is the cost of a single forward pass through the model. See"
" section 3.2 in `Fast Finite Width Neural Tangent Kernel "
"<https://arxiv.org/abs/2206.08720>`_ for details."
msgstr ""
"该方法的渐近时间复杂度为 :math:`N O [FP]`（计算雅可比矩阵的时间）+ :math:`N^2 O^2 P`（雅可比矩阵收缩的时间），其中 "
":math:`N` 是 :math:`x_1` 和 :math:`x_2` 的批量大小，:math:`O` 是模型的输出大小，:math:`P` "
"是参数总数，:math:`[FP]` 是通过模型单次正向传递的成本。详情参见 `快速有限宽度神经切面核 "
"<https://arxiv.org/abs/2206.08720>`_ 的第3.2节。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compute the NTK: method 2 (NTK-vector products)"
msgstr "计算 NTK：方法2（NTK-向量乘积法）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The next method we will discuss is a way to compute the NTK using NTK-vector"
" products."
msgstr "接下来我们将讨论一种使用 NTK-向量乘积计算 NTK 的方法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This method reformulates NTK as a stack of NTK-vector products applied to "
"columns of an identity matrix :math:`I_O` of size :math:`O\\times O` (where "
":math:`O` is the output size of the model):"
msgstr ""
"此方法将 NTK 重新定义为对大小为 :math:`O\\times O` 的单位矩阵 :math:`I_O` 的列执行 NTK-向量乘积的堆叠："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"J_{net}(x_1) J_{net}^T(x_2) = J_{net}(x_1) J_{net}^T(x_2) I_{O} = "
"\\left[J_{net}(x_1) \\left[J_{net}^T(x_2) e_o\\right]\\right]_{o=1}^{O},"
msgstr ""
"J_{net}(x_1) J_{net}^T(x_2) = J_{net}(x_1) J_{net}^T(x_2) I_{O} = "
"\\left[J_{net}(x_1) \\left[J_{net}^T(x_2) e_o\\right]\\right]_{o=1}^{O},"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"where :math:`e_o\\in \\mathbb{R}^O` are column vectors of the identity "
"matrix :math:`I_O`."
msgstr "其中 :math:`e_o\\in \\mathbb{R}^O` 是单位矩阵 :math:`I_O` 的列向量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let :math:`\\textrm{vjp}_o = J_{net}^T(x_2) e_o`. We can use a vector-"
"Jacobian product to compute this."
msgstr "令 :math:`\\textrm{vjp}_o = J_{net}^T(x_2) e_o`。我们可以使用向量-雅可比乘积计算这个值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, consider :math:`J_{net}(x_1) \\textrm{vjp}_o`. This is a Jacobian-"
"vector product!"
msgstr "现在考虑 :math:`J_{net}(x_1) \\textrm{vjp}_o`。这是一个雅可比-向量乘积！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, we can run the above computation in parallel over all columns "
":math:`e_o` of :math:`I_O` using ``vmap``."
msgstr "最后，我们可以使用 ``vmap`` 并行运行上述对矩阵 :math:`I_O` 的所有列 :math:`e_o` 的计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This suggests that we can use a combination of reverse-mode AD (to compute "
"the vector-Jacobian product) and forward-mode AD (to compute the Jacobian-"
"vector product) to compute the NTK."
msgstr "这表明我们可以结合反向模式自动微分（计算向量-雅可比乘积）和正向模式自动微分（计算雅可比-向量乘积）来计算 NTK。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's code that up:"
msgstr "让我们实现这一点："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our code for ``empirical_ntk_ntk_vps`` looks like a direct translation from "
"the math above! This showcases the power of function transforms: good luck "
"trying to write an efficient version of the above by only using "
"``torch.autograd.grad``."
msgstr ""
"我们的 ``empirical_ntk_ntk_vps`` 代码看起来像是上述数学公式的直接翻译！这展示了函数变换的强大功能：如果仅使用 "
"``torch.autograd.grad``，实现这段代码将非常困难。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The asymptotic time complexity of this method is :math:`N^2 O [FP]`, where "
":math:`N` is the batch size of :math:`x_1` and :math:`x_2`, :math:`O` is the"
" model's output size, and :math:`[FP]` is the cost of a single forward pass "
"through the model. Hence this method performs more forward passes through "
"the network than method 1, Jacobian contraction (:math:`N^2 O` instead of "
":math:`N O`), but avoids the contraction cost altogether (no :math:`N^2 O^2 "
"P` term, where :math:`P` is the total number of model's parameters). "
"Therefore, this method is preferable when :math:`O P` is large relative to "
":math:`[FP]`, such as fully-connected (not convolutional) models with many "
"outputs :math:`O`. Memory-wise, both methods should be comparable. See "
"section 3.3 in `Fast Finite Width Neural Tangent Kernel "
"<https://arxiv.org/abs/2206.08720>`_ for details."
msgstr ""
"该方法的渐近时间复杂度为 :math:`N^2 O [FP]`，其中 :math:`N` 是 :math:`x_1` 和 :math:`x_2` "
"的批量大小，:math:`O` 是模型的输出大小，:math:`[FP]` "
"是通过模型单次正向传递的成本。因此，此方法通过网络的正向传递次数多于方法1（雅可比收缩法）（:math:`N^2 O` 而不是 :math:`N "
"O`），但完全避免了收缩成本（没有 :math:`N^2 O^2 P` 项，其中 :math:`P` 是模型参数总数）。因此，当 :math:`O P`"
" 相对于 :math:`[FP]` 较大时，此方法更可取，例如具有多数输出 :math:`O` "
"的全连接（非卷积）模型。在内存方面，两种方法应该是可比的。详情参见 `快速有限宽度神经切面核 "
"<https://arxiv.org/abs/2206.08720>`_ 的第3.3节。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Total running time of the script:** ( 0 minutes  1.367 seconds)"
msgstr "**脚本总运行时间：** ( 0 分钟  1.367 秒)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: neural_tangent_kernels.py "
"<neural_tangent_kernels.py>`"
msgstr ""
":下载:`下载 Python 源代码: neural_tangent_kernels.py <neural_tangent_kernels.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: neural_tangent_kernels.ipynb "
"<neural_tangent_kernels.ipynb>`"
msgstr ""
":下载:`下载 Jupyter Notebook: neural_tangent_kernels.ipynb "
"<neural_tangent_kernels.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "NLP from Scratch"
msgstr "从零开始的 NLP"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In these three-part series you will build and train a basic character-level "
"Recurrent Neural Network (RNN) to classify words."
msgstr "在这一系列教程中，您将构建和训练一个基本的字符级循环神经网络（RNN）来分类单词。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You will learn:"
msgstr "您将学习："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to construct Recurrent Neural Networks from scratch"
msgstr "如何从零构建循环神经网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Essential data handling techniques for NLP"
msgstr "NLP 的基本数据处理技术"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to train an RNN to identify the language origin of words."
msgstr "如何训练 RNN 来识别单词的语言来源。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Before you begin, we recommend that you review the following:"
msgstr "在开始之前，我们建议您先学习以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`PyTorch Learn the Basics series "
"<https://pytorch.org/tutorials/beginner/basics/intro.html>`__"
msgstr ""
"`PyTorch 学习基础系列 "
"<https://pytorch.org/tutorials/beginner/basics/intro.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`How to install PyTorch <https://pytorch.org/get-started/locally/>`__"
msgstr "`如何安装 PyTorch <https://pytorch.org/get-started/locally/>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"NLP From Scratch - Part 1: Classifying Names with a Character-Level RNN"
msgstr "NLP 从零开始 - 第1部分：使用字符级 RNN 分类名称"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Learn how to use an RNN to classify names into their language of origin."
msgstr "学习如何使用 RNN 将名称分类到其语言来源。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ":octicon:`code;1em` Code"
msgstr "代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"NLP From Scratch - Part 2: Generating Names with a Character-Level RNN"
msgstr "NLP 从零开始 - 第2部分：使用字符级 RNN 生成名称"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Expand the RNN we created in Part 1 to generate names from languages."
msgstr "扩展我们在第1部分中创建的 RNN 从语言中生成名称。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"NLP From Scratch - Part 3: Translation with a Sequence to Sequence Network "
"and Attention"
msgstr "NLP 从零开始 - 第3部分：使用序列到序列网络和注意力机制进行翻译"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Create a sequence-to-sequence model that can translate your text from French"
" to English."
msgstr "创建一个序列到序列模型，可以将您的文本从法语翻译成英语。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Getting Started - Accelerate Your Scripts with nvFuser"
msgstr "入门 - 使用 nvFuser 加速脚本"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial has been deprecated. Redirecting to homepage in 3 seconds..."
msgstr "本教程已废弃。3 秒后重定向到主页..."

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_optimizer_step_in_backward_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_intermediate_optimizer_step_in_backward_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to save memory by fusing the optimizer step into the backward pass"
msgstr "如何通过将优化器步骤融合到反向传递中节省内存。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Hello there! This tutorial aims to showcase one way of reducing the memory "
"footprint of a training loop by reducing the memory taken by the "
"*gradients*. Say you have a model and you're interested in ways to optimize "
"memory to avoid ``Out of Memory`` (OOM) errors or simply to ooze more out of"
" your GPU. Well, you _might_ be in luck (if gradients take up a portion of "
"your memory and you do not need to do gradient accumulation). We will "
"explore the following:"
msgstr ""
"你好！本教程旨在展示通过减少 *梯度* 所占的内存来减少训练循环的内存占用的一种方法。假设您有一个模型，并且您对优化内存以避免 ``内存不足`` "
"(OOM) 错误或仅仅从 GPU 中获取更多内容感兴趣。那么，您 _可能_ "
"很幸运（如果梯度占用了很大部分内存并且不需要进行梯度累加）。我们将探讨以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "What takes up memory during your training or finetuning loop,"
msgstr "在训练或微调循环中占用内存的内容，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to capture and visualize memory snapshots to determine the bottleneck,"
msgstr "如何捕捉和可视化内存快照以确定瓶颈，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The new ``Tensor.register_post_accumulate_grad_hook(hook)`` API, and "
"finally,"
msgstr "新的 ``Tensor.register_post_accumulate_grad_hook(hook)`` API，最终，"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How everything fits together in 10 lines to achieve memory savings."
msgstr "如何将所有内容组合在10行代码中实现内存节省。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "To run this tutorial, you will need:"
msgstr "要运行本教程，您需要："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch 2.1.0 or newer with ``torchvision``"
msgstr "PyTorch 2.1.0 或更高版本，带有 ``torchvision``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"1 CUDA GPU if you'd like to run the memory visualizations locally. "
"Otherwise, this technique would benefit similarly on any device."
msgstr "1 个 CUDA GPU 如果您希望在本地运行内存可视化。否则，此技术在任何设备上都表现相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let us start by importing the required modules and models. We will use a "
"vision transformer model from torchvision, but feel free to substitute with "
"your own model. We will also use ``torch.optim.Adam`` as our optimizer, but,"
" again, feel free to substitute with your own optimizer."
msgstr ""
"现在让我们导入所需的模块和模型。我们将使用 torchvision 的视觉 Transformer 模型，但可以随意替换为自己的模型。我们还将使用 "
"``torch.optim.Adam`` 作为优化器，但是，您也可以随意替换为自己的优化器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now let's define our typical training loop. You should use real images when "
"training, but for the purposes of this tutorial, we are passing in fake "
"inputs and not worrying about loading any actual data."
msgstr "现在定义典型的训练循环。在训练时您应使用真实图像，但出于本教程的目的，我们传入假输入并不考虑加载任何实际数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Memory usage during training"
msgstr "训练期间的内存使用情况"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We are about to look at some memory snapshots, so we should be prepared to "
"analyze them properly. Typically, training memory consists of:"
msgstr "我们将查看一些内存快照，所以我们应该准备好正确地分析它们。通常，训练内存由以下组成："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model parameters (size P)"
msgstr "模型参数（大小为 P）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Activations that are saved for the backward pass (size A)"
msgstr "为了反向传播而保存的激活（大小为 A）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Gradients, which are the same size as the model parameters, so size G = P."
msgstr "梯度，大小与模型参数相同，因此 G = P。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimizer state, which is proportional to the size of the parameters. In "
"this case, the state for Adam requires 2x the model parameters, so size O = "
"2P."
msgstr "优化器状态，与参数大小成比例。在这里，使用 Adam 优化器需要 2 倍于模型参数的状态，因此 O = 2P。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Intermediate tensors, which are allocated throughout the compute. We will "
"not worry about them for now as they are usually small and ephemeral."
msgstr "中间张量，它们会在计算过程中分配。我们暂时不考虑它们，因为它们通常很小且是短暂的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Capturing and visualizing memory snapshots"
msgstr "捕获和可视化内存快照"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's get us a memory snapshot! As your code runs, consider what you may "
"expect the CUDA memory timeline to look like."
msgstr "让我们获取一个内存快照！当您的代码运行时，考虑您可能期望的 CUDA 内存时间线。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now open up the snapshot in the CUDA Memory Visualizer at "
"https://pytorch.org/memory_viz by dragging and dropping the "
"``snapshot.pickle`` file. Does the memory timeline match your expectations?"
msgstr ""
"现在通过拖放 ``snapshot.pickle`` 文件，在 https://pytorch.org/memory_viz 的 CUDA "
"内存可视化工具中打开快照。内存时间线是否符合您的预期？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "snapshot.png loaded into CUDA Memory Visualizer"
msgstr "snapshot.png 已加载到 CUDA 内存可视化工具中"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The model parameters have already been loaded in memory before the training "
"step, so we see a chunk of memory devoted to the weights right off the bat. "
"As we start our forward pass, memory is allocated gradually for the "
"activations, or the tensors we are saving to be able to compute gradients in"
" the backward pass. Once we start the backward pass, the activations are "
"gradually freed while memory of the gradients starts building up."
msgstr ""
"在训练步骤之前，模型参数已经加载到内存中，因此我们立即看到一块专用于权重的内存块。随着我们开始前向传播，内存逐渐被分配用于激活，即为了能够在反向传播中计算梯度而保存的张量。一旦开始反向传播，激活逐渐被释放，同时梯度的内存开始积累。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Lastly, as the optimizer kicks in, its state will be lazily initialized, so "
"we should see the optimizer state memory gradually increase during the "
"optimizer step of the first training loop only. In future loops, the "
"optimizer memory will remain and be updated in-place. The memory for the "
"gradients is then freed accordingly at the end of every training loop when "
"``zero_grad`` is called."
msgstr ""
"最后，当优化器启动时，其状态将被延迟初始化，因此我们应该看到优化器状态内存在第一次训练循环的优化步骤中逐渐增加。在后续循环中，优化器内存会保持不变，并就地更新。梯度内存随后在每次训练循环结束时调用"
" ``zero_grad`` 时被相应释放。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Where is the memory bottleneck in this training loop? Or, in other words, "
"where is the peak memory?"
msgstr "这个训练循环中的内存瓶颈在哪里？或者换句话说，内存峰值在哪里？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The peak memory usage is during the optimizer step! Note the memory then "
"consists of ~1.2GB of parameters, ~1.2GB of gradients, and ~2.4GB=2*1.2GB of"
" the optimizer state as expected. The last ~1.2GB comes from Adam optimizer "
"requiring memory for intermediates, totaling to ~6GB of peak memory. "
"Technically, you can remove the need for the last 1.2GB for optimizer "
"intermediates if you set ``Adam(model.parameters(), foreach=False)`` which "
"would trade off runtime for memory. If switching off the ``foreach`` runtime"
" optimization is sufficient in memory savings for you, nice, but please read"
" on if you're curious how this tutorial can help you do better! With the "
"technique we will soon introduce, we will reduce peak memory by removing the"
" need for the ~1.2GB of **gradients memory** as well as **optimizer "
"intermediates memory**. Now, what would you expect the new peak memory to "
"be? The answer will be revealed in the `next` snapshot."
msgstr ""
"内存峰值是在优化器步骤期间！注意，此时内存包括约 1.2GB 的参数、约 1.2GB 的梯度和约 2.4GB=2*1.2GB "
"的优化器状态，这符合预期。最后的约 1.2GB 来自 Adam 优化器需要中间值的内存，总计约 6GB 的峰值内存。从技术上讲，如果您设置 "
"``Adam(model.parameters(), foreach=False)``，则可以消除最后 1.2GB "
"优化器中间值的需求，从而以牺牲运行时优化换取内存节省。如果关闭 ``foreach`` "
"运行时优化能够为您节约足够的内存，那很好，但如果您好奇这个教程可以帮助您做得更好，请继续阅读！通过我们将要介绍的技术，我们将通过消除约 1.2GB "
"的梯度内存以及优化器中间值内存来减少峰值内存。那么，您认为新的内存峰值会是什么？答案将在 `next` 快照中揭晓。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "DISCLAIMER: This technique is **not** for all"
msgstr "免责声明：这种技术**并非**适用于所有情况"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before we get too excited, we have to consider whether this technique is "
"applicable for `your` use case. This is NOT a silver bullet! The technique "
"of fusing the optimizer step into the backward only targets reducing "
"*gradient* memory (and as a side effect also optimizer intermediates "
"memory). Thus, the more sizable the memory taken up by the gradients, the "
"more tantamount the memory reduction. In our example above, the gradients "
"eat up 20% of the memory pie, which is quite sizable!"
msgstr ""
"在我们过于兴奋之前，我们必须考虑这种技术是否适用于您的用例。这并不是万能之策！将优化器步骤融合到反向传播中的技术只针对于减少*梯度*内存（以及其副作用：优化器中间值内存）的目的。因此，梯度占用的内存越大，内存减少就越显著。在我们上面的示例中，梯度占用了"
" 20% 的内存比例，这相当可观！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This may not be the case for you, for example, if your weights are already "
"tiny, (say, due to applying LoRa,) then the gradients do not take much space"
" in your training loop and the wins are way less exciting. In that case, you"
" should first try other techniques like activations checkpointing, "
"distributed training, quantization, or reducing the batch size. Then, when "
"the gradients are part of the bottleneck again, come back to this tutorial!"
msgstr ""
"这可能不适合您，例如，如果您的权重已经很小（例如，由于应用了 "
"LoRa），那么梯度在您的训练循环中所占空间很少，收益就少得多。在这种情况下，您应该首先尝试其他技术，如激活检查点、分布式训练、量化或减小批量大小。然后，当梯度再次成为瓶颈时，请返回本教程！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Still here? Cool, let's introduce our new "
"``register_post_accumulate_grad_hook(hook)`` API on Tensor."
msgstr ""
"仍然在这里？很好，让我们介绍新的 ``register_post_accumulate_grad_hook(hook)`` API 到张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``Tensor.register_post_accumulate_grad_hook(hook)`` API and our technique"
msgstr "``Tensor.register_post_accumulate_grad_hook(hook)`` API 和我们的技术"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our technique relies on not having to save the gradients during "
"``backward()``. Instead, once a gradient has been accumulated, we will "
"immediately apply the optimizer to the corresponding parameter and drop that"
" gradient entirely! This removes the need for holding onto a big buffer of "
"gradients until the optimizer step."
msgstr ""
"我们的技术依赖于在 ``backward()`` "
"期间不保存梯度。相反，一旦梯度累积完成，我们将立即对对应参数应用优化器，并完全丢弃该梯度！这样就不需要在优化器步骤之前保存一个大的梯度缓冲区。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So how can we unlock the behavior of applying the optimizer more eagerly? In"
" our 2.1 release, we've added a new API "
":func:`torch.Tensor.register_post_accumulate_grad_hook` that would allow us "
"to add a hook onto a Tensor once its ``.grad`` field has been accumulated. "
"We will encapsulate the optimizer step into this hook. How?"
msgstr ""
"那么我们如何实现更急切地应用优化器呢？在我们的 2.1 版本中，我们添加了一个新的 API "
":func:`torch.Tensor.register_post_accumulate_grad_hook`，允许我们在张量的 ``.grad`` "
"字段累积后附加一个钩子。我们将在这个钩子中封装优化器步骤。如何做？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How everything fits together in 10 lines"
msgstr "如何用 10 行代码实现所有组合"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember our model and optimizer setup from the beginning? I'll leave them "
"commented out below so we don't spend resources rerunning the code."
msgstr "还记得我们之前的模型和优化器设置吗？我会将它们注释掉放在下面，这样我们就不需要重新运行代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That took about 10 lines of changes in our sample model, which is neat. "
"However, for real models, it could be a fairly intrusive change to switch "
"out the optimizer for an optimizer dictionary, especially for those who use "
"``LRScheduler``s or manipulate optimizer configuration throughout the "
"training epochs. Working out this API with those changes will be more "
"involved and will likely require moving more configuration into global state"
" but should not be impossible. That said, a next step for PyTorch is to make"
" this API easier to adopt with LRSchedulers and other features you are "
"already used to."
msgstr ""
"在我们的示例模型中，这大约需要 10 行代码更改，挺整洁。然而，对于真实模型，要将优化器切换为优化器字典可能会是一项相当侵入的变更，尤其是对于那些使用 "
"``LRScheduler`` "
"或在训练周期中操控优化器配置的人来说。这些变化可能更复杂，并可能需要将更多配置移到全局状态，但不应该是不可能的。话虽如此，PyTorch 的下一步是使该"
" API 更易于与 LRScheduler 和您已经习惯的其他功能一起采用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"But let me get back to convincing you that this technique is worth it. We "
"will consult our friend, the memory snapshot."
msgstr "但让我回到说服您这种技术值得实施的问题。我们将咨询我们的朋友，内存快照。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Yes, take some time to drag your snapshot into the CUDA Memory Visualizer."
msgstr "是的，花点时间将您的快照拖到 CUDA 内存可视化工具中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Several major observations:"
msgstr "几个主要观察结果："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There is no more optimizer step! Right...we fused that into the backward."
msgstr "不再有优化器步骤了！没错……我们将其融合到反向传播中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Likewise, the backward drags longer and there are more random allocations "
"for intermediates. This is expected, as the optimizer step requires "
"intermediates."
msgstr "同样，反向传播拖得更长，并且出现了更多随机的中间值分配。这是可以预料的，因为优化器步骤需要中间值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Most importantly! The peak memory is lower! It is now ~4GB (which I hope "
"maps closely to your earlier expectation)."
msgstr "最重要的是！峰值内存更低了！现在大约是 ~4GB（希望这与您之前的预期紧密匹配）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that there is no longer any big chunk of memory allocated for the "
"gradients compared to before, accounting for ~1.2GB of memory savings. "
"Instead, we've freed each gradient very quickly after they've been computed "
"by moving the optimizer step as far ahead as we can. Woohoo! By the way, the"
" other ~1.2GB of memory savings comes from breaking apart the optimizer into"
" per-parameter optimizers, so the intermediates have proportionally shrunk. "
"This detail is `less important` than the gradient memory savings, as you can"
" get optimizer intermediates savings from just turning ``foreach=False`` "
"without this technique."
msgstr ""
"注意，与之前相比，内存中不再有为梯度分配的大块内存，这节约了约 1.2GB "
"的内存。取而代之的是，我们通过尽可能提前移动优化器步骤，非常快地释放每个计算出的梯度。太棒了！顺便说一下，另约 1.2GB "
"的内存节约来自于将优化器拆分为每参数优化器，因此中间值比例减少了。这一细节`不如`梯度内存节约重要，因为您可以仅通过设置 "
"``foreach=False`` 而无需此技术来获得优化器中间值节约。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You may be correctly wondering: if we saved 2.4GB of memory, why is the peak"
" memory NOT 6GB - 2.4GB = 3.6GB? Well, the peak has moved! The peak is now "
"near the start of the backward step, when we still have activations in "
"memory, where before, the peak was during the optimizer step when the "
"activations had been freed. The ~0.4GB difference accounting for ~4.0GB - "
"~3.6GB is thus due to the activations memory. One can then imagine that this"
" technique can be coupled with activations checkpointing for more memory "
"wins."
msgstr ""
"您可能很正确地在思考：如果我们节省了 2.4GB 的内存，为什么峰值内存不是 6GB - 2.4GB = "
"3.6GB？嗯，峰值已经移动了！峰值现在几乎出现在反向传播步骤开始时，当我们仍然有激活在内存中，而之前峰值是在优化器步骤期间，此时激活已经被释放。大约 "
"~0.4GB 的差异，约 ~4.0GB - ~3.6GB，由激活内存引起。可以想象，此技术可以与激活检查点相结合以获得更多内存收益。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we learned about the memory saving technique of fusing the"
" optimizer into the backward step through the new "
"``Tensor.register_post_accumulate_grad_hook()`` API and *when* to apply this"
" technique (when gradients memory is significant). Along the way, we also "
"learned about memory snapshots, which are generally useful in memory "
"optimization."
msgstr ""
"在本教程中，我们学习了通过新的 ``Tensor.register_post_accumulate_grad_hook()`` API "
"将优化器融合到反向传播步骤中的内存节约技术，以及*何时*应用该技术（当梯度内存占用显著时）。同时，我们还学习了内存快照，它在内存优化中通常非常有用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Total running time of the script:** ( 1 minutes  3.554 seconds)"
msgstr "**脚本总运行时间：** ( 1 分钟 3.554 秒)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: "
"optimizer_step_in_backward_tutorial.py "
"<optimizer_step_in_backward_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码： optimizer_step_in_backward_tutorial.py "
"<optimizer_step_in_backward_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: "
"optimizer_step_in_backward_tutorial.ipynb "
"<optimizer_step_in_backward_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook： optimizer_step_in_backward_tutorial.ipynb "
"<optimizer_step_in_backward_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_parametrizations.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_parametrizations.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Parametrizations Tutorial"
msgstr "参数化教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Mario Lezcano <https://github.com/lezcano>`_"
msgstr "**作者**： `Mario Lezcano <https://github.com/lezcano>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Regularizing deep-learning models is a surprisingly challenging task. "
"Classical techniques such as penalty methods often fall short when applied "
"on deep models due to the complexity of the function being optimized. This "
"is particularly problematic when working with ill-conditioned models. "
"Examples of these are RNNs trained on long sequences and GANs. A number of "
"techniques have been proposed in recent years to regularize these models and"
" improve their convergence. On recurrent models, it has been proposed to "
"control the singular values of the recurrent kernel for the RNN to be well-"
"conditioned. This can be achieved, for example, by making the recurrent "
"kernel `orthogonal <https://en.wikipedia.org/wiki/Orthogonal_matrix>`_. "
"Another way to regularize recurrent models is via \"`weight normalization "
"<https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html>`_\"."
" This approach proposes to decouple the learning of the parameters from the "
"learning of their norms.  To do so, the parameter is divided by its "
"`Frobenius norm <https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm>`_"
" and a separate parameter encoding its norm is learned. A similar "
"regularization was proposed for GANs under the name of \"`spectral "
"normalization "
"<https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html>`_\"."
" This method controls the Lipschitz constant of the network by dividing its "
"parameters by their `spectral norm "
"<https://en.wikipedia.org/wiki/Matrix_norm#Special_cases>`_, rather than "
"their Frobenius norm."
msgstr ""
"对深度学习模型进行正则化是一项非常具有挑战性的任务。当应用于深度模型时，由于优化函数的复杂性，经典的正则化技术（例如惩罚方法）通常表现不佳。这在处理病态模型时尤其成问题。这类模型的例子包括在长序列上训练的"
" RNN 和 GAN。近年来提出了许多技术来正则化这些模型并改善其收敛性。对于循环模型，已经建议控制循环核的奇异值以使 RNN "
"成为病态良好的模型。例如，这可以通过使循环核为`正交矩阵 "
"<https://en.wikipedia.org/wiki/Orthogonal_matrix>`_ 来实现。对循环模型进行正则化的另一种方法是 "
"\"`权重归一化 "
"<https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html>`_\"。该方法建议将参数的学习与其模的学习进行解耦。为此，将参数除以其"
" `Frobenius 范数 "
"<https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm>`_，并学习一个单独的参数来记录其模。对于"
" GAN，也提出了一种类似的正则化，称为 \"`谱归一化 "
"<https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html>`_\"。该方法通过将参数除以其"
" `谱范数 <https://en.wikipedia.org/wiki/Matrix_norm#Special_cases>`_ 而非其 "
"Frobenius 范数来控制网络的 Lipschitz 常数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"All these methods have a common pattern: they all transform a parameter in "
"an appropriate way before using it. In the first case, they make it "
"orthogonal by using a function that maps matrices to orthogonal matrices. In"
" the case of weight and spectral normalization, they divide the original "
"parameter by its norm."
msgstr ""
"所有这些方法都有一个共同的模式：它们都在使用参数之前以适当的方式对参数进行变换。在第一种情况下，它们通过一个将矩阵映射到正交矩阵的函数使其正交。在权重归一化和谱归一化的情况下，它们通过将原始参数除以其范数来进行调整。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"More generally, all these examples use a function to put extra structure on "
"the parameters. In other words, they use a function to constrain the "
"parameters."
msgstr "更广泛地说，所有这些例子都使用一个函数在参数上施加额外的结构。换句话说，它们使用一个函数来约束参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, you will learn how to implement and use this pattern to "
"put constraints on your model. Doing so is as easy as writing your own "
"``nn.Module``."
msgstr "在本教程中，您将学习如何实现和使用此模式来为模型施加约束。这只需要像编写自己的``nn.Module``一样简单。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Requirements: ``torch>=1.9.0``"
msgstr "要求：``torch>=1.9.0``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Implementing parametrizations by hand"
msgstr "手动实现参数化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Assume that we want to have a square linear layer with symmetric weights, "
"that is, with weights ``X`` such that ``X = Xᵀ``. One way to do so is to "
"copy the upper-triangular part of the matrix into its lower-triangular part"
msgstr ""
"假设我们想要一个带对称权重的方形线性层，也就是具有权重``X``且满足``X = Xᵀ``的层。一种方法是将矩阵的上三角部分复制到其下三角部分。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can then use this idea to implement a linear layer with symmetric weights"
msgstr "然后我们可以使用这个想法来实现一个具有对称权重的线性层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The layer can be then used as a regular linear layer"
msgstr "然后可以像普通线性层一样使用该层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This implementation, although correct and self-contained, presents a number "
"of problems:"
msgstr "尽管该实现是正确的且自包含的，但它存在一些问题："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It reimplements the layer. We had to implement the linear layer as ``x @ "
"A``. This is not very problematic for a linear layer, but imagine having to "
"reimplement a CNN or a Transformer..."
msgstr ""
"它重新实现了该层。我们不得不将线性层实现为``x @ "
"A``。对于线性层来说，这不是很麻烦，但是想象一下必须重新实现一个CNN或Transformer......"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It does not separate the layer and the parametrization.  If the "
"parametrization were more difficult, we would have to rewrite its code for "
"each layer that we want to use it in."
msgstr "它没有分离层和参数化。如果参数化更复杂，我们将不得不为每个想要使用它的层重写其代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It recomputes the parametrization every time we use the layer. If we use the"
" layer several times during the forward pass, (imagine the recurrent kernel "
"of an RNN), it would compute the same ``A`` every time that the layer is "
"called."
msgstr ""
"每次使用该层时，它都会重新计算参数化。如果我们在前向传递期间多次使用该层（想象一下RNN的循环核），它将在每次调用该层时计算相同的``A``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introduction to parametrizations"
msgstr "参数化介绍"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Parametrizations can solve all these problems as well as others."
msgstr "参数化可解决所有这些问题以及其他问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's start by reimplementing the code above using "
"``torch.nn.utils.parametrize``. The only thing that we have to do is to "
"write the parametrization as a regular ``nn.Module``"
msgstr ""
"让我们开始使用``torch.nn.utils.parametrize``重新实现上面的代码。我们只需将参数化编写为一个常规的``nn.Module``即可。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is all we need to do. Once we have this, we can transform any regular "
"layer into a symmetric layer by doing"
msgstr "这就是我们需要做的一切。一旦完成，我们可以通过以下方式将任何常规层转换为对称层："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Now, the matrix of the linear layer is symmetric"
msgstr "现在，线性层的矩阵是对称的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can do the same thing with any other layer. For example, we can create a "
"CNN with `skew-symmetric <https://en.wikipedia.org/wiki/Skew-"
"symmetric_matrix>`_ kernels. We use a similar parametrization, copying the "
"upper-triangular part with signs reversed into the lower-triangular part"
msgstr ""
"我们可以对任何其他层做同样的事情。例如，我们可以创建一个具有`反对称内核 <https://en.wikipedia.org/wiki/Skew-"
"symmetric_matrix>`_的CNN。我们使用类似的参数化，将上三角部分的符号反转后复制到下三角部分。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inspecting a parametrized module"
msgstr "检验参数化的模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When a module is parametrized, we find that the module has changed in three "
"ways:"
msgstr "当模块被参数化时，我们发现模块发生了三种变化："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``model.weight`` is now a property"
msgstr "``model.weight``现在是一个属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "It has a new ``module.parametrizations`` attribute"
msgstr "它有了一个新的``module.parametrizations``属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The unparametrized weight has been moved to "
"``module.parametrizations.weight.original``"
msgstr "未参数化的权重已被移动到``module.parametrizations.weight.original``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After parametrizing ``weight``, ``layer.weight`` is turned into a `Python "
"property <https://docs.python.org/3/library/functions.html#property>`_. This"
" property computes ``parametrization(weight)`` every time we request "
"``layer.weight`` just as we did in our implementation of ``LinearSymmetric``"
" above."
msgstr ""
"在参数化``weight``之后，``layer.weight``变成了一个`Python属性 "
"<https://docs.python.org/3/library/functions.html#property>`_。此属性每次请求``layer.weight``时，都计算``parametrization(weight)``，正如我们在上面``LinearSymmetric``的实现中所做的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Registered parametrizations are stored under a ``parametrizations`` "
"attribute within the module."
msgstr "注册的参数化存储在模块中的``parametrizations``属性下。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This ``parametrizations`` attribute is an ``nn.ModuleDict``, and it can be "
"accessed as such"
msgstr "此``parametrizations``属性是一个``nn.ModuleDict``，可以像这样访问它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Each element of this ``nn.ModuleDict`` is a ``ParametrizationList``, which "
"behaves like an ``nn.Sequential``. This list will allow us to concatenate "
"parametrizations on one weight. Since this is a list, we can access the "
"parametrizations indexing it. Here's where our ``Symmetric`` parametrization"
" sits"
msgstr ""
"此``nn.ModuleDict``的每个元素都是一个``ParametrizationList``，它的行为类似于``nn.Sequential``。此列表允许我们在一个权重上级联参数化。由于这是一个列表，我们可以通过索引访问参数化。这是我们``Symmetric``参数化所在的位置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The other thing that we notice is that, if we print the parameters, we see "
"that the parameter ``weight`` has been moved"
msgstr "我们注意到的另一件事是，如果我们打印参数，我们会看到参数``weight``已被移到："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "It now sits under ``layer.parametrizations.weight.original``"
msgstr "现在位于``layer.parametrizations.weight.original``下。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Besides these three small differences, the parametrization is doing exactly "
"the same as our manual implementation"
msgstr "除了这三个小变化，参数化与我们的手动实现完全相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Parametrizations are first-class citizens"
msgstr "参数化是一级公民"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since ``layer.parametrizations`` is an ``nn.ModuleList``, it means that the "
"parametrizations are properly registered as submodules of the original "
"module. As such, the same rules for registering parameters in a module apply"
" to register a parametrization. For example, if a parametrization has "
"parameters, these will be moved from CPU to CUDA when calling ``model = "
"model.cuda()``."
msgstr ""
"由于``layer.parametrizations``是一个``nn.ModuleList``，这意味着参数化被正确注册为原始模块的子模块。因此，注册模块中参数的相同规则也适用于注册参数化。例如，如果参数化有参数，在调用``model"
" = model.cuda()``时，这些参数会从CPU移动到CUDA。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Caching the value of a parametrization"
msgstr "缓存参数化的值"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Parametrizations come with an inbuilt caching system via the context manager"
" ``parametrize.cached()``"
msgstr "参数化通过上下文管理器``parametrize.cached()``提供了一个内置的缓存系统。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Concatenating parametrizations"
msgstr "串联参数化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Concatenating two parametrizations is as easy as registering them on the "
"same tensor. We may use this to create more complex parametrizations from "
"simpler ones. For example, the `Cayley map "
"<https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map>`_ maps the skew-"
"symmetric matrices to the orthogonal matrices of positive determinant. We "
"can concatenate ``Skew`` and a parametrization that implements the Cayley "
"map to get a layer with orthogonal weights"
msgstr ""
"串联两个参数化就像在同一个张量上注册它们一样简单。我们可以使用它从更简单的参数化创建更复杂的参数化。例如，`Cayley映射 "
"<https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map>`_将反对称矩阵映射到正定确定性的正交矩阵。我们可以串联``Skew``和实现Cayley映射的参数化，以获得具有正交权重的层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This may also be used to prune a parametrized module, or to reuse "
"parametrizations. For example, the matrix exponential maps the symmetric "
"matrices to the Symmetric Positive Definite (SPD) matrices But the matrix "
"exponential also maps the skew-symmetric matrices to the orthogonal "
"matrices. Using these two facts, we may reuse the parametrizations before to"
" our advantage"
msgstr ""
"这也可以用来修剪参数化的模块或重用参数化。例如，矩阵指数将对称矩阵映射到对称正定(SPD)矩阵。但是矩阵指数也将反对称矩阵映射到正交矩阵。利用这两点，我们可以重新使用之前的参数化以为己所用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Initializing parametrizations"
msgstr "初始化参数化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Parametrizations come with a mechanism to initialize them. If we implement a"
" method ``right_inverse`` with signature"
msgstr "参数化附带一个初始化机制。如果我们实现了一个具有以下签名的方法``right_inverse``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "it will be used when assigning to the parametrized tensor."
msgstr "它将在分配给参数化张量时使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's upgrade our implementation of the ``Skew`` class to support this"
msgstr "让我们升级``Skew``类的实现以支持此功能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We may now initialize a layer that is parametrized with ``Skew``"
msgstr "现在我们可以初始化一个由``Skew``参数化的层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This ``right_inverse`` works as expected when we concatenate "
"parametrizations. To see this, let's upgrade the Cayley parametrization to "
"also support being initialized"
msgstr "当我们串联参数化时，这个``right_inverse``可以如预期那样工作。为了看清这一点，让我们升级Cayley参数化以支持初始化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This initialization step can be written more succinctly as"
msgstr "这个初始化步骤可以更简洁地写为："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The name of this method comes from the fact that we would often expect that "
"``forward(right_inverse(X)) == X``. This is a direct way of rewriting that "
"the forward after the initialization with value ``X`` should return the "
"value ``X``. This constraint is not strongly enforced in practice. In fact, "
"at times, it might be of interest to relax this relation. For example, "
"consider the following implementation of a randomized pruning method:"
msgstr ""
"这个方法的名称来源于这样一个事实：我们通常希望``forward(right_inverse(X)) == "
"X``。这是一种直接的方式，用于说明通过值``X``的初始化后调用前向应该返回值``X``。这种约束在实践中并不严格强制执行。事实上，有时可能有兴趣放宽此关系。例如，考虑以下随机修剪方法的实现："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this case, it is not true that for every matrix A "
"``forward(right_inverse(A)) == A``. This is only true when the matrix ``A`` "
"has zeros in the same positions as the mask. Even then, if we assign a "
"tensor to a pruned parameter, it will comes as no surprise that tensor will "
"be, in fact, pruned"
msgstr ""
"在这种情况下，对于每个矩阵A来说``forward(right_inverse(A)) == "
"A``并不总是正确。这仅在矩阵``A``在掩码(Mask)相同位置具有零值时才为真。即便如此，如果我们将一个张量分配给一个修剪参数，那么毫不奇怪，这个张量实际上将被修剪。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Removing parametrizations"
msgstr "移除参数化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We may remove all the parametrizations from a parameter or a buffer in a "
"module by using ``parametrize.remove_parametrizations()``"
msgstr "我们可以通过使用``parametrize.remove_parametrizations()``从模块中的参数或缓冲区中移除所有参数化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When removing a parametrization, we may choose to leave the original "
"parameter (i.e. that in ``layer.parametriations.weight.original``) rather "
"than its parametrized version by setting the flag "
"``leave_parametrized=False``"
msgstr ""
"在移除参数化时，我们可以选择是否保留原始参数（即位于``layer.parametriations.weight.original``中的那个）而不是其参数化版本，方法是设置标志``leave_parametrized=False``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: parametrizations.py "
"<parametrizations.py>`"
msgstr ":download:`下载Python源代码: parametrizations.py <parametrizations.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: parametrizations.ipynb "
"<parametrizations.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：parametrizations.ipynb "
"<parametrizations.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_per_sample_grads.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_per_sample_grads.py>` 下载完整示例代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Per-sample-gradients"
msgstr "逐样本梯度(Per-sample-gradients)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "What is it?"
msgstr "这是什么？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Per-sample-gradient computation is computing the gradient for each and every"
" sample in a batch of data. It is a useful quantity in differential privacy,"
" meta-learning, and optimization research."
msgstr "逐样本梯度计算是为一批数据中的每个样本计算其梯度。这是在差分隐私、元学习和优化研究中有用的量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s generate a batch of dummy data and pretend that we’re working with an "
"MNIST dataset. The dummy images are 28 by 28 and we use a minibatch of size "
"64."
msgstr "让我们生成一批虚拟数据，并假装我们正在处理MNIST数据集。虚拟图像大小为28x28，我们使用了一个64大小的迷你批次。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In regular model training, one would forward the minibatch through the "
"model, and then call .backward() to compute gradients.  This would generate "
"an 'average' gradient of the entire mini-batch:"
msgstr "在常规的模型训练中，会将迷你批次前向传播到模型中，然后调用.backward()来计算梯度。这将生成整个迷你批次的'平均'梯度："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In contrast to the above approach, per-sample-gradient computation is "
"equivalent to:"
msgstr "与上述方法相反，逐样本梯度计算等效于："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"for each individual sample of the data, perform a forward and a backward "
"pass to get an individual (per-sample) gradient."
msgstr "对于数据的每个单独样本，执行一次前向和反向传播，以获得单个（逐样本）梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``sample_grads[0]`` is the per-sample-grad for model.conv1.weight. "
"``model.conv1.weight.shape`` is ``[32, 1, 3, 3]``; notice how there is one "
"gradient, per sample, in the batch for a total of 64."
msgstr ""
"``sample_grads[0]``是模型``conv1.weight``的逐样本梯度。``model.conv1.weight.shape``为``[32,"
" 1, 3, 3]``；注意批次中每个样本都有一个梯度，总共有64个。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Per-sample-grads, *the efficient way*, using function transforms"
msgstr "逐样本梯度，*更高效的方法*，使用函数变换"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can compute per-sample-gradients efficiently by using function "
"transforms."
msgstr "我们可以通过使用函数变换来高效地计算逐样本梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``torch.func`` function transform API transforms over functions. Our "
"strategy is to define a function that computes the loss and then apply "
"transforms to construct a function that computes per-sample-gradients."
msgstr "``torch.func``函数变换API作用于函数之上。我们的策略是定义一个计算损失的函数，然后应用变换以构建一个计算逐样本梯度的函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll use the ``torch.func.functional_call`` function to treat an "
"``nn.Module`` like a function."
msgstr "我们将使用``torch.func.functional_call``函数将一个``nn.Module``视为一个函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, let’s extract the state from ``model`` into two dictionaries, "
"parameters and buffers. We'll be detaching them because we won't use regular"
" PyTorch autograd (e.g. Tensor.backward(), torch.autograd.grad)."
msgstr ""
"首先，让我们将``model``中的状态提取到两个字典中：参数和缓冲区。我们将分离它们，因为我们不会使用常规的PyTorch自动求导（例如Tensor.backward()，torch.autograd.grad）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, let's define a function to compute the loss of the model given a "
"single input rather than a batch of inputs. It is important that this "
"function accepts the parameters, the input, and the target, because we will "
"be transforming over them."
msgstr ""
"接下来，让我们定义一个函数来计算模型的损失，给定单个输入，而不是一批输入。重要的是，这个函数必须接受参数、输入和目标，因为我们将对它们进行变换。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note - because the model was originally written to handle batches, we’ll use"
" ``torch.unsqueeze`` to add a batch dimension."
msgstr "注意 - 由于模型最初是为处理批次而设计的，我们将使用``torch.unsqueeze``来添加一个批次维度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, let’s use the ``grad`` transform to create a new function that computes"
" the gradient with respect to the first argument of ``compute_loss`` (i.e. "
"the ``params``)."
msgstr "现在，让我们使用``grad``变换创建一个新函数，该函数计算``compute_loss``第一个参数（即``params``）的梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``ft_compute_grad`` function computes the gradient for a single (sample,"
" target) pair. We can use ``vmap`` to get it to compute the gradient over an"
" entire batch of samples and targets. Note that ``in_dims=(None, None, 0, "
"0)`` because we wish to map ``ft_compute_grad`` over the 0th dimension of "
"the data and targets, and use the same ``params`` and buffers for each."
msgstr ""
"``ft_compute_grad``函数计算单个（样本，目标）对的梯度。我们可以使用``vmap``使其计算整个批次样本和目标的梯度。请注意，``in_dims=(None,"
" None, 0, "
"0)``，因为我们希望在数据和目标的第0维上映射``ft_compute_grad``，并为每个样本使用相同的``params``和缓冲区。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, let's used our transformed function to compute per-sample-"
"gradients:"
msgstr "最后，让我们使用转换后的函数来计算逐样本梯度："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"we can double check that the results using ``grad`` and ``vmap`` match the "
"results of hand processing each one individually:"
msgstr "我们可以双重检查，使用``grad``和``vmap``得到的结果与手动对每个样本单独处理的结果是否匹配："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A quick note: there are limitations around what types of functions can be "
"transformed by ``vmap``. The best functions to transform are ones that are "
"pure functions: a function where the outputs are only determined by the "
"inputs, and that have no side effects (e.g. mutation). ``vmap`` is unable to"
" handle mutation of arbitrary Python data structures, but it is able to "
"handle many in-place PyTorch operations."
msgstr ""
"一个快速说明：关于``vmap``可以变换的函数类型存在一些限制。最适合转换的函数是纯函数：其输出仅由输入决定，并且没有副作用（如修改操作）。``vmap``无法处理任意Python数据结构的修改，但它可以处理许多PyTorch的就地操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Performance comparison"
msgstr "性能比较"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Curious about how the performance of ``vmap`` compares?"
msgstr "好奇``vmap``的性能比较如何？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Currently the best results are obtained on newer GPU's such as the A100 "
"(Ampere) where we've seen up to 25x speedups on this example, but here are "
"some results on our build machines:"
msgstr ""
"目前，最佳结果是在新款GPU（如A100（Ampere））上获得的，在该示例中我们看到了高达25倍的速度提升，但以下是我们构建机器上的一些结果："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are other optimized solutions (like in "
"https://github.com/pytorch/opacus) to computing per-sample-gradients in "
"PyTorch that also perform better than the naive method. But it’s cool that "
"composing ``vmap`` and ``grad`` give us a nice speedup."
msgstr ""
"在 PyTorch "
"中计算每样本梯度还有其他优化的解决方案（例如：https://github.com/pytorch/opacus），这些方法性能也比普通方法要好。不过，很酷的一点是，通过组合"
" ``vmap`` 和 ``grad`` 可以带来不错的性能提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In general, vectorization with ``vmap`` should be faster than running a "
"function in a for-loop and competitive with manual batching. There are some "
"exceptions though, like if we haven’t implemented the ``vmap`` rule for a "
"particular operation or if the underlying kernels weren’t optimized for "
"older hardware (GPUs). If you see any of these cases, please let us know by "
"opening an issue at on GitHub."
msgstr ""
"总体来说，与在循环中运行函数相比，使用 ``vmap`` 进行矢量化应该更快，并且与手动批处理性能相当。不过也有一些例外，例如我们没有针对某些操作实现 "
"``vmap`` 规则，或者底层内核未针对较旧硬件（例如 GPU）优化。如果您遇到这些情况，请通过 GitHub 提交问题告诉我们。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: per_sample_grads.py "
"<per_sample_grads.py>`"
msgstr ":download:`下载 Python 源代码: per_sample_grads.py <per_sample_grads.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: per_sample_grads.ipynb "
"<per_sample_grads.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: per_sample_grads.ipynb "
"<per_sample_grads.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_pinmem_nonblock.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_pinmem_nonblock.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A guide on good usage of ``non_blocking`` and ``pin_memory()`` in PyTorch"
msgstr "关于 PyTorch 中 ``non_blocking`` 和 ``pin_memory()`` 的良好使用指南"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Transferring data from the CPU to the GPU is fundamental in many PyTorch "
"applications. It's crucial for users to understand the most effective tools "
"and options available for moving data between devices. This tutorial "
"examines two key methods for device-to-device data transfer in PyTorch: "
":meth:`~torch.Tensor.pin_memory` and :meth:`~torch.Tensor.to` with the "
"``non_blocking=True`` option."
msgstr ""
"从 CPU 到 GPU 转移数据是在许多 PyTorch 应用中都会用到的一项基础操作。用户需要理解在设备间移动数据的最有效工具和选项。本教程将探讨 "
"PyTorch 中设备间数据转移的两种关键方法：:meth:`~torch.Tensor.pin_memory` 和 "
":meth:`~torch.Tensor.to` 配合 ``non_blocking=True`` 选项。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "What you will learn"
msgstr "您将学习的内容"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimizing the transfer of tensors from the CPU to the GPU can be achieved "
"through asynchronous transfers and memory pinning. However, there are "
"important considerations:"
msgstr "通过异步传输和内存固定可以优化从 CPU 到 GPU 的张量传输。然而，需关注以下几点重要考虑事项："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using ``tensor.pin_memory().to(device, non_blocking=True)`` can be up to "
"twice as slow as a straightforward ``tensor.to(device)``."
msgstr ""
"使用 ``tensor.pin_memory().to(device, non_blocking=True)`` 可能比直接使用 "
"``tensor.to(device)`` 慢至两倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Generally, ``tensor.to(device, non_blocking=True)`` is an effective choice "
"for enhancing transfer speed."
msgstr "通常来说，``tensor.to(device, non_blocking=True)`` 是提高传输速度的有效选择。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"While ``cpu_tensor.to(\"cuda\", non_blocking=True).mean()`` executes "
"correctly, attempting ``cuda_tensor.to(\"cpu\", non_blocking=True).mean()`` "
"will result in erroneous outputs."
msgstr ""
"尽管 ``cpu_tensor.to(\"cuda\", non_blocking=True).mean()`` 会正确执行，但尝试 "
"``cuda_tensor.to(\"cpu\", non_blocking=True).mean()`` 会导致错误的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preamble"
msgstr "序言"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The performance reported in this tutorial are conditioned on the system used"
" to build the tutorial. Although the conclusions are applicable across "
"different systems, the specific observations may vary slightly depending on "
"the hardware available, especially on older hardware. The primary objective "
"of this tutorial is to offer a theoretical framework for understanding CPU "
"to GPU data transfers. However, any design decisions should be tailored to "
"individual cases and guided by benchmarked throughput measurements, as well "
"as the specific requirements of the task at hand."
msgstr ""
"本教程中报告的性能表现取决于用于构建教程的系统。尽管结论适用范围广泛，但具体观察结果可能会因硬件而略有差异，特别是在旧硬件上。本教程的主要目标是提供一种理论框架，用于理解"
" CPU 到 GPU 的数据传输。然而，任何设计决策都应根据具体情况进行调整，并以基准吞吐量测量以及任务需求为指导。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial requires tensordict to be installed. If you don't have "
"tensordict in your environment yet, install it by running the following "
"command in a separate cell:"
msgstr "本教程需要安装 tensordict。如果您的环境中还没有 tensordict，请在单独的代码单元中运行以下命令安装它："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We start by outlining the theory surrounding these concepts, and then move "
"to concrete test examples of the features."
msgstr "我们首先概述与这些概念相关的理论内容，然后转向这些功能的具体测试示例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Background"
msgstr "背景"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Memory management basics"
msgstr "内存管理基础"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When one creates a CPU tensor in PyTorch, the content of this tensor needs "
"to be placed in memory. The memory we talk about here is a rather complex "
"concept worth looking at carefully. We distinguish two types of memory that "
"are handled by the Memory Management Unit: the RAM (for simplicity) and the "
"swap space on disk (which may or may not be the hard drive). Together, the "
"available space in disk and RAM (physical memory) make up the virtual "
"memory, which is an abstraction of the total resources available. In short, "
"the virtual memory makes it so that the available space is larger than what "
"can be found on RAM in isolation and creates the illusion that the main "
"memory is larger than it actually is."
msgstr ""
"当用户在 PyTorch 中创建一个 CPU "
"张量时，这个张量的内容需要被放置在内存中。我们这里所讨论的内存概念相对复杂，值得仔细研究。我们区分由内存管理单元处理的两种类型的内存：RAM（为了简化说明）以及磁盘上的交换空间（可能是硬盘，也可能不是）。可用磁盘和"
" RAM（即物理内存）上的总空间汇总构成了虚拟内存，这是总资源一种抽象表示。简而言之，虚拟内存让可用空间看起来比单独的 RAM "
"空间要大，从而制造了主内存实际上大于其真实大小的假象。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In normal circumstances, a regular CPU tensor is pageable which means that "
"it is divided in blocks called pages that can live anywhere in the virtual "
"memory (both in RAM or on disk). As mentioned earlier, this has the "
"advantage that the memory seems larger than what the main memory actually "
"is."
msgstr ""
"正常情况下，一个普通的 CPU 张量是分页的，这意味着它被划分为称为页面的块，这些页面可以存在于虚拟内存中的任何地方（包括 RAM "
"或磁盘）。正如前面所提到的，这样的好处是内存看起来比主内存实际大小更大。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Typically, when a program accesses a page that is not in RAM, a \"page "
"fault\" occurs and the operating system (OS) then brings back this page into"
" RAM (\"swap in\" or \"page in\"). In turn, the OS may have to swap out (or "
"\"page out\") another page to make room for the new page."
msgstr ""
"通常情况下，当程序访问某个不在 RAM 中的页面时，会发生“页面错误”，然后操作系统（OS）会将该页面带回 "
"RAM（“换入”或“页面载入”）。反过来操作系统可能需要交换出（或“页面卸载”）另一个页面以给新页面腾出空间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In contrast to pageable memory, a pinned (or page-locked or non-pageable) "
"memory is a type of memory that cannot be swapped out to disk. It allows for"
" faster and more predictable access times, but has the downside that it is "
"more limited than the pageable memory (aka the main memory)."
msgstr ""
"与分页内存对比，固定（或页面锁定或非分页）内存是一种无法交换到磁盘上的内存类型。它允许更快且时间更可预测的访问，但缺点是它比分页内存（即主内存）更加有限。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "CUDA and (non-)pageable memory"
msgstr "CUDA 和（非）分页内存"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To understand how CUDA copies a tensor from CPU to CUDA, let's consider the "
"two scenarios above:"
msgstr "为了理解 CUDA 如何从 CPU 到 CUDA 复制一个张量，让我们考虑上述两种情况："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If the memory is page-locked, the device can access the memory directly in "
"the main memory. The memory addresses are well defined and functions that "
"need to read these data can be significantly accelerated."
msgstr "如果内存是页面锁定的，设备可以直接从主内存访问数据。内存地址是定义良好的，因此需要读取这些数据的函数可以显著加速。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If the memory is pageable, all the pages will have to be brought to the main"
" memory before being sent to the GPU. This operation may take time and is "
"less predictable than when executed on page-locked tensors."
msgstr "如果内存是分页的，则所有页面需要首先被载入主内存，才能被发送到 GPU。这种操作可能耗时，并且比在页面锁定张量上执行时不可预测。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"More precisely, when CUDA sends pageable data from CPU to GPU, it must first"
" create a page-locked copy of that data before making the transfer."
msgstr "更确切地说，当 CUDA 从 CPU 向 GPU 发送分页数据时，它首先需要创建该数据的页面锁定副本，然后再进行传输。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Asynchronous vs. Synchronous Operations with ``non_blocking=True`` (CUDA "
"``cudaMemcpyAsync``)"
msgstr "使用 ``non_blocking=True`` 的异步与同步操作（CUDA ``cudaMemcpyAsync``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When executing a copy from a host (such as, CPU) to a device (such as, GPU),"
" the CUDA toolkit offers modalities to do these operations synchronously or "
"asynchronously with respect to the host."
msgstr "当从主机（例如 CPU）复制到设备（例如 GPU）时，CUDA 工具包提供了同步或异步操作主机执行的方式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In practice, when calling :meth:`~torch.Tensor.to`, PyTorch always makes a "
"call to `cudaMemcpyAsync <https://docs.nvidia.com/cuda/cuda-runtime-"
"api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g85073372f776b4c4d5f89f7124b7bf79>`_."
" If ``non_blocking=False`` (default), a ``cudaStreamSynchronize`` will be "
"called after each and every ``cudaMemcpyAsync``, making the call to "
":meth:`~torch.Tensor.to` blocking in the main thread. If "
"``non_blocking=True``, no synchronization is triggered, and the main thread "
"on the host is not blocked. Therefore, from the host perspective, multiple "
"tensors can be sent to the device simultaneously, as the thread does not "
"need to wait for one transfer to be completed to initiate the other."
msgstr ""
"实际上，当调用 :meth:`~torch.Tensor.to` 时，PyTorch 总是会调用 `cudaMemcpyAsync "
"<https://docs.nvidia.com/cuda/cuda-runtime-"
"api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1g85073372f776b4c4d5f89f7124b7bf79>`_。如果"
" ``non_blocking=False``（默认），会在每次 ``cudaMemcpyAsync`` 后调用 "
"``cudaStreamSynchronize`` ，从而使对 :meth:`~torch.Tensor.to` 的调用在主线程上阻塞。如果 "
"``non_blocking=True``，不会触发同步，主机上的主线程不会阻塞。因此，从主机的角度来看，多个张量可以同时发送到设备，因为线程不需要等待一次传输完成后才能发起另一传输。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In general, the transfer is blocking on the device side (even if it isn't on"
" the host side): the copy on the device cannot occur while another operation"
" is being executed. However, in some advanced scenarios, a copy and a kernel"
" execution can be done simultaneously on the GPU side. As the following "
"example will show, three requirements must be met to enable this:"
msgstr ""
"总体而言，传输在设备端是阻塞的（即使主机端并非如此）：设备上的复制操作不能与其他操作同时发生。然而，在某些高级场景中，设备端可以同时进行复制和内核执行。如以下示例所示，需要满足三个条件才能启用此功能："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The device must have at least one free DMA (Direct Memory Access) engine. "
"Modern GPU architectures such as Volterra, Tesla, or H100 devices have more "
"than one DMA engine."
msgstr ""
"设备必须至少有一个空闲的 DMA（直接内存访问）引擎。现代 GPU 架构如 Volterra、Tesla 或 H100 设备具有多个 DMA 引擎。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The transfer must be done on a separate, non-default cuda stream. In "
"PyTorch, cuda streams can be handles using :class:`~torch.cuda.Stream`."
msgstr ""
"传输必须发生在一个单独的非默认 cuda 流上。在 PyTorch 中，cuda 流可以用 :class:`~torch.cuda.Stream` "
"进行处理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The source data must be in pinned memory."
msgstr "源数据必须位于固定内存中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We demonstrate this by running profiles on the following script."
msgstr "我们通过运行以下脚本进行配置以证明这一点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Loading these profile traces in chrome (``chrome://tracing``) shows the "
"following results: first, let's see what happens if both the arithmetic "
"operation on ``t3_cuda`` is executed after the pageable tensor is sent to "
"GPU in the main stream:"
msgstr ""
"在 chrome 中加载这些跟踪文件（``chrome://tracing``）显示以下结果：首先，让我们看看在主流中，将页面张量发送到 GPU 后对 "
"``t3_cuda`` 执行算术操作时发生了什么："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using a pinned tensor doesn't change the trace much, both operations are "
"still executed consecutively:"
msgstr "使用固定张量并未显著改变跟踪结果，同一操作仍按照顺序执行："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Sending a pageable tensor to GPU on a separate stream is also a blocking "
"operation:"
msgstr "将分页张量发送到 GPU 在单独流上也是一个阻塞操作："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Only pinned tensors copies to GPU on a separate stream overlap with another "
"cuda kernel executed on the main stream:"
msgstr "只有将固定张量复制到 GPU 并在单独流中执行重叠操作时，另一个 cuda 内核才会在主流上执行："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "A PyTorch perspective"
msgstr "PyTorch 的视角"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``pin_memory()``"
msgstr "``pin_memory()``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch offers the possibility to create and send tensors to page-locked "
"memory through the :meth:`~torch.Tensor.pin_memory` method and constructor "
"arguments. CPU tensors on a machine where CUDA is initialized can be cast to"
" pinned memory through the :meth:`~torch.Tensor.pin_memory` method. "
"Importantly, ``pin_memory`` is blocking on the main thread of the host: it "
"will wait for the tensor to be copied to page-locked memory before executing"
" the next operation. New tensors can be directly created in pinned memory "
"with functions like :func:`~torch.zeros`, :func:`~torch.ones` and other "
"constructors."
msgstr ""
"PyTorch 提供了通过 :meth:`~torch.Tensor.pin_memory` "
"方法和构造函数参数创建和发送张量到页面锁定内存的可能性。在启用了 CUDA 的机器上，CPU 张量可以通过 "
":meth:`~torch.Tensor.pin_memory` 方法转换为固定内存。重要的是，``pin_memory`` "
"在主机主线程上是阻塞的：它会等待张量被复制到页面锁定内存后再继续执行下一操作。新张量可以通过像 "
":func:`~torch.zeros`、:func:`~torch.ones` 和其他构造函数直接创建于固定内存中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let us check the speed of pinning memory and sending tensors to CUDA:"
msgstr "让我们来检查固定内存和发送张量到 CUDA 的速度："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can observe that casting a pinned-memory tensor to GPU is indeed much "
"faster than a pageable tensor, because under the hood, a pageable tensor "
"must be copied to pinned memory before being sent to GPU."
msgstr "我们可以观察到，将固定内存张量转换到 GPU 的速度确实比分页张量快，因为在底层，分页张量必须先复制到固定内存，然后再发送到 GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, contrary to a somewhat common belief, calling "
":meth:`~torch.Tensor.pin_memory()` on a pageable tensor before casting it to"
" GPU should not bring any significant speed-up, on the contrary this call is"
" usually slower than just executing the transfer. This makes sense, since "
"we're actually asking Python to execute an operation that CUDA will perform "
"anyway before copying the data from host to device."
msgstr ""
"然而，与一些常见的印象相反，在将分页张量转换到 GPU 之前调用 "
":meth:`~torch.Tensor.pin_memory()`通常不会带来任何显著的速度提升，相反，此调用通常比直接执行传输还要慢。这很合理，因为我们实际上是请求"
" Python 执行一个 CUDA 无论如何都会在主机到设备复制数据之前完成的操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The PyTorch implementation of `pin_memory "
"<https://github.com/pytorch/pytorch/blob/5298acb5c76855bc5a99ae10016efc86b27949bd/aten/src/ATen/native/Memory.cpp#L58>`_"
" which relies on creating a brand new storage in pinned memory through "
"`cudaHostAlloc <https://docs.nvidia.com/cuda/cuda-runtime-"
"api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gb65da58f444e7230d3322b6126bb4902>`_"
" could be, in rare cases, faster than transitioning data in chunks as "
"``cudaMemcpy`` does. Here too, the observation may vary depending on the "
"available hardware, the size of the tensors being sent or the amount of "
"available RAM."
msgstr ""
"PyTorch 的 `pin_memory "
"<https://github.com/pytorch/pytorch/blob/5298acb5c76855bc5a99ae10016efc86b27949bd/aten/src/ATen/native/Memory.cpp#L58>`_"
" 实现依赖于通过 `cudaHostAlloc <https://docs.nvidia.com/cuda/cuda-runtime-"
"api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1gb65da58f444e7230d3322b6126bb4902>`_"
" 创建一个全新的页面锁定存储，在少数情况下可能比如 ``cudaMemcpy`` 转换数据的方式更快。同样，这种观察可能会因可用硬件、目标张量大小或可用"
" RAM 的多少而有所不同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``non_blocking=True``"
msgstr "``non_blocking=True``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As mentioned earlier, many PyTorch operations have the option of being "
"executed asynchronously with respect to the host through the "
"``non_blocking`` argument."
msgstr "如前所述，许多 PyTorch 操作可以通过 ``non_blocking`` 参数异步地与主机执行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here, to account accurately of the benefits of using ``non_blocking``, we "
"will design a slightly more complex experiment since we want to assess how "
"fast it is to send multiple tensors to GPU with and without calling "
"``non_blocking``."
msgstr ""
"为了准确评估使用 ``non_blocking`` 的好处，我们将设计一个稍复杂的实验，因为我们希望评估使用和不使用 ``non_blocking`` "
"时发送多个张量到 GPU 的速度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To get a better sense of what is happening here, let us profile these two "
"functions:"
msgstr "为了更好地理解这里发生的情况，让我们对这两个函数进行性能剖析："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's see the call stack with a regular ``to(device)`` first:"
msgstr "首先看看调用常规 ``to(device)`` 时的调用栈："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "and now the ``non_blocking`` version:"
msgstr "然后是 ``non_blocking`` 版本："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The results are without any doubt better when using ``non_blocking=True``, "
"as all transfers are initiated simultaneously on the host side and only one "
"synchronization is done."
msgstr "毫无疑问，使用 ``non_blocking=True`` 时的结果更为优秀，因为所有传输都在主机端同时启动，并且只进行一次同步操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The benefit will vary depending on the number and the size of the tensors as"
" well as depending on the hardware being used."
msgstr "收益将因张量的数量和大小以及所使用的硬件而异。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Interestingly, the blocking ``to(\"cuda\")`` actually performs the same "
"asynchronous device casting operation (``cudaMemcpyAsync``) as the one with "
"``non_blocking=True`` with a synchronization point after each copy."
msgstr ""
"有趣的是，阻塞的 ``to(\"cuda\")`` 实际上执行与设置 ``non_blocking=True`` "
"后一样的异步设备转换操作（``cudaMemcpyAsync``），但在每次复制后都有一个同步点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Synergies"
msgstr "协同效应"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have made the point that data transfer of tensors already in "
"pinned memory to GPU is faster than from pageable memory, and that we know "
"that doing these transfers asynchronously is also faster than synchronously,"
" we can benchmark combinations of these approaches. First, let's write a "
"couple of new functions that will call ``pin_memory`` and ``to(device)`` on "
"each tensor:"
msgstr ""
"现在我们已经提出，将已经处于固定内存中的张量数据传输到 GPU "
"比从分页内存中传输更快，并且异步传输比同步传输更快，我们可以对这些方法的组合进行基准测试。首先，让我们编写几个新函数，这些函数将在每个张量上调用 "
"``pin_memory`` 和 ``to(device)``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The benefits of using :meth:`~torch.Tensor.pin_memory` are more pronounced "
"for somewhat large batches of large tensors:"
msgstr "使用 :meth:`~torch.Tensor.pin_memory` 的优点在大批量大型张量时尤为显著："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Other copy directions (GPU -> CPU, CPU -> MPS)"
msgstr "其他方向的复制操作（GPU -> CPU，CPU -> MPS）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Until now, we have operated under the assumption that asynchronous copies "
"from the CPU to the GPU are safe. This is generally true because CUDA "
"automatically handles synchronization to ensure that the data being accessed"
" is valid at read time __whenever the tensor is in pageable memory__."
msgstr ""
"到目前为止，我们假设从 CPU 到 GPU 的异步复制是安全的。这通常是正确的，因为 CUDA "
"会自动处理同步，以确保在读取时访问的数据是有效的，__只要张量处于分页内存中__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, in other cases we cannot make the same assumption: when a tensor is"
" placed in pinned memory, mutating the original copy after calling the host-"
"to-device transfer may corrupt the data received on GPU. Similarly, when a "
"transfer is achieved in the opposite direction, from GPU to CPU, or from any"
" device that is not CPU or GPU to any device that is not a CUDA-handled GPU "
"(such as, MPS), there is no guarantee that the data read on GPU is valid "
"without explicit synchronization."
msgstr ""
"然而，在其他情况下，我们不能做出同样的假设：当一个张量被放置在固定内存中时，在调用主机到设备传输之后修改原始副本可能会导致 GPU "
"上接收到的数据损坏。同样，当传输方向为从 GPU 到 CPU，或从任何非 CPU 或 GPU 的设备到非 CUDA 管理的 "
"GPU（例如，MPS）时，如果没有显式同步，就无法保证 GPU 上读取的数据是有效的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In these scenarios, these transfers offer no assurance that the copy will be"
" complete at the time of data access. Consequently, the data on the host "
"might be incomplete or incorrect, effectively rendering it garbage."
msgstr "在这些场景中，这些传输无法确保在访问数据时复制已经完成。因此，主机上的数据可能不完整或不正确，从而导致数据无效。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's first demonstrate this with a pinned-memory tensor:"
msgstr "让我们首先用一个固定内存中的张量演示这一点："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using a pageable tensor always works:"
msgstr "使用分页张量总是有效的："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now let's demonstrate that CUDA to CPU also fails to produce reliable "
"outputs without synchronization:"
msgstr "现在让我们演示从 CUDA 到 CPU 的传输在没有同步时也无法产生可靠的输出："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Generally, asynchronous copies to a device are safe without explicit "
"synchronization only when the target is a CUDA-enabled device and the "
"original tensor is in pageable memory."
msgstr "通常情况下，只有当目标是支持 CUDA 的设备且原始张量在分页内存中时，异步复制到设备才在不显式同步的情况下是安全的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In summary, copying data from CPU to GPU is safe when using "
"``non_blocking=True``, but for any other direction, ``non_blocking=True`` "
"can still be used but the user must make sure that a device synchronization "
"is executed before the data is accessed."
msgstr ""
"总之，当使用 ``non_blocking=True`` 时，从 CPU 到 GPU 的数据复制是安全的，但对于任何其他方向，虽然可以使用 "
"``non_blocking=True``，但用户必须确保在访问数据之前执行设备同步。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Practical recommendations"
msgstr "实践建议"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can now wrap up some early recommendations based on our observations:"
msgstr "根据我们的观察，我们现在可以总结一些早期的建议："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In general, ``non_blocking=True`` will provide good throughput, regardless "
"of whether the original tensor is or isn't in pinned memory. If the tensor "
"is already in pinned memory, the transfer can be accelerated, but sending it"
" to pin memory manually from python main thread is a blocking operation on "
"the host, and hence will annihilate much of the benefit of using "
"``non_blocking=True`` (as CUDA does the `pin_memory` transfer anyway)."
msgstr ""
"一般来说，使用 ``non_blocking=True`` "
"会提供良好的吞吐量，无论原始张量是否位于固定内存中。如果张量已经在固定内存中，传输可以加速，但从 Python "
"主线程手动发送到固定内存是一种阻塞操作，从而抵消了使用 ``non_blocking=True`` 的大部分优势（因为 CUDA 已经自动执行 "
"`pin_memory` 转移）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One might now legitimately ask what use there is for the "
":meth:`~torch.Tensor.pin_memory` method. In the following section, we will "
"explore further how this can be used to accelerate the data transfer even "
"more."
msgstr ""
"现在可能有人会问，:meth:`~torch.Tensor.pin_memory` "
"方法有什么用途。在接下来的部分中，我们将进一步探讨如何使用它使数据传输更快。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Additional considerations"
msgstr "额外考虑"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch notoriously provides a :class:`~torch.utils.data.DataLoader` class "
"whose constructor accepts a ``pin_memory`` argument. Considering our "
"previous discussion on ``pin_memory``, you might wonder how the "
"``DataLoader`` manages to accelerate data transfers if memory pinning is "
"inherently blocking."
msgstr ""
"PyTorch 提供了一个著名的 :class:`~torch.utils.data.DataLoader` 类，其构造函数接受一个 "
"``pin_memory`` 参数。考虑到我们之前的讨论，你可能会想知道，如果内存固定本质上是阻塞的，``DataLoader`` "
"如何管理加速数据传输。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The key lies in the DataLoader's use of a separate thread to handle the "
"transfer of data from pageable to pinned memory, thus preventing any "
"blockage in the main thread."
msgstr "关键在于 DataLoader 使用一个单独的线程来处理从分页内存到固定内存的数据传输，从而防止主线程受阻。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To illustrate this, we will use the TensorDict primitive from the homonymous"
" library. When invoking :meth:`~tensordict.TensorDict.to`, the default "
"behavior is to send tensors to the device asynchronously, followed by a "
"single call to ``torch.device.synchronize()`` afterwards."
msgstr ""
"为了说明这一点，我们将使用同名库中的 TensorDict 原语。当调用 :meth:`~tensordict.TensorDict.to` "
"时，默认行为是将张量异步发送到设备，然后进行一次 ``torch.device.synchronize()`` 调用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Additionally, ``TensorDict.to()`` includes a ``non_blocking_pin`` option  "
"which initiates multiple threads to execute ``pin_memory()`` before "
"proceeding with to ``to(device)``. This approach can further accelerate data"
" transfers, as demonstrated in the following example."
msgstr ""
"此外， ``TensorDict.to()`` 包括一个 ``non_blocking_pin`` 选项，该选项启动多个线程来执行 "
"``pin_memory()``，然后进行 ``to(device)`` 操作。此方法可以进一步加速数据传输，如以下示例所示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example, we are transferring many large tensors from the CPU to the "
"GPU. This scenario is ideal for utilizing multithreaded ``pin_memory()``, "
"which can significantly enhance performance. However, if the tensors are "
"small, the overhead associated with multithreading may outweigh the "
"benefits. Similarly, if there are only a few tensors, the advantages of "
"pinning tensors on separate threads become limited."
msgstr ""
"在此示例中，我们将许多大张量从 CPU 传输到 GPU。对于使用多线程 ``pin_memory()`` "
"的场景非常理想，这可以显著提高性能。然而，如果张量很小，与多线程相关的开销可能超过其收益。同样，如果张量数量有限，在单独线程中固定张量的优势也会受到限制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As an additional note, while it might seem advantageous to create permanent "
"buffers in pinned memory to shuttle tensors from pageable memory before "
"transferring them to the GPU, this strategy does not necessarily expedite "
"computation. The inherent bottleneck caused by copying data into pinned "
"memory remains a limiting factor."
msgstr ""
"此外，尽管在固定内存中创建永久缓冲区以从分页内存传输张量到 GPU "
"看起来很有优势，但这种策略不一定能加快计算速度。将数据复制到固定内存的固有限制仍然是一个瓶颈因素。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Moreover, transferring data that resides on disk (whether in shared memory "
"or files) to the GPU typically requires an intermediate step of copying the "
"data into pinned memory (located in RAM). Utilizing non_blocking for large "
"data transfers in this context can significantly increase RAM consumption, "
"potentially leading to adverse effects."
msgstr ""
"此外，将磁盘上的数据（无论是共享内存还是文件）传输到 GPU 通常需要一个中间步骤，即将数据复制到固定内存（位于 RAM "
"中）。在此上下文中，对大数据传输使用 ``non_blocking`` 可以显著增加 RAM 消耗，从而可能导致不利影响。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In practice, there is no one-size-fits-all solution. The effectiveness of "
"using multithreaded ``pin_memory`` combined with ``non_blocking`` transfers "
"depends on a variety of  factors, including the specific system, operating "
"system, hardware, and the nature of the tasks being executed. Here is a list"
" of factors to check when trying to speed-up data transfers between CPU and "
"GPU, or comparing throughput's across scenarios:"
msgstr ""
"实际上，并没有一种通用的解决方案。结合使用多线程 ``pin_memory`` 和 ``non_blocking`` "
"的传输效果取决于各种因素，包括特定系统、操作系统、硬件以及任务的性质。以下是一些在尝试加速 CPU 和 GPU "
"之间的数据传输或比较不同情境下的吞吐量时需要检查的因素："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Number of available cores**"
msgstr "**可用核心数量**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How many CPU cores are available? Is the system shared with other users or "
"processes that might compete for resources?"
msgstr "有多少 CPU 核心可用？系统是否与其他用户或可能竞争资源的进程共享？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Core utilization**"
msgstr "**核心利用率**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Are the CPU cores heavily utilized by other processes? Does the application "
"perform other CPU-intensive tasks concurrently with data transfers?"
msgstr "CPU 核心是否被其他进程严重利用？应用程序是否在数据传输时并发执行其他 CPU 密集型任务？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Memory utilization**"
msgstr "**内存利用率**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How much pageable and page-locked memory is currently being used? Is there "
"sufficient free memory to allocate additional pinned memory without "
"affecting system performance? Remember that nothing comes for free, for "
"instance ``pin_memory`` will consume RAM and may impact other tasks."
msgstr ""
"当前使用了多少分页内存和页面锁定内存？是否有足够的可用内存来分配额外的固定内存而不影响系统性能？请记住，一切都是有代价的，例如 "
"``pin_memory`` 会消耗 RAM 并可能影响其他任务。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**CUDA Device Capabilities**"
msgstr "**CUDA 设备能力**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Does the GPU support multiple DMA engines for concurrent data transfers? "
"What are the specific capabilities and limitations of the CUDA device being "
"used?"
msgstr "GPU 是否支持多 DMA 引擎以进行并发数据传输？使用的 CUDA 设备的具体能力和限制是什么？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Number of tensors to be sent**"
msgstr "**需要发送的张量数量**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How many tensors are transferred in a typical operation?"
msgstr "在典型操作中会传输多少张量？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Size of the tensors to be sent**"
msgstr "**需要发送的张量大小**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"What is the size of the tensors being transferred? A few large tensors or "
"many small tensors may not benefit from the same transfer program."
msgstr "传输的张量大小是多少？少量大张量或大量小张量可能无法从相同的传输方案中受益。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**System Architecture**"
msgstr "**系统架构**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How is the system's architecture influencing data transfer speeds (for "
"example, bus speeds, network latency)?"
msgstr "系统架构如何影响数据传输速度（例如，总线速度、网络延迟）？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Additionally, allocating a large number of tensors or sizable tensors in "
"pinned memory can monopolize a substantial portion of RAM. This reduces the "
"available memory for other critical operations, such as paging, which can "
"negatively impact the overall performance of an algorithm."
msgstr ""
"此外，在固定内存中分配大量张量或大尺寸张量可能会占用 RAM "
"的相当一部分。这会减少可用内存用于其他关键操作（例如分页），从而对算法的整体性能产生负面影响。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Throughout this tutorial, we have explored several critical factors that "
"influence transfer speeds and memory management when sending tensors from "
"the host to the device. We've learned that using ``non_blocking=True`` "
"generally accelerates data transfers, and that "
":meth:`~torch.Tensor.pin_memory` can also enhance performance if implemented"
" correctly. However, these techniques require careful design and calibration"
" to be effective."
msgstr ""
"在整个教程中，我们探讨了从主机向设备发送张量时影响传输速度和内存管理的几个关键因素。我们了解到使用 ``non_blocking=True`` "
"通常可以加速数据传输，并且如果正确实施，:meth:`~torch.Tensor.pin_memory` "
"也可以提高性能。然而，这些技术需要精心设计和校准才能有效。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember that profiling your code and keeping an eye on the memory "
"consumption are essential to optimize resource usage and achieve the best "
"possible performance."
msgstr "请记住，分析你的代码并关注内存消耗对于优化资源使用和实现最佳性能至关重要。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Additional resources"
msgstr "额外资源"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are dealing with issues with memory copies when using CUDA devices or"
" want to learn more about what was discussed in this tutorial, check the "
"following references:"
msgstr "如果你在使用 CUDA 设备时遇到内存复制问题，或想了解本教程中讨论的更多内容，请查看以下参考资料："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`CUDA toolkit memory management doc <https://docs.nvidia.com/cuda/cuda-"
"runtime-api/group__CUDART__MEMORY.html>`_;"
msgstr ""
"`CUDA 工具包内存管理文档 <https://docs.nvidia.com/cuda/cuda-runtime-"
"api/group__CUDART__MEMORY.html>`_;"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`CUDA pin-memory note <https://forums.developer.nvidia.com/t/pinned-"
"memory/268474>`_;"
msgstr ""
"`CUDA 固定内存说明 <https://forums.developer.nvidia.com/t/pinned-memory/268474>`_;"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`How to Optimize Data Transfers in CUDA C/C++ "
"<https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/>`_;"
msgstr ""
"`如何优化 CUDA C/C++ 中的数据传输 <https://developer.nvidia.com/blog/how-optimize-"
"data-transfers-cuda-cc/>`_;"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`tensordict doc <https://pytorch.org/tensordict/stable/index.html>`_ and "
"`repo <https://github.com/pytorch/tensordict>`_."
msgstr ""
"`tensordict 文档 <https://pytorch.org/tensordict/stable/index.html>`_ 和 `仓库 "
"<https://github.com/pytorch/tensordict>`_;"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: pinmem_nonblock.py "
"<pinmem_nonblock.py>`"
msgstr ":download:`下载 Python 源代码: pinmem_nonblock.py <pinmem_nonblock.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: pinmem_nonblock.ipynb "
"<pinmem_nonblock.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: pinmem_nonblock.ipynb "
"<pinmem_nonblock.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training Transformer models using Pipeline Parallelism"
msgstr "使用流水线并行训练Transformer模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Redirecting to the latest parallelism APIs in 3 seconds..."
msgstr "将在3秒后重定向到最新的并行API..."

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introduction to Distributed Pipeline Parallelism"
msgstr "分布式流水线并行简介"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Authors**: `Howard Huang <https://github.com/H-Huang>`_"
msgstr "**作者**：`Howard Huang <https://github.com/H-Huang>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst>`__"
" 中查看并编辑本教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial uses a gpt-style transformer model to demonstrate implementing"
" distributed pipeline parallelism with `torch.distributed.pipelining "
"<https://pytorch.org/docs/main/distributed.pipelining.html>`__ APIs."
msgstr ""
"本教程使用 gpt 风格的 transformer 模型来演示如何使用 `torch.distributed.pipelining "
"<https://pytorch.org/docs/main/distributed.pipelining.html>`__ API "
"实现分布式流水线并行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to use ``torch.distributed.pipelining`` APIs"
msgstr "如何使用 ``torch.distributed.pipelining`` API"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to apply pipeline parallelism to a transformer model"
msgstr "如何将流水线并行应用于 transformer 模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to utilize different schedules on a set of microbatches"
msgstr "如何在一组微小批次上使用不同的调度方式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Familiarity with `basic distributed training  "
"<https://pytorch.org/tutorials/beginner/dist_overview.html>`__ in PyTorch"
msgstr ""
"熟悉 PyTorch 中 `基本分布式训练 "
"<https://pytorch.org/tutorials/beginner/dist_overview.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With ``torch.distributed.pipelining`` we will be partitioning the execution "
"of a model and scheduling computation on micro-batches. We will be using a "
"simplified version of a transformer decoder model. The model architecture is"
" for educational purposes and has multiple transformer decoder layers as we "
"want to demonstrate how to split the model into different chunks. First, let"
" us define the model:"
msgstr ""
"使用 ``torch.distributed.pipelining`` 时，我们将对模型的执行进行分片，并对微型批次进行计算调度。我们将使用 "
"transformer 解码器模型的简化版本。模型架构是出于教育目的，并且涵盖了多个 transformer "
"解码器层，因为我们希望演示如何将模型分成不同的块。首先，让我们定义模型："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Then, we need to import the necessary libraries in our script and initialize"
" the distributed training process. In this case, we are defining some global"
" variables to use later in the script:"
msgstr "接下来，我们需要在脚本中导入必要库并初始化分布式训练过程。在这种情况下，我们定义了一些全局变量以供以后在脚本中使用："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``rank``, ``world_size``, and ``init_process_group()`` code should seem "
"familiar to you as those are commonly used in all distributed programs. The "
"globals specific to pipeline parallelism include ``pp_group`` which is the "
"process group that will be used for send/recv communications, "
"``stage_index`` which, in this example, is a single rank per stage so the "
"index is equivalent to the rank, and ``num_stages`` which is equivalent to "
"world_size."
msgstr ""
"``rank``、``world_size`` 和 ``init_process_group()`` "
"的代码应该对你来说很熟悉，因为这些代码是所有分布式程序中常用的。与流水线并行相关的全局变量包括 "
"``pp_group``，即将用于发送/接收通信的进程组；``stage_index``，在这个示例中，每个阶段一个 rank，因此索引等同于 "
"rank；还有 ``num_stages``，等同于 world_size。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``num_stages`` is used to set the number of stages that will be used in "
"the pipeline parallelism schedule. For example, for ``num_stages=4``, a "
"microbatch will need to go through 4 forwards and 4 backwards before it is "
"completed. The ``stage_index`` is necessary for the framework to know how to"
" communicate between stages. For example, for the first stage "
"(``stage_index=0``), it will use data from the dataloader and does not need "
"to receive data from any previous peers to perform its computation."
msgstr ""
"``num_stages`` 用于设置流水线并行计划中将使用的阶段数。例如，对于 ``num_stages=4``，一个微小批次需要经过 4 "
"次前向传播和 4 次反向传播才能完成。``stage_index`` "
"是框架知道如何在阶段之间进行通信所必需的。例如，对于第一个阶段（``stage_index=0``），它将使用数据加载器中的数据，并且不需要从任何其他进程接收数据即可执行其计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 1: Partition the Transformer Model"
msgstr "步骤 1：分割 Transformer 模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "There are two different ways of partitioning the model:"
msgstr "有两种不同的模型分割方式："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First is the manual mode in which we can manually create two instances of "
"the model by deleting portions of attributes of the model. In this example "
"for two stages (2 ranks), the model is cut in half."
msgstr ""
"第一种是手动模式，我们可以通过删除模型属性的一部分来手动创建两个模型实例。在这个示例中，对于两个阶段（2 个 rank），模型被分割为两半。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As we can see the first stage does not have the layer norm or the output "
"layer, and it only includes the first four transformer blocks. The second "
"stage does not have the input embedding layers, but includes the output "
"layers and the final four transformer blocks. The function then returns the "
"``PipelineStage`` for the current rank."
msgstr ""
"我们可以看到，第一个阶段没有层归一化或输出层，它仅包含前四个 transformer 块。第二个阶段没有输入嵌入层，但包含输出层和最后四个 "
"transformer 块。然后函数返回当前 rank 的 ``PipelineStage``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The second method is the tracer-based mode which automatically splits the "
"model based on a ``split_spec`` argument. Using the pipeline specification, "
"we can instruct ``torch.distributed.pipelining`` where to split the model. "
"In the following code block, we are splitting before the before 4th "
"transformer decoder layer, mirroring the manual split described above. "
"Similarly, we can retrieve a ``PipelineStage`` by calling ``build_stage`` "
"after this splitting is done."
msgstr ""
"第二种方法是基于追踪器的模式，它基于 ``split_spec`` 参数自动分割模型。使用流水线规范，我们可以指示 "
"``torch.distributed.pipelining`` 在模型中分割的位置。在下面的代码块中，我们在第 4 个 transformer "
"解码器层之前进行分割，与上述的手动分割类似。同样，我们可以在分割完成后通过调用 ``build_stage`` 获取 "
"``PipelineStage``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 2: Define The Main Execution"
msgstr "步骤 2：定义主要执行流程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the main function we will create a particular pipeline schedule that the "
"stages should follow. ``torch.distributed.pipelining`` supports multiple "
"schedules including supports multiple schedules, including single-stage-per-"
"rank schedules ``GPipe`` and ``1F1B``, as well as multiple-stage-per-rank "
"schedules such as ``Interleaved1F1B`` and ``LoopedBFS``."
msgstr ""
"在主函数中，我们将创建一个特定的流水线计划供各个阶段遵循。``torch.distributed.pipelining`` 支持多种计划，包括单阶段单 "
"rank 的 ``GPipe`` 和 ``1F1B``，以及多阶段单 rank 的 ``Interleaved1F1B`` 和 "
"``LoopedBFS``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example above, we are using the manual method to split the model, but"
" the code can be uncommented to also try the tracer-based model splitting "
"function. In our schedule, we need to pass in the number of microbatches and"
" the loss function used to evaluate the targets."
msgstr ""
"在上面的示例中，我们使用手动方法分割模型，但代码可以通过取消注释尝试基于追踪器的模型分割功能。在我们的计划中，我们需要传入微小批次数量以及用于评估目标的损失函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``.step()`` function processes the entire minibatch and automatically "
"splits it into microbatches based on the ``n_microbatches`` passed "
"previously. The microbatches are then operated on according to the schedule "
"class. In the example above, we are using GPipe, which follows a simple all-"
"forwards and then all-backwards schedule. The output returned from rank 1 "
"will be the same as if the model was on a single GPU and run with the entire"
" batch. Similarly, we can pass in a ``losses`` container to store the "
"corresponding losses for each microbatch."
msgstr ""
"``.step()`` 函数处理整个小批量，并根据之前传入的 ``n_microbatches`` "
"自动将其分割为微小批次。然后根据计划类对微小批次进行操作。上述示例中，我们使用 GPipe，它遵循一个简单的全前向然后全反向的计划。返回的 rank 1"
" 的输出将与模型在单 GPU 上使用整个批量运行的结果相同。同样，我们可以传入一个 ``losses`` 容器来存储每个微小批次的相应损失。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 3: Launch the Distributed Processes"
msgstr "步骤 3：启动分布式进程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, we are ready to run the script. We will use ``torchrun`` to create "
"a single host, 2-process job. Our script is already written in a way rank 0 "
"that performs the required logic for pipeline stage 0, and rank 1 performs "
"the logic for pipeline stage 1."
msgstr ""
"最后，我们准备运行脚本。我们将使用 ``torchrun`` 创建一个单主机、2 个进程的任务。我们的脚本已经写好，rank 0 执行流水线阶段 0 "
"所需的逻辑，而 rank 1 执行流水线阶段 1 的逻辑。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torchrun --nnodes 1 --nproc_per_node 2 pipelining_tutorial.py``"
msgstr "``torchrun --nnodes 1 --nproc_per_node 2 pipelining_tutorial.py``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we have learned how to implement distributed pipeline "
"parallelism using PyTorch's ``torch.distributed.pipelining`` APIs. We "
"explored setting up the environment, defining a transformer model, and "
"partitioning it for distributed training. We discussed two methods of model "
"partitioning, manual and tracer-based, and demonstrated how to schedule "
"computations on micro-batches across different stages. Finally, we covered "
"the execution of the pipeline schedule and the launch of distributed "
"processes using ``torchrun``."
msgstr ""
"在本教程中，我们学习了如何使用 PyTorch 的 ``torch.distributed.pipelining`` API "
"实现分布式流水线并行性。我们探索了如何设置环境、定义 Transformer "
"模型并将其分割用于分布式训练。我们讨论了模型分割的两种方法：手动分割和基于追踪器的分割，并演示了如何跨不同阶段对微批次进行计算计划。最后，我们介绍了流水线计划的执行以及使用"
" ``torchrun`` 启动分布式进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Additional Resources"
msgstr "附加资源"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have successfully integrated ``torch.distributed.pipelining`` into the "
"`torchtitan repository <https://github.com/pytorch/torchtitan>`__. "
"TorchTitan is a clean, minimal code base for large-scale LLM training using "
"native PyTorch. For a production ready usage of pipeline parallelism as well"
" as composition with other distributed techniques, see `TorchTitan end to "
"end example of 3D parallelism <https://github.com/pytorch/torchtitan>`__."
msgstr ""
"我们已经成功将 ``torch.distributed.pipelining`` 集成到 `torchtitan 仓库 "
"<https://github.com/pytorch/torchtitan>`__ 中。TorchTitan 是一个简洁、最小化的大规模 LLM "
"训练代码库，使用原生 PyTorch。要了解生产级流水线并行性以及与其他分布式技术的组合，请参阅 `TorchTitan 的 3D 并行性端到端示例 "
"<https://github.com/pytorch/torchtitan>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Customize Process Group Backends Using Cpp Extensions"
msgstr "使用 Cpp 扩展定制进程组后端"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Author**: `Howard Huang <https://github.com/H-Huang>`__, `Feng Tian "
"<https://github.com/ftian1>`__, `Shen Li <https://mrshenli.github.io/>`__, "
"`Min Si <https://minsii.github.io/>`__"
msgstr ""
"**作者**: `Howard Huang <https://github.com/H-Huang>`__、`Feng Tian "
"<https://github.com/ftian1>`__、`Shen Li "
"<https://mrshenli.github.io/>`__、`Min Si <https://minsii.github.io/>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst>`__."
msgstr ""
"|编辑| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst>`__"
" 中查看和编辑本教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`PyTorch Collective Communication Package "
"<https://pytorch.org/docs/stable/distributed.html>`__"
msgstr "`PyTorch 集体通信包 <https://pytorch.org/docs/stable/distributed.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`PyTorch Cpp Extension "
"<https://pytorch.org/docs/stable/cpp_extension.html>`__"
msgstr ""
"`PyTorch Cpp 扩展 <https://pytorch.org/docs/stable/cpp_extension.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Writing Distributed Applications with PyTorch "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html>`__"
msgstr ""
"`使用 PyTorch 写分布式应用程序 "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to implement a custom ``Backend`` and plug "
"that into `PyTorch distributed package "
"<https://pytorch.org/docs/stable/distributed.html>`__ using `cpp extensions "
"<https://pytorch.org/docs/stable/cpp_extension.html>`__. This is helpful "
"when you need a specialized software stack for your hardware, or when you "
"would like to experiment with new collective communication algorithms."
msgstr ""
"本教程演示如何实现一个自定义 ``Backend`` 并将其接入到 `PyTorch 分布式包 "
"<https://pytorch.org/docs/stable/distributed.html>`__ 中，使用 `Cpp 扩展 "
"<https://pytorch.org/docs/stable/cpp_extension.html>`__。这在您需要针对硬件的专用软件栈或希望尝试新的集体通信算法时非常有用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Basics"
msgstr "基础知识"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch collective communications power several widely adopted distributed "
"training features, including `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
" and `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer>`__."
" In order to make the same collective communication API work with different "
"communication backends, the distributed package abstracts collective "
"communication operations into a `Backend "
"<https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/Backend.hpp>`__"
" class. Different backends can then be implemented as subclasses of "
"``Backend`` using preferred third-party libraries. PyTorch distributed comes"
" with three default backends, ``ProcessGroupNCCL``, ``ProcessGroupGloo``, "
"and ``ProcessGroupMPI``. However, beyond these three backends, there are "
"also other communication libraries (e.g., `UCC "
"<https://github.com/openucx/ucc>`__, `OneCCL <https://github.com/oneapi-"
"src/oneCCL>`__), different types of hardware (e.g., `TPU "
"<https://cloud.google.com/tpu>`__, `Trainum <https://aws.amazon.com/machine-"
"learning/trainium/>`__), and emerging communication algorithms (e.g., "
"`Herring <https://www.amazon.science/publications/herring-rethinking-the-"
"parameter-server-at-scale-for-the-cloud>`__, `Reduction Server "
"<https://cloud.google.com/blog/topics/developers-practitioners/optimize-"
"training-performance-reduction-server-vertex-ai>`__). Therefore, the "
"distributed package exposes extension APIs to allow customizing collective "
"communication backends."
msgstr ""
"PyTorch 集体通信支持多个被广泛采用的分布式训练功能，包括 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
" 和 `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer>`__。为了使同一集体通信"
" API 在不同通信后端中工作，分布式包将集体通信操作抽象为 `Backend "
"<https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/Backend.hpp>`__"
" 类。不同的后端可以通过 ``Backend`` 的子类来实现，并使用首选的第三方库。PyTorch "
"分布式包提供了三个默认后端：``ProcessGroupNCCL``、``ProcessGroupGloo`` 和 "
"``ProcessGroupMPI``。然而，除这三个后端外，还有其他通信库（如 `UCC "
"<https://github.com/openucx/ucc>`__、`OneCCL <https://github.com/oneapi-"
"src/oneCCL>`__）、不同类型的硬件（如 `TPU <https://cloud.google.com/tpu>`__、`Trainum "
"<https://aws.amazon.com/machine-learning/trainium/>`__）以及新兴通信算法（如 `Herring "
"<https://www.amazon.science/publications/herring-rethinking-the-parameter-"
"server-at-scale-for-the-cloud>`__、`Reduction Server "
"<https://cloud.google.com/blog/topics/developers-practitioners/optimize-"
"training-performance-reduction-server-vertex-ai>`__）。因此，分布式包提供扩展 API "
"以允许定制集体通信后端。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The 4 steps below show how to implement a dummy ``Backend`` backend and use "
"that in Python application code. Please note that this tutorial focuses on "
"demonstrating the extension APIs, instead of developing a functioning "
"communication backend. Hence, the ``dummy`` backend just covers a subset of "
"the APIs (``all_reduce`` and ``all_gather``), and simply sets the values of "
"tensors to 0."
msgstr ""
"下面的 4 个步骤展示了如何实现一个示例 ``Backend`` 后端并在 Python 应用代码中使用它。请注意，本教程旨在展示扩展 "
"API，而不是开发一个可用的通信后端。因此，``dummy`` 后端仅覆盖了部分 API（``all_reduce`` 和 "
"``all_gather``），并且只是将张量的值设置为 0。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 1: Implement a Subclass of ``Backend``"
msgstr "步骤 1：实现 ``Backend`` 的子类"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This first step is to implement a ``Backend`` subclass that overrides target"
" collective communication APIs and runs the custom communication algorithm. "
"The extension also needs to implement a ``Work`` subclass, which serves as a"
" future of communication results and allows asynchronous execution in "
"application code. If the extension uses third-party libraries, it can "
"include the headers and call into the library APIs from the ``BackendDummy``"
" subclass. The two code snippets below present the implementation of "
"``dummy.h`` and ``dummy.cpp``. See the `dummy collectives "
"<https://github.com/H-Huang/torch_collective_extension>`__ repository for "
"the full implementation."
msgstr ""
"第一步是实现一个 ``Backend`` 子类，该子类覆盖目标集体通信 API 并运行自定义通信算法。扩展还需要实现 ``Work`` "
"的子类，作为通信结果的未来并允许在应用代码中异步执行。如果扩展使用了第三方库，它可以包含头文件并从 ``BackendDummy`` 子类中调用库 "
"API。以下两个代码片段展示了 ``dummy.h`` 和 ``dummy.cpp`` 的实现。查看 `dummy collectives "
"<https://github.com/H-Huang/torch_collective_extension>`__ 仓库获取完整实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 2: Expose The Extension Python APIs"
msgstr "步骤 2：暴露扩展的 Python API"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The backend constructors are called `from Python side "
"<https://github.com/pytorch/pytorch/blob/v1.9.0/torch/distributed/distributed_c10d.py#L643-L650>`__,"
" so the extension also needs to expose the constructor APIs to Python. This "
"can be done by adding the following methods. In this example, ``store`` and "
"``timeout`` are ignored by the ``BackendDummy`` instantiation method, as "
"those are not used in this dummy implementation. However, real-world "
"extensions should consider using the ``store`` to perform rendezvous and "
"supporting the ``timeout`` argument."
msgstr ""
"后端构造函数是 `从 Python 侧调用的 "
"<https://github.com/pytorch/pytorch/blob/v1.9.0/torch/distributed/distributed_c10d.py#L643-L650>`__，因此扩展也需要将构造函数"
" API 暴露给 Python。可以通过添加以下方法实现。在本例中，``store`` 和 ``timeout`` 被 ``BackendDummy``"
" 实例化方法忽略，因为这些在此示例实现中未使用。然而，真实扩展应考虑使用 ``store`` 来执行 rendezvous 并支持 "
"``timeout`` 参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 3: Build The Custom Extension"
msgstr "步骤 3：构建自定义扩展"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, the extension source code files are ready. We can then use `cpp "
"extensions <https://pytorch.org/docs/stable/cpp_extension.html>`__ to build "
"it. To do that, create a ``setup.py`` file that prepares the paths and "
"commands. Then call ``python setup.py develop`` to install the extension."
msgstr ""
"现在，扩展源代码文件已经准备好了。然后我们可以使用 `Cpp 扩展 "
"<https://pytorch.org/docs/stable/cpp_extension.html>`__ 构建它。为此，创建一个 "
"``setup.py`` 文件，准备路径和命令。然后调用 ``python setup.py develop`` 安装扩展。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If the extension depends on third-party libraries, you can also specify "
"``libraries_dirs`` and ``libraries`` to the cpp extension APIs. See the "
"`torch ucc <https://github.com/openucx/torch-ucc>`__ project as a real-world"
" example."
msgstr ""
"如果扩展依赖于第三方库，您还可以将 ``libraries_dirs`` 和 ``libraries`` 指定给 Cpp 扩展 "
"API。查看作为真实示例的 `torch ucc <https://github.com/openucx/torch-ucc>`__ 项目。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 4: Use The Extension in Application"
msgstr "步骤 4：在应用中使用扩展"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After installation, you can conveniently use the ``dummy`` backend when "
"calling `init_process_group "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group>`__"
" as if it is an builtin backend."
msgstr ""
"安装完成后，您可以在调用 `init_process_group "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group>`__"
" 时使用 ``dummy`` 后端，就像它是一个内置后端一样。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can specify dispatching based on backend by changing the ``backend`` "
"argument of ``init_process_group``. We can dispatch collective with CPU "
"tensor to ``gloo`` backend and dispatch collective with CUDA tensor to "
"``dummy`` backend by specifying ``cpu:gloo,cuda:dummy`` as the backend "
"argument."
msgstr ""
"我们可以通过更改 ``init_process_group`` 的 ``backend`` 参数来基于后端指定调度。我们可以将 CPU "
"张量的集体调度指定给 ``gloo`` 后端，将 CUDA 张量的集体调度指定给 ``dummy`` 后端，只需将 "
"``cpu:gloo,cuda:dummy`` 作为 backend 参数传入即可。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To send all tensors to ``dummy`` backend, we can simply specify ``dummy`` as"
" the backend argument."
msgstr "要将所有张量发送到 ``dummy`` 后端，我们可以简单地将 ``dummy`` 作为 backend 参数指定。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_pruning_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_pruning_tutorial.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Pruning Tutorial"
msgstr "剪枝教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Michela Paganini <https://github.com/mickypaganini>`_"
msgstr "**作者**: `Michela Paganini <https://github.com/mickypaganini>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"State-of-the-art deep learning techniques rely on over-parametrized models "
"that are hard to deploy. On the contrary, biological neural networks are "
"known to use efficient sparse connectivity. Identifying optimal techniques "
"to compress models by reducing the number of parameters in them is important"
" in order to reduce memory, battery, and hardware consumption without "
"sacrificing accuracy. This in turn allows you to deploy lightweight models "
"on device, and guarantee privacy with private on-device computation. On the "
"research front, pruning is used to investigate the differences in learning "
"dynamics between over-parametrized and under-parametrized networks, to study"
" the role of lucky sparse subnetworks and initializations (\"`lottery "
"tickets <https://arxiv.org/abs/1803.03635>`_\") as a destructive neural "
"architecture search technique, and more."
msgstr ""
"最新的深度学习技术依赖于难以部署的过参数化模型。相反，生物神经网络以高效的稀疏连接而闻名。为了减少模型中的参数数量，从而降低内存、电池和硬件消耗而不牺牲准确性，寻找最佳压缩技术非常重要。这反过来允许您在设备上部署轻量级模型，并通过私有设备计算保证隐私。在研究方面，剪枝用于研究过参数化和欠参数化网络的学习动态之间的差异，研究幸运稀疏子网络和初始化（\"彩票票理论\"）作为破坏性的神经架构搜索技术等问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, you will learn how to use ``torch.nn.utils.prune`` to "
"sparsify your neural networks, and how to extend it to implement your own "
"custom pruning technique."
msgstr ""
"在本教程中，您将学习如何使用 ``torch.nn.utils.prune`` 对神经网络进行稀疏化，并了解如何扩展它以实现您自己的剪枝技术。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Requirements"
msgstr "需求"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``\"torch>=1.4.0a0+8e8a5e0\"``"
msgstr "``\"torch>=1.4.0a0+8e8a5e0\"``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Create a model"
msgstr "创建模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we use the `LeNet "
"<http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf>`_ architecture from "
"LeCun et al., 1998."
msgstr ""
"在本教程中，我们使用1998年LeCun等人的`LeNet "
"<http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf>`_架构。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inspect a Module"
msgstr "检查模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's inspect the (unpruned) ``conv1`` layer in our LeNet model. It will "
"contain two parameters ``weight`` and ``bias``, and no buffers, for now."
msgstr "让我们检查LeNet模型中的（未剪枝的）``conv1``层。它目前只包含两个参数``weight``和``bias``，暂时没有缓冲区。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Pruning a Module"
msgstr "剪枝模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To prune a module (in this example, the ``conv1`` layer of our LeNet "
"architecture), first select a pruning technique among those available in "
"``torch.nn.utils.prune`` (or `implement <#extending-torch-nn-utils-pruning-"
"with-custom-pruning-functions>`_ your own by subclassing "
"``BasePruningMethod``). Then, specify the module and the name of the "
"parameter to prune within that module. Finally, using the adequate keyword "
"arguments required by the selected pruning technique, specify the pruning "
"parameters."
msgstr ""
"要剪枝一个模块（在本例中是LeNet架构的``conv1``层），首先从``torch.nn.utils.prune``可用的剪枝技术中选择一种（或者通过继承``BasePruningMethod``来`实现"
" <#extending-torch-nn-utils-pruning-with-custom-pruning-functions>`_ "
"自定义剪枝技术）。然后需指定模块和模块中要剪枝的参数名称。最后，使用所选剪枝技术所需的合适关键字参数，指定剪枝参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example, we will prune at random 30% of the connections in the "
"parameter named ``weight`` in the ``conv1`` layer. The module is passed as "
"the first argument to the function; ``name`` identifies the parameter within"
" that module using its string identifier; and ``amount`` indicates either "
"the percentage of connections to prune (if it is a float between 0. and 1.),"
" or the absolute number of connections to prune (if it is a non-negative "
"integer)."
msgstr ""
"在此示例中，我们将随机剪枝``conv1``层中参数``weight``的30%连接。模块作为函数的第一个参数传递；``name``通过其字符串标识符标识模块中的参数；而``amount``则指示剪枝连接的百分比（如果是0到1之间的小数）或剪枝连接的绝对数量（如果是非负整数）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Pruning acts by removing ``weight`` from the parameters and replacing it "
"with a new parameter called ``weight_orig`` (i.e. appending ``\"_orig\"`` to"
" the initial parameter ``name``). ``weight_orig`` stores the unpruned "
"version of the tensor. The ``bias`` was not pruned, so it will remain "
"intact."
msgstr ""
"剪枝通过从参数中移除``weight``并用称为``weight_orig``的新参数代替它来执行（即为初始参数``name``添加``\"_orig\"``后缀）。``weight_orig``储存未剪枝版本的张量。``bias``未被剪枝，因此将保持完整。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The pruning mask generated by the pruning technique selected above is saved "
"as a module buffer named ``weight_mask`` (i.e. appending ``\"_mask\"`` to "
"the initial parameter ``name``)."
msgstr ""
"由上面选择的剪枝技术生成的剪枝掩码会被保存为模块缓冲区，名称为``weight_mask``（即为初始参数``name``添加``\"_mask\"``后缀）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the forward pass to work without modification, the ``weight`` attribute "
"needs to exist. The pruning techniques implemented in "
"``torch.nn.utils.prune`` compute the pruned version of the weight (by "
"combining the mask with the original parameter) and store them in the "
"attribute ``weight``. Note, this is no longer a parameter of the ``module``,"
" it is now simply an attribute."
msgstr ""
"为了使前向传播能够正常工作，``weight``属性需要存在。在``torch.nn.utils.prune``中实现的剪枝技术通过结合掩码与初始参数来计算剪枝后的权重，并将其存储在``weight``属性中。注意，这已不再是模块的参数，而只是一个属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, pruning is applied prior to each forward pass using PyTorch's "
"``forward_pre_hooks``. Specifically, when the ``module`` is pruned, as we "
"have done here, it will acquire a ``forward_pre_hook`` for each parameter "
"associated with it that gets pruned. In this case, since we have so far only"
" pruned the original parameter named ``weight``, only one hook will be "
"present."
msgstr ""
"最后，剪枝在每次前向传播之前通过PyTorch的``forward_pre_hooks``进行应用。具体来说，当模块被剪枝时，如我们已完成的那样，它会为每个与其相关的被剪枝参数获取一个``forward_pre_hook``。在这种情况下，由于我们到目前为止只剪枝了称为``weight``的原始参数，所以只会存在一个钩子。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For completeness, we can now prune the ``bias`` too, to see how the "
"parameters, buffers, hooks, and attributes of the ``module`` change. Just "
"for the sake of trying out another pruning technique, here we prune the 3 "
"smallest entries in the bias by L1 norm, as implemented in the "
"``l1_unstructured`` pruning function."
msgstr ""
"为了完整性，我们现在也可以剪枝``bias``，以观察模块的参数、缓冲区、钩子和属性的变化。为了尝试另一种剪枝技术，我们在此通过L1范数剪枝``bias``中最小的3个条目，使用``l1_unstructured``剪枝函数实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We now expect the named parameters to include both ``weight_orig`` (from "
"before) and ``bias_orig``. The buffers will include ``weight_mask`` and "
"``bias_mask``. The pruned versions of the two tensors will exist as module "
"attributes, and the module will now have two ``forward_pre_hooks``."
msgstr ""
"此时，我们期望命名参数包括``weight_orig``（之前的）和``bias_orig``。缓冲区将包括``weight_mask``和``bias_mask``。两个张量的剪枝版本将作为模块属性存在，模块现在将有两个``forward_pre_hooks``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Iterative Pruning"
msgstr "迭代剪枝"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The same parameter in a module can be pruned multiple times, with the effect"
" of the various pruning calls being equal to the combination of the various "
"masks applied in series. The combination of a new mask with the old mask is "
"handled by the ``PruningContainer``'s ``compute_mask`` method."
msgstr ""
"一个模块中的相同参数可以多次剪枝，这些剪枝调用的效果等同于系列中应用的各种掩码的组合。新掩码与旧掩码的组合由``PruningContainer``的``compute_mask``方法处理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Say, for example, that we now want to further prune ``module.weight``, this "
"time using structured pruning along the 0th axis of the tensor (the 0th axis"
" corresponds to the output channels of the convolutional layer and has "
"dimensionality 6 for ``conv1``), based on the channels' L2 norm. This can be"
" achieved using the ``ln_structured`` function, with ``n=2`` and ``dim=0``."
msgstr ""
"例如，我们现在想进一步剪枝``module.weight``，这次是沿着张量的第0轴进行结构化剪枝（第0轴对应卷积层的输出通道以及``conv1``层的维度为6），基于通道的L2范数。这可以通过``ln_structured``函数实现，设置``n=2``和``dim=0``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The corresponding hook will now be of type "
"``torch.nn.utils.prune.PruningContainer``, and will store the history of "
"pruning applied to the ``weight`` parameter."
msgstr ""
"相应的钩子类型现在将是``torch.nn.utils.prune.PruningContainer``，并存储``weight``参数的剪枝历史。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Serializing a pruned model"
msgstr "序列化剪枝模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"All relevant tensors, including the mask buffers and the original parameters"
" used to compute the pruned tensors are stored in the model's ``state_dict``"
" and can therefore be easily serialized and saved, if needed."
msgstr ""
"所有相关张量，包括用于计算剪枝张量的掩码缓冲区和原始参数，都存储在模型的``state_dict``中，因此可以轻松序列化并保存（如果需要）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Remove pruning re-parametrization"
msgstr "移除剪枝重新参数化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To make the pruning permanent, remove the re-parametrization in terms of "
"``weight_orig`` and ``weight_mask``, and remove the ``forward_pre_hook``, we"
" can use the ``remove`` functionality from ``torch.nn.utils.prune``. Note "
"that this doesn't undo the pruning, as if it never happened. It simply makes"
" it permanent, instead, by reassigning the parameter ``weight`` to the model"
" parameters, in its pruned version."
msgstr ""
"为了使剪枝永久化，移除以``weight_orig``和``weight_mask``为形式的重新参数化，并移除``forward_pre_hook``，可以使用``torch.nn.utils.prune``的``remove``功能。注意，这不会撤销剪枝，使其看似从未发生过。相反，它仅使其永久化，通过将剪枝后的参数``weight``分配为模型参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Prior to removing the re-parametrization:"
msgstr "移除重新参数化之前："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "After removing the re-parametrization:"
msgstr "移除重新参数化之后："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Pruning multiple parameters in a model"
msgstr "对模型中的多个参数进行剪枝"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By specifying the desired pruning technique and parameters, we can easily "
"prune multiple tensors in a network, perhaps according to their type, as we "
"will see in this example."
msgstr "通过指定所需剪枝技术和参数，我们可以轻松剪枝网络中的多个张量，或许根据它们的类型，如以下示例所示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Global pruning"
msgstr "全局剪枝"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So far, we only looked at what is usually referred to as \"local\" pruning, "
"i.e. the practice of pruning tensors in a model one by one, by comparing the"
" statistics (weight magnitude, activation, gradient, etc.) of each entry "
"exclusively to the other entries in that tensor. However, a common and "
"perhaps more powerful technique is to prune the model all at once, by "
"removing (for example) the lowest 20% of connections across the whole model,"
" instead of removing the lowest 20% of connections in each layer. This is "
"likely to result in different pruning percentages per layer. Let's see how "
"to do that using ``global_unstructured`` from ``torch.nn.utils.prune``."
msgstr ""
"到目前为止，我们只看了通常被称为\"局部\"剪枝的内容，即逐个剪枝模型中张量的做法，通过仅将每个条目的统计数据（权重幅度、激活、梯度等）与该张量中的其他条目进行比较。然而，一种常见且可能更强大的技术是一次性剪枝整个模型，例如删除整个模型中最低的20%的连接，而不是删除每层中最低的20%的连接。这可能导致不同层剪枝的百分比不同。让我们通过使用``global_unstructured``从``torch.nn.utils.prune``进行演示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we can check the sparsity induced in every pruned parameter, which will "
"not be equal to 20% in each layer. However, the global sparsity will be "
"(approximately) 20%."
msgstr "现在我们可以检查每个剪枝参数所产生的稀疏性，这在每层中可能不会等于20%。然而，全局稀疏性将（近似）为20%。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Extending ``torch.nn.utils.prune`` with custom pruning functions"
msgstr "使用自定义剪枝函数扩展``torch.nn.utils.prune``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To implement your own pruning function, you can extend the "
"``nn.utils.prune`` module by subclassing the ``BasePruningMethod`` base "
"class, the same way all other pruning methods do. The base class implements "
"the following methods for you: ``__call__``, ``apply_mask``, ``apply``, "
"``prune``, and ``remove``. Beyond some special cases, you shouldn't have to "
"reimplement these methods for your new pruning technique. You will, however,"
" have to implement ``__init__`` (the constructor), and ``compute_mask`` (the"
" instructions on how to compute the mask for the given tensor according to "
"the logic of your pruning technique). In addition, you will have to specify "
"which type of pruning this technique implements (supported options are "
"``global``, ``structured``, and ``unstructured``). This is needed to "
"determine how to combine masks in the case in which pruning is applied "
"iteratively. In other words, when pruning a prepruned parameter, the current"
" pruning technique is expected to act on the unpruned portion of the "
"parameter. Specifying the ``PRUNING_TYPE`` will enable the "
"``PruningContainer`` (which handles the iterative application of pruning "
"masks) to correctly identify the slice of the parameter to prune."
msgstr ""
"要实现自己的剪枝函数，您可以通过继承``BasePruningMethod``基础类扩展``nn.utils.prune``模块，其他所有剪枝方法也是这样实现的。基础类为您实现了以下方法：``__call__``、``apply_mask``、``apply``、``prune``和``remove``。除了一些特殊情况外，您不需要为新剪枝技术重新实现这些方法。然而，您需要实现``__init__``（构造函数）和``compute_mask``（根据剪枝技术逻辑计算给定张量掩码的说明）。此外，您需要指定此技术所实现的剪枝类型（支持选项包括``global``、``structured``和``unstructured``）。这是为了确定在剪枝迭代应用时如何结合掩码。换句话说，当剪枝一个已剪枝的参数时，当前剪枝技术预计将在参数的未剪枝部分上起作用。指定``PRUNING_TYPE``将使``PruningContainer``（处理剪枝掩码的迭代应用）能够正确识别需要剪枝的参数切片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's assume, for example, that you want to implement a pruning technique "
"that prunes every other entry in a tensor (or -- if the tensor has "
"previously been pruned -- in the remaining unpruned portion of the tensor). "
"This will be of ``PRUNING_TYPE='unstructured'`` because it acts on "
"individual connections in a layer and not on entire units/channels "
"(``'structured'``), or across different parameters (``'global'``)."
msgstr ""
"例如，假设您想实现一种剪枝技术，该技术会剪枝张量中的每隔一个条目（或者 - 如果该张量之前已剪枝 - "
"剩余未剪枝部分）。这属于``PRUNING_TYPE='unstructured'``，因为它作用于层中的单个连接，而不是整个单元/通道（``'structured'``），也不是不同参数之间（``'global'``）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, to apply this to a parameter in an ``nn.Module``, you should also "
"provide a simple function that instantiates the method and applies it."
msgstr "现在，为了将其应用于``nn.Module``中的参数，您还需要提供一个简单的函数来实例化该方法并应用它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's try it out!"
msgstr "让我们试试看！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: pruning_tutorial.py "
"<pruning_tutorial.py>`"
msgstr ":下载:`下载Python源码: pruning_tutorial.py <pruning_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: pruning_tutorial.ipynb "
"<pruning_tutorial.ipynb>`"
msgstr ""
":下载:`下载Jupyter notebook: pruning_tutorial.ipynb <pruning_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "(beta) Quantized Transfer Learning for Computer Vision Tutorial"
msgstr "(beta) 针对计算机视觉的量化迁移学习教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To get the most of this tutorial, we suggest using this `Colab Version "
"<https://colab.research.google.com/github/pytorch/tutorials/blob/gh-"
"pages/_downloads/quantized_transfer_learning_tutorial.ipynb>`_. This will "
"allow you to experiment with the information presented below."
msgstr ""
"为了充分利用本教程，我们建议使用此`Colab版本 "
"<https://colab.research.google.com/github/pytorch/tutorials/blob/gh-"
"pages/_downloads/quantized_transfer_learning_tutorial.ipynb>`_。这样您可以体验以下呈现的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Zafar Takhirov <https://github.com/z-a-f>`_"
msgstr "**作者**: `Zafar Takhirov <https://github.com/z-a-f>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial builds on the original `PyTorch Transfer Learning "
"<https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_ "
"tutorial, written by `Sasank Chilamkurthy <https://chsasank.github.io/>`_."
msgstr ""
"本教程基于`PyTorch迁移学习 "
"<https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_教程，由`Sasank"
" Chilamkurthy <https://chsasank.github.io/>`_编写。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Transfer learning refers to techniques that make use of a pretrained model "
"for application on a different data-set. There are two main ways the "
"transfer learning is used:"
msgstr "迁移学习是指利用预训练模型在不同数据集上的应用技术。迁移学习有两种主要使用方式："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**ConvNet as a fixed feature extractor**: Here, you `“freeze” "
"<https://arxiv.org/abs/1706.04983>`_ the weights of all the parameters in "
"the network except that of the final several layers (aka “the head”, usually"
" fully connected layers). These last layers are replaced with new ones "
"initialized with random weights and only these layers are trained."
msgstr ""
"**ConvNet作为固定特征提取器**: 在这里，您`“冻结” "
"<https://arxiv.org/abs/1706.04983>`_网络中所有参数的权重，除了最后几层（称为“头部”，通常是全连接层）。这些最后的层被替换为新的层，初始化为随机权重，并仅训练这些层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Finetuning the ConvNet**: Instead of random initializaion, the model is "
"initialized using a pretrained network, after which the training proceeds as"
" usual but with a different dataset. Usually the head (or part of it) is "
"also replaced in the network in case there is a different number of outputs."
" It is common in this method to set the learning rate to a smaller number. "
"This is done because the network is already trained, and only minor changes "
"are required to \"finetune\" it to a new dataset."
msgstr ""
"**微调ConvNet**: "
"与随机初始化不同，此方法使用预训练网络进行初始化，然后像平常一样进行不同数据集的训练。然而通常网络的头部（或部分头部）也会在输出数不同的情况下被替换。在此方法中，通常会设置较小的学习率，这是因为网络已经被训练过，只需要对其进行“小幅微调”以适应新数据集。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can also combine the above two methods: First you can freeze the feature"
" extractor, and train the head. After that, you can unfreeze the feature "
"extractor (or part of it), set the learning rate to something smaller, and "
"continue training."
msgstr "您也可以结合上述两个方法：首先冻结特征提取器，训练头部。之后可以解冻特征提取器（或部分特征提取器），设置一个较小的学习率，继续训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this part you will use the first method – extracting the features using a"
" quantized model."
msgstr "在这一部分，您将使用第一种方法——通过量化模型提取特征。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Part 0. Prerequisites"
msgstr "第0部分. 前置条件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before diving into the transfer learning, let us review the "
"\"prerequisites\", such as installations and data loading/visualizations."
msgstr "在深入了解迁移学习之前，让我们回顾一下“先决条件”，例如安装和数据加载/可视化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Installing the Nightly Build"
msgstr "安装测试版"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Because you will be using the beta parts of the PyTorch, it is recommended "
"to install the latest version of ``torch`` and ``torchvision``. You can find"
" the most recent instructions on local installation `here "
"<https://pytorch.org/get-started/locally/>`_. For example, to install "
"without GPU support:"
msgstr ""
"因为您将使用 PyTorch 的测试版部分，建议安装最新版本的 ``torch`` 和 ``torchvision``。您可以在 `这里 "
"<https://pytorch.org/get-started/locally/>`_ 找到最新的本地安装说明。例如，若不需要 GPU "
"支持，可以安装："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Load Data"
msgstr "加载数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This section is identical to the original transfer learning tutorial."
msgstr "本部分与原始迁移学习教程相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will use ``torchvision`` and ``torch.utils.data`` packages to load the "
"data."
msgstr "我们将使用 ``torchvision`` 和 ``torch.utils.data`` 包来加载数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The problem you are going to solve today is classifying **ants** and "
"**bees** from images. The dataset contains about 120 training images each "
"for ants and bees. There are 75 validation images for each class. This is "
"considered a very small dataset to generalize on. However, since we are "
"using transfer learning, we should be able to generalize reasonably well."
msgstr ""
"您今天将解决的问题是从图像中分类 **蚂蚁** 和 **蜜蜂**。数据集包含每类约 120 张训练图像，每类 75 "
"张验证图像。这被认为是一个用于泛化的非常小的数据集。然而，由于我们使用迁移学习，应该能够合理地泛化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "*This dataset is a very small subset of imagenet.*"
msgstr "*此数据集是 imagenet 的非常小的子集。*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Download the data from `here "
"<https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_ and extract "
"it to the ``data`` directory."
msgstr ""
"从 `这里 <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_ "
"下载数据并将其解压到 ``data`` 目录。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Visualize a few images"
msgstr "可视化几张图像"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s visualize a few training images so as to understand the data "
"augmentations."
msgstr "让我们可视化几张训练图像，以便了解数据增强。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Support Function for Model Training"
msgstr "模型训练的支持函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Below is a generic function for model training. This function also"
msgstr "下面是一个用于模型训练的通用函数。该函数还"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Schedules the learning rate"
msgstr "调度学习率"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Saves the best model"
msgstr "保存最佳模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Support Function for Visualizing the Model Predictions"
msgstr "可视化模型预测的支持函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Generic function to display predictions for a few images"
msgstr "通用函数，用于显示几个图像的预测结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Part 1. Training a Custom Classifier based on a Quantized Feature Extractor"
msgstr "第一部分: 基于量化特征提取器的自定义分类器训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section you will use a “frozen” quantized feature extractor, and "
"train a custom classifier head on top of it. Unlike floating point models, "
"you don’t need to set requires_grad=False for the quantized model, as it has"
" no trainable parameters. Please, refer to the `documentation "
"<https://pytorch.org/docs/stable/quantization.html>`_ for more details."
msgstr ""
"在此部分中，您将使用一个“冻结”的量化特征提取器，并在其之上训练一个自定义分类器头。与浮点模型不同，您不需要为量化模型设置 "
"requires_grad=False，因为它没有可训练的参数。请参考 `文档 "
"<https://pytorch.org/docs/stable/quantization.html>`_ 了解更多详情。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Load a pretrained model: for this exercise you will be using `ResNet-18 "
"<https://pytorch.org/hub/pytorch_vision_resnet/>`_."
msgstr ""
"加载预训练模型：在本练习中您将使用 `ResNet-18 "
"<https://pytorch.org/hub/pytorch_vision_resnet/>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At this point you need to modify the pretrained model. The model has the "
"quantize/dequantize blocks in the beginning and the end. However, because "
"you will only use the feature extractor, the dequantization layer has to "
"move right before the linear layer (the head). The easiest way to do that is"
" to wrap the model in the ``nn.Sequential`` module."
msgstr ""
"此时您需要修改预训练模型。模型在开头和结尾具有量化/反量化模块。然而，因为您只使用特征提取器，反量化层需要移到线性层（头部）之前。最简单的方法是将模型包装在"
" ``nn.Sequential`` 模块中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The first step is to isolate the feature extractor in the ResNet model. "
"Although in this example you are tasked to use all layers except ``fc`` as "
"the feature extractor, in reality, you can take as many parts as you need. "
"This would be useful in case you would like to replace some of the "
"convolutional layers as well."
msgstr ""
"第一步是将 ResNet 模型中的特征提取器隔离出来。尽管在本例中您需要使用所有层（除了 "
"``fc``），实际上，您可以根据需要提取任意部分。如果您希望替换一些卷积层，这将非常有用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When separating the feature extractor from the rest of a quantized model, "
"you have to manually place the quantizer/dequantized in the beginning and "
"the end of the parts you want to keep quantized."
msgstr "在将特征提取器从量化模型的其余部分分离时，您必须手动在希望保持量化的部分开始和结束处放置量化/反量化器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The function below creates a model with a custom head."
msgstr "以下函数创建一个具有自定义头的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Currently the quantized models can only be run on CPU. However, it is "
"possible to send the non-quantized parts of the model to a GPU."
msgstr "目前量化模型只能在 CPU 上运行。然而，可以将模型中非量化的部分发送到 GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Train and evaluate"
msgstr "训练和评估"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This step takes around 15-25 min on CPU. Because the quantized model can "
"only run on the CPU, you cannot run the training on GPU."
msgstr "此步骤在 CPU 上约需 15-25 分钟。因为量化模型只能在 CPU 上运行，您无法在 GPU 上进行训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Part 2. Finetuning the Quantizable Model"
msgstr "第二部分：微调可量化模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this part, we fine tune the feature extractor used for transfer learning,"
" and quantize the feature extractor. Note that in both part 1 and 2, the "
"feature extractor is quantized. The difference is that in part 1, we use a "
"pretrained quantized model. In this part, we create a quantized feature "
"extractor after fine tuning on the data-set of interest, so this is a way to"
" get better accuracy with transfer learning while having the benefits of "
"quantization. Note that in our specific example, the training set is really "
"small (120 images) so the benefits of fine tuning the entire model is not "
"apparent. However, the procedure shown here will improve accuracy for "
"transfer learning with larger datasets."
msgstr ""
"在此部分，我们微调用于迁移学习的特征提取器，并量化特征提取器。请注意，在第一部分和第二部分中，特征提取器都是量化的。不同之处在于第一部分使用预训练的量化模型。在本部分中，我们在感兴趣的数据集上微调后创建一个量化特征提取器，因此这是在享受量化优势的同时利用迁移学习提高精度的方法。请注意，在我们的具体示例中，训练集非常小（120"
" 张图像），因此微调整个模型的好处并不明显。但此处展示的流程将改进使用更大数据集的迁移学习的准确性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The pretrained feature extractor must be quantizable. To make sure it is "
"quantizable, perform the following steps:"
msgstr "预训练的特征提取器必须是可量化的。为确保它是可量化的，请执行以下步骤："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Fuse ``(Conv, BN, ReLU)``, ``(Conv, BN)``, and ``(Conv, ReLU)`` using "
"``torch.quantization.fuse_modules``."
msgstr ""
"使用 ``torch.quantization.fuse_modules`` 融合 ``(Conv, BN, ReLU)``, ``(Conv, "
"BN)``, 和 ``(Conv, ReLU)``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Connect the feature extractor with a custom head. This requires dequantizing"
" the output of the feature extractor."
msgstr "将特征提取器与一个自定义头部连接。这需要对特征提取器的输出进行反量化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Insert fake-quantization modules at appropriate locations in the feature "
"extractor to mimic quantization during training."
msgstr "在特征提取器中适当位置插入伪量化模块，以在训练期间模拟量化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For step (1), we use models from ``torchvision/models/quantization``, which "
"have a member method ``fuse_model``. This function fuses all the ``conv``, "
"``bn``, and ``relu`` modules. For custom models, this would require calling "
"the ``torch.quantization.fuse_modules`` API with the list of modules to fuse"
" manually."
msgstr ""
"对于步骤 (1)，我们使用 ``torchvision/models/quantization`` 中的模型，这些模型有一个成员方法 "
"``fuse_model``。此函数会融合所有的 ``conv``、``bn`` 和 ``relu`` 模块。对于自定义模型，这需要使用模块列表手动调用"
" ``torch.quantization.fuse_modules`` API。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Step (2) is performed by the ``create_combined_model`` function used in the "
"previous section."
msgstr "步骤 (2) 由上一节中使用的 ``create_combined_model`` 函数执行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Step (3) is achieved by using ``torch.quantization.prepare_qat``, which "
"inserts fake-quantization modules."
msgstr "使用 ``torch.quantization.prepare_qat`` 实现步骤 (3)，它会插入伪量化模块。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As step (4), you can start \"finetuning\" the model, and after that convert "
"it to a fully quantized version (Step 5)."
msgstr "作为步骤 (4)，您可以开始“微调”模型，之后将其转换为完全量化版本（步骤 5）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To convert the fine tuned model into a quantized model you can call the "
"``torch.quantization.convert`` function (in our case only the feature "
"extractor is quantized)."
msgstr ""
"要将微调后的模型转换为量化模型，可以调用 ``torch.quantization.convert`` 函数（在我们的例子中，仅特征提取器被量化）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Because of the random initialization your results might differ from the "
"results shown in this tutorial."
msgstr "由于随机初始化，您的结果可能与本教程中显示的结果有所不同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Finetuning the model"
msgstr "微调模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the current tutorial the whole model is fine tuned. In general, this will"
" lead to higher accuracy. However, due to the small training set used here, "
"we end up overfitting to the training set."
msgstr "在当前教程中，整个模型都被微调。一般来说，这会导致更高的准确性。然而，由于这里使用的小训练集，我们最终会过拟合到训练集。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 4. Fine tune the model"
msgstr "步骤 4：微调模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Step 5. Convert to quantized model"
msgstr "步骤 5：转换为量化模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Lets see how the quantized model performs on a few images"
msgstr "让我们看看量化模型在几张图像上的表现"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Real Time Inference on Raspberry Pi 4 (30 fps!)"
msgstr "Raspberry Pi 4 上的实时推断（30 帧/秒！）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Tristan Rice <https://github.com/d4l3k>`_"
msgstr "**作者**：`Tristan Rice <https://github.com/d4l3k>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch has out of the box support for Raspberry Pi 4. This tutorial will "
"guide you on how to setup a Raspberry Pi 4 for running PyTorch and run a "
"MobileNet v2 classification model in real time (30 fps+) on the CPU."
msgstr ""
"PyTorch 原生支持 Raspberry Pi 4。本教程将指导您如何设置 Raspberry Pi 4 以运行 PyTorch，并在 CPU "
"上实时运行 MobileNet v2 分类模型（30 帧/秒以上）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This was all tested with Raspberry Pi 4 Model B 4GB but should work with the"
" 2GB variant as well as on the 3B with reduced performance."
msgstr "所有测试均使用 Raspberry Pi 4 Model B 4GB，但也可以在 2GB 版本及性能较低的 3B 上运行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Prerequisites"
msgstr "前提条件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To follow this tutorial you'll need a Raspberry Pi 4, a camera for it and "
"all the other standard accessories."
msgstr "要跟随本教程，您需要 Raspberry Pi 4、一台相机以及所有其他标准配件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Raspberry Pi 4 Model B 2GB+ "
"<https://www.raspberrypi.com/products/raspberry-pi-4-model-b/>`_"
msgstr ""
"`Raspberry Pi 4 Model B 2GB+ "
"<https://www.raspberrypi.com/products/raspberry-pi-4-model-b/>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Raspberry Pi Camera Module <https://www.raspberrypi.com/products/camera-"
"module-v2/>`_"
msgstr ""
"`Raspberry Pi Camera Module <https://www.raspberrypi.com/products/camera-"
"module-v2/>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Heat sinks and Fan (optional but recommended)"
msgstr "散热片和风扇（可选但推荐）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "5V 3A USB-C Power Supply"
msgstr "5V 3A USB-C 电源适配器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "SD card (at least 8gb)"
msgstr "SD 卡（至少 8GB）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "SD card read/writer"
msgstr "SD 卡读卡器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Raspberry Pi 4 Setup"
msgstr "Raspberry Pi 4 设置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch only provides pip packages for Arm 64bit (aarch64) so you'll need to"
" install a 64 bit version of the OS on your Raspberry Pi"
msgstr ""
"PyTorch 仅提供适用于 Arm 64 位 (aarch64) 的 pip 包，因此您需要在 Raspberry Pi 上安装 64 "
"位版本的操作系统。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can download the latest arm64 Raspberry Pi OS from "
"https://downloads.raspberrypi.org/raspios_arm64/images/ and install it via "
"rpi-imager."
msgstr ""
"您可以从 https://downloads.raspberrypi.org/raspios_arm64/images/ 下载最新的 arm64 "
"Raspberry Pi OS，并通过 rpi-imager 安装它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**32-bit Raspberry Pi OS will not work.**"
msgstr "**32 位的 Raspberry Pi OS 无法工作。**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Installation will take at least a few minutes depending on your internet "
"speed and sdcard speed. Once it's done it should look like:"
msgstr "安装至少需要几分钟，具体取决于您的网络速度和 SD 卡速度。安装完成后应该看起来像这样："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Time to put your sdcard in your Raspberry Pi, connect the camera and boot it"
" up."
msgstr "现在是时候将您的 SD 卡插入 Raspberry Pi，连接相机并启动了。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Once that boots and you complete the initial setup you'll need to edit the "
"``/boot/config.txt`` file to enable the camera."
msgstr "启动后并完成初始设置后，您需要编辑 ``/boot/config.txt`` 文件以启用摄像头。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And then reboot. After you reboot the video4linux2 device ``/dev/video0`` "
"should exist."
msgstr "然后重启。重启后，video4linux2 设备 ``/dev/video0`` 应该会存在。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Installing PyTorch and OpenCV"
msgstr "安装 PyTorch 和 OpenCV"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch and all the other libraries we need have ARM 64-bit/aarch64 variants"
" so you can just install them via pip and have it work like any other Linux "
"system."
msgstr ""
"PyTorch 及我们需要的所有其他库都有 ARM 64 位/aarch64 变体，因此您可以通过 pip 安装并像在其他 Linux 系统上一样工作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We can now check that everything installed correctly:"
msgstr "我们现在可以检查是否所有内容正确安装："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Video Capture"
msgstr "视频捕获"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For video capture we're going to be using OpenCV to stream the video frames "
"instead of the more common ``picamera``. `picamera` isn't available on "
"64-bit Raspberry Pi OS and it's much slower than OpenCV. OpenCV directly "
"accesses the ``/dev/video0`` device to grab frames."
msgstr ""
"对于视频捕获，我们将使用 OpenCV 流式传输视频帧，而不是更常见的 ``picamera``。64 位 Raspberry Pi OS 上不支持 "
"`picamera`，且其速度比 OpenCV 慢得多。OpenCV 直接访问 ``/dev/video0`` 设备以抓取帧。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The model we're using (MobileNetV2) takes in image sizes of ``224x224`` so "
"we can request that directly from OpenCV at 36fps. We're targeting 30fps for"
" the model but we request a slightly higher framerate than that so there's "
"always enough frames."
msgstr ""
"我们使用的模型（MobileNetV2）接收 ``224x224`` 的图像尺寸，因此我们可以直接从 OpenCV 请求此大小的帧，帧率为 "
"36fps。我们目标是让模型在 30 帧每秒的速率运行，但请求稍高的帧率以确保始终有足够的帧。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"OpenCV returns a ``numpy`` array in BGR so we need to read and do a bit of "
"shuffling to get it into the expected RGB format."
msgstr "OpenCV 返回一个 BGR 格式的 ``numpy`` 数组，因此我们需要读取并做些调整以将其转换为所需的 RGB 格式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This data reading and processing takes about ``3.5 ms``."
msgstr "此数据读取和处理大约需要 ``3.5 毫秒``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Image Preprocessing"
msgstr "图像预处理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We need to take the frames and transform them into the format the model "
"expects. This is the same processing as you would do on any machine with the"
" standard torchvision transforms."
msgstr "我们需要将帧转换为模型所需的格式。这与任何机器上标准 torchvision 转换的处理方式相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model Choices"
msgstr "模型选择"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There's a number of models you can choose from to use with different "
"performance characteristics. Not all models provide a ``qnnpack`` pretrained"
" variant so for testing purposes you should chose one that does but if you "
"train and quantize your own model you can use any of them."
msgstr ""
"您可以选择许多模型以满足不同的性能特性。并非所有模型都提供 ``qnnpack`` "
"预训练变体，因此出于测试目的，建议选择一个具有此功能的模型，但如果您训练并量化了自己的模型，可以使用其中任何一个。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We're using ``mobilenet_v2`` for this tutorial since it has good performance"
" and accuracy."
msgstr "本教程中我们使用的是 ``mobilenet_v2``，因为它具有良好的性能和准确性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Raspberry Pi 4 Benchmark Results:"
msgstr "Raspberry Pi 4 基准结果："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model"
msgstr "模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FPS"
msgstr "帧率（FPS）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Total Time (ms/frame)"
msgstr "每帧总时间（毫秒）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Model Time (ms/frame)"
msgstr "每帧模型时间（毫秒）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "qnnpack Pretrained"
msgstr "qnnpack 预训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "mobilenet_v2"
msgstr "mobilenet_v2"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "33.7"
msgstr "33.7"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "29.7"
msgstr "29.7"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "26.4"
msgstr "26.4"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "True"
msgstr "True"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "mobilenet_v3_large"
msgstr "mobilenet_v3_large"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "29.3"
msgstr "29.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "34.1"
msgstr "34.1"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "30.7"
msgstr "30.7"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "resnet18"
msgstr "resnet18"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "9.2"
msgstr "9.2"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "109.0"
msgstr "109.0"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "100.3"
msgstr "100.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "False"
msgstr "False"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "resnet50"
msgstr "resnet50"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "4.3"
msgstr "4.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "233.9"
msgstr "233.9"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "225.2"
msgstr "225.2"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "resnext101_32x8d"
msgstr "resnext101_32x8d"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1.1"
msgstr "1.1"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "892.5"
msgstr "892.5"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "885.3"
msgstr "885.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "inception_v3"
msgstr "inception_v3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "4.9"
msgstr "4.9"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "204.1"
msgstr "204.1"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "195.5"
msgstr "195.5"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "googlenet"
msgstr "googlenet"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "7.4"
msgstr "7.4"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "135.3"
msgstr "135.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "132.0"
msgstr "132.0"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "shufflenet_v2_x0_5"
msgstr "shufflenet_v2_x0_5"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "46.7"
msgstr "46.7"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "21.4"
msgstr "21.4"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "18.2"
msgstr "18.2"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "shufflenet_v2_x1_0"
msgstr "shufflenet_v2_x1_0"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "24.4"
msgstr "24.4"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "41.0"
msgstr "41.0"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "37.7"
msgstr "37.7"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "shufflenet_v2_x1_5"
msgstr "shufflenet_v2_x1_5"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "16.8"
msgstr "16.8"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "59.6"
msgstr "59.6"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "56.3"
msgstr "56.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "shufflenet_v2_x2_0"
msgstr "shufflenet_v2_x2_0"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "11.6"
msgstr "11.6"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "86.3"
msgstr "86.3"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "82.7"
msgstr "82.7"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "MobileNetV2: Quantization and JIT"
msgstr "MobileNetV2：量化和 JIT"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For optimal performance we want a model that's quantized and fused. "
"Quantized means that it does the computation using int8 which is much more "
"performant than the standard float32 math. Fused means that consecutive "
"operations have been fused together into a more performant version where "
"possible. Commonly things like activations (``ReLU``) can be merged into the"
" layer before (``Conv2d``) during inference."
msgstr ""
"为了获得最佳性能，我们希望使用一个量化和融合的模型。量化意味着它使用 int8 进行计算，这比标准 float32 "
"计算更高效。融合意味着将连续的操作融合为更高效的版本（如果可能）。例如，推理期间，通常会将激活函数（``ReLU``）与前一层（``Conv2d``）合并。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The aarch64 version of pytorch requires using the ``qnnpack`` engine."
msgstr "aarch64 版本的 pytorch 需要使用 ``qnnpack`` 引擎。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For this example we'll use a prequantized and fused version of MobileNetV2 "
"that's provided out of the box by torchvision."
msgstr "对于本示例，我们将使用由 torchvision 提供的、量化和融合的 MobileNetV2 预训练版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We then want to jit the model to reduce Python overhead and fuse any ops. "
"Jit gives us ~30fps instead of ~20fps without it."
msgstr "然后我们希望通过 jit 减少 Python 开销并融合任何操作。Jit 使我们达到 ~30fps，而未使用时则仅为 ~20fps。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Putting It Together"
msgstr "将其整合在一起"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We can now put all the pieces together and run it:"
msgstr "现在我们可以将所有部分整合在一起并运行："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Running it shows that we're hovering at ~30 fps."
msgstr "运行结果显示我们维持在 ~30fps 左右。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is with all the default settings in Raspberry Pi OS. If you disabled "
"the UI and all the other background services that are enabled by default "
"it's more performant and stable."
msgstr "这是在树莓派操作系统中使用所有默认设置的表现。如果您禁用了用户界面和所有其他默认启用的后台服务，性能会更好且更稳定。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "If we check ``htop`` we see that we have almost 100% utilization."
msgstr "如果我们检查 ``htop``，会发现几乎有100%的利用率。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To verify that it's working end to end we can compute the probabilities of "
"the classes and `use the ImageNet class labels "
"<https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_ to print the "
"detections."
msgstr ""
"为了验证其端到端工作，我们可以计算类别的概率，并使用`ImageNet类别标签 "
"<https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_来打印检测结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``mobilenet_v3_large`` running in real time:"
msgstr "实时运行的``mobilenet_v3_large``:"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Detecting an orange:"
msgstr "检测橙子："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Detecting a mug:"
msgstr "检测杯子："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Troubleshooting: Performance"
msgstr "故障排除：性能问题"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch by default will use all of the cores available. If you have anything"
" running in the background on the Raspberry Pi it may cause contention with "
"the model inference causing latency spikes. To alleviate this you can reduce"
" the number of threads which will reduce the peak latency at a small "
"performance penalty."
msgstr ""
"PyTorch会默认使用所有可用的核心。如果在树莓派上有任何后台运行的程序，可能会与模型推断冲突，从而导致延迟波动。为了解决此问题，您可以减少线程数，这将以小幅性能损失换取降低最高延迟。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For ``shufflenet_v2_x1_5`` using ``2 threads`` instead of ``4 threads`` "
"increases best case latency to ``72 ms`` from ``60 ms`` but eliminates the "
"latency spikes of ``128 ms``."
msgstr ""
"对于 "
"``shufflenet_v2_x1_5``，使用``2个线程``而不是``4个线程``，虽然将最佳延迟从``60毫秒``增加到``72毫秒``，但消除了``128毫秒``的延迟波动。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Next Steps"
msgstr "下一步"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can create your own model or fine tune an existing one. If you fine tune"
" on one of the models from `torchvision.models.quantized "
"<https://pytorch.org/vision/stable/models.html#quantized-models>`_ most of "
"the work to fuse and quantize has already been done for you so you can "
"directly deploy with good performance on a Raspberry Pi."
msgstr ""
"您可以创建自己的模型或微调现有模型。如果您使用`torchvision.models.quantized "
"<https://pytorch.org/vision/stable/models.html#quantized-"
"models>`_中的模型进行微调，大部分用于融合和量化的工作已为您完成，因此您可以直接部署到树莓派上并获得良好的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "See more:"
msgstr "查看更多："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Quantization <https://pytorch.org/docs/stable/quantization.html>`_ for more"
" information on how to quantize and fuse your model."
msgstr ""
"`量化 <https://pytorch.org/docs/stable/quantization.html>`_ 了解如何量化和融合您的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Transfer Learning Tutorial "
"<https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_ "
"for how to use transfer learning to fine tune a pre-existing model to your "
"dataset."
msgstr ""
"`迁移学习教程 "
"<https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html>`_ "
"了解如何使用迁移学习将预先存在的模型微调到您的数据集上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_reinforcement_ppo.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_reinforcement_ppo.py>` 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Reinforcement Learning (PPO) with TorchRL Tutorial"
msgstr "使用TorchRL的强化学习（PPO）教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to use PyTorch and :py:mod:`torchrl` to train"
" a parametric policy network to solve the Inverted Pendulum task from the "
"`OpenAI-Gym/Farama-Gymnasium control library <https://github.com/Farama-"
"Foundation/Gymnasium>`__."
msgstr ""
"本教程演示了如何使用PyTorch和:py:mod:`torchrl`训练一个参数化策略网络解决来自`OpenAI-Gym/Farama-"
"Gymnasium控制库 <https://github.com/Farama-Foundation/Gymnasium>`__的倒立摆任务。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inverted pendulum"
msgstr "倒立摆"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to create an environment in TorchRL, transform its outputs, and collect "
"data from this environment;"
msgstr "如何在TorchRL中创建环境、转换其输出并从该环境中收集数据；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to make your classes talk to each other using "
":class:`~tensordict.TensorDict`;"
msgstr "如何使用:class:`~tensordict.TensorDict`让您的类相互通信；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The basics of building your training loop with TorchRL:"
msgstr "使用TorchRL构建训练循环的基础知识："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to compute the advantage signal for policy gradient methods;"
msgstr "如何为策略梯度方法计算优势信号；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to create a stochastic policy using a probabilistic neural network;"
msgstr "如何使用概率神经网络创建随机策略；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to create a dynamic replay buffer and sample from it without repetition."
msgstr "如何创建动态重放缓冲区并从中采样而无重复。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We will cover six crucial components of TorchRL:"
msgstr "我们将涵盖TorchRL的六个关键组件："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`environments <https://pytorch.org/rl/reference/envs.html>`__"
msgstr "`环境 <https://pytorch.org/rl/reference/envs.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`transforms <https://pytorch.org/rl/reference/envs.html#transforms>`__"
msgstr "`转换 <https://pytorch.org/rl/reference/envs.html#transforms>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`models (policy and value function) "
"<https://pytorch.org/rl/reference/modules.html>`__"
msgstr "`模型（策略和价值函数） <https://pytorch.org/rl/reference/modules.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`loss modules <https://pytorch.org/rl/reference/objectives.html>`__"
msgstr "`损失模块 <https://pytorch.org/rl/reference/objectives.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`data collectors <https://pytorch.org/rl/reference/collectors.html>`__"
msgstr "`数据收集器 <https://pytorch.org/rl/reference/collectors.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`replay buffers <https://pytorch.org/rl/reference/data.html#replay-"
"buffers>`__"
msgstr "`重放缓冲区 <https://pytorch.org/rl/reference/data.html#replay-buffers>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a "
"batch of data is being collected and directly consumed to train the policy "
"to maximise the expected return given some proximality constraints. You can "
"think of it as a sophisticated version of `REINFORCE "
"<https://link.springer.com/content/pdf/10.1007/BF00992696.pdf>`_, the "
"foundational policy-optimization algorithm. For more information, see the "
"`Proximal Policy Optimization Algorithms "
"<https://arxiv.org/abs/1707.06347>`_ paper."
msgstr ""
"近端策略优化（PPO）是一种策略梯度算法，先收集一批数据然后直接用于训练策略以在一定的近端性约束下最大化期望回报。您可以将其视为`REINFORCE "
"<https://link.springer.com/content/pdf/10.1007/BF00992696.pdf>`_（基础策略优化算法）的一个复杂版本。更多信息请参阅`近端策略优化算法"
" <https://arxiv.org/abs/1707.06347>`_论文。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PPO is usually regarded as a fast and efficient method for online, on-policy"
" reinforcement algorithm. TorchRL provides a loss-module that does all the "
"work for you, so that you can rely on this implementation and focus on "
"solving your problem rather than re-inventing the wheel every time you want "
"to train a policy."
msgstr ""
"PPO通常被认为是一种快速高效的在线、基于策略的强化算法。TorchRL提供了一个无需您再做任何工作即可运行的损失模块，因此您可以依靠此实现专注于解决问题，而无需每次训练策略时重新发明轮子。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For completeness, here is a brief overview of what the loss computes, even "
"though this is taken care of by our :class:`~torchrl.objectives.ClipPPOLoss`"
" module—the algorithm works as follows: 1. we will sample a batch of data by"
" playing the policy in the environment for a given number of steps. 2. Then,"
" we will perform a given number of optimization steps with random sub-"
"samples of this batch using a clipped version of the REINFORCE loss. 3. The "
"clipping will put a pessimistic bound on our loss: lower return estimates "
"will be favored compared to higher ones. The precise formula of the loss is:"
msgstr ""
"为了完整性，这里是对损失计算的简要概述，尽管这些已由我们的:class:`~torchrl.objectives.ClipPPOLoss`模块处理——算法的运行方式如下：1."
" 我们通过在环境中运行策略一定步数抽取一批数据。2. "
"随后，我们对该批数据进行若干次优化步骤，使用修剪版本的REINFORCE损失对该批数据的随机子批优化。3. "
"修剪操作会对我们的损失设置一个悲观界限：较低的回报估计比较高的更受青睐。损失的精确公式是："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"L(s,a,\\theta_k,\\theta) = \\min\\left(\n"
"\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n"
"g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n"
"\\right),"
msgstr ""
"L(s,a,\\theta_k,\\theta) = \\min\\left(\n"
"\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n"
"g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n"
"\\right),"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are two components in that loss: in the first part of the minimum "
"operator, we simply compute an importance-weighted version of the REINFORCE "
"loss (for example, a REINFORCE loss that we have corrected for the fact that"
" the current policy configuration lags the one that was used for the data "
"collection). The second part of that minimum operator is a similar loss "
"where we have clipped the ratios when they exceeded or were below a given "
"pair of thresholds."
msgstr ""
"在这个损失中有两个组成部分：在最小函数的第一项中，我们简单计算了一个经过重要性加权的REINFORCE损失（例如，我们修正了当前策略配置与用于收集数据时策略配置之间的迟滞差异）。最小函数的第二部分是一个相似的损失，但我们在比给定阈值高或低时对比率进行了修剪。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This loss ensures that whether the advantage is positive or negative, policy"
" updates that would produce significant shifts from the previous "
"configuration are being discouraged."
msgstr "此损失可确保无论优势为正还是负，都会阻止产生重大偏移于之前配置的策略更新。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This tutorial is structured as follows:"
msgstr "本教程结构如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, we will define a set of hyperparameters we will be using for "
"training."
msgstr "首先，我们将定义训练中使用的一组超参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we will focus on creating our environment, or simulator, using "
"TorchRL's wrappers and transforms."
msgstr "接下来，我们将专注于使用TorchRL的封装器和转换器来创建我们的环境或模拟器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we will design the policy network and the value model, which is "
"indispensable to the loss function. These modules will be used to configure "
"our loss module."
msgstr "然后，我们将设计策略网络和价值模型，这对于损失函数来说是必不可少的。这些模块将用于配置我们的损失模块。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Next, we will create the replay buffer and data loader."
msgstr "接下来，我们将创建重放缓冲区和数据加载器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Finally, we will run our training loop and analyze the results."
msgstr "最后，我们将运行训练循环并分析结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Throughout this tutorial, we'll be using the :mod:`tensordict` library. "
":class:`~tensordict.TensorDict` is the lingua franca of TorchRL: it helps us"
" abstract what a module reads and writes and care less about the specific "
"data description and more about the algorithm itself."
msgstr ""
"在本教程中，我们将使用:mod:`tensordict`库。:class:`~tensordict.TensorDict`是TorchRL的通用语言：它帮助我们抽象模块的读写内容，减少对具体数据描述的关注，并更多关注算法本身。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Define Hyperparameters"
msgstr "定义超参数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We set the hyperparameters for our algorithm. Depending on the resources "
"available, one may choose to execute the policy on GPU or on another device."
" The ``frame_skip`` will control how for how many frames is a single action "
"being executed. The rest of the arguments that count frames must be "
"corrected for this value (since one environment step will actually return "
"``frame_skip`` frames)."
msgstr ""
"我们为算法设置超参数。根据可用的资源，可以选择在GPU或其他设备上执行策略。``frame_skip``将控制单个动作被执行的帧数。所有计算帧的参数必须根据该值进行修正（因为一个环境步骤实际上返回``frame_skip``帧）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Data collection parameters"
msgstr "数据收集参数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When collecting data, we will be able to choose how big each batch will be "
"by defining a ``frames_per_batch`` parameter. We will also define how many "
"frames (such as the number of interactions with the simulator) we will allow"
" ourselves to use. In general, the goal of an RL algorithm is to learn to "
"solve the task as fast as it can in terms of environment interactions: the "
"lower the ``total_frames`` the better."
msgstr ""
"在收集数据时，我们可以通过定义一个``frames_per_batch``参数来选择每个批次的大小。我们还将定义允许使用的帧数（即与模拟器的交互次数）。一般来说，强化学习算法的目标是在环境交互次数上尽快解决任务：``total_frames``越低越好。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PPO parameters"
msgstr "PPO参数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At each data collection (or batch collection) we will run the optimization "
"over a certain number of *epochs*, each time consuming the entire data we "
"just acquired in a nested training loop. Here, the ``sub_batch_size`` is "
"different from the ``frames_per_batch`` here above: recall that we are "
"working with a \"batch of data\" coming from our collector, which size is "
"defined by ``frames_per_batch``, and that we will further split in smaller "
"sub-batches during the inner training loop. The size of these sub-batches is"
" controlled by ``sub_batch_size``."
msgstr ""
"在每次数据收集（或批量收集）时，我们将对数据进行一定数量的*训练周期*优化，每次在嵌套训练循环中消耗我们刚刚获得的所有数据。这里，``sub_batch_size``不同于上述的``frames_per_batch``：记住，我们与收集器处理的“数据批量”大小是``frames_per_batch``，但我们将在内层训练循环中进一步分割成更小的子批，这些子批的大小由``sub_batch_size``控制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Define an environment"
msgstr "定义一个环境"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In RL, an *environment* is usually the way we refer to a simulator or a "
"control system. Various libraries provide simulation environments for "
"reinforcement learning, including Gymnasium (previously OpenAI Gym), "
"DeepMind control suite, and many others. As a general library, TorchRL's "
"goal is to provide an interchangeable interface to a large panel of RL "
"simulators, allowing you to easily swap one environment with another. For "
"example, creating a wrapped gym environment can be achieved with few "
"characters:"
msgstr ""
"在强化学习中，称为*环境*的通常指模拟器或控制系统。各种库为强化学习提供了模拟环境，包括Gymnasium（前OpenAI "
"Gym）、DeepMind控制套件等。作为一个通用库，TorchRL的目标是为大量的RL模拟器提供一个可互换接口，让您可以轻松地用一个环境替换另一个。例如，创建一个封装的gym环境可以用很少代码完成："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are a few things to notice in this code: first, we created the "
"environment by calling the ``GymEnv`` wrapper. If extra keyword arguments "
"are passed, they will be transmitted to the ``gym.make`` method, hence "
"covering the most common environment construction commands. Alternatively, "
"one could also directly create a gym environment using ``gym.make(env_name, "
"**kwargs)`` and wrap it in a `GymWrapper` class."
msgstr ""
"在此代码中需要注意的几点：首先，我们通过调用``GymEnv``封装器创建了环境。如果传递了额外的关键词参数，它们将被传递到``gym.make``方法，因此涵盖了最常见的环境构造命令。或者，也可以直接使用``gym.make(env_name,"
" **kwargs)``创建gym环境并将其封装在`GymWrapper`类中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Also the ``device`` argument: for gym, this only controls the device where "
"input action and observed states will be stored, but the execution will "
"always be done on CPU. The reason for this is simply that gym does not "
"support on-device execution, unless specified otherwise. For other "
"libraries, we have control over the execution device and, as much as we can,"
" we try to stay consistent in terms of storing and execution backends."
msgstr ""
"还有``device``参数：对于gym，这仅控制输入动作和观察状态将存储的设备，但执行将始终在CPU上完成。其原因只是因为gym不支持设备上的执行，除非另行指定。对于其他库，我们可以控制执行设备，并尽可能在存储和执行后端保持一致性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Transforms"
msgstr "转换"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will append some transforms to our environments to prepare the data for "
"the policy. In Gym, this is usually achieved via wrappers. TorchRL takes a "
"different approach, more similar to other pytorch domain libraries, through "
"the use of transforms. To add transforms to an environment, one should "
"simply wrap it in a :class:`~torchrl.envs.transforms.TransformedEnv` "
"instance and append the sequence of transforms to it. The transformed "
"environment will inherit the device and meta-data of the wrapped "
"environment, and transform these depending on the sequence of transforms it "
"contains."
msgstr ""
"我们将给环境附加一些转换来为策略准备数据。在Gym中，这通常通过封装器实现。TorchRL采用了一种不同的方法，更类似于其他PyTorch领域库，通过使用转换器。为了向环境添加转换，应简单地将其封装在:class:`~torchrl.envs.transforms.TransformedEnv`实例中，并将转换序列附加到其上。转换后的环境将继承封装环境的设备和元数据，并根据其包含的转换序列转换这些属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Normalization"
msgstr "归一化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The first to encode is a normalization transform. As a rule of thumbs, it is"
" preferable to have data that loosely match a unit Gaussian distribution: to"
" obtain this, we will run a certain number of random steps in the "
"environment and compute the summary statistics of these observations."
msgstr "首先编码的是归一化转换。通常来说，建议数据与单位高斯分布大致匹配：为此，我们将在环境中运行若干随机步骤并计算这些观察值的统计摘要。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll append two other transforms: the "
":class:`~torchrl.envs.transforms.DoubleToFloat` transform will convert "
"double entries to single-precision numbers, ready to be read by the policy. "
"The :class:`~torchrl.envs.transforms.StepCounter` transform will be used to "
"count the steps before the environment is terminated. We will use this "
"measure as a supplementary measure of performance."
msgstr ""
"我们将附加另外两个转换：:class:`~torchrl.envs.transforms.DoubleToFloat`转换将双精度条目转换为单精度数，以便于策略读取。而:class:`~torchrl.envs.transforms.StepCounter`转换将用于计算环境终止之前的步骤数量。我们将使用此测量作为性能的补充测度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As we will see later, many of the TorchRL's classes rely on "
":class:`~tensordict.TensorDict` to communicate. You could think of it as a "
"python dictionary with some extra tensor features. In practice, this means "
"that many modules we will be working with need to be told what key to read "
"(``in_keys``) and what key to write (``out_keys``) in the ``tensordict`` "
"they will receive. Usually, if ``out_keys`` is omitted, it is assumed that "
"the ``in_keys`` entries will be updated in-place. For our transforms, the "
"only entry we are interested in is referred to as ``\"observation\"`` and "
"our transform layers will be told to modify this entry and this entry only:"
msgstr ""
"正如我们稍后将看到的，许多TorchRL类依赖于:class:`~tensordict.TensorDict`进行通信。可以将其视为一个具有额外张量功能的Python字典。实际上，这意味着我们使用的许多模块需要指定从`tensordict`中读取的键(``in_keys``)以及写入的键(``out_keys``)。通常，如果省略``out_keys``，则假定更新``in_keys``条目。对于我们的转换层，我们只感兴趣一个称为``\"observation\"``的条目，并指示我们的转换层仅修改该条目："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As you may have noticed, we have created a normalization layer but we did "
"not set its normalization parameters. To do this, "
":class:`~torchrl.envs.transforms.ObservationNorm` can automatically gather "
"the summary statistics of our environment:"
msgstr ""
"正如您可能注意到的，我们创建了一个归一化层，但是尚未设置其归一化参数。为此，:class:`~torchrl.envs.transforms.ObservationNorm`可以自动收集我们环境的汇总统计数据："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The :class:`~torchrl.envs.transforms.ObservationNorm` transform has now been"
" populated with a location and a scale that will be used to normalize the "
"data."
msgstr ""
":class:`~torchrl.envs.transforms.ObservationNorm`转换现在已填充用于归一化数据的位置和比例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let us do a little sanity check for the shape of our summary stats:"
msgstr "让我们做一个小的检查，验证我们的汇总统计的形状："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"An environment is not only defined by its simulator and transforms, but also"
" by a series of metadata that describe what can be expected during its "
"execution. For efficiency purposes, TorchRL is quite stringent when it comes"
" to environment specs, but you can easily check that your environment specs "
"are adequate. In our example, the :class:`~torchrl.envs.libs.gym.GymWrapper`"
" and :class:`~torchrl.envs.libs.gym.GymEnv` that inherits from it already "
"take care of setting the proper specs for your environment so you should not"
" have to care about this."
msgstr ""
"一个环境不仅由其模拟器和转换层定义，还由描述其执行过程中可以预期内容的一系列元数据定义。为了提高效率，TorchRL对环境规格有一定的严格要求，但可以很容易地检查您的环境规格是否合适。在我们的示例中，继承自:class:`~torchrl.envs.libs.gym.GymWrapper`和:class:`~torchrl.envs.libs.gym.GymEnv`已经处理了设置环境的正确规格，因此您通常不需要担心这些事情。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Nevertheless, let's see a concrete example using our transformed environment"
" by looking at its specs. There are three specs to look at: "
"``observation_spec`` which defines what is to be expected when executing an "
"action in the environment, ``reward_spec`` which indicates the reward domain"
" and finally the ``input_spec`` (which contains the ``action_spec``) and "
"which represents everything an environment requires to execute a single "
"step."
msgstr ""
"尽管如此，让我们通过查看其规格来具体了解使用我们的转换环境的情况。有三个规格需要注意：``observation_spec``定义在环境中执行动作时的预期内容，``reward_spec``表示奖励的范围，最后是``input_spec``（包含``action_spec``），表示环境执行单步所需的所有内容。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"the :func:`check_env_specs` function runs a small rollout and compares its "
"output against the environment specs. If no error is raised, we can be "
"confident that the specs are properly defined:"
msgstr ""
":func:`check_env_specs`函数通过小规模的试运行并将其输出与环境规格进行比较。如果没有出现错误，我们可以确信这些规格定义是正确的："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For fun, let's see what a simple random rollout looks like. You can call "
"`env.rollout(n_steps)` and get an overview of what the environment inputs "
"and outputs look like. Actions will automatically be drawn from the action "
"spec domain, so you don't need to care about designing a random sampler."
msgstr ""
"为了趣味，让我们看看一个简单的随机试运行会是什么样子。可以调用`env.rollout(n_steps)`并查看环境输入和输出的概览。动作将自动从动作规格域中抽取，因此不需要设计随机采样器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Typically, at each step, an RL environment receives an action as input, and "
"outputs an observation, a reward and a done state. The observation may be "
"composite, meaning that it could be composed of more than one tensor. This "
"is not a problem for TorchRL, since the whole set of observations is "
"automatically packed in the output :class:`~tensordict.TensorDict`. After "
"executing a rollout (for example, a sequence of environment steps and random"
" action generations) over a given number of steps, we will retrieve a "
":class:`~tensordict.TensorDict` instance with a shape that matches this "
"trajectory length:"
msgstr ""
"通常，在每一步中，RL环境接收一个动作作为输入，并输出一个观察值、奖励以及完成状态。观察值可能是复合的，这意味着它可能由多个张量组成。这对于TorchRL来说不是问题，因为整个观察值集会自动打包到输出:class:`~tensordict.TensorDict`中。在执行试运行（例如，环境步和随机动作生成的序列）后，我们会获取一个:class:`~tensordict.TensorDict`实例，其形状与此轨迹长度相匹配："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our rollout data has a shape of ``torch.Size([3])``, which matches the "
"number of steps we ran it for. The ``\"next\"`` entry points to the data "
"coming after the current step. In most cases, the ``\"next\"`` data at time "
"`t` matches the data at ``t+1``, but this may not be the case if we are "
"using some specific transformations (for example, multi-step)."
msgstr ""
"我们的试运行数据形状为``torch.Size([3])``，与我们运行的步数相匹配。``\"next\"``条目指向当前步之后的数据。在大多数情况下，时间`t`的``\"next\"``数据与`t+1``的数据一致，但如果我们使用某些特定的转换（例如多步），可能不会如此。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PPO utilizes a stochastic policy to handle exploration. This means that our "
"neural network will have to output the parameters of a distribution, rather "
"than a single value corresponding to the action taken."
msgstr "PPO利用一种随机策略来进行探索。这意味着我们的神经网络需要输出一个分布的参数，而不是一个单一的值来表示所采取的动作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the data is continuous, we use a Tanh-Normal distribution to respect the "
"action space boundaries. TorchRL provides such distribution, and the only "
"thing we need to care about is to build a neural network that outputs the "
"right number of parameters for the policy to work with (a location, or mean,"
" and a scale):"
msgstr ""
"由于数据是连续性，我们使用Tanh-"
"正态分布来遵守动作空间的边界。TorchRL提供了这样的分布，我们需要关注的仅仅是构建能够输出正确数量参数的神经网络，以供策略使用（即位置或均值，以及一个比例）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), "
"\\sigma^{+}_{\\theta}(\\text{observation})"
msgstr ""
"f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), "
"\\sigma^{+}_{\\theta}(\\text{observation})"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The only extra-difficulty that is brought up here is to split our output in "
"two equal parts and map the second to a strictly positive space."
msgstr "此处唯一的额外难点是将输出拆分为两部分，并将第二部分映射到一个严格正值的空间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We design the policy in three steps:"
msgstr "我们分三步设计策略："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Define a neural network ``D_obs`` -> ``2 * D_action``. Indeed, our ``loc`` "
"(mu) and ``scale`` (sigma) both have dimension ``D_action``."
msgstr ""
"定义一个神经网络 ``D_obs`` -> ``2 * "
"D_action``。实际上，我们的``loc``（均值）和``scale``（标准差）都具有维度``D_action``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Append a :class:`~tensordict.nn.distributions.NormalParamExtractor` to "
"extract a location and a scale (for example, splits the input in two equal "
"parts and applies a positive transformation to the scale parameter)."
msgstr ""
"附加一个:class:`~tensordict.nn.distributions.NormalParamExtractor`用于提取位置和比例（例如，将输入拆分为两部分，并对比例参数应用一个正值转换）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Create a probabilistic :class:`~tensordict.nn.TensorDictModule` that can "
"generate this distribution and sample from it."
msgstr "创建一个概率性的:class:`~tensordict.nn.TensorDictModule`，它能够生成该分布并从中进行采样。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To enable the policy to \"talk\" with the environment through the "
"``tensordict`` data carrier, we wrap the ``nn.Module`` in a "
":class:`~tensordict.nn.TensorDictModule`. This class will simply ready the "
"``in_keys`` it is provided with and write the outputs in-place at the "
"registered ``out_keys``."
msgstr ""
"为了使策略可以通过``tensordict``数据载体与环境进行“对话”，我们将``nn.Module``包装在:class:`~tensordict.nn.TensorDictModule`中。这个类只需要读取给定的``in_keys``并将输出写入到注册的``out_keys``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We now need to build a distribution out of the location and scale of our "
"normal distribution. To do so, we instruct the "
":class:`~torchrl.modules.tensordict_module.ProbabilisticActor` class to "
"build a :class:`~torchrl.modules.TanhNormal` out of the location and scale "
"parameters. We also provide the minimum and maximum values of this "
"distribution, which we gather from the environment specs."
msgstr ""
"现在我们需要基于正态分布的位置和比例创建一个分布。为此，我们指示:class:`~torchrl.modules.tensordict_module.ProbabilisticActor`类构建一个:class:`~torchrl.modules.TanhNormal`，并提供该分布的最小值和最大值，这些值可以从环境规格中获取。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The name of the ``in_keys`` (and hence the name of the ``out_keys`` from the"
" :class:`~tensordict.nn.TensorDictModule` above) cannot be set to any value "
"one may like, as the :class:`~torchrl.modules.TanhNormal` distribution "
"constructor will expect the ``loc`` and ``scale`` keyword arguments. That "
"being said, :class:`~torchrl.modules.tensordict_module.ProbabilisticActor` "
"also accepts ``Dict[str, str]`` typed ``in_keys`` where the key-value pair "
"indicates what ``in_key`` string should be used for every keyword argument "
"that is to be used."
msgstr ""
"``in_keys``的名称（以及上面:class:`~tensordict.nn.TensorDictModule`的``out_keys``名称）不能随意设置，因为:class:`~torchrl.modules.TanhNormal`分布构造函数将需要``loc``和``scale``关键词参数。不过，:class:`~torchrl.modules.tensordict_module.ProbabilisticActor`也接受类型为``Dict[str,"
" str]``的``in_keys``，其中键值对指明每个关键词参数使用什么``in_key``字符串。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Value network"
msgstr "价值网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The value network is a crucial component of the PPO algorithm, even though "
"it won't be used at inference time. This module will read the observations "
"and return an estimation of the discounted return for the following "
"trajectory. This allows us to amortize learning by relying on the some "
"utility estimation that is learned on-the-fly during training. Our value "
"network share the same structure as the policy, but for simplicity we assign"
" it its own set of parameters."
msgstr ""
"价值网络是PPO算法的一个关键部分，尽管它不会在推理时使用。该模块读取观察值并返回接下来轨迹的折现回报估计值。这使我们在训练过程中通过学习在线估计的某些效用来缓解学习负担。我们的价值网络与策略具有相同的结构，但为了简便，我们为它分配了一组独立的参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"let's try our policy and value modules. As we said earlier, the usage of "
":class:`~tensordict.nn.TensorDictModule` makes it possible to directly read "
"the output of the environment to run these modules, as they know what "
"information to read and where to write it:"
msgstr ""
"让我们尝试一下我们的策略和价值模块。正如我们之前所说，使用:class:`~tensordict.nn.TensorDictModule`使这两个模块可以直接读取环境的输出进行操作，因为它们知道要读取什么信息以及将写入何处："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Data collector"
msgstr "数据收集器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRL provides a set of `DataCollector classes "
"<https://pytorch.org/rl/reference/collectors.html>`__. Briefly, these "
"classes execute three operations: reset an environment, compute an action "
"given the latest observation, execute a step in the environment, and repeat "
"the last two steps until the environment signals a stop (or reaches a done "
"state)."
msgstr ""
"TorchRL提供了一组`DataCollector类 "
"<https://pytorch.org/rl/reference/collectors.html>`__。简而言之，这些类执行三个操作：重置环境，根据最新观察值计算动作，在环境中执行步骤，然后重复后两个步骤直到环境发送停止信号（或达到完成状态）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"They allow you to control how many frames to collect at each iteration "
"(through the ``frames_per_batch`` parameter), when to reset the environment "
"(through the ``max_frames_per_traj`` argument), on which ``device`` the "
"policy should be executed, etc. They are also designed to work efficiently "
"with batched and multiprocessed environments."
msgstr ""
"这些类允许您控制每轮收集多少帧（通过``frames_per_batch``参数）、何时重置环境（通过``max_frames_per_traj``参数）、策略应该在哪个``device``上执行等。它们还旨在与批处理和多进程环境高效地协作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The simplest data collector is the "
":class:`~torchrl.collectors.collectors.SyncDataCollector`: it is an iterator"
" that you can use to get batches of data of a given length, and that will "
"stop once a total number of frames (``total_frames``) have been collected. "
"Other data collectors "
"(:class:`~torchrl.collectors.collectors.MultiSyncDataCollector` and "
":class:`~torchrl.collectors.collectors.MultiaSyncDataCollector`) will "
"execute the same operations in synchronous and asynchronous manner over a "
"set of multiprocessed workers."
msgstr ""
"最简单的数据收集器是:class:`~torchrl.collectors.collectors.SyncDataCollector`：它是一个迭代器，可以用于获取给定长度的数据批，并在收集总帧数（``total_frames``）后停止。其他数据收集器（:class:`~torchrl.collectors.collectors.MultiSyncDataCollector`和:class:`~torchrl.collectors.collectors.MultiaSyncDataCollector`）则在一组多进程工作者中以同步和异步方式执行相同操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As for the policy and environment before, the data collector will return "
":class:`~tensordict.TensorDict` instances with a total number of elements "
"that will match ``frames_per_batch``. Using :class:`~tensordict.TensorDict` "
"to pass data to the training loop allows you to write data loading pipelines"
" that are 100% oblivious to the actual specificities of the rollout content."
msgstr ""
"与之前的策略和环境一样，数据收集器将返回与``frames_per_batch``匹配总元素数的:class:`~tensordict.TensorDict`实例。使用:class:`~tensordict.TensorDict`将数据传递到训练循环中允许您编写完全独立于试运行内容具体方面的数据加载管道。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Replay buffer"
msgstr "重放缓冲区"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Replay buffers are a common building piece of off-policy RL algorithms. In "
"on-policy contexts, a replay buffer is refilled every time a batch of data "
"is collected, and its data is repeatedly consumed for a certain number of "
"epochs."
msgstr ""
"重放缓冲区是离线策略RL算法中常见的构建块。在在线策略上下文中，重放缓冲区会在每次收集数据批时重新填充，并在一定次数的epoch中重复使用其数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRL's replay buffers are built using a common container "
":class:`~torchrl.data.ReplayBuffer` which takes as argument the components "
"of the buffer: a storage, a writer, a sampler and possibly some transforms. "
"Only the storage (which indicates the replay buffer capacity) is mandatory. "
"We also specify a sampler without repetition to avoid sampling multiple "
"times the same item in one epoch. Using a replay buffer for PPO is not "
"mandatory and we could simply sample the sub-batches from the collected "
"batch, but using these classes make it easy for us to build the inner "
"training loop in a reproducible way."
msgstr ""
"TorchRL的重放缓冲区是使用:class:`~torchrl.data.ReplayBuffer`作为通用容器构建的，该容器接受缓冲区的组件：存储器、写入器、采样器以及可能的转换器。只有存储器（表示重放缓冲区容量）是必需的。我们还指定一个无重复采样器，以避免在一个epoch中对同一项进行多次采样。对于PPO使用重放缓冲区不是必须的，也可以直接从收集的数据中采样子批，但使用这些类使我们能够以更具复现性的方式构建内部训练循环。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Loss function"
msgstr "损失函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The PPO loss can be directly imported from TorchRL for convenience using the"
" :class:`~torchrl.objectives.ClipPPOLoss` class. This is the easiest way of "
"utilizing PPO: it hides away the mathematical operations of PPO and the "
"control flow that goes with it."
msgstr ""
"可以通过使用:class:`~torchrl.objectives.ClipPPOLoss`类直接从TorchRL导入PPO损失以方便使用。这是使用PPO的最简单方式：它隐藏了PPO的数学运算和相关的控制流程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PPO requires some \"advantage estimation\" to be computed. In short, an "
"advantage is a value that reflects an expectancy over the return value while"
" dealing with the bias / variance tradeoff. To compute the advantage, one "
"just needs to (1) build the advantage module, which utilizes our value "
"operator, and (2) pass each batch of data through it before each epoch. The "
"GAE module will update the input ``tensordict`` with new ``\"advantage\"`` "
"and ``\"value_target\"`` entries. The ``\"value_target\"`` is a gradient-"
"free tensor that represents the empirical value that the value network "
"should represent with the input observation. Both of these will be used by "
":class:`~torchrl.objectives.ClipPPOLoss` to return the policy and value "
"losses."
msgstr ""
"PPO需要计算一些\"优势估计\"。简而言之，优势是一种反映回报值期望的值，同时处理偏差与方差之间的权衡。为了计算优势，只需（1）构造使用我们的价值运算符的优势模块，然后（2）在每个epoch之前将每批数据传递给它。GAE模块会用新的``\"advantage\"``和``\"value_target\"``条目更新输入的``tensordict``。``\"value_target\"``是一个无梯度张量，表示价值网络在输入观察条件下应该预测的经验价值。这两者都将被:class:`~torchrl.objectives.ClipPPOLoss`用于返回策略和价值损失。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We now have all the pieces needed to code our training loop. The steps "
"include:"
msgstr "现在我们已经拥有编写训练循环所需的所有组件。这些步骤包括："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Collect data"
msgstr "数据收集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compute advantage"
msgstr "计算优势"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Loop over the collected to compute loss values"
msgstr "循环使用所收集数据来计算损失值"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Back propagate"
msgstr "反向传播"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Optimize"
msgstr "优化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Repeat"
msgstr "重复"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Results"
msgstr "结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before the 1M step cap is reached, the algorithm should have reached a max "
"step count of 1000 steps, which is the maximum number of steps before the "
"trajectory is truncated."
msgstr "在达到1M步数限制之前，该算法应该已经达到1000步的最大步数，这是一条轨迹被截断之前的最大步数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Conclusion and next steps"
msgstr "总结与下一步"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In this tutorial, we have learned:"
msgstr "在本教程中，我们学习了以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to create and customize an environment with :py:mod:`torchrl`;"
msgstr "如何使用 :py:mod:`torchrl` 创建和自定义环境；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to write a model and a loss function;"
msgstr "如何编写模型和损失函数；"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to set up a typical training loop."
msgstr "如何设置一个典型的训练循环。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you want to experiment with this tutorial a bit more, you can apply the "
"following modifications:"
msgstr "如果您想在本教程中进行更多实验，可以进行以下修改："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"From an efficiency perspective, we could run several simulations in parallel"
" to speed up data collection. Check :class:`~torchrl.envs.ParallelEnv` for "
"further information."
msgstr ""
"从效率的角度讲，我们可以并行运行多个模拟以加快数据收集。请查看 :class:`~torchrl.envs.ParallelEnv` 以获取更多信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"From a logging perspective, one could add a "
":class:`torchrl.record.VideoRecorder` transform to the environment after "
"asking for rendering to get a visual rendering of the inverted pendulum in "
"action. Check :py:mod:`torchrl.record` to know more."
msgstr ""
"从日志记录的角度讲，可以在请求呈现后，为环境添加一个 :class:`torchrl.record.VideoRecorder` "
"转换，从而获得倒立摆动作的视觉呈现。查看 :py:mod:`torchrl.record` 了解更多信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: reinforcement_ppo.py "
"<reinforcement_ppo.py>`"
msgstr ""
":download:`下载 Python 源代码: reinforcement_ppo.py <reinforcement_ppo.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: reinforcement_ppo.ipynb "
"<reinforcement_ppo.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: reinforcement_ppo.ipynb "
"<reinforcement_ppo.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_reinforcement_q_learning.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_reinforcement_q_learning.py>` "
"以下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Reinforcement Learning (DQN) Tutorial"
msgstr "强化学习 (DQN) 教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Adam Paszke <https://github.com/apaszke>`_"
msgstr "**作者**: `Adam Paszke <https://github.com/apaszke>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`Mark Towers <https://github.com/pseudo-rnd-thoughts>`_"
msgstr "`Mark Towers <https://github.com/pseudo-rnd-thoughts>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) "
"agent on the CartPole-v1 task from `Gymnasium "
"<https://gymnasium.farama.org>`__."
msgstr ""
"本教程展示了如何使用 PyTorch 在 `Gymnasium <https://gymnasium.farama.org>`__ 的 "
"CartPole-v1 任务上训练深度Q学习（DQN）智能体。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You might find it helpful to read the original `Deep Q Learning (DQN) "
"<https://arxiv.org/abs/1312.5602>`__ paper"
msgstr "您可能会觉得阅读原始的 `深度Q学习 (DQN) <https://arxiv.org/abs/1312.5602>`__ 论文很有帮助。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Task**"
msgstr "**任务**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The agent has to decide between two actions - moving the cart left or right "
"- so that the pole attached to it stays upright. You can find more "
"information about the environment and other more challenging environments at"
" `Gymnasium's website "
"<https://gymnasium.farama.org/environments/classic_control/cart_pole/>`__."
msgstr ""
"智能体需要在两个动作之间进行选择——使推车向左或向右移动——使与之连接的杆子保持直立。您可以在 `Gymnasium 的网站 "
"<https://gymnasium.farama.org/environments/classic_control/cart_pole/>`__ "
"上找到有关此环境以及其他更具挑战性的环境的更多信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "CartPole"
msgstr "CartPole"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As the agent observes the current state of the environment and chooses an "
"action, the environment *transitions* to a new state, and also returns a "
"reward that indicates the consequences of the action. In this task, rewards "
"are +1 for every incremental timestep and the environment terminates if the "
"pole falls over too far or the cart moves more than 2.4 units away from "
"center. This means better performing scenarios will run for longer duration,"
" accumulating larger return."
msgstr ""
"当智能体观察环境的当前状态并选择一个动作时，环境会 *过渡* 到一个新的状态，同时返回一个指示动作后果的奖励。在任务中，每个增加的时间步都会获得 +1 "
"的奖励，如果杆子倾倒过多或推车超过中心 2.4 单位的距离，环境将结束。表现更好的场景会运行更长的时间，从而累积更大的回报。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The CartPole task is designed so that the inputs to the agent are 4 real "
"values representing the environment state (position, velocity, etc.). We "
"take these 4 inputs without any scaling and pass them through a small fully-"
"connected network with 2 outputs, one for each action. The network is "
"trained to predict the expected value for each action, given the input "
"state. The action with the highest expected value is then chosen."
msgstr ""
"CartPole 任务的设计使得智能体的输入是 4 个表示环境状态（位置、速度等）的实数值。我们直接使用这 4 个输入，将它们通过一个具有 2 "
"个输出的简单全连接网络，每个输出对应一个动作。该网络被训练以预测每个动作的期望值，给定输入状态，选择期望值最高的动作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Packages**"
msgstr "**包**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, let's import needed packages. Firstly, we need `gymnasium "
"<https://gymnasium.farama.org/>`__ for the environment, installed by using "
"`pip`. This is a fork of the original OpenAI Gym project and maintained by "
"the same team since Gym v0.19. If you are running this in Google Colab, run:"
msgstr ""
"首先，让我们导入需要的包。首先我们需要使用 `pip` 安装 `gymnasium <https://gymnasium.farama.org/>`__"
" 来构建环境。这是原始 OpenAI Gym 项目的一个分支，由其自 Gym v0.19 版本起的团队维护。如果您在 Google Colab "
"上运行，请执行："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We'll also use the following from PyTorch:"
msgstr "我们还将使用 PyTorch 的以下组件："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "neural networks (``torch.nn``)"
msgstr "神经网络 （``torch.nn``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "optimization (``torch.optim``)"
msgstr "优化工具 （``torch.optim``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "automatic differentiation (``torch.autograd``)"
msgstr "自动微分 （``torch.autograd``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Replay Memory"
msgstr "重放记忆"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll be using experience replay memory for training our DQN. It stores the "
"transitions that the agent observes, allowing us to reuse this data later. "
"By sampling from it randomly, the transitions that build up a batch are "
"decorrelated. It has been shown that this greatly stabilizes and improves "
"the DQN training procedure."
msgstr ""
"我们将使用经验回放记忆来训练我们的 "
"DQN。它存储了智能体观察到的过渡，使我们可以稍后重新利用这些数据。通过随机采样，这些构建批次的过渡行为可以去相关化。这被证明能够显著稳定和改善 DQN"
" 的培训过程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For this, we're going to need two classes:"
msgstr "为此，我们需要两个类："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``Transition`` - a named tuple representing a single transition in our "
"environment. It essentially maps (state, action) pairs to their (next_state,"
" reward) result, with the state being the screen difference image as "
"described later on."
msgstr ""
"``Transition`` - 一个命名元组，表示我们环境中的单次过渡。它本质上将 (状态，动作) 映射到 (下一个状态，奖励) "
"的结果，状态描述为后面提到的屏幕差异图像。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``ReplayMemory`` - a cyclic buffer of bounded size that holds the "
"transitions observed recently. It also implements a ``.sample()`` method for"
" selecting a random batch of transitions for training."
msgstr ""
"``ReplayMemory`` - 一个有界大小的循环缓存，用于存储最近观察到的过渡。它还实现了 ``.sample()`` "
"方法，用于随机选择批量的过渡进行训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, let's define our model. But first, let's quickly recap what a DQN is."
msgstr "现在，让我们定义我们的模型。但首先，让我们快速回顾一下什么是 DQN。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "DQN algorithm"
msgstr "DQN 算法"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our environment is deterministic, so all equations presented here are also "
"formulated deterministically for the sake of simplicity. In the "
"reinforcement learning literature, they would also contain expectations over"
" stochastic transitions in the environment."
msgstr "我们的环境是确定性的，因此这里呈现的所有公式也都被简化为确定性形式。在强化学习文献中，它们还会包含对环境中随机过渡的期望值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our aim will be to train a policy that tries to maximize the discounted, "
"cumulative reward :math:`R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0}"
" r_t`, where :math:`R_{t_0}` is also known as the *return*. The discount, "
":math:`\\gamma`, should be a constant between :math:`0` and :math:`1` that "
"ensures the sum converges. A lower :math:`\\gamma` makes rewards from the "
"uncertain far future less important for our agent than the ones in the near "
"future that it can be fairly confident about. It also encourages agents to "
"collect reward closer in time than equivalent rewards that are temporally "
"far away in the future."
msgstr ""
"我们的目标是训练一个策略，该策略试图最大化折扣后的累积奖励：:math:`R_{t_0} = \\sum_{t=t_0}^{\\infty} "
"\\gamma^{t - t_0} r_t`，其中 :math:`R_{t_0}` 也被称为 *回报*。折扣因子 :math:`\\gamma` 应在 "
":math:`0` 和 :math:`1` 之间的常量，以确保总和收敛。较低的 :math:`\\gamma` "
"值使得不确定的远期未来奖励对智能体的价值低于近未来可靠的奖励。它还鼓励智能体在时间上更靠近的奖励比远未来奖励等效的时间点更重要。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The main idea behind Q-learning is that if we had a function :math:`Q^*: "
"State \\times Action \\rightarrow \\mathbb{R}`, that could tell us what our "
"return would be, if we were to take an action in a given state, then we "
"could easily construct a policy that maximizes our rewards:"
msgstr ""
"Q学习的核心理念是，如果我们拥有一个函数 :math:`Q^*: State \\times Action \\rightarrow "
"\\mathbb{R}`，它可以告诉我们在给定状态下采取某个动作后我们将获得的回报，那么我们可以轻松构建一个最大化奖励的策略："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)"
msgstr ""
"\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, we don't know everything about the world, so we don't have access "
"to :math:`Q^*`. But, since neural networks are universal function "
"approximators, we can simply create one and train it to resemble "
":math:`Q^*`."
msgstr ""
"然而，我们并不了解世界上的所有信息，因此无法访问 :math:`Q^*`。但由于神经网络是通用函数逼近器，我们可以简单地创建一个网络，并训练它以类似于 "
":math:`Q^*`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For our training update rule, we'll use a fact that every :math:`Q` function"
" for some policy obeys the Bellman equation:"
msgstr "对于我们的训练更新规则，我们将使用一个事实：每个策略的 :math:`Q` 函数都遵循贝尔曼方程："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))"
msgstr ""
"Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s&apos;, \\pi(s&apos;))\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The difference between the two sides of the equality is known as the "
"temporal difference error, :math:`\\delta`:"
msgstr "方程两边之间的差异称为时间差分误差 :math:`\\delta`："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))"
msgstr ""
"\\delta = Q(s, a) - (r + \\gamma \\max_a&apos; Q(s&apos;, a))\n"
"\n"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To minimize this error, we will use the `Huber loss "
"<https://en.wikipedia.org/wiki/Huber_loss>`__. The Huber loss acts like the "
"mean squared error when the error is small, but like the mean absolute error"
" when the error is large - this makes it more robust to outliers when the "
"estimates of :math:`Q` are very noisy. We calculate this over a batch of "
"transitions, :math:`B`, sampled from the replay memory:"
msgstr ""
"为了最小化此误差，我们将使用 `Huber 损失 <https://en.wikipedia.org/wiki/Huber_loss>`__。Huber"
" 损失在误差较小时表现得像均方误差，而在误差较大时表现得像平均绝对误差——这使它在 :math:`Q` "
"的估计非常嘈杂时更能对抗异常值的影响。我们在一个从回放记忆中随机选择的过渡批次 :math:`B` 上计算此误差："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)"
msgstr ""
"\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s&apos;, r) \\ \\in \\ B} "
"\\mathcal{L}(\\delta)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n"
"  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n"
"  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n"
"\\end{cases}"
msgstr ""
"\\text{其中} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n"
"  \\frac{1}{2}{\\delta^2}  & \\text{对于 } |\\delta| \\le 1, \\\\\n"
"  |\\delta| - \\frac{1}{2} & \\text{否则.}\n"
"\\end{cases}"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Q-network"
msgstr "Q网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Our model will be a feed forward  neural network that takes in the "
"difference between the current and previous screen patches. It has two "
"outputs, representing :math:`Q(s, \\mathrm{left})` and :math:`Q(s, "
"\\mathrm{right})` (where :math:`s` is the input to the network). In effect, "
"the network is trying to predict the *expected return* of taking each action"
" given the current input."
msgstr ""
"我们的模型将是一个前馈神经网络，它接收当前和之前屏幕块的差异作为输入。它有两个输出，分别表示 :math:`Q(s, \\mathrm{left})` "
"和 :math:`Q(s, \\mathrm{right})`（其中 :math:`s` 是网络的输入）。实际上，网络试图预测每个动作给定当前输入的 "
"*期望回报*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Hyperparameters and utilities"
msgstr "超参数和工具"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This cell instantiates our model and its optimizer, and defines some "
"utilities:"
msgstr "此单元实例化了我们的模型及其优化器，并定义了一些工具："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``select_action`` - will select an action according to an epsilon greedy "
"policy. Simply put, we'll sometimes use our model for choosing the action, "
"and sometimes we'll just sample one uniformly. The probability of choosing a"
" random action will start at ``EPS_START`` and will decay exponentially "
"towards ``EPS_END``. ``EPS_DECAY`` controls the rate of the decay."
msgstr ""
"``select_action`` - 将根据 epsilon "
"贪婪策略选择一个动作。简单来说，我们有时会使用模型来选择动作，有时我们会随机选择一个动作。随机选择动作的概率将从 ``EPS_START`` "
"开始指数递减至 ``EPS_END``。``EPS_DECAY`` 控制递减的速率。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``plot_durations`` - a helper for plotting the duration of episodes, along "
"with an average over the last 100 episodes (the measure used in the official"
" evaluations). The plot will be underneath the cell containing the main "
"training loop, and will update after every episode."
msgstr ""
"``plot_durations`` - 一个帮助工具，用于绘制每集的持续时间，以及过去 100 "
"集的平均值（正式评估中使用的衡量标准）。图表将位于包含主要训练循环的单元下方，并在每一集之后更新。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Finally, the code for training our model."
msgstr "最后是训练模型的代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here, you can find an ``optimize_model`` function that performs a single "
"step of the optimization. It first samples a batch, concatenates all the "
"tensors into a single one, computes :math:`Q(s_t, a_t)` and "
":math:`V(s_{t+1}) = \\max_a Q(s_{t+1}, a)`, and combines them into our loss."
" By definition we set :math:`V(s) = 0` if :math:`s` is a terminal state. We "
"also use a target network to compute :math:`V(s_{t+1})` for added stability."
" The target network is updated at every step with a `soft update "
"<https://arxiv.org/pdf/1509.02971.pdf>`__ controlled by the hyperparameter "
"``TAU``, which was previously defined."
msgstr ""
"此处您可以找到一个 ``optimize_model`` 函数，它执行优化的单步操作。它首先对一个批次进行采样，将所有张量拼接成一个单一的张量，计算 "
":math:`Q(s_t, a_t)` 和 :math:`V(s_{t+1}) = \\max_a Q(s_{t+1}, "
"a)`，并将它们组合到我们的损失中。根据定义，如果 :math:`s` 是终止状态，我们将设置 :math:`V(s) = "
"0`。我们还使用一个目标网络来计算 :math:`V(s_{t+1})` 以增强稳定性。目标网络在每一步通过一个 `软更新 "
"<https://arxiv.org/pdf/1509.02971.pdf>`__ 控制的超参数 ``TAU`` 进行更新，该参数之前定义过。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Below, you can find the main training loop. At the beginning we reset the "
"environment and obtain the initial ``state`` Tensor. Then, we sample an "
"action, execute it, observe the next state and the reward (always 1), and "
"optimize our model once. When the episode ends (our model fails), we restart"
" the loop."
msgstr ""
"以下是主要的训练循环。在开始时我们重置环境并获取初始 ``state`` 张量。然后我们采样一个动作，执行它，观察下一个状态和奖励（总为 "
"1），并优化我们的模型一次。当一集结束（我们的模型失败）时，我们重新开始循环。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Below, `num_episodes` is set to 600 if a GPU is available, otherwise 50 "
"episodes are scheduled so training does not take too long. However, 50 "
"episodes is insufficient for to observe good performance on CartPole. You "
"should see the model constantly achieve 500 steps within 600 training "
"episodes. Training RL agents can be a noisy process, so restarting training "
"can produce better results if convergence is not observed."
msgstr ""
"以下将 `num_episodes` 设置为 600，如果有 GPU 可用；否则，将安排 50 集以使训练时间不会太长。然而，50 集不足以观察到在 "
"CartPole 上的良好表现。您应该会看到模型在 600 集训练后能够持续达到 500 步。训练 RL "
"智能体可能是一个噪声较大的过程，因此如果没有观察到收敛，可以重新启动训练以获得更好的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Here is the diagram that illustrates the overall resulting data flow."
msgstr "这里是一个图示，展示了整体结果的数据流。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Actions are chosen either randomly or based on a policy, getting the next "
"step sample from the gym environment. We record the results in the replay "
"memory and also run optimization step on every iteration. Optimization picks"
" a random batch from the replay memory to do training of the new policy. The"
" \"older\" target_net is also used in optimization to compute the expected Q"
" values. A soft update of its weights are performed at every step."
msgstr ""
"动作的选择可以随机生成，也可以基于策略，在 gym "
"环境中获得下一步的样本。我们将在回放记忆中记录结果，并在每次迭代中运行优化步骤。优化会从回放记忆中选取随机批次进行新策略的训练。“较旧”的 "
"target_net 也会在优化中用于计算预期 Q 值。在每一步对其权重进行软更新。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: reinforcement_q_learning.py "
"<reinforcement_q_learning.py>`"
msgstr ""
":download:`下载 Python 源代码: reinforcement_q_learning.py "
"<reinforcement_q_learning.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: reinforcement_q_learning.ipynb "
"<reinforcement_q_learning.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: reinforcement_q_learning.ipynb "
"<reinforcement_q_learning.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Implementing Batch RPC Processing Using Asynchronous Executions"
msgstr "使用异步执行实现批量 RPC 处理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst>`__"
" 查看并编辑此教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`Getting started with Distributed RPC Framework <rpc_tutorial.html>`__"
msgstr "`开始使用分布式 RPC 框架 <rpc_tutorial.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Implementing a Parameter Server using Distributed RPC Framework "
"<rpc_param_server_tutorial.html>`__"
msgstr "`使用分布式 RPC 框架实现参数服务 <rpc_param_server_tutorial.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`RPC Asynchronous Execution Decorator "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
msgstr ""
"`RPC 异步执行装饰器 "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to build batch-processing RPC applications "
"with the `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" decorator, which helps to speed up training by reducing the number of "
"blocked RPC threads and consolidating CUDA operations on the callee. This "
"shares the same idea as `Batch Inference with TorchServe "
"<https://pytorch.org/serve/batch_inference_with_ts.html>`__."
msgstr ""
"本教程演示了如何使用 `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 装饰器构建批处理 RPC 应用程序，该装饰器通过减少阻塞的 RPC 线程数量和整合被调用端的 CUDA 操作来加速训练。这与 `使用 "
"TorchServe 的批量推理 <https://pytorch.org/serve/batch_inference_with_ts.html>`__"
" 的理念相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "This tutorial requires PyTorch v1.6.0 or above."
msgstr "本教程要求 PyTorch v1.6.0 及以上版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Previous tutorials have shown the steps to build distributed training "
"applications using `torch.distributed.rpc "
"<https://pytorch.org/docs/stable/rpc.html>`__, but they didn't elaborate on "
"what happens on the callee side when processing an RPC request. As of "
"PyTorch v1.5, each RPC request will block one thread on the callee to "
"execute the function in that request until that function returns. This works"
" for many use cases, but there is one caveat. If the user function blocks on"
" IO, e.g., with nested RPC invocation, or signaling, e.g., waiting for a "
"different RPC request to unblock, the RPC thread on the callee will have to "
"idle waiting until the IO finishes or the signaling event occurs. As a "
"result, RPC callees are likely to use more threads than necessary. The cause"
" of this problem is that RPC treats user functions as black boxes, and knows"
" very little about what happens in the function. To allow user functions to "
"yield and free RPC threads, more hints need to be provided to the RPC "
"system."
msgstr ""
"之前的教程已经展示了如何使用 `torch.distributed.rpc "
"<https://pytorch.org/docs/stable/rpc.html>`__ 构建分布式训练应用程序的步骤，但它们没有详细说明处理 RPC"
" 请求时被调用方的行为。从 PyTorch v1.5 开始，每个 RPC "
"请求将在被调用方阻塞一个线程以执行该请求中的函数，直到函数返回。这适用于许多用例，但存在一个问题。如果用户函数在 IO（例如嵌套 RPC "
"调用）或信号（例如等待另一个 RPC 请求解除阻塞）上阻塞，被调用方的 RPC 线程将不得不空闲等待，直到 IO "
"完成或信号触发。因此，被调用方可能会使用比必要更多的线程。这是因为 RPC 将用户函数视为黑盒，对函数内部的操作知之甚少。为允许用户函数释放并让 RPC"
" 线程让出资源，RPC 系统需要提供更多提示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since v1.6.0, PyTorch addresses this problem by introducing two new "
"concepts:"
msgstr "从 v1.6.0 起，PyTorch 通过引入以下两个新概念解决了此问题："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A `torch.futures.Future <https://pytorch.org/docs/master/futures.html>`__ "
"type that encapsulates an asynchronous execution, which also supports "
"installing callback functions."
msgstr ""
"一种 `torch.futures.Future <https://pytorch.org/docs/master/futures.html>`__ "
"类型，用于封装异步执行，同时支持安装回调函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"An `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" decorator that allows applications to tell the callee that the target "
"function will return a future and can pause and yield multiple times during "
"execution."
msgstr ""
"一个 `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 装饰器，允许应用程序通知被调用方目标函数将返回一个 Future，并且在执行过程中可以多次暂停和让出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With these two tools, the application code can break a user function into "
"multiple smaller functions, chain them together as callbacks on ``Future`` "
"objects, and return the ``Future`` that contains the final result. On the "
"callee side, when getting the ``Future`` object, it installs subsequent RPC "
"response preparation and communication as callbacks as well, which will be "
"triggered when the final result is ready. In this way, the callee no longer "
"needs to block one thread and wait until the final return value is ready. "
"Please refer to the API doc of `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" for simple examples."
msgstr ""
"借助这两个工具，应用程序代码可以将用户函数分解为多个较小的函数，将它们链式连接为 ``Future`` 对象上的回调，并返回包含最终结果的 "
"``Future``。在被调用方接收到 ``Future`` 对象时，还会安装后续的 RPC "
"响应准备和通信为回调，这些回调将在最终结果准备就绪时触发。这样，被调用方不再需要阻塞一个线程等待最终返回值。有关简单示例，请参阅 "
"`@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 的 API 文档。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Besides reducing the number of idle threads on the callee, these tools also "
"help to make batch RPC processing easier and faster. The following two "
"sections of this tutorial demonstrate how to build distributed batch-"
"updating parameter server and batch-processing reinforcement learning "
"applications using the `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" decorator."
msgstr ""
"除了减少被调用方空闲线程数量，这些工具还使批量 RPC 处理变得更容易更快。本教程接下来的两部分将演示如何使用 "
"`@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 装饰器构建分布式批量更新参数服务器和批处理强化学习应用程序。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Batch-Updating Parameter Server"
msgstr "批量更新参数服务器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Consider a synchronized parameter server training application with one "
"parameter server (PS) and multiple trainers. In this application, the PS "
"holds the parameters and waits for all trainers to report gradients. In "
"every iteration, it waits until receiving gradients from all trainers and "
"then updates all parameters in one shot. The code below shows the "
"implementation of the PS class. The ``update_and_fetch_model`` method is "
"decorated using ``@rpc.functions.async_execution`` and will be called by "
"trainers. Each invocation returns a ``Future`` object that will be populated"
" with the updated model. Invocations launched by most trainers just "
"accumulate gradients to the ``.grad`` field, return immediately, and yield "
"the RPC thread on the PS. The last arriving trainer will trigger the "
"optimizer step and consume all previously reported gradients. Then it sets "
"the ``future_model`` with the updated model, which in turn notifies all "
"previous requests from other trainers through the ``Future`` object and "
"sends out the updated model to all trainers."
msgstr ""
"考虑一个使用同步参数服务器进行训练的应用程序，该应用包括一个参数服务器（PS）和多个训练器。在此应用中，PS "
"持有参数并等待所有训练器报告梯度。在每次迭代中，它等待接收到所有训练器的梯度后，一次性更新所有参数。以下代码展示了 PS "
"类的实现。``update_and_fetch_model`` 方法使用 ``@rpc.functions.async_execution`` "
"装饰，并由训练器调用。每次调用都会返回一个 ``Future`` 对象，其中填充了更新后的模型。大多数训练器发起的调用只是将梯度累积到 "
"``.grad`` 字段，立即返回并释放 PS 上的 RPC 线程。最后一个到达的训练器将触发优化器的步长操作并处理之前报告的所有梯度。然后，它通过 "
"``Future`` 对象将更新的模型设置到 ``future_model``，从而通知所有之前来自其他训练器的请求，并将更新的模型发送给所有训练器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the trainers, they are all initialized using the same set of parameters "
"from the PS. In every iteration, each trainer first runs the forward and the"
" backward passes to generate gradients locally. Then, each trainer reports "
"its gradients to the PS using RPC, and fetches back the updated parameters "
"through the return value of the same RPC request. In the trainer's "
"implementation, whether the target function is marked with "
"``@rpc.functions.async_execution`` or not makes no difference. The trainer "
"simply calls ``update_and_fetch_model`` using ``rpc_sync`` which will block "
"on the trainer until the updated model is returned."
msgstr ""
"对训练器来说，它们都是使用来自 PS 的同一组参数初始化的。在每次迭代中，每个训练器首先运行前向和后向传播以在本地生成梯度。然后，每个训练器通过 RPC"
" 向 PS 报告其梯度，并通过同一 RPC 请求的返回值获取更新后的参数。在训练器的实现中，无论目标函数是否标记为 "
"``@rpc.functions.async_execution`` 都没有区别。训练器简单地使用 ``rpc_sync`` 调用 "
"``update_and_fetch_model``，该调用在返回更新的模型之前会在训练器上阻塞。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We skip the code that launches multiple processes in this tutorial and "
"please refer to the `examples "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc>`__ repo "
"for the full implementation. Note that, it is possible to implement batch "
"processing without the `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" decorator. However, that would require either blocking more RPC threads on "
"the PS or use another round of RPC to fetch updated models, where the latter"
" would add both more code complexity and more communication overhead."
msgstr ""
"我们在此教程中跳过了启动多个进程的代码，请参考 `examples "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc>`__ "
"仓库以获取完整实现。需要注意的是，可以在不使用 `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 装饰器的情况下实现批处理。然而，这样做需要阻塞 PS 上的更多 RPC 线程，或者使用另一轮 RPC "
"来获取更新后的模型，后者会增加代码复杂性和通信开销。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This section uses a simple parameter sever training example to show how to "
"implement batch RPC applications using the `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" decorator. In the next section, we re-implement the reinforcement learning "
"example in the previous `Getting started with Distributed RPC Framework "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__ tutorial "
"using batch processing, and demonstrate its impact on the training speed."
msgstr ""
"本节使用一个简单的参数服务器训练示例，展示如何使用 `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 装饰器实现批量 RPC 应用程序。在下一节中，我们将使用批处理重新实现上一节 `分布式 RPC 框架入门 "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__ "
"教程中的强化学习示例，并展示其对训练速度的影响。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Batch-Processing CartPole Solver"
msgstr "批处理 CartPole 求解器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This section uses CartPole-v1 from OpenAI Gym as an example to show the "
"performance impact of batch processing RPC. Please note that since the goal "
"is to demonstrate the usage of `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" instead of building the best CartPole solver or solving most different RL "
"problems, we use very simple policies and reward calculation strategies and "
"focus on the multi-observer single-agent batch RPC implementation. We use a "
"similar ``Policy`` model as the previous tutorial which is shown below. "
"Compared to the previous tutorial, the difference is that its constructor "
"takes an additional ``batch`` argument which controls the ``dim`` parameter "
"for ``F.softmax`` because with batching, the ``x`` argument in the "
"``forward`` function contains states from multiple observers and hence the "
"dimension needs to change properly. Everything else stays intact."
msgstr ""
"本节以 OpenAI Gym 的 CartPole-v1 为例，展示批处理 RPC 的性能影响。请注意，由于目标是演示 "
"`@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 的用法，而不是构建最佳 CartPole 求解器或解决大多数 RL 问题，我们采用非常简单的策略和奖励计算方法，并专注于多观察者单代理的批量 RPC "
"实现。我们采用与上一教程类似的 ``Policy`` 模型如下所示。与上一教程相比，其构造函数增加了一个 ``batch`` 参数，用于控制 "
"``F.softmax`` 的 ``dim`` 参数，因为启用批处理时，``forward`` 函数中的 ``x`` "
"参数包含来自多个观察者的状态，因此需要相应调整维度。其余部分保持不变。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The constructor of the ``Observer`` adjusts accordingly as well. It also "
"takes a ``batch`` argument, which governs which ``Agent`` function it uses "
"to select actions. In batch mode, it calls ``select_action_batch`` function "
"on ``Agent`` which will be presented shortly, and this function will be "
"decorated with `@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__."
msgstr ""
"``Observer`` 的构造函数也做了相应调整。它也接受一个 ``batch`` 参数，用于决定使用哪个 ``Agent`` "
"函数来选择动作。在批量模式中，它会调用 ``Agent`` 上的 ``select_action_batch`` 函数，该函数将使用 "
"`@rpc.functions.async_execution "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.functions.async_execution>`__"
" 进行装饰。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compared to the previous tutorial `Getting started with Distributed RPC "
"Framework <https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__,"
" observers behave a little differently. Instead of exiting when the "
"environment is stopped, it always runs ``n_steps`` iterations in every "
"episode. When the environment returns, the observer simply resets the "
"environment and start over again. With this design, the agent will receive a"
" fixed number of states from every observer and hence can pack them into a "
"fixed-size tensor. In every step, the ``Observer`` uses RPC to send its "
"state to the ``Agent`` and fetches the action through the return value. At "
"the end of every episode, it returns the rewards of all steps to ``Agent``. "
"Note that this ``run_episode`` function will be called by the ``Agent`` "
"using RPC. So the ``rpc_sync`` call in this function will be a nested RPC "
"invocation. We could mark this function as "
"``@rpc.functions.async_execution`` too to avoid blocking one thread on the "
"``Observer``. However, as the bottleneck is the ``Agent`` instead of the "
"``Observer``, it should be OK to block one thread on the ``Observer`` "
"process."
msgstr ""
"与之前的 `分布式 RPC 框架入门 "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__ "
"教程相比，观察者的行为略有不同。它不会在环境停止时退出，而是每轮都执行 ``n_steps`` "
"次迭代。当环境返回时，观察者只是重置环境并重新开始。通过此设计，代理将从每个观察者接收到固定数量的状态，因此可以将状态打包为固定大小的张量。在每步中，``Observer``"
" 使用 RPC 将其状态发送到 ``Agent``，并通过返回值获取动作。在每轮结束时，它将所有步骤的奖励返回给 ``Agent``。需要注意的是，该 "
"``run_episode`` 函数将由 ``Agent`` 使用 RPC 调用。因此，该函数中的 ``rpc_sync`` 调用是嵌套的 RPC "
"调用。我们也可以将此函数标记为 ``@rpc.functions.async_execution``，以避免在 ``Observer`` "
"上阻塞一个线程。然而，由于瓶颈在于 ``Agent`` 而不是 ``Observer``，因此在 ``Observer`` "
"进程上阻塞一个线程是可以接受的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The constructor of the ``Agent`` also takes a ``batch`` argument, which "
"controls how action probs are batched. In batch mode, the "
"``saved_log_probs`` contains a list of tensors, where each tensor contains "
"action robs from all observers in one step. Without batching, the "
"``saved_log_probs`` is a dictionary where the key is the observer id and the"
" value is a list of action probs for that observer."
msgstr ""
"``Agent`` 的构造函数也接受一个 ``batch`` 参数，用于控制如何批量化动作概率。在批量模式下，``saved_log_probs`` "
"包含一个张量列表，其中每个张量包含所有观察者在一个步骤中的动作概率。不使用批量处理时，``saved_log_probs`` 是一个字典，其中键是观察者"
" ID，值是该观察者的一系列动作概率。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The non-batching ``select_acion`` simply runs the state throw the policy, "
"saves the action prob, and returns the action to the observer right away."
msgstr "非批量处理的 ``select_acion`` 只是将状态通过策略运行，保存动作概率，并立即将动作返回给观察者。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With batching, the state is stored in a 2D tensor ``self.states``, using the"
" observer id as the row id. Then, it chains a ``Future`` by installing a "
"callback function to the batch-generated ``self.future_actions`` ``Future`` "
"object, which will be populated with the specific row indexed using the id "
"of that observer. The last arriving observer runs all batched states through"
" the policy in one shot and set  ``self.future_actions`` accordingly. When "
"this occurs, all the callback functions installed on ``self.future_actions``"
" will be triggered and their return values will be used to populate the "
"chained ``Future`` object, which in turn notifies the ``Agent`` to prepare "
"and communicate responses for all previous RPC requests from other "
"observers."
msgstr ""
"在批量处理中，状态存储在一个以观察者 ID 作为行 ID 的二维张量 ``self.states`` 中。然后，通过将回调函数安装到批处理生成的 "
"``self.future_actions`` ``Future`` 对象上来链式链接一个 ``Future``，该 ``Future`` "
"对象将通过观察者的 ID 索引填充特定行的结果。最后一个到达的观察者一次性运行所有批处理状态通过策略，并更新设置 "
"``self.future_actions``。此时，安装在 ``self.future_actions`` "
"上的所有回调函数将被触发，其返回值将用于填充链式的 ``Future`` 对象，从而通知 ``Agent`` 为所有之前来自其他观察者的 RPC "
"请求准备和发送响应。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now let's define how different RPC functions are stitched together. The "
"``Agent`` controls the execution of every episode. It first uses "
"``rpc_async`` to kick off the episode on all observers and block on the "
"returned futures which will be populated with observer rewards. Note that "
"the code below uses the RRef helper ``ob_rref.rpc_async()`` to launch the "
"``run_episode`` function on the owner of the ``ob_rref`` RRef with the "
"provided arguments. It then converts the saved action probs and returned "
"observer rewards into expected data format, and launch the training step. "
"Finally, it resets all states and returns the reward of the current episode."
" This function is the entry point to run one episode."
msgstr ""
"现在让我们定义如何将不同的RPC函数连接在一起。“Agent”控制每个周期的执行。首先，它使用“rpc_async”来启动周期，并阻塞在返回的期望值上，这些值将被观察者的奖励填充。注意，以下代码使用RRef助手“ob_rref.rpc_async()”在“ob_rref”RRef的拥有者上，通过提供的参数启动“run_episode”函数。然后它将保存的动作概率和返回的观察者奖励转换为预期的数据格式，并启动训练步骤。最后，它重置所有状态并返回当前周期的奖励。此函数是运行一个周期的入口点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The rest of the code is normal processes launching and logging which are "
"similar to other RPC tutorials. In this tutorial, all observers passively "
"waiting for commands from the agent. Please refer to the `examples "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc>`__ repo "
"for the full implementation."
msgstr ""
"其余代码是常规的进程启动和日志记录，与其他RPC教程类似。在本教程中，所有观察者都被动地等待来自Agent的命令。请参阅`examples "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc>`__仓库的完整实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Batch RPC helps to consolidate the action inference into less CUDA "
"operations, and hence reduces the amortized overhead. The above ``main`` "
"function runs the same code on both batch and no-batch modes using different"
" numbers of observers, ranging from 1 to 10. The figure below plots the "
"execution time of different world sizes using default argument values. The "
"results confirmed our expectation that batch processing helped to speed up "
"training."
msgstr ""
"批量RPC有助于将动作推断整合到更少的CUDA操作中，从而减少平均开销。上述“main”函数分别以批量和非批量模式运行相同代码，使用不同数量的观察者，从1到10不等。下图绘制了使用默认参数值的不同规模的执行时间。结果验证了我们的预期，即批量处理有助于加速训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Learn More"
msgstr "了解更多"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Batch-Updating Parameter Server Source Code "
"<https://github.com/pytorch/examples/blob/master/distributed/rpc/batch/parameter_server.py>`__"
msgstr ""
"`批量更新参数服务器源码 "
"<https://github.com/pytorch/examples/blob/master/distributed/rpc/batch/parameter_server.py>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Batch-Processing CartPole Solver "
"<https://github.com/pytorch/examples/blob/master/distributed/rpc/batch/reinforce.py>`__"
msgstr ""
"`批量处理CartPole求解器 "
"<https://github.com/pytorch/examples/blob/master/distributed/rpc/batch/reinforce.py>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Distributed Autograd <https://pytorch.org/docs/master/rpc.html#distributed-"
"autograd-framework>`__"
msgstr ""
"`分布式自动梯度 <https://pytorch.org/docs/master/rpc.html#distributed-autograd-"
"framework>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Implementing a Parameter Server Using Distributed RPC Framework"
msgstr "使用分布式RPC框架实现参数服务器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**\\ : `Rohan Varma <https://github.com/rohan-varma>`_"
msgstr "**作者**: `Rohan Varma <https://github.com/rohan-varma>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst>`__."
msgstr ""
"|edit| 在`github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst>`__中查看并编辑本教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`RPC API documents <https://pytorch.org/docs/master/rpc.html>`__"
msgstr "`RPC API文档 <https://pytorch.org/docs/master/rpc.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial walks through a simple example of implementing a parameter "
"server using PyTorch's `Distributed RPC framework "
"<https://pytorch.org/docs/stable/rpc.html>`_. The parameter server framework"
" is a paradigm in which a set of servers store parameters, such as large "
"embedding tables, and several trainers query the parameter servers in order "
"to retrieve the most up to date parameters. These trainers can run a "
"training loop locally and occasionally synchronize with the parameter server"
" to get the latest parameters. For more reading on the parameter server "
"approach, check out `this paper "
"<https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf>`_."
msgstr ""
"本教程通过一个简单示例讲解了如何使用PyTorch的`分布式RPC框架 "
"<https://pytorch.org/docs/stable/rpc.html>`_实现参数服务器。参数服务器框架是一种范式，其中一组服务器存储参数，比如大的嵌入表，而几个训练器查询参数服务器以获取最新的参数。这些训练器可以在本地运行训练循环，并偶尔与参数服务器同步以获取最新参数。关于参数服务器方法的更多阅读，请查看`这篇论文"
" <https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using the Distributed RPC Framework, we'll build an example where multiple "
"trainers use RPC to communicate with the same parameter server and use `RRef"
" <https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef>`_ to "
"access states on the remote parameter server instance. Each trainer will "
"launch its dedicated backward pass in a distributed fashion through "
"stitching of the autograd graph across multiple nodes using distributed "
"autograd."
msgstr ""
"使用分布式RPC框架，我们将构建一个示例，其中多个训练器使用RPC与同一参数服务器通信，并通过`RRef "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef>`_访问远程参数服务器实例的状态。每个训练器将通过分布式方式发起它的专属逆向传播步骤，通过跨多个节点的自动梯度图拼接实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Note**\\ : This tutorial covers the use of the Distributed RPC Framework, "
"which is useful for splitting a model onto multiple machines, or for "
"implementing a parameter-server training strategy where network trainers "
"fetch parameters hosted on a different machine. If instead you are looking "
"for replicating your model across many GPUs, please see the `Distributed "
"Data Parallel tutorial "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_. There is "
"also another `RPC tutorial "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`_ that covers"
" reinforcement learning and RNN use cases."
msgstr ""
"**注意**: "
"本教程涵盖了分布式RPC框架的使用，这对于将模型拆分到多个机器上，或者实现参数服务器训练策略（网络训练器从另一台机器获取参数）非常有用。如果您希望在多个GPU上复制您的模型，请参阅`分布式数据并行教程"
" "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_。还有另一个`RPC教程"
" "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`_涵盖了强化学习和RNN的用例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's start with the familiar: importing our required modules and defining a"
" simple ConvNet that will train on the MNIST dataset. The below network is "
"largely adopted from the network defined in the `pytorch/examples repo "
"<https://github.com/pytorch/examples/tree/master/mnist>`_."
msgstr ""
"让我们从熟悉的内容开始：导入所需模块并定义一个简单的用于MNIST数据集训练的卷积网络。以下网络主要采用于`pytorch/examples repo "
"<https://github.com/pytorch/examples/tree/master/mnist>`_中定义的网络。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, let's define some helper functions that will be useful for the rest of"
" our script. The following uses `rpc_sync "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.rpc_sync>`_ "
"and `RRef "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef>`_ in "
"order to define a function that invokes a given method on an object living "
"on a remote node. Below, our handle to the remote object is given by the "
"``rref`` argument, and we run it on its owning node: ``rref.owner()``. On "
"the caller node, we run this command synchronously through the use of "
"``rpc_sync``\\ , meaning that we will block until a response is received."
msgstr ""
"接下来，让我们定义一些辅助函数，这些函数将在后续脚本中用到。以下使用`rpc_sync "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.rpc_sync>`_和`RRef"
" "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef>`_定义一个函数，该函数调用远程节点上的某个对象的方法。在下面代码中，我们使用``rref``作为远程对象的句柄，并在其所属节点上运行它：``rref.owner()``。在调用节点上，我们通过使用``rpc_sync``同步运行该命令，这意味着我们会阻塞直到接收到响应。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, we're ready to define our parameter server. We will subclass "
"``nn.Module`` and save a handle to our network defined above. We'll also "
"save an input device which will be the device our input is transferred to "
"before invoking the model."
msgstr ""
"现在，我们可以定义参数服务器了。我们将继承``nn.Module``并保存对上面定义的网络的句柄。同时还会保存一个输入设备，这将是将输入传输到模型之前的数据传输的设备。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we'll define our forward pass. Note that regardless of the device of "
"the model output, we move the output to CPU, as the Distributed RPC "
"Framework currently only supports sending CPU tensors over RPC. We have "
"intentionally disabled sending CUDA tensors over RPC due to the potential "
"for different devices (CPU/GPU) on on the caller/callee, but may support "
"this in future releases."
msgstr ""
"接下来，我们将定义前向传播。注意，无论模型输出的设备是什么，我们都将输出移到CPU，因为分布式RPC框架目前只支持通过RPC发送CPU张量。我们有意禁用了通过RPC发送CUDA张量的功能，以避免调用者/被调者上的设备（CPU/GPU）可能不同的问题，但未来版本可能会支持这一点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we'll define a few miscellaneous functions useful for training and "
"verification purposes. The first, ``get_dist_gradients``\\ , will take in a "
"Distributed Autograd context ID and call into the "
"``dist_autograd.get_gradients`` API in order to retrieve gradients computed "
"by distributed autograd. More information can be found in the `distributed "
"autograd documentation "
"<https://pytorch.org/docs/stable/rpc.html#distributed-autograd-framework>`_."
" Note that we also iterate through the resulting dictionary and convert each"
" tensor to a CPU tensor, as the framework currently only supports sending "
"tensors over RPC. Next, ``get_param_rrefs`` will iterate through our model "
"parameters and wrap them as a (local) `RRef "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef>`_. "
"This method will be invoked over RPC by trainer nodes and will return a list"
" of the parameters to be optimized. This is required as input to the "
"`Distributed Optimizer <https://pytorch.org/docs/stable/rpc.html#module-"
"torch.distributed.optim>`_\\ , which requires all parameters it must "
"optimize as a list of ``RRef``\\ s."
msgstr ""
"接下来，我们将定义一些用于训练和验证的杂项函数。第一个``get_dist_gradients``将接收一个分布式自动梯度上下文ID并调用``dist_autograd.get_gradients``"
" API以检索由分布式自动梯度计算出的梯度。更多信息请参阅`分布式自动梯度文档 "
"<https://pytorch.org/docs/stable/rpc.html#distributed-autograd-"
"framework>`_。注意，我们还遍历结果字典并将每个张量转换为CPU张量，因为该框架目前只支持通过RPC发送张量。接着，``get_param_rrefs``会遍历我们的模型参数，并将它们包装为（本地）`RRef"
" "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.RRef>`_。该方法将通过RPC由训练器节点调用，并返回需要优化的参数列表。这是分布式优化器的输入所必需的，`分布式优化器"
" <https://pytorch.org/docs/stable/rpc.html#module-"
"torch.distributed.optim>`_需要所有待优化的参数作为``RRef``列表。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, we'll create methods to initialize our parameter server. Note that "
"there will only be one instance of a parameter server across all processes, "
"and all trainers will talk to the same parameter server and update the same "
"stored model. As seen in ``run_parameter_server``\\ , the server itself does"
" not take any independent actions; it waits for requests from trainers "
"(which are yet to be defined) and responds to them by running the requested "
"function."
msgstr ""
"最后，我们将创建初始化参数服务器的方法。注意，所有进程中仅会有一个参数服务器实例，所有训练器都会与同一个参数服务器通信并更新同存储的模型。正如``run_parameter_server``中所见，服务器本身不采取任何独立动作；它等待训练器的请求（尚未定义），并通过运行请求的函数来响应。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that above, ``rpc.shutdown()`` will not immediately shut down the "
"Parameter Server. Instead, it will wait for all workers (trainers in this "
"case) to also call into ``rpc.shutdown()``. This gives us the guarantee that"
" the parameter server will not go offline before all trainers (yet to be "
"define) have completed their training process."
msgstr ""
"注意，上述``rpc.shutdown()``不会立即关闭参数服务器。相反，它会等待所有工作者（这里是训练器）也调用``rpc.shutdown()``。这保证了参数服务器不会在所有训练器（尚未定义）完成训练之前下线。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we'll define our ``TrainerNet`` class. This will also be a subclass of"
" ``nn.Module``\\ , and our ``__init__`` method will use the ``rpc.remote`` "
"API to obtain an RRef, or Remote Reference, to our parameter server. Note "
"that here we are not copying the parameter server to our local process, "
"instead, we can think of ``self.param_server_rref`` as a distributed shared "
"pointer to the parameter server that lives on a separate process."
msgstr ""
"接下来，我们将定义``TrainerNet``类。这也将是``nn.Module``的子类，我们的``__init__``方法将使用``rpc.remote``"
" "
"API来获取参数服务器的RRef（远程引用）。注意，这里我们没有将参数服务器复制到本地进程，而是可以将``self.param_server_rref``视为分布式共享指针，指向位于单独进程上的参数服务器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we'll define a method called ``get_global_param_rrefs``. To motivate "
"the need for this method, it is worth it to read through the documentation "
"on `DistributedOptimizer <https://pytorch.org/docs/stable/rpc.html#module-"
"torch.distributed.optim>`_, specifically the API signature.  The optimizer "
"must be passed a list of ``RRef``\\ s corresponding to the remote parameters"
" to be optimized, so here we obtain the necessary ``RRef``\\ s. Since the "
"only remote worker that a given ``TrainerNet`` interacts with is the "
"``ParameterServer``\\ , we simply invoke a ``remote_method`` on the "
"``ParameterServer``. We use the ``get_param_rrefs`` method which we defined "
"in the ``ParameterServer`` class. This method will return a list of "
"``RRef``\\ s to the parameters that need to be optimized. Note that in this "
"case our ``TrainerNet`` does not define its own paramaters; if it did, we "
"would need to wrap each parameter in an ``RRef`` as well and include it into"
" our input to ``DistributedOptimizer``."
msgstr ""
"接下来，我们将定义一个名为``get_global_param_rrefs``的方法。为了说明该方法的必要性，值得阅读`分布式优化器 "
"<https://pytorch.org/docs/stable/rpc.html#module-"
"torch.distributed.optim>`_的文档，尤其是API签名。优化器必须传递一个列表，其中包含要优化的远程参数的``RRef``。因此，我们在这里获取必要的``RRef``。由于给定的``TrainerNet``仅与``ParameterServer``交互，我们简单地对``ParameterServer``调用``remote_method``。我们使用在``ParameterServer``类中定义的``get_param_rrefs``方法。该方法将返回需要优化的参数的``RRef``列表。注意，在此情况下，我们的``TrainerNet``未定义自己的参数；如果定义了，我们需要将每个参数包装为``RRef``并将其包含到``DistributedOptimizer``的输入中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, we're ready to define our ``forward`` method, which will invoke "
"(synchronous) RPC to run the forward pass of the network defined on the "
"``ParameterServer``. Note that we pass in ``self.param_server_rref``\\ , "
"which is a remote handle to our ``ParameterServer``\\ , to our RPC call. "
"This call will send an RPC to the node on which our ``ParameterServer`` is "
"running, invoke the ``forward`` pass, and return the ``Tensor`` "
"corresponding to the model's output."
msgstr ""
"现在，我们将定义``forward``方法，它将调用（同步）RPC以运行在``ParameterServer``上定义的网络的前向传播。注意，我们将``self.param_server_rref``（一个指向``ParameterServer``的远程句柄）传递给我们的RPC调用。该调用将向运行``ParameterServer``的节点发送RPC，调用前向传播，并返回与模型输出对应的``Tensor``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With our trainer fully defined, it's now time to write our neural network "
"training loop that will create our network and optimizer, run some inputs "
"through the network and compute the loss. The training loop looks a lot like"
" that of a local training program, with some modifications due to the nature"
" of our network being distributed across machines."
msgstr ""
"随着训练器的完全定义，现在是时候编写我们的神经网络训练循环。在该循环中，我们将创建网络和优化器，运行一些输入并计算损失。训练循环看起来很像本地训练程序，有一些修改，因为我们的网络是分布在多台机器上的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Below, we initialize our ``TrainerNet`` and build a "
"``DistributedOptimizer``. Note that as mentioned above, we must pass in all "
"of the global (across all nodes participating in distributed training) "
"parameters that we want to be optimized. In addition, we pass in the local "
"optimizer to be used, in this case, SGD. Note that we can configure the "
"underlying optimizer algorithm in the same way as creating a local optimizer"
" - all arguments for ``optimizer.SGD`` will be forwarded properly. As an "
"example, we pass in a custom learning rate that will be used as the learning"
" rate for all local optimizers."
msgstr ""
"以下，我们初始化``TrainerNet``并构建``DistributedOptimizer``。注意，如上所述，我们必须传入所有需要优化的全局（参与分布式训练的所有节点）参数。此外，我们传入要使用的本地优化器，这里是SGD。注意，我们可以像创建本地优化器一样配置底层优化器算法——所有参数都会正确传递给``optimizer.SGD``。举例来说，我们传入一个自定义学习率，它将作为所有本地优化器的学习率使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we define our main training loop. We loop through iterables given by "
"PyTorch's `DataLoader <https://pytorch.org/docs/stable/data.html>`_. Before "
"writing our typical forward/backward/optimizer loop, we first wrap the logic"
" within a `Distributed Autograd context "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.context>`_."
" Note that this is needed to record RPCs invoked in the model's forward "
"pass, so that an appropriate graph can be constructed which includes all "
"participating distributed workers in the backward pass. The distributed "
"autograd context returns a ``context_id`` which serves as an identifier for "
"accumulating and optimizing gradients corresponding to a particular "
"iteration."
msgstr ""
"接下来，我们定义主要的训练循环。我们通过 PyTorch 的 `DataLoader "
"<https://pytorch.org/docs/stable/data.html>`_ "
"提供的可迭代对象进行循环。在编写典型的前向/后向/优化器循环之前，我们首先将逻辑包装在一个 `分布式自动梯度上下文 "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.autograd.context>`_"
" 中。请注意，这对于记录模型前向传播过程中调用的 RPC "
"是必要的，以便能够构建一个图，包括所有在后向传播中参与的分布式工作人员。分布式自动梯度上下文返回一个 "
"``context_id``，该标识符用于积累和优化与特定迭代相对应的梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As opposed to calling the typical ``loss.backward()`` which would kick off "
"the backward pass on this local worker, we call ``dist_autograd.backward()``"
" and pass in our context_id as well as ``loss``\\ , which is the root at "
"which we want the backward pass to begin. In addition, we pass this "
"``context_id`` into our optimizer call, which is required to be able to look"
" up the corresponding gradients computed by this particular backwards pass "
"across all nodes."
msgstr ""
"与调用典型的 ``loss.backward()`` 会在本地工作器上启动后向传播不同，我们调用 "
"``dist_autograd.backward()`` 并传入 `context_id` 以及 ``loss``\\ "
"，后者是我们希望后向传播开始的根节点。此外，我们还将这个 ``context_id`` "
"传入优化器调用中，这对于查找通过此特定后向传播步骤跨所有节点计算的对应梯度是必要的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following simply computes the accuracy of our model after we're done "
"training, much like a traditional local model. However, note that the "
"``net`` we pass into this function above is an instance of ``TrainerNet`` "
"and therefore the forward pass invokes RPC in a transparent fashion."
msgstr ""
"以下代码简单计算模型在完成训练后的准确性，这类似于传统的本地模型。然而，请注意我们传入上述函数的 ``net`` 是 ``TrainerNet`` "
"的实例，因此前向传播以透明的方式触发 RPC。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, similar to how we defined ``run_parameter_server`` as the main loop "
"for our ``ParameterServer`` that is responsible for initializing RPC, let's "
"define a similar loop for our trainers. The difference will be that our "
"trainers must run the training loop we defined above:"
msgstr ""
"接下来，类似于我们为负责初始化 RPC 的 ``ParameterServer`` 定义的主要循环 "
"``run_parameter_server``，我们为训练器定义一个类似的循环。区别在于，训练器必须运行我们在上面定义的训练循环："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that similar to ``run_parameter_server``\\ , ``rpc.shutdown()`` will by"
" default wait for all workers, both trainers and ParameterServers, to call "
"into ``rpc.shutdown()`` before this node exits. This ensures that nodes are "
"terminated gracefully and no node goes offline while another is expecting it"
" to be online."
msgstr ""
"请注意，类似于 ``run_parameter_server``\\ ，``rpc.shutdown()`` "
"默认会等待所有工作器，包括训练器和参数服务器，调用 ``rpc.shutdown()`` "
"后才会让该节点退出。这确保节点能够优雅地终止，且没有任何节点在另一个节点期望其处于在线状态时离线。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We've now completed our trainer and parameter server specific code, and all "
"that's left is to add code to launch trainers and parameter servers. First, "
"we must take in various arguments that apply to our parameter server and "
"trainers. ``world_size`` corresponds to the total number of nodes that will "
"participate in training, and is the sum of all trainers and the parameter "
"server. We also must pass in a unique ``rank`` for each individual process, "
"from 0 (where we will run our single parameter server) to ``world_size - "
"1``. ``master_addr`` and ``master_port`` are arguments that can be used to "
"identify where the rank 0 process is running, and will be used by individual"
" nodes to discover each other. To test this example out locally, simply pass"
" in ``localhost`` and the same ``master_port`` to all instances spawned. "
"Note that for demonstration purposes, this example supports only between 0-2"
" GPUs, although the pattern can be extended to make use of additional GPUs."
msgstr ""
"到目前为止，我们已经完成了特定于训练器和参数服务器的代码，现在只剩下启动训练器和参数服务器的代码。首先，我们需要获取适用于参数服务器和训练器的各种参数。``world_size``"
" 对应于参与训练的所有节点的总数，即训练器和参数服务器总数的和。我们还必须为每个独立进程传入一个唯一的 ``rank``，范围从 "
"0（用于运行单个参数服务器）到 ``world_size - 1``。``master_addr`` 和 ``master_port`` 是用于标识 "
"`rank 0` 进程所在地址的参数，供单个节点相互发现。在本地测试这个示例时，只需将 ``localhost`` 和相同的 "
"``master_port`` 传递给所有启动的实例。请注意，出于演示目的，该示例仅支持 0-2 个 GPU，但可以扩展用于更多 GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, we'll create a process corresponding to either a parameter server or "
"trainer depending on our command line arguments. We'll create a "
"``ParameterServer`` if our passed in rank is 0, and a ``TrainerNet`` "
"otherwise. Note that we're using ``torch.multiprocessing`` to launch a "
"subprocess corresponding to the function that we want to execute, and "
"waiting on this process's completion from the main thread with ``p.join()``."
" In the case of initializing our trainers, we also use PyTorch's "
"`dataloaders <https://pytorch.org/docs/stable/data.html>`_ in order to "
"specify train and test data loaders on the MNIST dataset."
msgstr ""
"现在，我们将根据命令行参数创建参数服务器或训练器对应的进程。如果传入的 rank 为 0，我们会创建一个 "
"``ParameterServer``，否则创建一个 ``TrainerNet``。请注意，我们使用 ``torch.multiprocessing``"
" 启动对应执行函数的子进程，并在主线程上使用 ``p.join()`` 等待该进程完成。在初始化训练器的情况下，我们还使用 PyTorch 的 "
"`数据加载器 <https://pytorch.org/docs/stable/data.html>`_ 来指定 MNIST "
"数据集的训练和测试数据加载器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To run the example locally, run the following command worker for the server "
"and each worker you wish to spawn, in separate terminal windows: ``python "
"rpc_parameter_server.py --world_size=WORLD_SIZE --rank=RANK``. For example, "
"for a master node with world size of 2, the command would be ``python "
"rpc_parameter_server.py --world_size=2 --rank=0``. The trainer can then be "
"launched with the command ``python rpc_parameter_server.py --world_size=2 "
"--rank=1`` in a separate window, and this will begin training with one "
"server and a single trainer. Note that this tutorial assumes that training "
"occurs using between 0 and 2 GPUs, and this argument can be configured by "
"passing ``--num_gpus=N`` into the training script."
msgstr ""
"为了在本地运行该示例，请在单独的终端窗口中为服务器及每个工作器运行以下命令：``python rpc_parameter_server.py "
"--world_size=WORLD_SIZE --rank=RANK``。例如，对于一个世界大小为 2 的主节点，命令为 ``python "
"rpc_parameter_server.py --world_size=2 --rank=0``。然后可以通过命令 ``python "
"rpc_parameter_server.py --world_size=2 --rank=1`` "
"在单独的窗口中启动训练器，这将开始使用一个服务器和一个训练器进行训练。请注意，此教程假定使用 0 至 2 个 GPU 进行训练，可以通过向训练脚本传递 "
"``--num_gpus=N`` 参数进行配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can pass in the command line arguments ``--master_addr=ADDRESS`` and "
"``--master_port=PORT`` to indicate the address and port that the master "
"worker is listening on, for example, to test functionality where trainers "
"and master nodes run on different machines."
msgstr ""
"可以通过命令行参数 ``--master_addr=ADDRESS`` 和 ``--master_port=PORT`` "
"指定主工作器的监听地址和端口，例如测试训练器和主节点在不同机器上运行的功能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Getting Started with Distributed RPC Framework"
msgstr "分布式 RPC 框架入门"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst>`__"
" 中查看和编辑此教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial uses two simple examples to demonstrate how to build "
"distributed training with the `torch.distributed.rpc "
"<https://pytorch.org/docs/stable/rpc.html>`__ package which was first "
"introduced as an experimental feature in PyTorch v1.4. Source code of the "
"two examples can be found in `PyTorch examples "
"<https://github.com/pytorch/examples>`__."
msgstr ""
"本教程使用两个简单示例演示如何利用具有实验性特性的 `torch.distributed.rpc "
"<https://pytorch.org/docs/stable/rpc.html>`__ 包（在 PyTorch v1.4 "
"首次引入）创建分布式训练。两个示例的源码可在 `PyTorch examples "
"<https://github.com/pytorch/examples>`__ 中找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Previous tutorials, `Getting Started With Distributed Data Parallel "
"<ddp_tutorial.html>`__ and `Writing Distributed Applications With PyTorch "
"<dist_tuto.html>`__, described `DistributedDataParallel "
"<https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html>`__"
" which supports a specific training paradigm where the model is replicated "
"across multiple processes and each process handles a split of the input "
"data. Sometimes, you might run into scenarios that require different "
"training paradigms. For example:"
msgstr ""
"前面的教程 `分布式数据并行入门 <ddp_tutorial.html>`__ 和 `使用 PyTorch 编写分布式应用程序 "
"<dist_tuto.html>`__ 描述了 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/_modules/torch/nn/parallel/distributed.html>`__，它支持一种特定的训练范式，将模型复制到多个进程中，每个进程处理输入数据的一部分。有时可能会遇到需要不同训练范式的场景，例如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In reinforcement learning, it might be relatively expensive to acquire "
"training data from environments while the model itself can be quite small. "
"In this case, it might be useful to spawn multiple observers running in "
"parallel and share a single agent. In this case, the agent takes care of the"
" training locally, but the application would still need libraries to send "
"and receive data between observers and the trainer."
msgstr ""
"在强化学习中，从环境中获取训练数据可能相对昂贵，而模型本身可能相对较小。在这种情况下，可能需要生成多个观察者并行运行，并共享单个代理。在这个情况下，代理负责本地训练，但应用仍然需要库来在观察者和训练器之间发送和接收数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Your model might be too large to fit in GPUs on a single machine, and hence "
"would need a library to help split the model onto multiple machines. Or you "
"might be implementing a `parameter server "
"<https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf>`__ training "
"framework, where model parameters and trainers live on different machines."
msgstr ""
"你的模型可能太大，无法容纳在单台机器的 GPU 上，因此需要一个库来帮助将模型拆分到多台机器上。或者你可能在实现一种 `参数服务器 "
"<https://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf>`__ "
"训练框架，其中模型参数和训练器位于不同的机器上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The `torch.distributed.rpc <https://pytorch.org/docs/stable/rpc.html>`__ "
"package can help with the above scenarios. In case 1, `RPC "
"<https://pytorch.org/docs/stable/rpc.html#rpc>`__ and `RRef "
"<https://pytorch.org/docs/stable/rpc.html#rref>`__ allow sending data from "
"one worker to another while easily referencing remote data objects. In case "
"2, `distributed autograd "
"<https://pytorch.org/docs/stable/rpc.html#distributed-autograd-framework>`__"
" and `distributed optimizer "
"<https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim>`__"
" make executing backward pass and optimizer step as if it is local training."
" In the next two sections, we will demonstrate APIs of "
"`torch.distributed.rpc <https://pytorch.org/docs/stable/rpc.html>`__ using a"
" reinforcement learning example and a language model example. Please note, "
"this tutorial does not aim at building the most accurate or efficient models"
" to solve given problems, instead, the main goal here is to show how to use "
"the `torch.distributed.rpc <https://pytorch.org/docs/stable/rpc.html>`__ "
"package to build distributed training applications."
msgstr ""
"`torch.distributed.rpc <https://pytorch.org/docs/stable/rpc.html>`__ "
"包可以帮助解决以上场景。在场景 1 中，`RPC <https://pytorch.org/docs/stable/rpc.html#rpc>`__ 和"
" `RRef <https://pytorch.org/docs/stable/rpc.html#rref>`__ "
"允许从一个工作器向另一个工作器发送数据，并轻松引用远程数据对象。在场景 2 中，`分布式自动梯度 "
"<https://pytorch.org/docs/stable/rpc.html#distributed-autograd-framework>`__"
" 和 `分布式优化器 <https://pytorch.org/docs/stable/rpc.html#module-"
"torch.distributed.optim>`__ "
"使得执行后向传播和优化器步骤时与本地训练类似。在接下来的两个部分中，我们将使用一个强化学习示例和一个语言模型示例演示 "
"`torch.distributed.rpc <https://pytorch.org/docs/stable/rpc.html>`__ 的 "
"API。请注意，本教程并不是为了构建最准确或最高效的模型来解决问题，其主要目的是展示如何使用 `torch.distributed.rpc "
"<https://pytorch.org/docs/stable/rpc.html>`__ 包来构建分布式训练应用程序。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed Reinforcement Learning using RPC and RRef"
msgstr "使用 RPC 和 RRef 进行分布式强化学习"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This section describes steps to build a toy distributed reinforcement "
"learning model using RPC to solve CartPole-v1 from `OpenAI Gym "
"<https://www.gymlibrary.dev/environments/classic_control/cart_pole/>`__. The"
" policy code is mostly borrowed from the existing single-thread `example "
"<https://github.com/pytorch/examples/blob/master/reinforcement_learning>`__ "
"as shown below. We will skip details of the ``Policy`` design, and focus on "
"RPC usages."
msgstr ""
"本节描述了使用 RPC 构建一个示例性分布式强化学习模型以解决来自 `OpenAI Gym "
"<https://www.gymlibrary.dev/environments/classic_control/cart_pole/>`__ 的 "
"CartPole-v1。策略代码主要借鉴了现有的单线程 `示例 "
"<https://github.com/pytorch/examples/blob/master/reinforcement_learning>`__，如下所示。我们将略过"
" ``Policy`` 设计的细节，而专注于 RPC 的使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We are ready to present the observer. In this example, each observer creates"
" its own environment, and waits for the agent's command to run an episode. "
"In each episode, one observer loops at most ``n_steps`` iterations, and in "
"each iteration, it uses RPC to pass its environment state to the agent and "
"gets an action back. Then it applies that action to its environment, and "
"gets the reward and the next state from the environment. After that, the "
"observer uses another RPC to report the reward to the agent. Again, please "
"note that, this is obviously not the most efficient observer implementation."
" For example, one simple optimization could be packing current state and "
"last reward in one RPC to reduce the communication overhead. However, the "
"goal is to demonstrate RPC API instead of building the best solver for "
"CartPole. So, let's keep the logic simple and the two steps explicit in this"
" example."
msgstr ""
"我们准备好展示观察者。在此示例中，每个观察者创建自己的环境，并等待代理的指令以运行一个回合。在每个回合中，一个观察者最多循环 ``n_steps`` "
"迭代，在每次迭代中，它使用 RPC "
"将其环境状态传递给代理并获得一个动作作为响应。然后它对其环境应用该动作，并从环境中获得奖励和下一个状态。此后，观察者使用另一个 RPC "
"向代理报告奖励。同样，请注意，这显然不是最高效的观察者实现。例如，一个简单的优化可以是将当前状态和最后奖励打包到一个 RPC "
"中以减少通信开销。然而，这里的目标是展示 RPC API 而不是构建 CartPole "
"的最佳求解器。因此，让我们在此示例中保持逻辑简单，并将这两个步骤显式分开。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The code for agent is a little more complex, and we will break it into "
"multiple pieces. In this example, the agent serves as both the trainer and "
"the master, such that it sends command to multiple distributed observers to "
"run episodes, and it also records all actions and rewards locally which will"
" be used during the training phase after each episode. The code below shows "
"``Agent`` constructor where most lines are initializing various components. "
"The loop at the end initializes observers remotely on other workers, and "
"holds ``RRefs`` to those observers locally. The agent will use those "
"observer ``RRefs`` later to send commands. Applications don't need to worry "
"about the lifetime of ``RRefs``. The owner of each ``RRef`` maintains a "
"reference counting map to track its lifetime, and guarantees the remote data"
" object will not be deleted as long as there is any live user of that "
"``RRef``. Please refer to the ``RRef`` `design doc "
"<https://pytorch.org/docs/stable/rpc/rref.html>`__ for details."
msgstr ""
"代理的代码稍微复杂一些，我们将其分成多个部分。在此示例中，代理既充当训练器又充当主节点，因此它向多个分布式观察者发送运行回合的指令，还会在本地记录所有动作和奖励，这些将在每个回合后训练阶段使用。下面的代码展示了"
" ``Agent`` 的构造函数，其中大多数行在初始化各种组件。最后的循环在其他工作器上远程初始化观察者，并在本地保留这些观察者的 "
"``RRef``。代理稍后将使用这些观察者的 ``RRefs`` 发送指令。应用程序无需担心 ``RRefs`` 的生命周期。每个 ``RRef`` "
"的所有者维护一个引用计数映射以跟踪其生命周期，并保证只要有任何活动用户使用该 ``RRef``，远程数据对象不会被删除。请参考 ``RRef`` "
"`设计文档 <https://pytorch.org/docs/stable/rpc/rref.html>`__ 了解详情。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, the agent exposes two APIs to observers for selecting actions and "
"reporting rewards. Those functions only run locally on the agent, but will "
"be triggered by observers through RPC."
msgstr "接下来，代理向观察者公开两个 API，用于选择动作和报告奖励。那些函数仅在代理上本地运行，但将通过 RPC 被观察者触发。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's add a ``run_episode`` function on agent which tells all observers to "
"execute an episode. In this function, it first creates a list to collect "
"futures from asynchronous RPCs, and then loop over all observer ``RRefs`` to"
" make asynchronous RPCs. In these RPCs, the agent also passes an ``RRef`` of"
" itself to the observer, so that the observer can call functions on the "
"agent as well. As shown above, each observer will make RPCs back to the "
"agent, which are nested RPCs. After each episode, the ``saved_log_probs`` "
"and ``rewards`` will contain the recorded action probs and rewards."
msgstr ""
"让我们在代理（agent）上添加一个``run_episode``函数，该函数告诉所有观察者执行一个回合。在此函数中，首先创建一个列表，用于收集异步RPC的futures，然后遍历所有观察者的``RRefs``，进行异步RPC调用。在这些RPC中，代理还将自己的``RRef``传递给观察者，以便观察者也可以调用代理上的函数。如上所示，每个观察者将向代理发起RPC请求，这些是嵌套的RPC调用。在每个回合之后，``saved_log_probs``和``rewards``将包含记录的动作概率和奖励。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, after one episode, the agent needs to train the model, which is "
"implemented in the ``finish_episode`` function below. There is no RPCs in "
"this function and it is mostly borrowed from the single-thread `example "
"<https://github.com/pytorch/examples/blob/master/reinforcement_learning>`__."
" Hence, we skip describing its contents."
msgstr ""
"最后，在一个回合之后，代理需要训练模型，这在下面的``finish_episode``函数中实现。该函数中没有RPC，且大部分内容借鉴自单线程的`示例 "
"<https://github.com/pytorch/examples/blob/master/reinforcement_learning>`__。因此，我们略过其具体内容的描述。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With ``Policy``, ``Observer``, and ``Agent`` classes, we are ready to launch"
" multiple processes to perform the distributed training. In this example, "
"all processes run the same ``run_worker`` function, and they use the rank to"
" distinguish their role. Rank 0 is always the agent, and all other ranks are"
" observers. The agent serves as master by repeatedly calling ``run_episode``"
" and ``finish_episode`` until the running reward surpasses the reward "
"threshold specified by the environment. All observers passively waiting for "
"commands from the agent. The code is wrapped by `rpc.init_rpc "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.init_rpc>`__"
" and `rpc.shutdown "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.shutdown>`__,"
" which initializes and terminates RPC instances respectively. More details "
"are available in the `API page "
"<https://pytorch.org/docs/stable/rpc.html>`__."
msgstr ""
"有了``Policy``、``Observer``和``Agent``类，我们便可以启动多个进程进行分布式训练。在此示例中，所有进程运行相同的``run_worker``函数，并通过排名区分其角色。排名为0的进程总是作为代理，其余排名的进程为观察者。代理作为主控，通过反复调用``run_episode``和``finish_episode``，直到运行奖励超过环境指定的奖励阈值。所有观察者被动等待来自代理的命令。代码被`rpc.init_rpc"
" "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.init_rpc>`__和`rpc.shutdown"
" "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.shutdown>`__包装，这分别用于初始化和终止RPC实例。更多详情请参见`API页"
" <https://pytorch.org/docs/stable/rpc.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Below are some sample outputs when training with `world_size=2`."
msgstr "以下是使用`world_size=2`进行训练时的一些示例输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example, we show how to use RPC as the communication vehicle to pass"
" data across workers, and how to use RRef to reference remote objects. It is"
" true that you could build the entire structure directly on top of "
"``ProcessGroup`` ``send`` and ``recv`` APIs or use other communication/RPC "
"libraries. However, by using `torch.distributed.rpc`, you can get the native"
" support and continuously optimized performance under the hood."
msgstr ""
"在这个示例中，我们展示了如何使用RPC作为通信媒介来跨工作节点传递数据，以及如何使用RRef来引用远程对象。虽然你可以直接基于``ProcessGroup``的``send``和``recv``API构建整个结构，或者使用其他的通信/RPC库，但通过使用`torch.distributed.rpc`，你可以获得原生支持，并在底层实现持续优化的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we will show how to combine RPC and RRef with distributed autograd and"
" distributed optimizer to perform distributed model parallel training."
msgstr ""
"接下来，我们将展示如何将RPC与RRef结合分布式自动微分（distributed autograd）和分布式优化器（distributed "
"optimizer），以实现分布式模型并行训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed RNN using Distributed Autograd and Distributed Optimizer"
msgstr "使用分布式自动微分和分布式优化器的分布式RNN"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we use an RNN model to show how to build distributed model "
"parallel training with the RPC API. The example RNN model is very small and "
"can easily fit into a single GPU, but we still divide its layers onto two "
"different workers to demonstrate the idea. Developer can apply the similar "
"techniques to distribute much larger models across multiple devices and "
"machines."
msgstr ""
"在本节中，我们使用一个RNN模型来展示如何利用RPC "
"API构建分布式模型并行训练。示例中的RNN模型非常小，可以轻松放入单个GPU，但我们仍然将其层分布到两个不同的工作节点上以演示该方法。开发者可以采用类似技术，将更大的模型分布到多个设备和机器上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The RNN model design is borrowed from the word language model in PyTorch "
"`example "
"<https://github.com/pytorch/examples/tree/master/word_language_model>`__ "
"repository, which contains three main components, an embedding table, an "
"``LSTM`` layer, and a decoder. The code below wraps the embedding table and "
"the decoder into sub-modules, so that their constructors can be passed to "
"the RPC API. In the ``EmbeddingTable`` sub-module, we intentionally put the "
"``Embedding`` layer on GPU to cover the use case. In v1.4, RPC always "
"creates CPU tensor arguments or return values on the destination worker. If "
"the function takes a GPU tensor, you need to move it to the proper device "
"explicitly."
msgstr ""
"RNN模型的设计借鉴自PyTorch `示例 "
"<https://github.com/pytorch/examples/tree/master/word_language_model>`__库中的词语言模型，它包含三个主要组件：一个嵌入表、一个``LSTM``层和一个解码器。以下代码将嵌入表和解码器封装为子模块，以便它们的构造函数可以传递给RPC"
" "
"API。在``EmbeddingTable``子模块中，我们有意将``Embedding``层放置在GPU上，以涵盖该用例。在1.4版本中，RPC总是会在目标工作节点上创建CPU张量参数或返回值。如果函数接收GPU张量参数，你需要显式地将其移动到适当的设备上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With the above sub-modules, we can now piece them together using RPC to "
"create an RNN model. In the code below ``ps`` represents a parameter server,"
" which hosts parameters of the embedding table and the decoder. The "
"constructor uses the `remote "
"<https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.remote>`__ "
"API to create an ``EmbeddingTable`` object and a ``Decoder`` object on the "
"parameter server, and locally creates the ``LSTM`` sub-module. During the "
"forward pass, the trainer uses the ``EmbeddingTable`` ``RRef`` to find the "
"remote sub-module and passes the input data to the ``EmbeddingTable`` using "
"RPC and fetches the lookup results. Then, it runs the embedding through the "
"local ``LSTM`` layer, and finally uses another RPC to send the output to the"
" ``Decoder`` sub-module. In general, to implement distributed model parallel"
" training, developers can divide the model into sub-modules, invoke RPC to "
"create sub-module instances remotely, and use on ``RRef`` to find them when "
"necessary. As you can see in the code below, it looks very similar to "
"single-machine model parallel training. The main difference is replacing "
"``Tensor.to(device)`` with RPC functions."
msgstr ""
"有了上述子模块，我们现在可以使用RPC将它们组合在一起，创建一个RNN模型。在以下代码中，``ps``表示参数服务器，它托管嵌入表和解码器的参数。构造函数使用`remote"
" <https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.remote>`__ "
"API在参数服务器上创建一个``EmbeddingTable``对象和一个``Decoder``对象，并在本地创建``LSTM``子模块。在前向传播过程中，训练器使用``EmbeddingTable``的``RRef``查找到远程子模块，并通过RPC将输入数据传递给``EmbeddingTable``获取查找结果。然后将结果通过本地``LSTM``层，最后利用另一个RPC调用将输出传递给``Decoder``子模块。一般来说，为了实现分布式模型并行训练，开发者可以将模型分解为子模块，调用RPC在远程创建子模块实例，并在需要时通过``RRef``查找到它们。如以下代码所示，它看起来与单机的模型并行训练非常相似。主要区别在于将``Tensor.to(device)``替换为RPC函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before introducing the distributed optimizer, let's add a helper function to"
" generate a list of RRefs of model parameters, which will be consumed by the"
" distributed optimizer. In local training, applications could call "
"``Module.parameters()`` to grab references to all parameter tensors, and "
"pass it to the local optimizer for subsequent updates. However, the same API"
" does not work in distributed training scenarios as some parameters live on "
"remote machines. Therefore, instead of taking a list of parameter "
"``Tensors``, the distributed optimizer takes a list of ``RRefs``, one "
"``RRef`` per model parameter for both local and remote model parameters. The"
" helper function is pretty simple, just call ``Module.parameters()`` and "
"creates a local ``RRef`` on each of the parameters."
msgstr ""
"在介绍分布式优化器之前，添加一个辅助函数来生成模型参数的``RRefs``列表，这些列表将由分布式优化器使用。在本地训练中，应用程序可以调用``Module.parameters()``以获取所有参数张量的引用，并将其传递给本地优化器以进行后续更新。然而，在分布式训练场景中，由于一些参数位于远程机器上，无法使用相同的API。因此，分布式优化器接受一个``RRefs``列表，每个模型参数（无论本地或远程）对应一个``RRef``。辅助函数非常简单，只需调用``Module.parameters()``并在每个参数上创建一个本地``RRef``即可。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Then, as the ``RNNModel`` contains three sub-modules, we need to call "
"``_parameter_rrefs`` three times, and wrap that into another helper "
"function."
msgstr ""
"然后，由于``RNNModel``包含三个子模块，我们需要三次调用``_parameter_rrefs``，并将它们封装到另一个辅助函数中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, we are ready to implement the training loop. After initializing model "
"arguments, we create the ``RNNModel`` and the ``DistributedOptimizer``. The "
"distributed optimizer will take a list of parameter ``RRefs``, find all "
"distinct owner workers, and create the given local optimizer (i.e., ``SGD`` "
"in this case, you can use other local optimizers as well) on each of the "
"owner worker using the given arguments (i.e., ``lr=0.05``)."
msgstr ""
"现在，我们准备好实现训练循环。在初始化模型参数后，创建``RNNModel``和``DistributedOptimizer``。分布式优化器将接受一个参数``RRefs``列表，找到所有不同的所有者工作节点，并在每个所有者工作节点上使用给定的参数（如``lr=0.05``）创建给定的本地优化器（例如此处的``SGD``，也可以使用其他本地优化器）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the training loop, it first creates a distributed autograd context, which"
" will help the distributed autograd engine to find gradients and involved "
"RPC send/recv functions. The design details of the distributed autograd "
"engine can be found in its `design note "
"<https://pytorch.org/docs/stable/rpc/distributed_autograd.html>`__. Then, it"
" kicks off the forward pass as if it is a local model, and run the "
"distributed backward pass. For the distributed backward, you only need to "
"specify a list of roots, in this case, it is the loss ``Tensor``. The "
"distributed autograd engine will traverse the distributed graph "
"automatically and write gradients properly. Next, it runs the ``step`` "
"function on the distributed optimizer, which will reach out to all involved "
"local optimizers to update model parameters. Compared to local training, one"
" minor difference is that you don't need to run ``zero_grad()`` because each"
" autograd context has dedicated space to store gradients, and as we create a"
" context per iteration, those gradients from different iterations will not "
"accumulate to the same set of ``Tensors``."
msgstr ""
"在训练循环中，首先创建一个分布式自动微分上下文，它将帮助分布式自动微分引擎查找梯度以及涉及的RPC发送/接收函数。分布式自动微分引擎的设计详见其`设计说明"
" "
"<https://pytorch.org/docs/stable/rpc/distributed_autograd.html>`__。接着，启动前向传播过程，就像使用本地模型一样，并运行分布式反向传播。对于分布式反向传播，只需指定一个根节点列表，在这种情况下即损失``Tensor``。分布式自动微分引擎会自动遍历分布式计算图并正确写入梯度。接下来运行分布式优化器的``step``函数，该函数将联系所有相关的本地优化器以更新模型参数。与本地训练相比，区别是你不需要调用``zero_grad()``，因为每个自动微分上下文都有专用空间存储梯度，并且由于每次迭代都会创建一个新的上下文，不同迭代的梯度不会累积到相同的``Tensors``集合中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, let's add some glue code to launch the parameter server and the "
"trainer processes."
msgstr "最后，添加一些粘合代码以启动参数服务器和训练器进程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_scaled_dot_product_attention_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击:ref:`here "
"<sphx_glr_download_intermediate_scaled_dot_product_attention_tutorial.py>`下载完整示例代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"(Beta) Implementing High-Performance Transformers with Scaled Dot Product "
"Attention (SDPA)"
msgstr "（Beta）实现基于缩放点积注意力（SDPA）的高性能Transformer"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author:** `Driss Guessous <https://github.com/drisspg>`_"
msgstr "**作者：** `Driss Guessous <https://github.com/drisspg>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we want to highlight a new ``torch.nn.functional`` "
"function that can be helpful for implementing transformer architectures. The"
" function is named ``torch.nn.functional.scaled_dot_product_attention``. For"
" detailed description of the function, see the `PyTorch documentation "
"<https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention>`__."
" This function has already been incorporated into "
"``torch.nn.MultiheadAttention`` and ``torch.nn.TransformerEncoderLayer``."
msgstr ""
"在本教程中，我们希望突出一个有助于实现Transformer架构的新``torch.nn.functional``函数。该函数名为``torch.nn.functional.scaled_dot_product_attention``。关于此函数的详细描述，请参阅`PyTorch文档"
" "
"<https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention>`__。该函数已被集成到``torch.nn.MultiheadAttention``和``torch.nn.TransformerEncoderLayer``中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At a high level, this PyTorch function calculates the scaled dot product "
"attention (SDPA) between query, key, and value according to the definition "
"found in the paper `Attention is all you need "
"<https://arxiv.org/abs/1706.03762>`__. While this function can be written in"
" PyTorch using existing functions, a fused implementation can provide large "
"performance benefits over a naive implementation."
msgstr ""
"从高层次来看，此PyTorch函数根据论文`Attention is all you need "
"<https://arxiv.org/abs/1706.03762>`__中的定义，计算查询(Query)、键(Key)和值(Value)之间的缩放点积注意力(SDPA)。尽管可以使用现有函数在PyTorch中编写此功能，但融合实现相比于简单实现能够提供显著的性能提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fused implementations"
msgstr "融合实现"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For CUDA tensor inputs, the function will dispatch into one of the following"
" implementations:"
msgstr "对于CUDA张量输入，该函数将调度到以下实现之一："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
" <https://arxiv.org/abs/2205.14135>`__"
msgstr ""
"`FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
" <https://arxiv.org/abs/2205.14135>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Memory-Efficient Attention "
"<https://github.com/facebookresearch/xformers>`__"
msgstr ""
"`Memory-Efficient Attention "
"<https://github.com/facebookresearch/xformers>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "A PyTorch implementation defined in C++"
msgstr "用C++定义的PyTorch实现"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Explicit Dispatcher Control"
msgstr "显式调度器控制"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"While the function will implicitly dispatch to one of the three "
"implementations, the user can also explicitly control the dispatch via the "
"use of a context manager. This context manager allows users to explicitly "
"disable certain implementations. If a user wants to ensure the function is "
"indeed using the fastest implementation for their specific inputs, the "
"context manager can be used to sweep through measuring performance."
msgstr ""
"尽管该函数会自动调度到三种实现之一，用户也可以通过上下文管理器显式控制调度。这个上下文管理器允许用户明确禁用某些实现。如果用户想确保某种特定输入使用最快的实现，可以通过上下文管理器进行性能测试。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Hardware dependence"
msgstr "硬件依赖性"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Depending on what machine you ran the above cell on and what hardware is "
"available, your results might be different. - If you don’t have a GPU and "
"are running on CPU then with FP32 the context manager will have no effect "
"and all three runs should return similar timings. - Depending on what "
"compute capability your graphics card supports flash attention or memory "
"efficient might have failed."
msgstr ""
"根据运行代码的机器和可用的硬件，你的运行结果可能不同。- "
"如果你没有GPU并在CPU上运行，那么以FP32运行时上下文管理器无效，三次运行时间应该相似。- "
"根据你的显卡支持的计算能力，闪存注意力或内存高效注意力可能会失败。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Causal Self Attention"
msgstr "因果自注意力"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Below is an example implementation of a multi-headed causal self attention "
"block inspired by `Andrej Karpathy NanoGPT "
"<https://github.com/karpathy/nanoGPT>`__ repository."
msgstr ""
"以下是一个多头因果自注意力块的实现示例，灵感来自`Andrej Karpathy NanoGPT "
"<https://github.com/karpathy/nanoGPT>`__库。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``NestedTensor`` and Dense tensor support"
msgstr "``NestedTensor``和密集张量的支持"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"SDPA supports both ``NestedTensor`` and Dense tensor inputs. "
"``NestedTensors`` handle the case where the input is a batch of variable "
"length sequences without needing to pad each sequence to the maximum length "
"in the batch. For more information about ``NestedTensors`` see `torch.nested"
" <https://pytorch.org/docs/stable/nested.html>`__ and `NestedTensors "
"Tutorial <https://pytorch.org/tutorials/prototype/nestedtensor.html>`__."
msgstr ""
"SDPA支持``NestedTensor``和密集张量输入。``NestedTensors``处理输入是可变长度序列的批量，而无需为每个序列填充到批次的最大长度。有关``NestedTensors``的更多信息，请参阅`torch.nested"
" <https://pytorch.org/docs/stable/nested.html>`__和`NestedTensors教程 "
"<https://pytorch.org/tutorials/prototype/nestedtensor.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using SDPA with ``torch.compile``"
msgstr "将SDPA与``torch.compile``结合使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With the release of PyTorch 2.0, a new feature called ``torch.compile()`` "
"has been introduced, which can provide significant performance improvements "
"over eager mode. Scaled dot product attention is fully composable with "
"``torch.compile()``. To demonstrate this, let's compile the "
"``CausalSelfAttention`` module using ``torch.compile()`` and observe the "
"resulting performance improvements."
msgstr ""
"随着PyTorch "
"2.0的发布，引入了一个新功能“torch.compile()”，该功能可以显著提升急切模式的性能。缩放点积注意力完全可以与“torch.compile()”组合使用。为了演示这一点，我们将使用“torch.compile()”编译“CausalSelfAttention”模块，并观察由此带来的性能提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The exact execution time is dependent on machine, however the results for "
"mine: The non compiled module runs in  166.616 microseconds The compiled "
"module runs in  166.726 microseconds That is not what we were expecting. "
"Let's dig a little deeper. PyTorch comes with an amazing built-in profiler "
"that you can use to inspect the performance characteristics of your code."
msgstr ""
"具体的执行时间取决于机器，然而在我的机器上的结果如下：未编译模块运行时间为166.616微秒，编译模块运行时间为166.726微秒。这不是我们预期的结果。让我们深入一下。PyTorch内置了一个非常出色的性能分析工具，可以用来检查代码的性能特性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The previous code snippet generates a report of the top 10 PyTorch functions"
" that consumed the most GPU execution time, for both the compiled and non-"
"compiled module. The analysis reveals that the majority of time spent on the"
" GPU is concentrated on the same set of functions for both modules. The "
"reason for this here is that ``torch.compile`` is very good at removing the "
"framework overhead associated with PyTorch. If your model is launching "
"large, efficient CUDA kernels, which in this case ``CausalSelfAttention`` "
"is, then the overhead of PyTorch can be hidden."
msgstr ""
"前面的代码片段生成了一个报告，列出了消耗GPU执行时间最多的前10个PyTorch函数，适用于编译和未编译的模块。这项分析显示，大部分GPU时间都集中在两种模块中同一组函数的执行上。这里的原因是“torch.compile”非常擅长去除与PyTorch相关的框架开销。如果您的模型正在启动大型、效率高的CUDA内核（如在此例中的“CausalSelfAttention”），那么PyTorch的开销可以被隐藏。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In reality, your module does not normally consist of a singular "
"``CausalSelfAttention`` block. When experimenting with `Andrej Karpathy "
"NanoGPT <https://github.com/karpathy/nanoGPT>`__ repository, compiling the "
"module took the time per train step from: ``6090.49ms`` to ``3273.17ms``! "
"This was done on commit: ``ae3a8d5`` of NanoGPT training on the Shakespeare "
"dataset."
msgstr ""
"实际上，您的模块通常不会仅仅由一个单一的“CausalSelfAttention”块组成。在尝试“Andrej Karpathy NanoGPT "
"<https://github.com/karpathy/nanoGPT>`__存储库时，编译模块将每次训练步骤时间从“6090.49毫秒”减少到“3273.17毫秒”！这是在NanoGPT使用Shakespeare数据集进行训练的提交版本“ae3a8d5”下完成的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Using SDPA with attn_bias subclasses"
msgstr "使用带有attn_bias子类的SDPA"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we have demonstrated the basic usage of "
"``torch.nn.functional.scaled_dot_product_attention``. We have shown how the "
"``sdpa_kernel`` context manager can be used to assert a certain "
"implementation is used on GPU. As well, we built a simple "
"``CausalSelfAttention`` module that works with ``NestedTensor`` and is torch"
" compilable. In the process we have shown how to the profiling tools can be "
"used to explore the performance characteristics of a user defined module."
msgstr ""
"在本教程中，我们演示了“torch.nn.functional.scaled_dot_product_attention”的基本用法。我们展示了如何使用“sdpa_kernel”上下文管理器在GPU上断言某个具体的实现被使用。同时，我们构建了一个简单的“CausalSelfAttention”模块，能够与“NestedTensor”一起工作并且可以使用torch编译。在此过程中，我们展示了如何使用性能分析工具来探索用户定义模块的性能特性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: "
"scaled_dot_product_attention_tutorial.py "
"<scaled_dot_product_attention_tutorial.py>`"
msgstr ""
":download:`下载Python源码: scaled_dot_product_attention_tutorial.py "
"<scaled_dot_product_attention_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: "
"scaled_dot_product_attention_tutorial.ipynb "
"<scaled_dot_product_attention_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: scaled_dot_product_attention_tutorial.ipynb "
"<scaled_dot_product_attention_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_seq2seq_translation_tutorial.py>` to "
"download the full example code"
msgstr ""
"单击:ref:`这里 "
"<sphx_glr_download_intermediate_seq2seq_translation_tutorial.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"NLP From Scratch: Translation with a Sequence to Sequence Network and "
"Attention"
msgstr "从零开始的NLP：使用序列到序列网络和注意力进行翻译"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is the third and final tutorial on doing **NLP From Scratch**, where we"
" write our own classes and functions to preprocess the data to do our NLP "
"modeling tasks."
msgstr "这是关于从零开始进行**NLP**的第三个也是最后一个教程，我们将编写自己的类和函数来预处理数据，以完成我们的NLP建模任务。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this project we will be teaching a neural network to translate from "
"French to English."
msgstr "在这个项目中，我们将教一个神经网络从法语翻译到英语。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "... to varying degrees of success."
msgstr "...成功的程度各不相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is made possible by the simple but powerful idea of the `sequence to "
"sequence network <https://arxiv.org/abs/1409.3215>`__, in which two "
"recurrent neural networks work together to transform one sequence to "
"another. An encoder network condenses an input sequence into a vector, and a"
" decoder network unfolds that vector into a new sequence."
msgstr ""
"这得益于一个简单但强大的想法，即`序列到序列网络 "
"<https://arxiv.org/abs/1409.3215>`__，其中两个循环神经网络协同工作，将一个序列转换为另一个序列。编码器网络将输入序列浓缩为一个向量，解码器网络再将该向量扩展为一个新的序列。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To improve upon this model we'll use an `attention mechanism "
"<https://arxiv.org/abs/1409.0473>`__, which lets the decoder learn to focus "
"over a specific range of the input sequence."
msgstr ""
"为了改善这个模型，我们将使用一个`注意力机制 "
"<https://arxiv.org/abs/1409.0473>`__，这使解码器能够学习在输入序列的特定范围上集中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It would also be useful to know about Sequence to Sequence networks and how "
"they work:"
msgstr "了解有关序列到序列网络及其工作原理的内容也很有用："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Learning Phrase Representations using RNN Encoder-Decoder for Statistical "
"Machine Translation <https://arxiv.org/abs/1406.1078>`__"
msgstr "`使用RNN编码器解码器进行统计机器翻译的短语表示学习 <https://arxiv.org/abs/1406.1078>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Sequence to Sequence Learning with Neural Networks "
"<https://arxiv.org/abs/1409.3215>`__"
msgstr "`使用神经网络进行序列到序列学习 <https://arxiv.org/abs/1409.3215>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Neural Machine Translation by Jointly Learning to Align and Translate "
"<https://arxiv.org/abs/1409.0473>`__"
msgstr "`通过联合学习对齐和翻译的神经机器翻译 <https://arxiv.org/abs/1409.0473>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`A Neural Conversational Model <https://arxiv.org/abs/1506.05869>`__"
msgstr "`一个神经对话模型 <https://arxiv.org/abs/1506.05869>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You will also find the previous tutorials on "
":doc:`/intermediate/char_rnn_classification_tutorial` and "
":doc:`/intermediate/char_rnn_generation_tutorial` helpful as those concepts "
"are very similar to the Encoder and Decoder models, respectively."
msgstr ""
"您还会发现前面的教程 :doc:`/intermediate/char_rnn_classification_tutorial` 和 "
":doc:`/intermediate/char_rnn_generation_tutorial` 很有帮助，因为这些概念与编码器和解码器模型非常相似。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Requirements**"
msgstr "**要求**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Loading data files"
msgstr "加载数据文件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The data for this project is a set of many thousands of English to French "
"translation pairs."
msgstr "这个项目的数据是一组包含成千上万句英语到法语翻译对的数据集。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`This question on Open Data Stack Exchange "
"<https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-"
"translated-into-many-languages>`__ pointed me to the open translation site "
"https://tatoeba.org/ which has downloads available at "
"https://tatoeba.org/eng/downloads - and better yet, someone did the extra "
"work of splitting language pairs into individual text files here: "
"https://www.manythings.org/anki/"
msgstr ""
"`Open Data Stack Exchange上的这个问题 "
"<https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-"
"translated-into-many-languages>`__ "
"指引我去查找开放翻译网站https://tatoeba.org/，这个网站在https://tatoeba.org/eng/downloads "
"上提供了下载选项，更好的是，有人额外提供了将语言对拆分为单独的文本文件的工作，可以在这里找到: "
"https://www.manythings.org/anki/"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The English to French pairs are too big to include in the repository, so "
"download to ``data/eng-fra.txt`` before continuing. The file is a tab "
"separated list of translation pairs:"
msgstr ""
"英语到法语的数据对太大，无法包含在存储库中，请下载到``data/eng-fra.txt``后再继续。该文件是一份以制表符分隔的翻译对列表："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Similar to the character encoding used in the character-level RNN tutorials,"
" we will be representing each word in a language as a one-hot vector, or "
"giant vector of zeros except for a single one (at the index of the word). "
"Compared to the dozens of characters that might exist in a language, there "
"are many many more words, so the encoding vector is much larger. We will "
"however cheat a bit and trim the data to only use a few thousand words per "
"language."
msgstr ""
"类似于字符级RNN教程中使用的字符编码，我们将使用单热向量（即一个巨大的零向量，除了单个以单词索引为索引的位置为一）来表示每种语言中的每个单词。与一个语言中的几十个字符相比，单词会更多，因此编码向量要大得多。不过我们会稍作取舍，只使用每种语言中的几千个单词。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll need a unique index per word to use as the inputs and targets of the "
"networks later. To keep track of all this we will use a helper class called "
"``Lang`` which has word → index (``word2index``) and index → word "
"(``index2word``) dictionaries, as well as a count of each word "
"``word2count`` which will be used to replace rare words later."
msgstr ""
"为了在后续使用这些数据时作为网络的输入和目标，我们需要为每个单词分配一个唯一索引。为此我们将使用一个名为``Lang``的辅助类，包含单词到索引（``word2index``）和索引到单词（``index2word``）的字典，以及每个单词的计数``word2count``，这些计数将用于后续替换罕见单词。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The files are all in Unicode, to simplify we will turn Unicode characters to"
" ASCII, make everything lowercase, and trim most punctuation."
msgstr "这些文件全部为Unicode格式，为了简化，我们会将Unicode字符转换为ASCII，将所有内容转为小写并去掉大部分标点符号。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To read the data file we will split the file into lines, and then split "
"lines into pairs. The files are all English → Other Language, so if we want "
"to translate from Other Language → English I added the ``reverse`` flag to "
"reverse the pairs."
msgstr ""
"读取数据文件的方法是，将文件拆分为行，然后将行拆分为翻译对。由于这些文件的方向是英语到其他语言，为了实现从其他语言到英语的翻译，我添加了一个``reverse``标志用于翻转翻译对。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since there are a *lot* of example sentences and we want to train something "
"quickly, we'll trim the data set to only relatively short and simple "
"sentences. Here the maximum length is 10 words (that includes ending "
"punctuation) and we're filtering to sentences that translate to the form \"I"
" am\" or \"He is\" etc. (accounting for apostrophes replaced earlier)."
msgstr ""
"由于有*很多*示例句子并希望快速训练一个模型，我们会将数据集裁减为相对短小简单的句子。在这里的最大长度为10个单词（包括结束标点符号），并会过滤出翻译形式为“我是”或“他是”等句子的句子（考虑前面处理掉的撇号）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The full process for preparing the data is:"
msgstr "准备数据的完整流程如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Read text file and split into lines, split lines into pairs"
msgstr "读取文本文件并拆分为行，将行拆分成翻译对"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Normalize text, filter by length and content"
msgstr "规范化文本，按照长度和内容进行过滤"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Make word lists from sentences in pairs"
msgstr "从翻译对的句子中生成单词词表"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The Seq2Seq Model"
msgstr "Seq2Seq模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A Recurrent Neural Network, or RNN, is a network that operates on a sequence"
" and uses its own output as input for subsequent steps."
msgstr "循环神经网络（RNN）是一种根据序列操作并使用自身输出作为后续步骤输入的网络。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or "
"seq2seq network, or `Encoder Decoder network "
"<https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model consisting of two "
"RNNs called the encoder and decoder. The encoder reads an input sequence and"
" outputs a single vector, and the decoder reads that vector to produce an "
"output sequence."
msgstr ""
"一个`序列到序列网络 <https://arxiv.org/abs/1409.3215>`__，又称为seq2seq网络或者`编码器解码器网络 "
"<https://arxiv.org/pdf/1406.1078v3.pdf>`__，是一种由两个RNN组成的模型，分别称为编码器和解码器。编码器读取输入序列并输出一个向量，解码器读取该向量以生成输出序列。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Unlike sequence prediction with a single RNN, where every input corresponds "
"to an output, the seq2seq model frees us from sequence length and order, "
"which makes it ideal for translation between two languages."
msgstr ""
"与单一RNN进行的序列预测不同，在seq2seq模型中，每个输入对应一个输出，而seq2seq模型使我们摆脱了序列长度和顺序的限制，非常适合用于两种语言之间的翻译。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Consider the sentence ``Je ne suis pas le chat noir`` → ``I am not the black"
" cat``. Most of the words in the input sentence have a direct translation in"
" the output sentence, but are in slightly different orders, e.g. ``chat "
"noir`` and ``black cat``. Because of the ``ne/pas`` construction there is "
"also one more word in the input sentence. It would be difficult to produce a"
" correct translation directly from the sequence of input words."
msgstr ""
"考虑句子``Je ne suis pas le chat noir`` → ``I am not the black "
"cat``。输入句子中的大多数单词在输出句子中都有直接翻译，但顺序略有不同，例如``chat noir``和``black "
"cat``。由于``ne/pas``结构，输入句子中还有一个额外的词。如果直接根据输入单词序列生成正确的翻译会比较困难。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With a seq2seq model the encoder creates a single vector which, in the ideal"
" case, encodes the \"meaning\" of the input sequence into a single vector — "
"a single point in some N dimensional space of sentences."
msgstr "使用seq2seq模型，编码器创建一个单一的向量，在理想情况下，它将输入序列的“意义”编码为一个向量——某个N维空间中的单一节点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The Encoder"
msgstr "编码器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The encoder of a seq2seq network is a RNN that outputs some value for every "
"word from the input sentence. For every input word the encoder outputs a "
"vector and a hidden state, and uses the hidden state for the next input "
"word."
msgstr ""
"seq2seq网络的编码器是一种RNN，它为输入句子的每个单词输出一个值。对于每个输入单词，编码器输出一个向量和一个隐层状态，并将该隐层状态用于下一个输入单词。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The Decoder"
msgstr "解码器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The decoder is another RNN that takes the encoder output vector(s) and "
"outputs a sequence of words to create the translation."
msgstr "解码器是另一种RNN，它接收编码器输出的向量并生成一个词序列以完成翻译。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Simple Decoder"
msgstr "简单解码器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the simplest seq2seq decoder we use only last output of the encoder. This"
" last output is sometimes called the *context vector* as it encodes context "
"from the entire sequence. This context vector is used as the initial hidden "
"state of the decoder."
msgstr ""
"在最简单的seq2seq解码器中，我们只使用编码器的最后一个输出。这最后一个输出有时被称为*上下文向量*，因为它编码了整个序列的上下文信息。上下文向量用作解码器的初始隐层状态。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At every step of decoding, the decoder is given an input token and hidden "
"state. The initial input token is the start-of-string ``<SOS>`` token, and "
"the first hidden state is the context vector (the encoder's last hidden "
"state)."
msgstr ""
"在解码的每一步中，解码器会接收一个输入标记和隐层状态。初始输入标记是“字符串开始``<SOS>``”标记，第一个隐层状态是上下文向量（编码器的最后一个隐层状态）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"I encourage you to train and observe the results of this model, but to save "
"space we'll be going straight for the gold and introducing the Attention "
"Mechanism."
msgstr "我鼓励您训练并观察该模型的结果，但为了节省空间，我们将直接介绍更加高级的部分：注意力机制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Attention Decoder"
msgstr "注意力解码器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If only the context vector is passed between the encoder and decoder, that "
"single vector carries the burden of encoding the entire sentence."
msgstr "如果仅在编码器和解码器之间传递上下文向量，那么单一向量就需要承担起编码整个句子的负担。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Attention allows the decoder network to \"focus\" on a different part of the"
" encoder's outputs for every step of the decoder's own outputs. First we "
"calculate a set of *attention weights*. These will be multiplied by the "
"encoder output vectors to create a weighted combination. The result (called "
"``attn_applied`` in the code) should contain information about that specific"
" part of the input sequence, and thus help the decoder choose the right "
"output words."
msgstr ""
"注意力使解码器网络能够在每一步选择对编码器的输出的不同部分进行“关注”。首先我们计算一组*注意力权重*。这些权重将乘以编码器的输出向量以创建一个加权组合。结果（在代码中称为``attn_applied``）应该包含输入序列中特定部分的信息，因此能够帮助解码器选择正确的输出单词。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Calculating the attention weights is done with another feed-forward layer "
"``attn``, using the decoder's input and hidden state as inputs. Because "
"there are sentences of all sizes in the training data, to actually create "
"and train this layer we have to choose a maximum sentence length (input "
"length, for encoder outputs) that it can apply to. Sentences of the maximum "
"length will use all the attention weights, while shorter sentences will only"
" use the first few."
msgstr ""
"计算注意力权重是通过另一个前馈层``attn``来完成的，使用解码器的输入和隐层状态作为输入。由于训练数据中有长度不同的句子，要真正创建和训练这一层，我们需要选择一个最大句子长度（作为编码器输出的输入长度），该层能够适用于这些句子。最大长度的句子将使用所有注意力权重，而较短的句子只使用前几个权重。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Bahdanau attention, also known as additive attention, is a commonly used "
"attention mechanism in sequence-to-sequence models, particularly in neural "
"machine translation tasks. It was introduced by Bahdanau et al. in their "
"paper titled `Neural Machine Translation by Jointly Learning to Align and "
"Translate <https://arxiv.org/pdf/1409.0473.pdf>`__. This attention mechanism"
" employs a learned alignment model to compute attention scores between the "
"encoder and decoder hidden states. It utilizes a feed-forward neural network"
" to calculate alignment scores."
msgstr ""
"Bahdanau注意机制，也称为加性注意机制，是序列到序列模型中常用的注意机制，特别是在神经机器翻译任务中。它由Bahdanau等人在其论文《通过联合学习对齐和翻译进行神经机器翻译》中提出（<https://arxiv.org/pdf/1409.0473.pdf>）。该注意机制采用一个学习的对齐模型来计算编码器和解码器隐藏状态之间的注意分数。它使用一个前馈神经网络来计算对齐分数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, there are alternative attention mechanisms available, such as Luong"
" attention, which computes attention scores by taking the dot product "
"between the decoder hidden state and the encoder hidden states. It does not "
"involve the non-linear transformation used in Bahdanau attention."
msgstr ""
"然而，还有其他可选的注意机制，例如Luong注意机制，它通过解码器隐藏状态与编码器隐藏状态之间的点积来计算注意分数。它不涉及Bahdanau注意机制中使用的非线性变换。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will be using Bahdanau attention. However, it would be "
"a valuable exercise to explore modifying the attention mechanism to use "
"Luong attention."
msgstr "在本教程中，我们将使用Bahdanau注意机制。不过，探索修改注意机制以使用Luong注意机制是一个有价值的练习。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are other forms of attention that work around the length limitation by"
" using a relative position approach. Read about \"local attention\" in "
"`Effective Approaches to Attention-based Neural Machine Translation "
"<https://arxiv.org/abs/1508.04025>`__."
msgstr ""
"还有一些其他形式的注意机制通过使用相对位置的方法来解决长度限制问题。阅读关于《基于注意力的神经机器翻译的有效方法》中描述的“局部注意”机制（<https://arxiv.org/abs/1508.04025>）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Preparing Training Data"
msgstr "准备训练数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To train, for each pair we will need an input tensor (indexes of the words "
"in the input sentence) and target tensor (indexes of the words in the target"
" sentence). While creating these vectors we will append the EOS token to "
"both sequences."
msgstr ""
"为了训练，对于每一对数据，我们需要一个输入张量（输入句子中单词的索引）和一个目标张量（目标句子中单词的索引）。在创建这些向量时，我们会在两个序列后附加EOS标记。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training the Model"
msgstr "模型训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To train we run the input sentence through the encoder, and keep track of "
"every output and the latest hidden state. Then the decoder is given the "
"``<SOS>`` token as its first input, and the last hidden state of the encoder"
" as its first hidden state."
msgstr ""
"在训练过程中，我们将输入句子通过编码器运行，并跟踪每一个输出以及最新的隐藏状态。然后，解码器接收``<SOS>``标记作为其第一个输入，并使用编码器的最后隐藏状态作为其第一个隐藏状态。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"\"Teacher forcing\" is the concept of using the real target outputs as each "
"next input, instead of using the decoder's guess as the next input. Using "
"teacher forcing causes it to converge faster but `when the trained network "
"is exploited, it may exhibit instability "
"<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>`__."
msgstr ""
"“教师强制”是一种使用真实目标输出作为每个下一输入的概念，而不是使用解码器的预测作为下一输入。使用教师强制能够加速收敛，但在`当使用训练后的网络时，可能会表现出不稳定性`（<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf>）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can observe outputs of teacher-forced networks that read with coherent "
"grammar but wander far from the correct translation - intuitively it has "
"learned to represent the output grammar and can \"pick up\" the meaning once"
" the teacher tells it the first few words, but it has not properly learned "
"how to create the sentence from the translation in the first place."
msgstr ""
"你可以观察到教师强制网络的输出，其虽然具有连贯的语法，但可能远离正确的翻译——直观上，它已经学习如何表现输出语法，并“拾取”意义，一旦教师告诉他前几个词，但它并没有从一开始就正确地从翻译创建句子的能力。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Because of the freedom PyTorch's autograd gives us, we can randomly choose "
"to use teacher forcing or not with a simple if statement. Turn "
"``teacher_forcing_ratio`` up to use more of it."
msgstr ""
"由于PyTorch的自动微分功能赋予了我们的自由，我们可以使用一个简单的if语句随机选择是否使用教师强制。将``teacher_forcing_ratio``调高以使用更多的教师强制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This is a helper function to print time elapsed and estimated time remaining"
" given the current time and progress %."
msgstr "这是一个辅助函数，用于根据当前时间和进度百分比打印已用时间和估计剩余时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The whole training process looks like this:"
msgstr "整个训练过程如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Start a timer"
msgstr "启动计时器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Initialize optimizers and criterion"
msgstr "初始化优化器和损失函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Create set of training pairs"
msgstr "创建训练对集合"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Start empty losses array for plotting"
msgstr "启动用于绘图的空损失数组"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Then we call ``train`` many times and occasionally print the progress (% of "
"examples, time so far, estimated time) and average loss."
msgstr "然后我们调用``train``多次，并偶尔打印进度（示例的百分比、到目前为止的时间、估计时间）和平均损失。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Plotting results"
msgstr "绘制结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Plotting is done with matplotlib, using the array of loss values "
"``plot_losses`` saved while training."
msgstr "使用matplotlib绘图，使用训练期间保存的损失值数组``plot_losses``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Evaluation"
msgstr "评估"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Evaluation is mostly the same as training, but there are no targets so we "
"simply feed the decoder's predictions back to itself for each step. Every "
"time it predicts a word we add it to the output string, and if it predicts "
"the EOS token we stop there. We also store the decoder's attention outputs "
"for display later."
msgstr ""
"评估与训练大致相同，但没有目标标注，因此我们简单地将解码器的预测结果在每一步反馈给自身。每次它预测一个词，我们将其添加到输出字符串中，如果它预测到EOS标记，我们就在那里停止。同样，我们存储解码器的注意力输出以供稍后显示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can evaluate random sentences from the training set and print out the "
"input, target, and output to make some subjective quality judgements:"
msgstr "我们可以从训练集随机评估句子，并打印出输入、目标和输出，以进行一些主观质量判断："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training and Evaluating"
msgstr "训练和评估"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With all these helper functions in place (it looks like extra work, but it "
"makes it easier to run multiple experiments) we can actually initialize a "
"network and start training."
msgstr "有了所有这些辅助函数（看起来像是额外工作，但使运行多个试验更容易），我们实际上可以初始化一个网络并开始训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember that the input sentences were heavily filtered. For this small "
"dataset we can use relatively small networks of 256 hidden nodes and a "
"single GRU layer. After about 40 minutes on a MacBook CPU we'll get some "
"reasonable results."
msgstr ""
"请记住，输入句子被进行了大量过滤。对于这个小型数据集，我们可以使用256个隐藏节点和单层GRU的相对较小的网络。在MacBook "
"CPU上运行约40分钟后，会得到一些不错的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you run this notebook you can train, interrupt the kernel, evaluate, and "
"continue training later. Comment out the lines where the encoder and decoder"
" are initialized and run ``trainIters`` again."
msgstr ""
"如果你运行这个笔记本，你可以先训练，随后中断内核，评估结果，稍后继续训练。注释掉初始化编码器和解码器的代码行，并再次运行``trainIters``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Set dropout layers to ``eval`` mode"
msgstr "将dropout层设置为``eval``模式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Visualizing Attention"
msgstr "可视化注意机制"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A useful property of the attention mechanism is its highly interpretable "
"outputs. Because it is used to weight specific encoder outputs of the input "
"sequence, we can imagine looking where the network is focused most at each "
"time step."
msgstr "注意机制的一个有用属性是其输出高度可解释。由于它被用于加权输入序列中特定的编码器输出，我们可以想象观察网络在每个时间步专注的地方。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You could simply run ``plt.matshow(attentions)`` to see attention output "
"displayed as a matrix. For a better viewing experience we will do the extra "
"work of adding axes and labels:"
msgstr "你可以简单运行``plt.matshow(attentions)``以矩阵形式显示注意输出。为了更好的观看体验，我们将额外添加轴和标签："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Try with a different dataset"
msgstr "尝试使用不同的数据集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Another language pair"
msgstr "另一种语言对"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Human → Machine (e.g. IOT commands)"
msgstr "人类 → 机器（例如，物联网命令）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Chat → Response"
msgstr "聊天 → 回复"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Question → Answer"
msgstr "问题 → 答案"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Replace the embeddings with pretrained word embeddings such as ``word2vec`` "
"or ``GloVe``"
msgstr "用预训练的词嵌入（如``word2vec``或``GloVe``）替换嵌入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Try with more layers, more hidden units, and more sentences. Compare the "
"training time and results."
msgstr "尝试使用更多的层、更多的隐藏单元和更多的句子。比较训练时间和结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you use a translation file where pairs have two of the same phrase (``I "
"am test \\t I am test``), you can use this as an autoencoder. Try this:"
msgstr ""
"如果你使用一个翻译文件，其中的对是两个相同的短语（``I am test \\t I am test``），你可以将其作为一个自动编码器。试试这个："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Train as an autoencoder"
msgstr "训练作为一个自动编码器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Save only the Encoder network"
msgstr "只保存编码器网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Train a new Decoder for translation from there"
msgstr "从这里训练一个新的解码器用于翻译"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: seq2seq_translation_tutorial.py "
"<seq2seq_translation_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：seq2seq_translation_tutorial.py "
"<seq2seq_translation_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: seq2seq_translation_tutorial.ipynb "
"<seq2seq_translation_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook：seq2seq_translation_tutorial.ipynb "
"<seq2seq_translation_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_spatial_transformer_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`此处 "
"<sphx_glr_download_intermediate_spatial_transformer_tutorial.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Spatial Transformer Networks Tutorial"
msgstr "空间变换网络教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author**: `Ghassen HAMROUNI <https://github.com/GHamrouni>`_"
msgstr "**作者**：`Ghassen HAMROUNI <https://github.com/GHamrouni>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, you will learn how to augment your network using a visual "
"attention mechanism called spatial transformer networks. You can read more "
"about the spatial transformer networks in the `DeepMind paper "
"<https://arxiv.org/abs/1506.02025>`__"
msgstr ""
"在此教程中，你将学习如何使用一种称为空间变换网络的视觉注意机制来增强你的网络。你可以在《DeepMind论文》（<https://arxiv.org/abs/1506.02025>）中了解有关空间变换网络的更多信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Spatial transformer networks are a generalization of differentiable "
"attention to any spatial transformation. Spatial transformer networks (STN "
"for short) allow a neural network to learn how to perform spatial "
"transformations on the input image in order to enhance the geometric "
"invariance of the model. For example, it can crop a region of interest, "
"scale and correct the orientation of an image. It can be a useful mechanism "
"because CNNs are not invariant to rotation and scale and more general affine"
" transformations."
msgstr ""
"空间变换网络是任何空间变换的可微注意力的泛化。空间变换网络（简称STN）使神经网络能够学习如何在输入图像上进行空间变换，以增强模型的几何不变性。例如，它可以裁剪感兴趣区域、缩放和校正图像的方向。它是一种有用的机制，因为CNN对旋转和缩放以及更一般的仿射变换不具有不变性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One of the best things about STN is the ability to simply plug it into any "
"existing CNN with very little modification."
msgstr "关于STN的最棒事情之一是它可以简单地插入任何现有的CNN中，只需很少的修改。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Loading the data"
msgstr "加载数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this post we experiment with the classic MNIST dataset. Using a standard "
"convolutional network augmented with a spatial transformer network."
msgstr "在本文中，我们实验经典的MNIST数据集。使用一个增添了空间变换网络的标准卷积网络。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Depicting spatial transformer networks"
msgstr "空间变换网络的图示"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Spatial transformer networks boils down to three main components :"
msgstr "空间变换网络归结为以下三个主要组件："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The localization network is a regular CNN which regresses the transformation"
" parameters. The transformation is never learned explicitly from this "
"dataset, instead the network learns automatically the spatial "
"transformations that enhances the global accuracy."
msgstr "定位网络是一个常规的CNN，它对变换参数进行回归。此数据集不会显式地学习变换，而是网络会自动学习增强整体准确性的空间变换。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The grid generator generates a grid of coordinates in the input image "
"corresponding to each pixel from the output image."
msgstr "网格生成器生成一个输入图像中的坐标网格，与输出图像的每个像素对应。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The sampler uses the parameters of the transformation and applies it to the "
"input image."
msgstr "采样器使用变换的参数并将其应用于输入图像。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We need the latest version of PyTorch that contains affine_grid and "
"grid_sample modules."
msgstr "我们需要最新版本的PyTorch，其中包含affine_grid和grid_sample模块。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Training the model"
msgstr "训练模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, let's use the SGD algorithm to train the model. The network is learning"
" the classification task in a supervised way. In the same time the model is "
"learning STN automatically in an end-to-end fashion."
msgstr "现在，让我们使用SGD算法来训练模型。网络以监督方式学习分类任务。同时，模型以端到端方式自动学习STN。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Visualizing the STN results"
msgstr "可视化STN的结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now, we will inspect the results of our learned visual attention mechanism."
msgstr "现在，我们将检查我们学习的视觉注意机制的结果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We define a small helper function in order to visualize the transformations "
"while training."
msgstr "我们定义一个小的辅助函数以便在训练时可视化变换。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: spatial_transformer_tutorial.py "
"<spatial_transformer_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：spatial_transformer_tutorial.py "
"<spatial_transformer_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: spatial_transformer_tutorial.ipynb "
"<spatial_transformer_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook：spatial_transformer_tutorial.ipynb "
"<spatial_transformer_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Speech Recognition with Wav2Vec2"
msgstr "使用Wav2Vec2进行语音识别"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html"
msgstr ""
"此教程已迁移至https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_tensorboard_profiler_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`此处 "
"<sphx_glr_download_intermediate_tensorboard_profiler_tutorial.py>`下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch Profiler With TensorBoard"
msgstr "使用TensorBoard的PyTorch性能分析工具"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial demonstrates how to use TensorBoard plugin with PyTorch "
"Profiler to detect performance bottlenecks of the model."
msgstr "此教程演示了如何使用TensorBoard插件与PyTorch性能分析工具来检测模型的性能瓶颈。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The TensorBoard integration with the PyTorch profiler is now deprecated. "
"Instead, use Perfetto or the Chrome trace to view ``trace.json`` files. "
"After `generating a trace "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-"
"tracing-functionality>`__, simply drag the ``trace.json`` into `Perfetto UI "
"<https://ui.perfetto.dev/>`__ or ``chrome://tracing`` to visualize your "
"profile."
msgstr ""
"与PyTorch性能分析工具集成的TensorBoard现已弃用。相反，使用Perfetto或Chrome追踪查看``trace.json``文件。在`生成追踪文件`（<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-"
"tracing-functionality>）之后，只需将``trace.json``拖到`Perfetto "
"UI`（<https://ui.perfetto.dev/>）或``chrome://tracing``中即可可视化你的配置文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"PyTorch 1.8 includes an updated profiler API capable of recording the CPU "
"side operations as well as the CUDA kernel launches on the GPU side. The "
"profiler can visualize this information in TensorBoard Plugin and provide "
"analysis of the performance bottlenecks."
msgstr ""
"PyTorch "
"1.8引入了更新的性能分析工具API，它可以记录CPU端操作以及GPU端的CUDA内核启动。性能分析工具可以在TensorBoard插件中可视化这些信息并提供性能瓶颈分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will use a simple Resnet model to demonstrate how to "
"use TensorBoard plugin to analyze model performance."
msgstr "在本教程中，我们将使用一个简单的Resnet模型来演示如何使用TensorBoard插件分析模型性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "To install ``torch`` and ``torchvision`` use the following command:"
msgstr "安装``torch``和``torchvision``可以使用以下命令："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Steps"
msgstr "步骤"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Prepare the data and model"
msgstr "准备数据和模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Use profiler to record execution events"
msgstr "使用性能分析工具记录执行事件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Run the profiler"
msgstr "运行性能分析工具"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Use TensorBoard to view results and analyze model performance"
msgstr "使用TensorBoard查看结果并分析模型性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Improve performance with the help of profiler"
msgstr "借助性能分析工具提升性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Analyze performance with other advanced features"
msgstr "结合其他高级功能分析性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Additional Practices: Profiling PyTorch on AMD GPUs"
msgstr "附加实践：在AMD GPU上进行PyTorch性能分析"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1. Prepare the data and model"
msgstr "1. 准备数据和模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "First, import all necessary libraries:"
msgstr "首先，导入所有必需的库："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Then prepare the input data. For this tutorial, we use the CIFAR10 dataset. "
"Transform it to the desired format and use ``DataLoader`` to load each "
"batch."
msgstr "然后准备输入数据。对于本教程，我们使用CIFAR10数据集。将其转换为所需格式并使用``DataLoader``加载每批数据。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, create Resnet model, loss function, and optimizer objects. To run on "
"GPU, move model and loss to GPU device."
msgstr "接下来，创建Resnet模型、损失函数和优化器对象。为了在GPU上运行，将模型和损失函数移动到GPU设备。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Define the training step for each batch of input data."
msgstr "定义每批输入数据的训练步骤。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2. Use profiler to record execution events"
msgstr "2. 使用性能分析工具记录执行事件"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The profiler is enabled through the context manager and accepts several "
"parameters, some of the most useful are:"
msgstr "性能分析工具通过上下文管理器启用，并接受多个参数，其中一些最有用的是："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``schedule`` - callable that takes step (int) as a single parameter and "
"returns the profiler action to perform at each step."
msgstr "``schedule`` - 一个可调用对象，它以步长（int）作为单个参数，并返回每一步执行的性能分析操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example with ``wait=1, warmup=1, active=3, repeat=1``, profiler will"
" skip the first step/iteration, start warming up on the second, record the "
"following three iterations, after which the trace will become available and "
"on_trace_ready (when set) is called. In total, the cycle repeats once. Each "
"cycle is called a \"span\" in TensorBoard plugin."
msgstr ""
"在此示例中，设置 ``wait=1, warmup=1, active=3, "
"repeat=1``，分析器将跳过第一个步骤/迭代，从第二个开始预热，记录接下来的三个迭代，之后分析结果将可用，并调用设置的 "
"on_trace_ready（如果设置了）。总体而言，此循环重复一次。在 TensorBoard 插件中，每个循环称为一个“跨度”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"During ``wait`` steps, the profiler is disabled. During ``warmup`` steps, "
"the profiler starts tracing but the results are discarded. This is for "
"reducing the profiling overhead. The overhead at the beginning of profiling "
"is high and easy to bring skew to the profiling result. During ``active`` "
"steps, the profiler works and records events."
msgstr ""
"在 ``wait`` 步骤期间，分析器被禁用。在 ``warmup`` "
"步骤期间，分析器开始跟踪但结果会被丢弃，这是为了减少分析开销。在分析开始时的开销较高，容易对分析结果产生偏差。在 ``active`` "
"步骤期间，分析器正常工作并记录事件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``on_trace_ready`` - callable that is called at the end of each cycle; In "
"this example we use ``torch.profiler.tensorboard_trace_handler`` to generate"
" result files for TensorBoard. After profiling, result files will be saved "
"into the ``./log/resnet18`` directory. Specify this directory as a "
"``logdir`` parameter to analyze profile in TensorBoard."
msgstr ""
"``on_trace_ready`` - 每个循环结束时调用的函数；在此示例中，我们使用 "
"``torch.profiler.tensorboard_trace_handler`` 来为 TensorBoard "
"生成结果文件。分析完成后，结果文件将保存到 ``./log/resnet18`` 目录。指定此目录作为 ``logdir`` 参数以在 "
"TensorBoard 中分析配置文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``record_shapes`` - whether to record shapes of the operator inputs."
msgstr "``record_shapes`` - 是否记录操作输入的形状。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``profile_memory`` - Track tensor memory allocation/deallocation. Note, for "
"old version of pytorch with version before 1.10, if you suffer long "
"profiling time, please disable it or upgrade to new version."
msgstr ""
"``profile_memory`` - 跟踪张量内存分配/释放。注意，对于版本低于 1.10 的旧版本 "
"PyTorch，如果分析时间过长，请禁用它或升级到新版本。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``with_stack`` - Record source information (file and line number) for the "
"ops. If the TensorBoard is launched in VS Code (`reference "
"<https://code.visualstudio.com/docs/datascience/pytorch-"
"support#_tensorboard-integration>`_), clicking a stack frame will navigate "
"to the specific code line."
msgstr ""
"``with_stack`` - 记录操作的源信息（文件和行号）。如果在 VS Code 中启动 TensorBoard（`参考 "
"<https://code.visualstudio.com/docs/datascience/pytorch-"
"support#_tensorboard-integration>`_），单击堆栈帧将导航到特定代码行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Alternatively, the following non-context manager start/stop is supported as "
"well."
msgstr "另外，也支持以下非上下文管理器的启动/停止方式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3. Run the profiler"
msgstr "3. 运行分析器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Run the above code. The profiling result will be saved under "
"``./log/resnet18`` directory."
msgstr "运行上述代码。分析结果将保存到 ``./log/resnet18`` 目录下。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "4. Use TensorBoard to view results and analyze model performance"
msgstr "4. 使用 TensorBoard 查看结果并分析模型性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TensorBoard Plugin support has been deprecated, so some of these functions "
"may not work as previously. Please take a look at the replacement, `HTA "
"<https://github.com/pytorch/kineto/tree/main#holistic-trace-analysis>`_."
msgstr ""
"TensorBoard 插件支持已废弃，因此其中一些功能可能不再像以前那样有效。请查看其替代内容，`HTA "
"<https://github.com/pytorch/kineto/tree/main#holistic-trace-analysis>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Install PyTorch Profiler TensorBoard Plugin."
msgstr "安装 PyTorch 分析器 TensorBoard 插件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Launch the TensorBoard."
msgstr "启动 TensorBoard。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Open the TensorBoard profile URL in Google Chrome browser or Microsoft Edge "
"browser (**Safari is not supported**)."
msgstr ""
"在 Google Chrome 浏览器或 Microsoft Edge 浏览器中打开 TensorBoard 配置文件 URL（**不支持 "
"Safari**）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You could see Profiler plugin page as shown below."
msgstr "您可以看到 Profiler 插件页面，如下所示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The overview shows a high-level summary of model performance."
msgstr "概览显示模型性能的高级摘要。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The \"GPU Summary\" panel shows the GPU configuration, GPU usage and Tensor "
"Cores usage. In this example, the GPU Utilization is low. The details of "
"these metrics are `here "
"<https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md>`_."
msgstr ""
"“GPU 概况”面板显示 GPU 配置、GPU 使用率和 Tensor Cores 使用情况。在本示例中，GPU 使用率较低。这些指标的详细信息在`此处"
" "
"<https://github.com/pytorch/kineto/blob/main/tb_plugin/docs/gpu_utilization.md>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The \"Step Time Breakdown\" shows distribution of time spent in each step "
"over different categories of execution. In this example, you can see the "
"``DataLoader`` overhead is significant."
msgstr "“步骤时间分解”显示每个步骤在不同执行类别上花费时间的分布。在本示例中，您可以看到 ``DataLoader`` 的开销很大。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The bottom \"Performance Recommendation\" uses the profiling data to "
"automatically highlight likely bottlenecks, and gives you actionable "
"optimization suggestions."
msgstr "底部的“性能建议”使用分析数据自动突出可能的瓶颈，并提供可执行的优化建议。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You can change the view page in left \"Views\" dropdown list."
msgstr "您可以在左侧“视图”下拉列表中更改视图页面。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Operator view"
msgstr "操作符视图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The operator view displays the performance of every PyTorch operator that is"
" executed either on the host or device."
msgstr "操作符视图显示在主机或设备上执行的每个 PyTorch 操作符的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The \"Self\" duration does not include its child operators’ time. The "
"\"Total\" duration includes its child operators’ time."
msgstr "“Self”持续时间不包括其子操作符的时间。“Total”持续时间包括其子操作符的时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "View call stack"
msgstr "查看调用堆栈"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click the ``View Callstack`` of an operator, the operators with same name "
"but different call stacks will be shown. Then click a ``View Callstack`` in "
"this sub-table, the call stack frames will be shown."
msgstr ""
"单击操作符的 ``View Callstack``，将显示具有相同名称但不同调用堆栈的操作符。然后在该子表中单击 ``View "
"Callstack``，调用堆栈帧将被显示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If the TensorBoard is launched inside VS Code (`Launch Guide "
"<https://devblogs.microsoft.com/python/python-in-visual-studio-code-"
"february-2021-release/#tensorboard-integration>`_), clicking a call stack "
"frame will navigate to the specific code line."
msgstr ""
"如果在 VS Code 中启动 TensorBoard（`启动指南 "
"<https://devblogs.microsoft.com/python/python-in-visual-studio-code-"
"february-2021-release/#tensorboard-integration>`_），单击调用堆栈帧将导航到特定代码行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Kernel view"
msgstr "内核视图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The GPU kernel view shows all kernels’ time spent on GPU."
msgstr "GPU 内核视图显示所有内核在 GPU 上花费的时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Tensor Cores Used: Whether this kernel uses Tensor Cores."
msgstr "使用 Tensor Cores：是否此内核使用 Tensor Cores。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Mean Blocks per SM: Blocks per SM = Blocks of this kernel / SM number of "
"this GPU. If this number is less than 1, it indicates the GPU "
"multiprocessors are not fully utilized. \"Mean Blocks per SM\" is weighted "
"average of all runs of this kernel name, using each run’s duration as "
"weight."
msgstr ""
"每个 SM 平均 Blocks：Blocks per SM = 此内核的 Blocks / 此 GPU 的 SM 数。如果此数值小于 1，则表示 GPU"
" 多处理器未得到充分利用。“每个 SM 平均 Blocks”是此内核名称的所有运行的加权平均值，每次运行的持续时间作为权重。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Mean Est. Achieved Occupancy: Est. Achieved Occupancy is defined in this "
"column’s tooltip. For most cases such as memory bandwidth bounded kernels, "
"the higher the better. \"Mean Est. Achieved Occupancy\" is weighted average "
"of all runs of this kernel name, using each run’s duration as weight."
msgstr ""
"平均预计实现占用率：预计实现占用率在此列的工具提示中定义。对于大多数情况，例如内存带宽受限的内核，数值越高越好。“平均预计实现占用率”是此内核名称所有运行的加权平均值，每次运行的持续时间作为权重。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Trace view"
msgstr "跟踪视图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The trace view shows timeline of profiled operators and GPU kernels. You can"
" select it to see details as below."
msgstr "跟踪视图显示被分析的操作符和 GPU 内核的时间轴。您可以选择它以查看更多详情，如下所示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can move the graph and zoom in/out with the help of right side toolbar. "
"And keyboard can also be used to zoom and move around inside the timeline. "
"The ‘w’ and ‘s’ keys zoom in centered around the mouse, and the ‘a’ and ‘d’ "
"keys move the timeline left and right. You can hit these keys multiple times"
" until you see a readable representation."
msgstr ""
"通过右侧工具栏可以移动图表并放大/缩小，同时键盘也可用于缩放和在时间轴中移动。“w”和“s”键围绕鼠标中心放大，“a”和“d”键左右移动时间轴。您可以多次按这些键，直到看到易于读取的表示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If a backward operator's \"Incoming Flow\" field is with value \"forward "
"correspond to backward\", you can click the text to get its launching "
"forward operator."
msgstr "如果某个后向操作符的“传入流量”字段的值为“前向对应后向”，您可以单击文本以获得其启动的前向操作符。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example, we can see the event prefixed with "
"``enumerate(DataLoader)`` costs a lot of time. And during most of this "
"period, the GPU is idle. Because this function is loading data and "
"transforming data on host side, during which the GPU resource is wasted."
msgstr ""
"在此示例中，我们可以看到前缀为 ``enumerate(DataLoader)`` 的事件花费了大量时间。在此期间大部分时间内，GPU "
"都处于空闲状态。因为该功能在主机端加载数据并对数据进行转换，在此期间 GPU 资源被浪费。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "5. Improve performance with the help of profiler"
msgstr "5. 在分析器的帮助下提高性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At the bottom of \"Overview\" page, the suggestion in \"Performance "
"Recommendation\" hints the bottleneck is ``DataLoader``. The PyTorch "
"``DataLoader`` uses single process by default. User could enable multi-"
"process data loading by setting the parameter ``num_workers``. `Here "
"<https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-"
"loading>`_ is more details."
msgstr ""
"在“概览”页面底部，“性能建议”中的提示表明瓶颈是 ``DataLoader``。PyTorch 的 ``DataLoader`` "
"默认使用单进程。用户可以通过设置参数 ``num_workers`` 启用多进程数据加载。`更多详情请参考此处 "
"<https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-"
"loading>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this example, we follow the \"Performance Recommendation\" and set "
"``num_workers`` as below, pass a different name such as "
"``./log/resnet18_4workers`` to ``tensorboard_trace_handler``, and run it "
"again."
msgstr ""
"在此示例中，我们遵循“性能建议”，设置 ``num_workers`` 如下，传递一个不同的名称比如 "
"``./log/resnet18_4workers`` 给 ``tensorboard_trace_handler``，然后再次运行它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Then let’s choose the recently profiled run in left \"Runs\" dropdown list."
msgstr "然后让我们在左侧“运行”下拉列表中选择最近分析的运行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"From the above view, we can find the step time is reduced to about 76ms "
"comparing with previous run's 132ms, and the time reduction of "
"``DataLoader`` mainly contributes."
msgstr ""
"从上述视图中，我们可以发现步骤时间减少到约 76 毫秒，与之前运行的 132 毫秒相比下降了很多，而时间减少主要归功于 ``DataLoader``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"From the above view, we can see that the runtime of "
"``enumerate(DataLoader)`` is reduced, and the GPU utilization is increased."
msgstr "从上述视图中，我们可以看到 ``enumerate(DataLoader)`` 的运行时间有所减少，并且 GPU 利用率有所提高。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "6. Analyze performance with other advanced features"
msgstr "6. 使用其他高级功能分析性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Memory view"
msgstr "内存视图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To profile memory, ``profile_memory`` must be set to ``True`` in arguments "
"of ``torch.profiler.profile``."
msgstr ""
"要分析内存，必须在 ``torch.profiler.profile`` 的参数中将 ``profile_memory`` 设置为 ``True``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You can try it by using existing example on Azure"
msgstr "您可以通过使用 Azure 上的现有示例尝试它。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The profiler records all memory allocation/release events and allocator's "
"internal state during profiling. The memory view consists of three "
"components as shown in the following."
msgstr "分析器在分析期间记录所有内存分配/释放事件以及分配器的内部状态。内存视图包括如下所示的三个组件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The components are memory curve graph, memory events table and memory "
"statistics table, from top to bottom, respectively."
msgstr "这些组件分别是从上到下的内存曲线图、内存事件表和内存统计表。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The memory type could be selected in \"Device\" selection box. For example, "
"\"GPU0\" means the following table only shows each operator's memory usage "
"on GPU 0, not including CPU or other GPUs."
msgstr ""
"可以在“设备”选择框中选择内存类型。例如，“GPU0”表示以下表格仅显示每个操作符在 GPU 0 上的内存使用情况，不包括 CPU 或其他 GPU。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The memory curve shows the trends of memory consumption. The \"Allocated\" "
"curve shows the total memory that is actually in use, e.g., tensors. In "
"PyTorch, caching mechanism is employed in CUDA allocator and some other "
"allocators. The \"Reserved\" curve shows the total memory that is reserved "
"by the allocator. You can left click and drag on the graph to select events "
"in the desired range:"
msgstr ""
"内存曲线显示内存消耗的趋势。“Allocated”曲线显示实际上正在使用的总内存，例如，张量。在 PyTorch 中，CUDA "
"分配器和其他分配器采用缓存机制。“Reserved”曲线显示分配器保留的总内存。您可以左键单击并拖动图表以选择所需范围的事件："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After selection, the three components will be updated for the restricted "
"time range, so that you can gain more information about it. By repeating "
"this process, you can zoom into a very fine-grained detail. Right click on "
"the graph will reset the graph to the initial state."
msgstr "选择后，三个组件将针对所选时间范围更新，以便您获得更多信息。重复此过程，可缩放至非常详细的水平。右键单击图表将重置图表到初始状态。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the memory events table, the allocation and release events are paired "
"into one entry. The \"operator\" column shows the immediate ATen operator "
"that is causing the allocation. Notice that in PyTorch, ATen operators "
"commonly use ``aten::empty`` to allocate memory. For example, ``aten::ones``"
" is implemented as ``aten::empty`` followed by an ``aten::fill_``. Solely "
"display the operator name as ``aten::empty`` is of little help. It will be "
"shown as ``aten::ones (aten::empty)`` in this special case. The \"Allocation"
" Time\", \"Release Time\" and \"Duration\" columns' data might be missing if"
" the event occurs outside of the time range."
msgstr ""
"在内存事件表中，分配和释放事件配对为一个条目。“操作符”列显示导致分配的直接 ATen 操作符。注意，在 PyTorch 中，ATen 操作符通常使用 "
"``aten::empty`` 分配内存。例如，``aten::ones`` 实现为 ``aten::empty`` 后跟 "
"``aten::fill_``。单独显示操作符名为 ``aten::empty`` 帮助不大。在这种特殊情况下将显示为 ``aten::ones "
"(aten::empty)``。“分配时间”、“释放时间”和“持续时间”列的数据可能会缺失，若事件发生在选定时间范围之外。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the memory statistics table, the \"Size Increase\" column sums up all "
"allocation size and minus all the memory release size, that is, the net "
"increase of memory usage after this operator. The \"Self Size Increase\" "
"column is similar to \"Size Increase\", but it does not count children "
"operators' allocation. With regards to ATen operators' implementation "
"detail, some operators might call other operators, so memory allocations can"
" happen at any level of the call stack. That says, \"Self Size Increase\" "
"only count the memory usage increase at current level of call stack. "
"Finally, the \"Allocation Size\" column sums up all allocation without "
"considering the memory release."
msgstr ""
"在内存统计表中，“大小增加”列汇总所有分配大小并减去所有内存释放大小，即该操作符后内存使用的净增加。“自身大小增加”列类似于“大小增加”，但不统计子操作符的分配。考虑到"
" ATen "
"操作符的实现细节，有些操作符可能调用其他操作符，因此内存分配可能发生在调用堆栈的任何级别。这就是说，“自身大小增加”只统计当前调用堆栈级别的内存使用增加。最后，“分配大小”列汇总所有分配，不考虑内存释放。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed view"
msgstr "分布式视图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The plugin now supports distributed view on profiling DDP with NCCL/GLOO as "
"backend."
msgstr "该插件现在支持使用 NCCL/GLOO 作为后端的分析 DDP 分布式视图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You can try it by using existing example on Azure:"
msgstr "您可以通过使用 Azure 上的现有示例尝试它："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The \"Computation/Communication Overview\" shows computation/communication "
"ratio and their overlapping degree. From this view, User can figure out load"
" balance issue among workers. For example, if the computation + overlapping "
"time of one worker is much larger than others, there may be a problem of "
"load balance or this worker may be a straggler."
msgstr ""
"“计算/通信概览”显示计算/通信比例及其重叠程度。从该视图中，用户可以发现工人的负载平衡问题。例如，如果一个工人的计算+重叠时间远大于其他工人，则可能存在负载平衡问题或该工人可能是慢速工人。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The \"Synchronizing/Communication Overview\" shows the efficiency of "
"communication. \"Data Transfer Time\" is the time for actual data "
"exchanging. \"Synchronizing Time\" is the time for waiting and synchronizing"
" with other workers."
msgstr "“同步/通信概览”显示通信效率。“数据传输时间”是实际数据交换的时间。“同步时间”是等待和与其他工人同步的时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If one worker’s \"Synchronizing Time\" is much shorter than that of other "
"workers’, this worker may be a straggler which may have more computation "
"workload than other workers’."
msgstr "如果一个工人的“同步时间”远短于其他工人的，此工人可能是一个慢速工人，可能计算负载高于其他工人。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The \"Communication Operations Stats\" summarizes the detailed statistics of"
" all communication ops in each worker."
msgstr "“通信操作统计”总结每个工人中所有通信操作的详细统计信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "7. Additional Practices: Profiling PyTorch on AMD GPUs"
msgstr "7. 额外实践：在 AMD GPU 上分析 PyTorch"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The AMD ROCm Platform is an open-source software stack designed for GPU "
"computation, consisting of drivers, development tools, and APIs. We can run "
"the above mentioned steps on AMD GPUs. In this section, we will use Docker "
"to install the ROCm base development image before installing PyTorch."
msgstr ""
"AMD ROCm 平台是专为 GPU 计算设计的开源软件栈，包括驱动程序、开发工具和 API。我们可以在 AMD GPU "
"上运行上述步骤。在本节中，我们将使用 Docker 安装 ROCm 基础开发镜像，然后安装 PyTorch。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the purpose of example, let's create a directory called "
"``profiler_tutorial``, and save the code in **Step 1** as "
"``test_cifar10.py`` in this directory."
msgstr ""
"为了举个例子，我们创建一个名为 ``profiler_tutorial`` 的目录，并将 **步骤 1** 中的代码保存为 "
"``test_cifar10.py`` 文件，保存在这个目录中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At the time of this writing, the Stable(``2.1.1``) Linux version of PyTorch "
"on ROCm Platform is `ROCm 5.6 <https://pytorch.org/get-started/locally/>`_."
msgstr ""
"在撰写本文时，PyTorch 在 ROCm 平台上的稳定版（``2.1.1``）Linux 版本为 `ROCm 5.6 "
"<https://pytorch.org/get-started/locally/>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Obtain a base Docker image with the correct user-space ROCm version "
"installed from `Docker Hub "
"<https://hub.docker.com/repository/docker/rocm/dev-ubuntu-20.04>`_."
msgstr ""
"从 `Docker Hub <https://hub.docker.com/repository/docker/rocm/dev-"
"ubuntu-20.04>`_ 获取已安装正确的用户空间 ROCm 版本的基础 Docker 镜像。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "It is ``rocm/dev-ubuntu-20.04:5.6``."
msgstr "镜像为 ``rocm/dev-ubuntu-20.04:5.6``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Start the ROCm base Docker container:"
msgstr "启动 ROCm 基础 Docker 容器："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Inside the container, install any dependencies needed for installing the "
"wheels package."
msgstr "在容器内部，安装任何用于安装 wheels 包的依赖项。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Install the wheels:"
msgstr "安装 wheels 包："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Install the ``torch_tb_profiler``, and then, run the Python file "
"``test_cifar10.py``:"
msgstr "安装 ``torch_tb_profiler``，然后运行 Python 文件 ``test_cifar10.py``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Now, we have all the data needed to view in TensorBoard:"
msgstr "现在，我们已经有了在 TensorBoard 中查看所需的数据："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Choose different views as described in **Step 4**. For example, below is the"
" **Operator** View:"
msgstr "选择不同的视图，如 **步骤 4** 中所描述。例如，下面是 **Operator** 视图："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"At the time this section is written, **Trace** view does not work and it "
"displays nothing. You can work around by typing ``chrome://tracing`` in your"
" Chrome Browser."
msgstr ""
"在撰写本节内容时，**Trace** 视图无法正常工作，显示为空。可以通过在 Chrome 浏览器中输入 ``chrome://tracing`` "
"来解决。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Copy the ``trace.json`` file under ``~/profiler_tutorial/log/resnet18`` "
"directory to the Windows."
msgstr ""
"将 ``~/profiler_tutorial/log/resnet18`` 目录下的 ``trace.json`` 文件复制到 Windows 中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You may need to copy the file by using ``scp`` if the file is located in a "
"remote location."
msgstr "如果文件位于远程位置，您可能需要使用 ``scp`` 来复制文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click **Load** button to load the trace JSON file from the "
"``chrome://tracing`` page in the browser."
msgstr "点击 **Load** 按钮，从浏览器中的 ``chrome://tracing`` 页面加载 trace JSON 文件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As mentioned previously, you can move the graph and zoom in and out. You can"
" also use keyboard to zoom and move around inside the timeline. The ``w`` "
"and ``s`` keys zoom in centered around the mouse, and the ``a`` and ``d`` "
"keys move the timeline left and right. You can hit these keys multiple times"
" until you see a readable representation."
msgstr ""
"如之前提到的，您可以移动图表并进行放大或缩小。您还可以使用键盘在时间轴内缩放和移动。``w`` 和 ``s`` 键以鼠标为中心放大，``a`` 和 "
"``d`` 键左右移动时间轴。您可以多次按下这些键，直到看到可读的表示形式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Take a look at the following documents to continue your learning, and feel "
"free to open an issue `here <https://github.com/pytorch/kineto/issues>`_."
msgstr ""
"查看以下文档以继续学习，并欢迎随时在 `此处 <https://github.com/pytorch/kineto/issues>`_ 提出问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`PyTorch TensorBoard Profiler Github "
"<https://github.com/pytorch/kineto/tree/master/tb_plugin>`_"
msgstr ""
"`PyTorch TensorBoard Profiler GitHub "
"<https://github.com/pytorch/kineto/tree/master/tb_plugin>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`torch.profiler API <https://pytorch.org/docs/master/profiler.html>`_"
msgstr "`torch.profiler API <https://pytorch.org/docs/master/profiler.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`HTA <https://github.com/pytorch/kineto/tree/main#holistic-trace-analysis>`_"
msgstr ""
"`HTA <https://github.com/pytorch/kineto/tree/main#holistic-trace-analysis>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: tensorboard_profiler_tutorial.py "
"<tensorboard_profiler_tutorial.py>`"
msgstr ""
":下载:`下载 Python 源代码: tensorboard_profiler_tutorial.py "
"<tensorboard_profiler_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: tensorboard_profiler_tutorial.ipynb "
"<tensorboard_profiler_tutorial.ipynb>`"
msgstr ""
":下载:`下载 Jupyter notebook: tensorboard_profiler_tutorial.ipynb "
"<tensorboard_profiler_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Visualizing Models, Data, and Training with TensorBoard"
msgstr "使用 TensorBoard 可视化模型、数据和训练过程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the `60 Minute Blitz "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`_, "
"we show you how to load in data, feed it through a model we define as a "
"subclass of ``nn.Module``, train this model on training data, and test it on"
" test data. To see what's happening, we print out some statistics as the "
"model is training to get a sense for whether training is progressing. "
"However, we can do much better than that: PyTorch integrates with "
"TensorBoard, a tool designed for visualizing the results of neural network "
"training runs. This tutorial illustrates some of its functionality, using "
"the `Fashion-MNIST dataset <https://github.com/zalandoresearch/fashion-"
"mnist>`__ which can be read into PyTorch using `torchvision.datasets`."
msgstr ""
"在 `60 分钟入门教程 "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`_ "
"中，我们讲解了如何加载数据，将数据传入我们定义为 ``nn.Module`` "
"子类的模型，在训练数据上训练模型，并在测试数据上测试模型。为了观察训练过程，我们打印了模型训练时的一些统计信息，但我们可以做得更好：PyTorch "
"集成了 TensorBoard，一个为可视化神经网络训练结果而设计的工具。本教程将展示其一些功能，使用 `Fashion-MNIST 数据集 "
"<https://github.com/zalandoresearch/fashion-mnist>`__，该数据集可以通过 "
"`torchvision.datasets` 加载到 PyTorch 中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In this tutorial, we'll learn how to:"
msgstr "在本教程中，我们将学习如何："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Read in data and with appropriate transforms (nearly identical to the prior "
"tutorial)."
msgstr "读取并转换数据（与前面的教程几乎相同）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Set up TensorBoard."
msgstr "设置 TensorBoard。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Write to TensorBoard."
msgstr "写入数据到 TensorBoard。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Inspect a model architecture using TensorBoard."
msgstr "使用 TensorBoard 检查模型结构。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Use TensorBoard to create interactive versions of the visualizations we "
"created in last tutorial, with less code"
msgstr "使用 TensorBoard 创建前教程可视化的交互式版本，同时减少代码量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Specifically, on point #5, we'll see:"
msgstr "特别是在第5点中，我们将看到："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "A couple of ways to inspect our training data"
msgstr "几种检查训练数据的方法"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to track our model's performance as it trains"
msgstr "如何跟踪模型在训练过程中的性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to assess our model's performance once it is trained."
msgstr "如何评估训练后的模型性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll begin with similar boilerplate code as in the `CIFAR-10 tutorial "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`__:"
msgstr ""
"我们将从类似于 `CIFAR-10 教程 "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`__ "
"的基础代码开始："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll define a similar model architecture from that tutorial, making only "
"minor modifications to account for the fact that the images are now one "
"channel instead of three and 28x28 instead of 32x32:"
msgstr "我们将定义与该教程中类似的模型架构，仅做出一些轻微修改以适应图像现在是单通道而不是三通道且大小为28x28而不是32x32的情况："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We'll define the same ``optimizer`` and ``criterion`` from before:"
msgstr "我们将定义与之前相同的 ``optimizer`` 和 ``criterion``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1. TensorBoard setup"
msgstr "1. TensorBoard 设置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we'll set up TensorBoard, importing ``tensorboard`` from ``torch.utils``"
" and defining a ``SummaryWriter``, our key object for writing information to"
" TensorBoard."
msgstr ""
"现在我们将设置 TensorBoard，从 ``torch.utils`` 导入 ``tensorboard`` 并定义一个 "
"``SummaryWriter``，这是我们写入 TensorBoard 的关键对象。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that this line alone creates a ``runs/fashion_mnist_experiment_1`` "
"folder."
msgstr "请注意，仅这一行代码就创建了一个 ``runs/fashion_mnist_experiment_1`` 文件夹。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2. Writing to TensorBoard"
msgstr "2. 写入数据到 TensorBoard"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now let's write an image to our TensorBoard - specifically, a grid - using "
"`make_grid "
"<https://pytorch.org/vision/stable/utils.html#torchvision.utils.make_grid>`__."
msgstr ""
"现在让我们向 TensorBoard 写入一个图像 - 确切来说是一个网格 - 使用 `make_grid "
"<https://pytorch.org/vision/stable/utils.html#torchvision.utils.make_grid>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Now running"
msgstr "现在运行"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"from the command line and then navigating to `http://localhost:6006 "
"<http://localhost:6006>`_ should show the following."
msgstr "从命令行导航到 `http://localhost:6006 <http://localhost:6006>`_，应该能看到以下内容。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now you know how to use TensorBoard! This example, however, could be done in"
" a Jupyter Notebook - where TensorBoard really excels is in creating "
"interactive visualizations. We'll cover one of those next, and several more "
"by the end of the tutorial."
msgstr ""
"现在您已经知道如何使用 TensorBoard！然而，这个例子可以在 Jupyter Notebook 中完成 - TensorBoard "
"的真正优秀之处在于创建交互式可视化。接下来我们将在教程结束前介绍一个这样的例子。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3. Inspect the model using TensorBoard"
msgstr "3. 使用 TensorBoard 检查模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One of TensorBoard's strengths is its ability to visualize complex model "
"structures. Let's visualize the model we built."
msgstr "TensorBoard 的一个优势是能够可视化复杂的模型结构。我们来可视化我们构建的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now upon refreshing TensorBoard you should see a \"Graphs\" tab that looks "
"like this:"
msgstr "刷新 TensorBoard 后，您应该能看到一个 \"Graphs\" 标签页，如下图所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Go ahead and double click on \"Net\" to see it expand, seeing a detailed "
"view of the individual operations that make up the model."
msgstr "继续双击 \"Net\"，可以展开它，查看组成模型的各个操作的详细视图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TensorBoard has a very handy feature for visualizing high dimensional data "
"such as image data in a lower dimensional space; we'll cover this next."
msgstr "TensorBoard 对于将高维数据（如图像数据）可视化为低维空间非常有用；接下来我们将介绍这一功能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "4. Adding a \"Projector\" to TensorBoard"
msgstr "4. 为 TensorBoard 添加 \"Projector\""

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can visualize the lower dimensional representation of higher dimensional "
"data via the `add_embedding "
"<https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_embedding>`__"
" method"
msgstr ""
"我们可以通过 `add_embedding "
"<https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_embedding>`__"
" 方法可视化高维数据的低维表示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now in the \"Projector\" tab of TensorBoard, you can see these 100 images - "
"each of which is 784 dimensional - projected down into three dimensional "
"space. Furthermore, this is interactive: you can click and drag to rotate "
"the three dimensional projection. Finally, a couple of tips to make the "
"visualization easier to see: select \"color: label\" on the top left, as "
"well as enabling \"night mode\", which will make the images easier to see "
"since their background is white:"
msgstr ""
"现在在 TensorBoard 的 \"Projector\" 标签页中，您可以看到这些100张图像 - 每张图像是784维 - "
"投射到三维空间中。此外，这是可交互的：您可以点击并拖曳来旋转三维投影。最后，有两个小技巧可以让可视化更容易观察：在左上角选择 \"color: "
"label\"，并启用 \"夜间模式\"，这样图像会更容易看清，因为它们的背景是白色的："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we've thoroughly inspected our data, let's show how TensorBoard can make"
" tracking model training and evaluation clearer, starting with training."
msgstr "现在我们已对数据进行了深入检查，接下来我们将展示 TensorBoard 如何使模型训练和评估的跟踪更加清晰，从训练开始。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "5. Tracking model training with TensorBoard"
msgstr "5. 使用 TensorBoard 跟踪模型训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the previous example, we simply *printed* the model's running loss every "
"2000 iterations. Now, we'll instead log the running loss to TensorBoard, "
"along with a view into the predictions the model is making via the "
"``plot_classes_preds`` function."
msgstr ""
"在之前的例子中，我们每2000次迭代通过 *print* 输出模型的运行损失。现在，我们将通过 ``plot_classes_preds`` "
"函数将运行损失记录到 TensorBoard，同时查看模型做出的预测。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Finally, let's train the model using the same model training code from the "
"prior tutorial, but writing results to TensorBoard every 1000 batches "
"instead of printing to console; this is done using the `add_scalar "
"<https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar>`__"
" function."
msgstr ""
"最后，使用前教程中的同样模型训练代码，但将每1000批次的结果写入 TensorBoard，而不是打印到控制台；这通过使用 `add_scalar "
"<https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar>`__"
" 函数完成。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In addition, as we train, we'll generate an image showing the model's "
"predictions vs. the actual results on the four images included in that "
"batch."
msgstr "此外，在训练过程中，我们将生成一个图像，显示模型的预测与该批次中包含的四张图像的实际结果的比较。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can now look at the scalars tab to see the running loss plotted over the"
" 15,000 iterations of training:"
msgstr "您现在可以查看标量标签页，看到持续损失在15000次迭代中的绘制："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In addition, we can look at the predictions the model made on arbitrary "
"batches throughout learning. See the \"Images\" tab and scroll down under "
"the \"predictions vs. actuals\" visualization to see this; this shows us "
"that, for example, after just 3000 training iterations, the model was "
"already able to distinguish between visually distinct classes such as "
"shirts, sneakers, and coats, though it isn't as confident as it becomes "
"later on in training:"
msgstr ""
"此外，我们可以查看模型在整个学习过程中对任意批次的预测。查看 \"Images\" 标签页，并向下滚动到 \"predictions vs. "
"actuals\" "
"可视化以查看这一点；例如，在仅训练3000次迭代后，模型已能够区分视觉上明显不同的类别，如衬衫、运动鞋和外套，虽然它在训练后期更有信心："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the prior tutorial, we looked at per-class accuracy once the model had "
"been trained; here, we'll use TensorBoard to plot precision-recall curves "
"(good explanation `here <https://www.scikit-"
"yb.org/en/latest/api/classifier/prcurve.html>`__) for each class."
msgstr ""
"在之前的教程中，我们在模型训练完成后查看了每个类别的准确率；在这里，我们将使用 TensorBoard 来绘制每个类别的精确率-"
"召回率曲线（关于此主题的优秀解释见 `这里 <https://www.scikit-"
"yb.org/en/latest/api/classifier/prcurve.html>`__）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "6. Assessing trained models with TensorBoard"
msgstr "6. 使用 TensorBoard 评估训练过的模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You will now see a \"PR Curves\" tab that contains the precision-recall "
"curves for each class. Go ahead and poke around; you'll see that on some "
"classes the model has nearly 100% \"area under the curve\", whereas on "
"others this area is lower:"
msgstr ""
"您现在可以看到 \"PR Curves\" 标签页，其中包含每个类别的精确率-召回率曲线。继续探索一下；您会看到对于某些类别，模型几乎达到100%的 "
"\"曲线下面积\"（AUC），而对其他类别而言，这一面积较低："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And that's an intro to TensorBoard and PyTorch's integration with it. Of "
"course, you could do everything TensorBoard does in your Jupyter Notebook, "
"but with TensorBoard, you gets visuals that are interactive by default."
msgstr ""
"这就是 TensorBoard 和其与 PyTorch集成的入门介绍。当然，您可以在 Jupyter Notebook 中完成所有 "
"TensorBoard 提供的功能，但使用 TensorBoard，您可以获得默认即为交互式的可视化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Text-to-speech with Tacotron2"
msgstr "使用 Tacotron2 的文本转语音"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html"
msgstr ""
"本教程已移动至 "
"https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Whole Slide Image Classification Using PyTorch and TIAToolbox"
msgstr "使用 PyTorch 和 TIAToolbox 进行整体切片图像分类"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To get the most of this tutorial, we suggest using this `Colab Version "
"<https://colab.research.google.com/github/pytorch/tutorials/blob/main/_static/tiatoolbox_tutorial.ipynb>`_."
" This will allow you to experiment with the information presented below."
msgstr ""
"为了充分利用本教程，我们建议使用此 `Colab 版本 "
"<https://colab.research.google.com/github/pytorch/tutorials/blob/main/_static/tiatoolbox_tutorial.ipynb>`_，以便实验下面展示的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will show how to classify Whole Slide Images (WSIs) "
"using PyTorch deep learning models with help from TIAToolbox. A WSI is an "
"image of a sample of human tissue taken through a surgery or biopsy and "
"scanned using specialized scanners. They are used by pathologists and "
"computational pathology researchers to `study diseases such as cancer at the"
" microscopic level "
"<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/>`__ in order to "
"understand for example tumor growth and help improve treatment for patients."
msgstr ""
"在本教程中，我们将展示如何利用 PyTorch 深度学习模型和 TIAToolbox 来分类整体切片图像（WSIs）。WSI "
"是通过手术或活组织检查提取的人体组织样本的图像，并使用专业扫描仪进行扫描。这些图像被病理学家和计算病理学研究人员用于 `研究癌症等疾病的微观水平 "
"<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/>`__，以便例如了解肿瘤生长并帮助改善患者的治疗。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"What makes WSIs challenging to process is their enormous size. For example, "
"a typical slide image has in the order of `100,000x100,000 pixels "
"<https://doi.org/10.1117%2F12.912388>`__ where each pixel can correspond to "
"about 0.25x0.25 microns on the slide. This introduces challenges in loading "
"and processing such images, not to mention hundreds or even thousands of "
"WSIs in a single study (larger studies produce better results)!"
msgstr ""
"处理 WSIs 的挑战在于其巨大的尺寸。例如，一个典型的切片图像的大小约为 `100,000x100,000 像素 "
"<https://doi.org/10.1117%2F12.912388>`__，其中每个像素在切片上对应约0.25x0.25微米。这带来了加载和处理此类图像的挑战，更不用说在单个研究中处理数百甚至数千张WSI了（更大的研究能产生更好的结果）！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Conventional image processing pipelines are not suitable for WSI processing "
"so we need better tools. This is where `TIAToolbox "
"<https://github.com/TissueImageAnalytics/tiatoolbox>`__ can help as it "
"brings a set of useful tools to import and process tissue slides in a fast "
"and computationally efficient manner. Typically, WSIs are saved in a pyramid"
" structure with multiple copies of the same image at various magnification "
"levels optimized for visualization. The level 0 (or the bottom level) of the"
" pyramid contains the image at the highest magnification or zoom level, "
"whereas the higher levels in the pyramid have a lower resolution copy of the"
" base image. The pyramid structure is sketched below."
msgstr ""
"传统的图像处理管道不适用于WSI处理，因此我们需要更好的工具。而 `TIAToolbox "
"<https://github.com/TissueImageAnalytics/tiatoolbox>`__ "
"可以提供帮助，它提供了一组实用工具，能够快速且高效地导入和处理组织切片。通常，WSI以金字塔结构保存，同一图像在不同放大级别的多个副本被优化以便于可视化。金字塔的第0级（或底层）包含最高放大或缩放级别的图像，而金字塔的较高层则是基础图像的低分辨率副本。下面展示了金字塔结构的草图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"|WSI pyramid stack| *WSI pyramid stack (*\\ `source <https://tia-"
"toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#>`__\\"
" *)*"
msgstr ""
"|WSI金字塔堆栈| *WSI金字塔堆栈 (*\\ `来源 <https://tia-"
"toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#>`__\\"
" *)*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "WSI pyramid stack"
msgstr "WSI金字塔堆栈"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TIAToolbox allows us to automate common downstream analysis tasks such as "
"`tissue classification <https://doi.org/10.1016/j.media.2022.102685>`__. In "
"this tutorial we show how you can: 1. Load WSI images using TIAToolbox; and "
"2. Use different PyTorch models to classify slides at the patch-level. In "
"this tutorial, we will provide an example of using TorchVision ``ResNet18`` "
"model and custom `HistoEncoder` <https://github.com/jopo666/HistoEncoder>`__"
" model."
msgstr ""
"TIAToolbox 允许我们自动化常见的下游分析任务，如 `组织分类 "
"<https://doi.org/10.1016/j.media.2022.102685>`__。在本教程中，我们将展示如何：1. 使用 "
"TIAToolbox 加载 WSI 图像；2. 使用不同的 PyTorch 模型在切片级别对图像进行分类。在本教程中，我们将演示如何使用 "
"TorchVision 的 ``ResNet18`` 模型以及自定义 `HistoEncoder` "
"<https://github.com/jopo666/HistoEncoder>`__ 模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let’s get started!"
msgstr "让我们开始吧！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Setting up the environment"
msgstr "设置环境"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To run the examples provided in this tutorial, the following packages are "
"required as prerequisites."
msgstr "要运行本教程中提供的示例，需要以下软件包作为前提。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "OpenJpeg"
msgstr "OpenJpeg"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "OpenSlide"
msgstr "OpenSlide"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Pixman"
msgstr "Pixman"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TIAToolbox"
msgstr "TIAToolbox"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "HistoEncoder (for a custom model example)"
msgstr "HistoEncoder（用于自定义模型示例）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Please run the following command in your terminal to install these packages:"
msgstr "请在终端中运行以下命令以安装这些软件包："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`apt-get -y -qq install libopenjp2-7-dev libopenjp2-tools openslide-tools "
"libpixman-1-dev` `pip install -q 'tiatoolbox<1.5' histoencoder && echo "
"\"Installation is done.\"`"
msgstr ""
"`apt-get -y -qq install libopenjp2-7-dev libopenjp2-tools openslide-tools "
"libpixman-1-dev` `pip install -q &apos;tiatoolbox<1.5&apos; histoencoder && "
"echo \"安装完成。\"`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Alternatively, you can run ``brew install openjpeg openslide`` to install "
"the prerequisite packages on MacOS instead of ``apt-get``. Further "
"information on installation can be `found here <https://tia-"
"toolbox.readthedocs.io/en/latest/installation.html>`__."
msgstr ""
"或者，你可以运行 ``brew install openjpeg openslide`` 来在 MacOS 上安装所需软件包，而不是使用 ``apt-"
"get``。更多安装信息可以在 `此处找到 <https://tia-"
"toolbox.readthedocs.io/en/latest/installation.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Importing related libraries"
msgstr "导入相关库"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Clean-up before a run"
msgstr "运行前清理"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To ensure proper clean-up (for example in abnormal termination), all files "
"downloaded or created in this run are saved in a single directory "
"``global_save_dir``, which we set equal to “./tmp/”. To simplify "
"maintenance, the name of the directory occurs only at this one place, so "
"that it can easily be changed, if desired."
msgstr ""
"为了确保适当的清理（例如异常终止时），此运行中下载或创建的所有文件都保存在单个目录 ``global_save_dir`` 中，我们将其设置为 "
"“./tmp/”。为了简化维护，目录名称仅在此处出现，因此可以轻松修改。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Downloading the data"
msgstr "下载数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For our sample data, we will use one whole-slide image, and patches from the"
" validation subset of `Kather 100k <https://zenodo.org/record/1214456#.YJ-"
"tn3mSkuU>`__ dataset."
msgstr ""
"对于我们的示例数据，我们将使用一张全切片图像以及来自 `Kather 100k "
"<https://zenodo.org/record/1214456#.YJ-tn3mSkuU>`__ 数据集验证子集中的一些补丁。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Reading the data"
msgstr "读取数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We create a list of patches and a list of corresponding labels. For example,"
" the first label in ``label_list`` will indicate the class of the first "
"image patch in ``patch_list``."
msgstr ""
"我们创建一个补丁列表和一个对应的标签列表。例如，``label_list`` 中的第一个标签将指示 ``patch_list`` "
"中第一个图像补丁的类别。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As you can see for this patch dataset, we have 9 classes/labels with IDs 0-8"
" and associated class names. describing the dominant tissue type in the "
"patch:"
msgstr "正如你所看到的，这个补丁数据集中有9个类别/标签，ID从0到8，并有相关的类别名称，描述补丁中主要的组织类型："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "BACK ⟶ Background (empty glass region)"
msgstr "BACK ⟶ 背景（空玻璃区域）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "LYM ⟶ Lymphocytes"
msgstr "LYM ⟶ 淋巴细胞"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "NORM ⟶ Normal colon mucosa"
msgstr "NORM ⟶ 正常结肠黏膜"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "DEB ⟶ Debris"
msgstr "DEB ⟶ 碎屑"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "MUS ⟶ Smooth muscle"
msgstr "MUS ⟶ 平滑肌"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "STR ⟶ Cancer-associated stroma"
msgstr "STR ⟶ 与癌症相关的间质"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "ADI ⟶ Adipose"
msgstr "ADI ⟶ 脂肪组织"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "MUC ⟶ Mucus"
msgstr "MUC ⟶ 粘液"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TUM ⟶ Colorectal adenocarcinoma epithelium"
msgstr "TUM ⟶ 结直肠腺癌上皮"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Classify image patches"
msgstr "分类图像补丁"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We demonstrate how to obtain a prediction for each patch within a digital "
"slide first with the ``patch`` mode and then with a large slide using "
"``wsi`` mode."
msgstr "我们演示如何先使用 ``patch`` 模式，然后使用 ``wsi`` 模式对数字切片中的每个补丁进行预测。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Define ``PatchPredictor`` model"
msgstr "定义 ``PatchPredictor`` 模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The PatchPredictor class runs a CNN-based classifier written in PyTorch."
msgstr "PatchPredictor 类运行一个基于 CNN 的分类器，使用 PyTorch 编写。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``model`` can be any trained PyTorch model with the constraint that it "
"should follow the ``tiatoolbox.models.abc.ModelABC`` `(docs)` <https://tia-"
"toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html>`__"
" class structure. For more information on this matter, please refer to `our "
"example notebook on advanced model techniques "
"<https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-"
"modeling.ipynb>`__. In order to load a custom model, you need to write a "
"small preprocessing function, as in ``preproc_func(img)``, which makes sure "
"the input tensors are in the right format for the loaded network."
msgstr ""
"``model`` 可以是任何训练好的 PyTorch 模型，条件是它应该遵循 ``tiatoolbox.models.abc.ModelABC`` "
"`(文档)` <https://tia-"
"toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html>`__"
" 类结构。关于这一点的更多信息，请参阅 `高级模型技术示例笔记本 "
"<https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-"
"modeling.ipynb>`__。为了加载自定义模型，你需要编写一个小的预处理函数，例如 "
"``preproc_func(img)``，以确保输入张量格式正确，适用于加载的网络。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Alternatively, you can pass ``pretrained_model`` as a string argument. This "
"specifies the CNN model that performs the prediction, and it must be one of "
"the models listed `here <https://tia-"
"toolbox.readthedocs.io/en/stable/_autosummary/tiatoolbox.models.architecture.get_pretrained_model.html#tiatoolbox.models.architecture.get_pretrained_model>`__."
" The command will look like this: ``predictor = "
"PatchPredictor(pretrained_model='resnet18-kather100k', "
"pretrained_weights=weights_path, batch_size=32)``."
msgstr ""
"或者，你可以将 ``pretrained_model`` 作为字符串参数传递。这指定了执行预测的 CNN 模型，它必须是列出在 `此处 "
"<https://tia-"
"toolbox.readthedocs.io/en/stable/_autosummary/tiatoolbox.models.architecture.get_pretrained_model.html#tiatoolbox.models.architecture.get_pretrained_model>`__"
" 的模型之一。命令将如下所示：``predictor = "
"PatchPredictor(pretrained_model=&apos;resnet18-kather100k&apos;, "
"pretrained_weights=weights_path, batch_size=32)``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``pretrained_weights``: When using a ``pretrained_model``, the corresponding"
" pretrained weights will also be downloaded by default. You can override the"
" default with your own set of weights via the ``pretrained_weight`` "
"argument."
msgstr ""
"``pretrained_weights``：使用 ``pretrained_model`` 时，相应的预训练权重默认也会下载。你可以通过 "
"``pretrained_weight`` 参数用自己的一组权重覆盖默认值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``batch_size``: Number of images fed into the model each time. Higher values"
" for this parameter require a larger (GPU) memory capacity."
msgstr "``batch_size``：每次输入模型的图像数量。该参数的较高值需要更大的（GPU）内存容量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Predict patch labels"
msgstr "预测补丁标签"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We create a predictor object and then call the ``predict`` method using the "
"``patch`` mode. We then compute the classification accuracy and confusion "
"matrix."
msgstr "我们创建一个预测器对象，然后使用 ``patch`` 模式调用 ``predict`` 方法。我们随后计算分类准确率和混淆矩阵。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Predict patch labels for a whole slide"
msgstr "为整个切片预测补丁标签"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We now introduce ``IOPatchPredictorConfig``, a class that specifies the "
"configuration of image reading and prediction writing for the model "
"prediction engine. This is required to inform the classifier which level of "
"the WSI pyramid the classifier should read, process data and generate "
"output."
msgstr ""
"我们现在介绍 ``IOPatchPredictorConfig``，它是一个指定图像读取和预测写入配置的类，用于模型预测引擎。这是为了通知分类器 WSI"
" 金字塔的哪个级别应该被读取、处理数据以及生成输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Parameters of ``IOPatchPredictorConfig`` are defined as:"
msgstr "``IOPatchPredictorConfig`` 的参数定义如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``input_resolutions``: A list, in the form of a dictionary, specifying the "
"resolution of each input. List elements must be in the same order as in the "
"target ``model.forward()``. If your model accepts only one input, you just "
"need to put one dictionary specifying ``'units'`` and ``'resolution'``. Note"
" that TIAToolbox supports a model with more than one input. For more "
"information on units and resolution, please see `TIAToolbox documentation "
"<https://tia-"
"toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect>`__."
msgstr ""
"``input_resolutions``：一个字典形式的列表，指定每个输入的分辨率。列表元素必须与目标 ``model.forward()`` "
"中的顺序相同。如果你的模型仅接受一个输入，则只需要放一个字典指定 ``&apos;units&apos;`` 和 "
"``&apos;resolution&apos;``。请注意，TIAToolbox 支持具有多个输入的模型。有关单位和分辨率的更多信息，请参阅 "
"`TIAToolbox 文档 <https://tia-"
"toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``patch_input_shape``: Shape of the largest input in (height, width) format."
msgstr "``patch_input_shape``：最大输入的形状，以 (高度, 宽度) 格式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``stride_shape``: The size of a stride (steps) between two consecutive "
"patches, used in the patch extraction process. If the user sets "
"``stride_shape`` equal to ``patch_input_shape``, patches will be extracted "
"and processed without any overlap."
msgstr ""
"``stride_shape``：补丁提取过程中过两连续补丁之间的步幅大小。如果用户将 ``stride_shape`` 设置为 "
"``patch_input_shape``，补丁将被提取并处理而没有任何重叠。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``predict`` method applies the CNN on the input patches and get the "
"results. Here are the arguments and their descriptions:"
msgstr "``predict`` 方法将 CNN 应用于输入补丁上并获得结果。以下是参数及其描述："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``mode``: Type of input to be processed. Choose from ``patch``, ``tile`` or "
"``wsi`` according to your application."
msgstr "``mode``：要处理的输入类型。根据你的应用选择 ``patch``、``tile`` 或 ``wsi``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``imgs``: List of inputs, which should be a list of paths to the input tiles"
" or WSIs."
msgstr "``imgs``：输入列表，应为输入图块或 WSI 的路径列表。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``return_probabilities``: Set to **True** to get per class probabilities "
"alongside predicted labels of input patches. If you wish to merge the "
"predictions to generate prediction maps for ``tile`` or ``wsi`` modes, you "
"can set ``return_probabilities=True``."
msgstr ""
"``return_probabilities``：设置为 **True** 以获取每个类别的概率以及输入补丁的预测标签。如果你希望合并预测以生成 "
"``tile`` 或 ``wsi`` 模式的预测地图，可以设置 ``return_probabilities=True``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``ioconfig``: set the IO configuration information using the "
"``IOPatchPredictorConfig`` class."
msgstr "``ioconfig``：使用 ``IOPatchPredictorConfig`` 类设置 IO 配置信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``resolution`` and ``unit`` (not shown below): These arguments specify the "
"level or micron-per-pixel resolution of the WSI levels from which we plan to"
" extract patches and can be used instead of ``ioconfig``. Here we specify "
"the WSI level as ``'baseline'``, which is equivalent to level 0. In general,"
" this is the level of greatest resolution. In this particular case, the "
"image has only one level. More information can be found in the "
"`documentation <https://tia-"
"toolbox.readthedocs.io/en/stable/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect>`__."
msgstr ""
"``resolution`` 和 ``unit``（下文未显示）：这些参数指定 WSI "
"金字塔级别或每像素微米分辨率，根据我们计划提取补丁的内容，可以替代 ``ioconfig`` 使用。在这里，我们将 WSI 级别指定为 "
"``&apos;baseline&apos;``，即等同于第0级。通常，这是最高清分辨率的级别。在这个特殊情况下，图像只有一个级别。更多信息可以在 "
"`文档 <https://tia-"
"toolbox.readthedocs.io/en/stable/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect>`__"
" 中找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``masks``: A list of paths corresponding to the masks of WSIs in the "
"``imgs`` list. These masks specify the regions in the original WSIs from "
"which we want to extract patches. If the mask of a particular WSI is "
"specified as ``None``, then the labels for all patches of that WSI (even "
"background regions) would be predicted. This could cause unnecessary "
"computation."
msgstr ""
"``masks``：对应于 ``imgs`` 列表中 WSI 掩膜路径的列表。这些掩膜指定我们想要从原始 WSI 中提取补丁的区域。如果某个 WSI "
"的掩膜指定为 ``None``，则该 WSI 所有补丁（甚至是背景区域）的标签都将被预测。这可能会导致不必要的计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``merge_predictions``: You can set this parameter to ``True`` if it’s "
"required to generate a 2D map of patch classification results. However, for "
"large WSIs this will require large available memory. An alternative "
"(default) solution is to set ``merge_predictions=False``, and then generate "
"the 2D prediction maps using the ``merge_predictions`` function as you will "
"see later on."
msgstr ""
"``merge_predictions``：如果需要生成补丁分类结果的二维地图，你可以将此参数设置为 ``True``。然而，对于大型 "
"WSI，这将需要大量的可用内存。一种替代（默认）解决方案是设置 ``merge_predictions=False``，然后使用你后面将看到的 "
"``merge_predictions`` 函数生成二维预测地图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since we are using a large WSI the patch extraction and prediction processes"
" may take some time (make sure to set the ``ON_GPU=True`` if you have access"
" to Cuda enabled GPU and PyTorch+Cuda)."
msgstr ""
"由于我们正在使用大型 WSI，补丁提取和预测过程可能需要一些时间（请确保在拥有支持 Cuda 的 GPU 和 PyTorch+Cuda 的情况下设置 "
"``ON_GPU=True``）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We see how the prediction model works on our whole-slide images by "
"visualizing the ``wsi_output``. We first need to merge patch prediction "
"outputs and then visualize them as an overlay on the original image. As "
"before, the ``merge_predictions`` method is used to merge the patch "
"predictions. Here we set the parameters ``resolution=1.25, units='power'`` "
"to generate the prediction map at 1.25x magnification. If you would like to "
"have higher/lower resolution (bigger/smaller) prediction maps, you need to "
"change these parameters accordingly. When the predictions are merged, use "
"the ``overlay_patch_prediction`` function to overlay the prediction map on "
"the WSI thumbnail, which should be extracted at the resolution used for "
"prediction merging."
msgstr ""
"通过可视化 "
"``wsi_output``，我们可以看到预测模型在我们的全切片图像上的表现。我们首先需要合并补丁预测输出，然后将其作为原始图像上的叠加层进行可视化。与之前一样，``merge_predictions``"
" 方法用于合并补丁预测。在这里，我们将参数 ``resolution=1.25, units=&apos;power&apos;`` "
"设置为1.25倍放大率生成预测地图。如果你想要更高/更低分辨率（更大/更小）的预测地图，需要相应地更改这些参数。当预测被合并后，使用 "
"``overlay_patch_prediction`` 函数将预测地图叠加到WSI缩略图上，该缩略图应根据预测合并所用的分辨率提取。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Overlaying the prediction map on this image as below gives:"
msgstr "将预测地图叠加到此图像中，如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Feature extraction with a pathology-specific model"
msgstr "使用特定于病理学的模型进行特征提取"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we will show how to extract features from a pretrained "
"PyTorch model that exists outside TIAToolbox, using the WSI inference "
"engines provided by TIAToolbox. To illustrate this we will use HistoEncoder,"
" a computational-pathology specific model that has been trained in a self-"
"supervised fashion to extract features from histology images. The model has "
"been made available here:"
msgstr ""
"在本节中，我们将展示如何使用 TIAToolbox 提供的 WSI 推理引擎，从 TIAToolbox 之外的预训练 PyTorch "
"模型中提取特征。为了说明这一点，我们将使用 "
"HistoEncoder，一种特定于计算病理学的模型，它以自监督的方式训练以从组织学图像中提取特征。这个模型已在以下位置提供："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"‘HistoEncoder: Foundation models for digital pathology’ "
"(https://github.com/jopo666/HistoEncoder) by Pohjonen, Joona and team at the"
" University of Helsinki."
msgstr ""
"‘HistoEncoder: "
"基于数字病理学的基础模型’（https://github.com/jopo666/HistoEncoder），由赫尔辛基大学的 Joona "
"Pohjonen 和团队开发。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will plot a umap reduction into 3D (RGB) of the feature map to visualize "
"how the features capture the differences between some of the above mentioned"
" tissue types."
msgstr "我们将绘制一个特征地图的 umap 降维到三维（RGB）以可视化这些特征如何捕获上述某些组织类型之间的差异。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TIAToolbox defines a ModelABC which is a class inheriting PyTorch `nn.Module"
" <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`__ and "
"specifies how a model should look in order to be used in the TIAToolbox "
"inference engines. The histoencoder model doesn’t follow this structure, so "
"we need to wrap it in a class whose output and methods are those that the "
"TIAToolbox engine expects."
msgstr ""
"TIAToolbox 定义了一个 ModelABC，它是一个继承了 PyTorch `nn.Module "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`__ "
"的类，并规定了一个模型应如何配置才能在 TIAToolbox 推理引擎中使用。histoencoder "
"模型并未遵循该结构，因此我们需要将其封装到一个类中，这个类的输出和方法符合 TIAToolbox 引擎的要求。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have our wrapper, we will create our feature extraction model "
"and instantiate a `DeepFeatureExtractor <https://tia-"
"toolbox.readthedocs.io/en/v1.4.1/_autosummary/tiatoolbox.models.engine.semantic_segmentor.DeepFeatureExtractor.html>`__"
" to allow us to use this model over a WSI. We will use the same WSI as "
"above, but this time we will extract features from the patches of the WSI "
"using the HistoEncoder model, rather than predicting some label for each "
"patch."
msgstr ""
"现在我们有了封装器，我们将创建我们的特征提取模型，并实例化一个 `DeepFeatureExtractor <https://tia-"
"toolbox.readthedocs.io/en/v1.4.1/_autosummary/tiatoolbox.models.engine.semantic_segmentor.DeepFeatureExtractor.html>`__，以便我们可以在整个WSI上使用此模型。我们将使用前面相同的WSI，但这次我们将使用HistoEncoder模型从WSI的切片中提取特征，而不是为每个切片预测一些标签。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When we create the ``DeepFeatureExtractor``, we will pass the "
"``auto_generate_mask=True`` argument. This will automatically create a mask "
"of the tissue region using otsu thresholding, so that the extractor "
"processes only those patches containing tissue."
msgstr ""
"当我们创建 ``DeepFeatureExtractor`` 时，我们会传递 ``auto_generate_mask=True`` "
"参数。这将使用Otsu阈值法自动生成组织区域的掩码，从而提取器仅处理包含组织的切片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"These features could be used to train a downstream model, but here in order "
"to get some intuition for what the features represent, we will use a UMAP "
"reduction to visualize the features in RGB space. The points labeled in a "
"similar color should have similar features, so we can check if the features "
"naturally separate out into the different tissue regions when we overlay the"
" UMAP reduction on the WSI thumbnail. We will plot it along with the patch-"
"level prediction map from above to see how the features compare to the "
"patch-level predictions in the following cells."
msgstr ""
"这些特征可以用于训练下游模型，但在这里，为了直观了解这些特征代表了什么，我们将使用UMAP降维在RGB空间中可视化这些特征。用相似颜色标记的点应具有相似的特征，因此当我们将UMAP降维叠加到WSI缩略图上时，可以检查这些特征是否自然分离到不同的组织区域中。在接下来的单元格中，我们将绘制它，以及之前的切片级预测图，以查看特征与切片级预测之间的比较。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We see that the prediction map from our patch-level predictor, and the "
"feature map from our self-supervised feature encoder, capture similar "
"information about the tissue types in the WSI. This is a good sanity check "
"that our models are working as expected. It also shows that the features "
"extracted by the HistoEncoder model are capturing the differences between "
"the tissue types, and so that they are encoding histologically relevant "
"information."
msgstr ""
"我们看到，从切片级预测模型的预测图和我们的自监督特征编码器的特征图，都捕获了WSI中组织类型的相似信息。这是对我们模型按预期工作的一个很好的验证。这也表明由HistoEncoder模型提取的特征捕获了组织类型之间的差异，因此它们编码了与组织学相关的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Where to Go From Here"
msgstr "下一步做什么"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this notebook, we show how we can use the ``PatchPredictor`` and "
"``DeepFeatureExtractor`` classes and their ``predict`` method to predict the"
" label, or extract features, for patches of big tiles and WSIs. We introduce"
" ``merge_predictions`` and ``overlay_prediction_mask`` helper functions that"
" merge the patch prediction outputs and visualize the resulting prediction "
"map as an overlay on the input image/WSI."
msgstr ""
"在本笔记中，我们展示了如何使用``PatchPredictor``和``DeepFeatureExtractor``类及其``predict``方法来预测大块切片和WSI中切片的标签或提取特征。我们介绍了``merge_predictions``和``overlay_prediction_mask``辅助函数，这些函数将切片预测输出合并，并将生成的预测图作为叠加显示在输入图像/WSI上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"All the processes take place within TIAToolbox and we can easily put the "
"pieces together, following our example code. Please make sure to set inputs "
"and options correctly. We encourage you to further investigate the effect on"
" the prediction output of changing ``predict`` function parameters. We have "
"demonstrated how to use your own pretrained model or one provided by the "
"research community for a specific task in the TIAToolbox framework to do "
"inference on large WSIs even if the model structure is not defined in the "
"TIAToolbox model class."
msgstr ""
"所有这些过程在TIAToolbox内进行，我们可以按照示例代码轻松拼接各个部分。请务必正确设置输入和选项。我们鼓励您进一步研究改变``predict``函数参数对预测输出的影响。我们已展示如何在TIAToolbox框架中使用您自己的预训练模型或研究社区提供的模型为特定任务在大型WSI上进行推理，即使模型结构未在TIAToolbox模型类中定义。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "You can learn more through the following resources:"
msgstr "您可以通过以下资源了解更多信息："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Advanced model handling with PyTorch and TIAToolbox <https://tia-"
"toolbox.readthedocs.io/en/latest/_notebooks/jnb/07-advanced-"
"modeling.html>`__"
msgstr ""
"`使用PyTorch和TIAToolbox的高级模型处理 <https://tia-"
"toolbox.readthedocs.io/en/latest/_notebooks/jnb/07-advanced-"
"modeling.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Creating slide graphs for WSI with a custom PyTorch graph neural network "
"<https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/full-"
"pipelines/slide-graph.html>`__"
msgstr ""
"`用自定义PyTorch图神经网络为WSI创建幻灯片图 <https://tia-"
"toolbox.readthedocs.io/en/latest/_notebooks/jnb/full-pipelines/slide-"
"graph.html>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_torch_compile_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_torch_compile_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introduction to ``torch.compile``"
msgstr "``torch.compile`` 简介"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author:** William Wen"
msgstr "**作者:** William Wen"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.compile`` is the latest method to speed up your PyTorch code! "
"``torch.compile`` makes PyTorch code run faster by JIT-compiling PyTorch "
"code into optimized kernels, all while requiring minimal code changes."
msgstr ""
"``torch.compile`` 是一种最新方法，可以加速您的PyTorch代码！``torch.compile`` "
"通过JIT编译PyTorch代码为优化内核，从而让PyTorch代码运行得更快，同时需要的代码改动最少。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we cover basic ``torch.compile`` usage, and demonstrate "
"the advantages of ``torch.compile`` over previous PyTorch compiler "
"solutions, such as `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`__ and `FX Tracing "
"<https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace>`__."
msgstr ""
"在本教程中，我们介绍 ``torch.compile`` 的基础用法，并演示其相对于之前PyTorch编译器解决方案（如`TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`__ 和 `FX Tracing "
"<https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace>`__）的优势。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Contents**"
msgstr "**目录**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Required pip Dependencies**"
msgstr "**所需的pip依赖**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torch >= 2.0``"
msgstr "``torch >= 2.0``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torchvision``"
msgstr "``torchvision``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``numpy``"
msgstr "``numpy``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``scipy``"
msgstr "``scipy``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``tabulate``"
msgstr "``tabulate``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**System Requirements** - A C++ compiler, such as ``g++`` - Python "
"development package (``python-devel``/``python-dev``)"
msgstr ""
"**系统要求** - 一个C++编译器，例如 ``g++`` - Python开发包 (``python-devel``/``python-dev``)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"NOTE: a modern NVIDIA GPU (H100, A100, or V100) is recommended for this "
"tutorial in order to reproduce the speedup numbers shown below and "
"documented elsewhere."
msgstr "注意：推荐使用现代NVIDIA GPU（H100、A100或V100）以再现以下所示和其他地方记录的加速数值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.compile`` is included in the latest PyTorch. Running TorchInductor "
"on GPU requires Triton, which is included with the PyTorch 2.0 nightly "
"binary. If Triton is still missing, try installing ``torchtriton`` via pip "
"(``pip install torchtriton --extra-index-url "
"\"https://download.pytorch.org/whl/nightly/cu117\"`` for CUDA 11.7)."
msgstr ""
"``torch.compile`` "
"包含在最新的PyTorch中。在GPU上运行TorchInductor需要Triton，Triton包含在PyTorch "
"2.0夜间版中。如果仍缺少Triton，请尝试通过pip安装``torchtriton``（``pip install torchtriton "
"--extra-index-url \"https://download.pytorch.org/whl/nightly/cu117\"`` "
"适用于CUDA 11.7）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Arbitrary Python functions can be optimized by passing the callable to "
"``torch.compile``. We can then call the returned optimized function in place"
" of the original function."
msgstr ""
"可以通过将可调用对象传递给``torch.compile``来优化任意Python函数。然后，我们可以用返回的优化函数代替原始函数进行调用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Alternatively, we can decorate the function."
msgstr "或者，我们也可以装饰该函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We can also optimize ``torch.nn.Module`` instances."
msgstr "我们也可以优化``torch.nn.Module``实例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "torch.compile and Nested Calls"
msgstr "torch.compile 和嵌套调用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Nested function calls within the decorated function will also be compiled."
msgstr "装饰函数中的嵌套函数调用也会被编译。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the same fashion, when compiling a module all sub-modules and methods "
"within it, that are not in a skip list, are also compiled."
msgstr "同样，当编译一个模块时，所有非跳过列表中的子模块和方法也都会被编译。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can also disable some functions from being compiled by using "
"``torch.compiler.disable``. Suppose you want to disable the tracing on just "
"the ``complex_function`` function, but want to continue the tracing back in "
"``complex_conjugate``. In this case, you can use "
"``torch.compiler.disable(recursive=False)`` option. Otherwise, the default "
"is ``recursive=True``."
msgstr ""
"我们还可以使用``torch.compiler.disable``禁用某些函数的编译。假设您想仅对``complex_function``函数禁用跟踪，但希望在``complex_conjugate``中恢复跟踪。在这种情况下，您可以使用"
" ``torch.compiler.disable(recursive=False)`` 选项。否则，默认是``recursive=True``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Best Practices and Recommendations"
msgstr "最佳实践与建议"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Behavior of ``torch.compile`` with Nested Modules and Function Calls"
msgstr "``torch.compile``在嵌套模块和函数调用中的行为"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When you use ``torch.compile``, the compiler will try to recursively compile"
" every function call inside the target function or module inside the target "
"function or module that is not in a skip list (such as built-ins, some "
"functions in the torch.* namespace)."
msgstr ""
"使用``torch.compile``时，编译器会尝试递归编译目标函数或模块内的每个函数调用，前提是这些函数或模块不在跳过列表中（例如内置函数，一些torch.*命名空间中的函数）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Best Practices:**"
msgstr "**最佳实践:**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"1. **Top-Level Compilation:** One approach is to compile at the highest "
"level possible (i.e., when the top-level module is initialized/called) and "
"selectively disable compilation when encountering excessive graph breaks or "
"errors. If there are still many compile issues, compile individual "
"subcomponents instead."
msgstr ""
"1. **顶层编译:** "
"一种方法是在最高可能的级别（即当顶级模块初始化/调用时）进行编译，并在遇到过多图中断或错误时选择性地禁用编译。若仍存在许多编译问题，可仅编译单独的子组件。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"2. **Modular Testing:** Test individual functions and modules with "
"``torch.compile`` before integrating them into larger models to isolate "
"potential issues."
msgstr ""
"2. **模块化测试:** 用 ``torch.compile`` 单独测试各个函数和模块，然后再将它们整合到更大的模型中，以隔离潜在问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"3. **Disable Compilation Selectively:** If certain functions or sub-modules "
"cannot be handled by `torch.compile`, use the `torch.compiler.disable` "
"context managers to recursively exclude them from compilation."
msgstr ""
"3. **选择性禁用编译:** "
"如果某些函数或子模块无法被``torch.compile``处理，可使用``torch.compiler.disable``上下文管理器递归地将它们排除在编译之外。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"4. **Compile Leaf Functions First:** In complex models with multiple nested "
"functions and modules, start by compiling the leaf functions or modules "
"first. For more information see `TorchDynamo APIs for fine-grained tracing "
"<https://pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html>`__."
msgstr ""
"4. **优先编译叶子函数:** 在具有多个嵌套函数和模块的复杂模型中，可先从编译叶子函数或模块开始。有关更多信息，请参阅`TorchDynamo "
"APIs for fine-grained tracing "
"<https://pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Prefer ``mod.compile()`` over ``torch.compile(mod)``:** Avoids ``_orig_`` "
"prefix issues in ``state_dict``."
msgstr ""
"**推荐使用 ``mod.compile()`` 而不是 ``torch.compile(mod)``:** "
"避免``state_dict``中的``_orig_``前缀问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"6. **Use ``fullgraph=True`` to catch graph breaks:** Helps ensure end-to-end"
" compilation, maximizing speedup and compatibility with ``torch.export``."
msgstr ""
"6. **使用``fullgraph=True``捕获图中断:** 有助于确保端到端编译，从而最大限度提高加速并与``torch.export``兼容。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Demonstrating Speedups"
msgstr "演示加速效果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's now demonstrate that using ``torch.compile`` can speed up real models."
" We will compare standard eager mode and ``torch.compile`` by evaluating and"
" training a ``torchvision`` model on random data."
msgstr ""
"现在让我们演示使用``torch.compile``可以加速真实模型。我们将在随机数据上评估和训练一个``torchvision``模型，并比较其标准模式(eager)和``torch.compile``模式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Before we start, we need to define some utility functions."
msgstr "在开始前，我们需要定义一些实用函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "First, let's compare inference."
msgstr "首先，让我们比较推论。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that in the call to ``torch.compile``, we have the additional ``mode`` "
"argument, which we will discuss below."
msgstr "请注意在调用``torch.compile``时，我们还有一个附加的``mode``参数，我们将随后讨论。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Notice that ``torch.compile`` takes a lot longer to complete compared to "
"eager. This is because ``torch.compile`` compiles the model into optimized "
"kernels as it executes. In our example, the structure of the model doesn't "
"change, and so recompilation is not needed. So if we run our optimized model"
" several more times, we should see a significant improvement compared to "
"eager."
msgstr ""
"注意``torch.compile``完成的时间比标准模式(eager)长得多，因为``torch.compile``在执行时编译模型为优化内核。在我们的示例中，模型的结构不会变化，因此不需要重新编译。所以如果我们多次运行已经优化的模型，应该能看到与标准模式相比显著的改进。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And indeed, we can see that running our model with ``torch.compile`` results"
" in a significant speedup. Speedup mainly comes from reducing Python "
"overhead and GPU read/writes, and so the observed speedup may vary on "
"factors such as model architecture and batch size. For example, if a model's"
" architecture is simple and the amount of data is large, then the bottleneck"
" would be GPU compute and the observed speedup may be less significant."
msgstr ""
"确实，我们可以看到运行我们的模型时，``torch.compile``相对于标准模式具有显著的加速效果。加速主要是通过减少Python的开销和GPU的读写开销实现的，因此观察到的加速可能会因模型架构和批次大小等因素而有所不同。例如，如果模型架构相对简单并且数据量较大，则瓶颈可能是GPU计算，观察到的加速效果可能相对较小。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You may also see different speedup results depending on the chosen ``mode`` "
"argument. The ``\"reduce-overhead\"`` mode uses CUDA graphs to further "
"reduce the overhead of Python. For your own models, you may need to "
"experiment with different modes to maximize speedup. You can read more about"
" modes `here <https://pytorch.org/get-started/pytorch-2.0/#user-"
"experience>`__."
msgstr ""
"您可能还会发现基于所选择的``mode``参数会有不同的加速结果。``\"reduce-"
"overhead\"``模式使用CUDA图进一步减少Python的开销。对于您自己的模型，可能需要尝试不同的模式以获得最大加速效果。您可以在`此处 "
"<https://pytorch.org/get-started/pytorch-2.0/#user-"
"experience>`__阅读更多有关模式的信息。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You may might also notice that the second time we run our model with "
"``torch.compile`` is significantly slower than the other runs, although it "
"is much faster than the first run. This is because the ``\"reduce-"
"overhead\"`` mode runs a few warm-up iterations for CUDA graphs."
msgstr ""
"您可能还会注意到，我们第二次运行模型时比其他运行慢得多，尽管比第一次运行快得多。这是因为``\"reduce-"
"overhead\"``模式会为CUDA图运行一些热身迭代。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For general PyTorch benchmarking, you can try using "
"``torch.utils.benchmark`` instead of the ``timed`` function we defined "
"above. We wrote our own timing function in this tutorial to show "
"``torch.compile``'s compilation latency."
msgstr ""
"对于一般的PyTorch基准测试，可以尝试使用``torch.utils.benchmark``，而不是我们定义的``timed``函数。我们在本教程中编写自己的计时函数是为了展示``torch.compile``的编译延迟。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Now, let's consider comparing training."
msgstr "现在，让我们考虑比较训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Again, we can see that ``torch.compile`` takes longer in the first "
"iteration, as it must compile the model, but in subsequent iterations, we "
"see significant speedups compared to eager."
msgstr ""
"同样，我们可以看到，``torch.compile``在第一次迭代中花费了更长时间，因为必须编译模型，但在后续迭代中，与标准模式相比，我们看到了显著的加速效果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We remark that the speedup numbers presented in this tutorial are for "
"demonstration purposes only. Official speedup values can be seen at the "
"`TorchInductor performance dashboard "
"<https://hud.pytorch.org/benchmark/compilers>`__."
msgstr ""
"我们提醒，本教程中展示的加速数字仅供演示。官方加速数据可在`TorchInductor性能仪表盘 "
"<https://hud.pytorch.org/benchmark/compilers>`__中查看。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Comparison to TorchScript and FX Tracing"
msgstr "与TorchScript和FX Tracing的比较"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have seen that ``torch.compile`` can speed up PyTorch code. Why else "
"should we use ``torch.compile`` over existing PyTorch compiler solutions, "
"such as TorchScript or FX Tracing? Primarily, the advantage of "
"``torch.compile`` lies in its ability to handle arbitrary Python code with "
"minimal changes to existing code."
msgstr ""
"我们已经看到``torch.compile``可以加速PyTorch代码。那么，为什么还要选择``torch.compile``而不是现有的PyTorch编译器解决方案，例如TorchScript或FX"
" Tracing？主要原因是``torch.compile``能够以最小的代码改动处理任意Python代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One case that ``torch.compile`` can handle that other compiler solutions "
"struggle with is data-dependent control flow (the ``if x.sum() < 0:`` line "
"below)."
msgstr ""
"一个``torch.compile``可以处理但其他编译器解决方案难以处理的情况是数据依赖性的控制流（如下所示的``if x.sum() < "
"0:``行）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchScript tracing ``f1`` results in silently incorrect results, since only"
" the actual control flow path is traced."
msgstr "对函数``f1``的TorchScript跟踪会导致无声的错误结果，因为仅实际的控制流路径被跟踪。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"FX tracing ``f1`` results in an error due to the presence of data-dependent "
"control flow."
msgstr "对函数``f1``的FX跟踪会因为存在数据依赖性的控制流而导致错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If we provide a value for ``x`` as we try to FX trace ``f1``, then we run "
"into the same problem as TorchScript tracing, as the data-dependent control "
"flow is removed in the traced function."
msgstr ""
"如果我们为``x``提供一个值并尝试对``f1``进行FX跟踪，则会遇到与TorchScript跟踪同样的问题，因为数据依赖性的控制流在跟踪函数中被移除了。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we can see that ``torch.compile`` correctly handles data-dependent "
"control flow."
msgstr "现在我们可以看到``torch.compile``正确地处理了与数据有关的控制流。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchScript scripting can handle data-dependent control flow, but this "
"solution comes with its own set of problems. Namely, TorchScript scripting "
"can require major code changes and will raise errors when unsupported Python"
" is used."
msgstr ""
"TorchScript脚本可以处理与数据有关的控制流，但是这种解决方案也有其自身的问题。即，TorchScript脚本可能需要进行大量代码更改，并且在使用不支持的Python时会引发错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example below, we forget TorchScript type annotations and we receive "
"a TorchScript error because the input type for argument ``y``, an ``int``, "
"does not match with the default argument type, ``torch.Tensor``."
msgstr ""
"在下面的示例中，我们忘记了TorchScript类型注解，结果因为参数``y``的输入类型是``int``，而不是默认的``torch.Tensor``类型而收到TorchScript错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "However, ``torch.compile`` is easily able to handle ``f2``."
msgstr "然而，``torch.compile``能够轻松地处理``f2``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Another case that ``torch.compile`` handles well compared to previous "
"compilers solutions is the usage of non-PyTorch functions."
msgstr "另一个``torch.compile``相较于之前的编译器解决方案处理得很好的情况是对非PyTorch函数的使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchScript tracing treats results from non-PyTorch function calls as "
"constants, and so our results can be silently wrong."
msgstr "TorchScript跟踪将非PyTorch函数调用的结果视为常量，因此我们的结果可能会默默地出错。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchScript scripting and FX tracing disallow non-PyTorch function calls."
msgstr "TorchScript脚本和FX跟踪不允许调用非PyTorch函数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In comparison, ``torch.compile`` is easily able to handle the non-PyTorch "
"function call."
msgstr "相比之下，``torch.compile``能够轻松地处理非PyTorch函数调用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TorchDynamo and FX Graphs"
msgstr "TorchDynamo和FX图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One important component of ``torch.compile`` is TorchDynamo. TorchDynamo is "
"responsible for JIT compiling arbitrary Python code into `FX graphs "
"<https://pytorch.org/docs/stable/fx.html#torch.fx.Graph>`__, which can then "
"be further optimized. TorchDynamo extracts FX graphs by analyzing Python "
"bytecode during runtime and detecting calls to PyTorch operations."
msgstr ""
"``torch.compile``的一个重要组件是TorchDynamo。TorchDynamo负责将任意Python代码JIT编译为FX图，从而进行进一步优化。TorchDynamo通过在运行时分析Python字节码并检测对PyTorch操作的调用来提取FX图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Normally, TorchInductor, another component of ``torch.compile``, further "
"compiles the FX graphs into optimized kernels, but TorchDynamo allows for "
"different backends to be used. In order to inspect the FX graphs that "
"TorchDynamo outputs, let us create a custom backend that outputs the FX "
"graph and simply returns the graph's unoptimized forward method."
msgstr ""
"通常情况下，``torch.compile``的另一个组件TorchInductor会进一步将FX图编译为优化的内核，但TorchDynamo允许使用不同的后端。为了检查TorchDynamo输出的FX图，我们创建了一个自定义后端，该后端输出FX图并简单地返回图的未优化前向方法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using our custom backend, we can now see how TorchDynamo is able to handle "
"data-dependent control flow. Consider the function below, where the line "
"``if b.sum() < 0`` is the source of data-dependent control flow."
msgstr ""
"使用我们的自定义后端，现在我们可以看到TorchDynamo如何处理与数据有关的控制流。考虑下面的函数，其中``if b.sum() < "
"0``一行是数据相关控制流的来源。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The output reveals that TorchDynamo extracted 3 different FX graphs "
"corresponding the following code (order may differ from the output above):"
msgstr "输出显示TorchDynamo提取了与以下代码对应的3个不同的FX图（顺序可能与上面的输出不同）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``x = a / (torch.abs(a) + 1)``"
msgstr "``x = a / (torch.abs(a) + 1)``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``b = b * -1; return x * b``"
msgstr "``b = b * -1; return x * b``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``return x * b``"
msgstr "``return x * b``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When TorchDynamo encounters unsupported Python features, such as data-"
"dependent control flow, it breaks the computation graph, lets the default "
"Python interpreter handle the unsupported code, then resumes capturing the "
"graph."
msgstr ""
"当TorchDynamo遇到不支持的Python特性（例如数据相关的控制流）时，它会中断计算图，让默认的Python解释器处理不支持的代码，然后恢复捕获图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's investigate by example how TorchDynamo would step through ``bar``. If "
"``b.sum() < 0``, then TorchDynamo would run graph 1, let Python determine "
"the result of the conditional, then run graph 2. On the other hand, if ``not"
" b.sum() < 0``, then TorchDynamo would run graph 1, let Python determine the"
" result of the conditional, then run graph 3."
msgstr ""
"让我们通过一个示例调查TorchDynamo如何逐步处理``bar``。如果``b.sum() < "
"0``，TorchDynamo会运行图1，让Python确定条件的结果，然后运行图2。另一方面，如果``not b.sum() < "
"0``，TorchDynamo会运行图1，让Python确定条件的结果，然后运行图3。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This highlights a major difference between TorchDynamo and previous PyTorch "
"compiler solutions. When encountering unsupported Python features, previous "
"solutions either raise an error or silently fail. TorchDynamo, on the other "
"hand, will break the computation graph."
msgstr ""
"这突出体现了TorchDynamo与之前PyTorch编译器解决方案的主要区别。当遇到不支持的Python特性时，之前的解决方案要么引发错误，要么静默失败。而TorchDynamo则会中断计算图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can see where TorchDynamo breaks the graph by using "
"``torch._dynamo.explain``:"
msgstr "我们可以通过使用``torch._dynamo.explain``来查看TorchDynamo中断图的地方："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to maximize speedup, graph breaks should be limited. We can force "
"TorchDynamo to raise an error upon the first graph break encountered by "
"using ``fullgraph=True``:"
msgstr ""
"为了最大限度地提高速度，应尽量限制图中断。我们可以通过使用``fullgraph=True``强制TorchDynamo在遇到第一个图中断时引发错误："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And below, we demonstrate that TorchDynamo does not break the graph on the "
"model we used above for demonstrating speedups."
msgstr "下面，我们演示了TorchDynamo在我们上面用于演示加速的模型上并未中断图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can use ``torch.export`` (from PyTorch 2.1+) to extract a single, "
"exportable FX graph from the input PyTorch program. The exported graph is "
"intended to be run on different (i.e. Python-less) environments. One "
"important restriction is that the ``torch.export`` does not support graph "
"breaks. Please check `this tutorial "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html>`__ "
"for more details on ``torch.export``."
msgstr ""
"我们可以使用``torch.export``（从PyTorch "
"2.1+开始支持）从输入的PyTorch程序中提取单个、可导出的FX图。导出的图旨在在不同的（即无需Python的）环境中运行。一项重要的限制是``torch.export``不支持图中断。有关``torch.export``的更多详细信息，请查看`本教程"
" <https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we introduced ``torch.compile`` by covering basic usage, "
"demonstrating speedups over eager mode, comparing to previous PyTorch "
"compiler solutions, and briefly investigating TorchDynamo and its "
"interactions with FX graphs. We hope that you will give ``torch.compile`` a "
"try!"
msgstr ""
"在本教程中，我们通过涵盖基本用法、展示相较于即时模式的加速效果、与之前的PyTorch编译器解决方案进行比较，及简要研究TorchDynamo与FX图的交互，介绍了``torch.compile``。我们希望您能尝试一下``torch.compile``！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: torch_compile_tutorial.py "
"<torch_compile_tutorial.py>`"
msgstr ""
":download:`下载Python源码文件: torch_compile_tutorial.py "
"<torch_compile_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: torch_compile_tutorial.ipynb "
"<torch_compile_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: torch_compile_tutorial.ipynb "
"<torch_compile_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "torch.export Nightly Tutorial"
msgstr "torch.export实验性教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html"
msgstr ""
"本教程已迁移至https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_torch_export_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_torch_export_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "torch.export Tutorial"
msgstr "torch.export教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author:** William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan"
msgstr "**作者:** William Wen, Zhengxu Chen, Angela Yi, Pian Pawakapan"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.export`` and its related features are in prototype status and are "
"subject to backwards compatibility breaking changes. This tutorial provides "
"a snapshot of ``torch.export`` usage as of PyTorch 2.5."
msgstr ""
"``torch.export``及其相关功能处于原型状态，可能会发生向后兼容性破坏的变化。本教程提供了PyTorch "
"2.5中``torch.export``使用的快照。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":func:`torch.export` is the PyTorch 2.X way to export PyTorch models into "
"standardized model representations, intended to be run on different (i.e. "
"Python-less) environments. The official documentation can be found `here "
"<https://pytorch.org/docs/main/export.html>`__."
msgstr ""
":func:`torch.export` 是PyTorch "
"2.X提出的一种将PyTorch模型导出为标准模型表示的方法，目的是在不同的（即无需Python的）环境中运行。官方文档可以在`这里 "
"<https://pytorch.org/docs/main/export.html>`__找到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, you will learn how to use :func:`torch.export` to extract "
"``ExportedProgram``'s (i.e. single-graph representations) from PyTorch "
"programs. We also detail some considerations/modifications that you may need"
" to make in order to make your model compatible with ``torch.export``."
msgstr ""
"在本教程中，您将学习如何使用:func:`torch.export`从PyTorch程序中提取``ExportedProgram``（即单图表示）。我们还详细说明了一些可能需要进行的修改，以便让您的模型兼容``torch.export``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.export`` extracts single-graph representations from PyTorch programs"
" by tracing the target function, given example inputs. "
"``torch.export.export()`` is the main entry point for ``torch.export``."
msgstr ""
"``torch.export``通过跟踪目标函数生成单图表示，给定示例输入。``torch.export.export()``是``torch.export``的主要入口。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, ``torch.export`` and ``torch.export.export()`` are "
"practically synonymous, though ``torch.export`` generally refers to the "
"PyTorch 2.X export process, and ``torch.export.export()`` generally refers "
"to the actual function call."
msgstr ""
"在本教程中，``torch.export``和``torch.export.export()``在实际应用中几乎是同义的，不过``torch.export``通常指PyTorch"
" 2.X的导出过程，而``torch.export.export()``通常指实际的函数调用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The signature of ``torch.export.export()`` is:"
msgstr "``torch.export.export()``的函数签名为："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.export.export()`` traces the tensor computation graph from calling "
"``mod(*args, **kwargs)`` and wraps it in an ``ExportedProgram``, which can "
"be serialized or executed later with different inputs. To execute the "
"``ExportedProgram`` we can call ``.module()`` on it to return a "
"``torch.nn.Module`` which is callable, just like the original program. We "
"will detail the ``dynamic_shapes`` argument later in the tutorial."
msgstr ""
"``torch.export.export()``通过调用``mod(*args, "
"**kwargs)``跟踪张量计算图，并将其封装在一个``ExportedProgram``中，该程序可以序列化或稍后使用不同的输入执行。要执行``ExportedProgram``，我们可以调用``.module()``来返回一个可调用的``torch.nn.Module``，就像原始程序一样。我们将在教程中详细说明``dynamic_shapes``参数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's review some attributes of ``ExportedProgram`` that are of interest."
msgstr "让我们回顾一下``ExportedProgram``的一些感兴趣的属性。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``graph`` attribute is an `FX graph "
"<https://pytorch.org/docs/stable/fx.html#torch.fx.Graph>`__ traced from the "
"function we exported, that is, the computation graph of all PyTorch "
"operations. The FX graph is in \"ATen IR\" meaning that it contains only "
"\"ATen-level\" operations."
msgstr ""
"``graph``属性是从我们导出的函数中跟踪的一个`FX图 "
"<https://pytorch.org/docs/stable/fx.html#torch.fx.Graph>`__，即所有PyTorch操作的计算图。FX图是“ATen"
" IR”，意味着它仅包含“ATen级”操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``graph_signature`` attribute gives a more detailed description of the "
"input and output nodes in the exported graph, describing which ones are "
"parameters, buffers, user inputs, or user outputs."
msgstr "``graph_signature``属性提供了导出图中输入和输出节点的更详细描述，描述了哪些是参数、缓冲区、用户输入或用户输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The ``range_constraints`` attributes will be covered later."
msgstr "``range_constraints``属性将在后面介绍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"See the ``torch.export`` `documentation "
"<https://pytorch.org/docs/main/export.html#torch.export.export>`__ for more "
"details."
msgstr ""
"有关更多详细信息，请参阅``torch.export`` `文档 "
"<https://pytorch.org/docs/main/export.html#torch.export.export>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Graph Breaks"
msgstr "图中断"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Although ``torch.export`` shares components with ``torch.compile``, the key "
"limitation of ``torch.export``, especially when compared to "
"``torch.compile``, is that it does not support graph breaks. This is because"
" handling graph breaks involves interpreting the unsupported operation with "
"default Python evaluation, which is incompatible with the export use case. "
"Therefore, in order to make your model code compatible with "
"``torch.export``, you will need to modify your code to remove graph breaks."
msgstr ""
"尽管``torch.export``与``torch.compile``共享组件，但``torch.export``的关键限制，特别是与``torch.compile``相比，是它不支持图中断。这是因为处理图中断需要使用默认的Python评估解释不支持的操作，这与导出的使用场景不兼容。因此，为了使您的模型代码与``torch.export``兼容，您需要修改代码以消除图中断。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "A graph break is necessary in cases such as:"
msgstr "在以下情况下需要图中断："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "data-dependent control flow"
msgstr "数据相关的控制流"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "accessing tensor data with ``.data``"
msgstr "使用``.data``访问张量数据"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "calling unsupported functions (such as many built-in functions)"
msgstr "调用不支持的函数（如许多内置函数）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Non-Strict Export"
msgstr "非严格导出"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To trace the program, ``torch.export`` uses TorchDynamo by default, a byte "
"code analysis engine, to symbolically analyze the Python code and build a "
"graph based on the results. This analysis allows ``torch.export`` to provide"
" stronger guarantees about safety, but not all Python code is supported, "
"causing these graph breaks."
msgstr ""
"为了跟踪程序，``torch.export``默认使用TorchDynamo，一个字节码分析引擎，来符号化分析Python代码，并基于结果构建图。这种分析允许``torch.export``提供更强的安全性保证，但并非所有Python代码都受支持，导致这些图中断。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To address this issue, in PyTorch 2.3, we introduced a new mode of exporting"
" called non-strict mode, where we trace through the program using the Python"
" interpreter executing it exactly as it would in eager mode, allowing us to "
"skip over unsupported Python features. This is done through adding a "
"``strict=False`` flag."
msgstr ""
"为了解决这个问题，在PyTorch "
"2.3版本中，我们引入了一种新的非严格模式导出方式，我们通过Python解释器以即时模式的方式准确执行程序来跟踪程序，从而跳过不支持的Python特性。可以通过添加``strict=False``标志来实现这一点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Looking at some of the previous examples which resulted in graph breaks:"
msgstr "回顾一些导致图中断的先例："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Calling unsupported functions (such as many built-in functions) traces"
msgstr "调用不支持的函数（如许多内置函数）会跟踪"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"through, but in this case, ``id(x)`` gets specialized as a constant integer "
"in the graph. This is because ``id(x)`` is not a tensor operation, so the "
"operation is not recorded in the graph."
msgstr ""
"调用``id(x)``的情况，但是在这种情况下，``id(x)``在图中被专用化为一个常量整数。这是因为``id(x)``不是一个张量操作，因此操作未被记录在图中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"However, there are still some features that require rewrites to the original"
" module:"
msgstr "但是，仍然存在一些需要重写原始模块的功能："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Control Flow Ops"
msgstr "控制流操作"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.export`` actually does support data-dependent control flow. But "
"these need to be expressed using control flow ops. For example, we can fix "
"the control flow example above using the ``cond`` op, like so:"
msgstr ""
"``torch.export``实际上支持数据相关的控制流。但这些需要使用控制流操作来表达。例如，我们可以使用``cond``操作修复上述控制流示例，如下所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "There are limitations to ``cond`` that one should be aware of:"
msgstr "``cond``存在一些需要注意的限制："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The predicate (i.e. ``x.sum() > 0``) must result in a boolean or a single-"
"element tensor."
msgstr "谓词（即``x.sum() > 0``）必须是布尔值或单个元素的张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The operands (i.e. ``[x]``) must be tensors."
msgstr "操作数（即``[x]``）必须是张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The branch function (i.e. ``true_fn`` and ``false_fn``) signature must match"
" with the operands and they must both return a single tensor with the same "
"metadata (for example, ``dtype``, ``shape``, etc.)."
msgstr ""
"分支函数（即``true_fn``和``false_fn``）的签名必须与操作数匹配，并且它们都必须返回一个具有相同元数据（例如``dtype``、``shape``等）的单个张量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Branch functions cannot mutate input or global variables."
msgstr "分支函数不能更改输入或全局变量。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Branch functions cannot access closure variables, except for ``self`` if the"
" function is defined in the scope of a method."
msgstr "分支函数不能访问闭包变量，除了函数在方法作用域内定义时的``self``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For more details about ``cond``, check out the `cond documentation "
"<https://pytorch.org/docs/main/cond.html>`__."
msgstr ""
"有关``cond``的更多详细信息，请查看`cond文档 <https://pytorch.org/docs/main/cond.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can also use ``map``, which applies a function across the first dimension"
" of the first tensor argument."
msgstr "我们还可以使用``map``，它将一个函数应用于第一个张量参数的第一个维度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Other control flow ops include ``while_loop``, ``associative_scan``, and "
"``scan``. For more documentation on each operator, please refer to `this "
"page "
"<https://github.com/pytorch/pytorch/tree/main/torch/_higher_order_ops>`__."
msgstr ""
"其他控制流操作包括``while_loop``、``associative_scan``和``scan``。有关每个操作的更多文档，请参阅`此页面 "
"<https://github.com/pytorch/pytorch/tree/main/torch/_higher_order_ops>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Constraints/Dynamic Shapes"
msgstr "约束/动态形状"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This section covers dynamic behavior and representation of exported "
"programs. Dynamic behavior is subjective to the particular model being "
"exported, so for the most part of this tutorial, we'll focus on this "
"particular toy model (with the resulting tensor shapes annotated):"
msgstr ""
"本部分介绍导出的程序的动态行为和表示。动态行为是针对特定模型的，因此在本教程的大部分内容中，我们将重点介绍这个特定的玩具模型（带有结果张量形状的注释）："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By default, ``torch.export`` produces a static program. One consequence of "
"this is that at runtime, the program won't work on inputs with different "
"shapes, even if they're valid in eager mode."
msgstr ""
"默认情况下，``torch.export`` 会生成静态程序。其结果是，在运行时，即使输入形状在即时模式下是有效的，程序也无法处理不同形状的输入。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Basic concepts: symbols and guards"
msgstr "基本概念：符号和守护"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To enable dynamism, ``export()`` provides a ``dynamic_shapes`` argument. The"
" easiest way to work with dynamic shapes is using ``Dim.AUTO`` and looking "
"at the program that's returned. Dynamic behavior is specified at a input "
"dimension-level; for each input we can specify a tuple of values:"
msgstr ""
"为了支持动态性，``export()`` 提供了一个 ``dynamic_shapes`` 参数。处理动态形状的最简单方法是使用 "
"``Dim.AUTO`` 并查看返回的程序。动态行为是针对每个输入维度指定的；对于每个输入，我们可以指定一个值元组："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before we look at the program that's produced, let's understand what "
"specifying ``dynamic_shapes`` entails, and how that interacts with export. "
"For every input dimension where a ``Dim`` object is specified, a symbol is "
"`allocated "
"<https://pytorch.org/docs/main/export.programming_model.html#basics-of-"
"symbolic-shapes>`_, taking on a range of ``[2, inf]`` (why not ``[0, inf]`` "
"or ``[1, inf]``? we'll explain later in the 0/1 specialization section)."
msgstr ""
"在我们查看生成的程序之前，让我们了解指定 ``dynamic_shapes`` 的意义，以及它与导出的交互方式。对于每个指定了 ``Dim`` "
"对象的输入维度，将分配一个符号 `allocated "
"<https://pytorch.org/docs/main/export.programming_model.html#basics-of-"
"symbolic-shapes>`_，范围为 ``[2, inf]``（为什么不是 ``[0, inf]`` 或 ``[1, inf]``？我们将在 "
"0/1 专门化部分 later 解释）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Export then runs model tracing, looking at each operation that's performed "
"by the model. Each individual operation can emit what's called \"guards\"; "
"basically boolean condition that are required to be true for the program to "
"be valid. When guards involve symbols allocated for input dimensions, the "
"program contains restrictions on what input shapes are valid; i.e. the "
"program's dynamic behavior. The symbolic shapes subsystem is the part "
"responsible for taking in all the emitted guards and producing a final "
"program representation that adheres to all of these guards. Before we see "
"this \"final representation\" in an ``ExportedProgram``, let's look at the "
"guards emitted by the toy model we're tracing."
msgstr ""
"导出然后运行模型追踪，查看模型执行的每个操作。每个单独的操作都可以发出所谓的“守护”；基本上是程序有效所需的布尔条件。当守护涉及为输入维度分配的符号时，程序包含关于有效输入形状的限制；即程序的动态行为。符号形状子系统负责接受所有发出的守护并生成符合这些守护的最终程序表示。在我们看到"
" ``ExportedProgram`` 中的这种“最终表示”之前，让我们看看我们正在追踪的玩具模型发出的守护。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here, each forward input tensor is annotated with the symbol allocated at "
"the start of tracing:"
msgstr "在这里，每个前向输入张量都被注释为在追踪开始时分配的符号："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's understand each of the operations and the emitted guards:"
msgstr "让我们了解每个操作和发出的守护："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``x0 = x + y``: This is an element-wise add with broadcasting, since ``x`` "
"is a 1-d tensor and ``y`` a 2-d tensor. ``x`` is broadcasted along the last "
"dimension of ``y``, emitting the guard ``s2 == s4``."
msgstr ""
"``x0 = x + y``: 这是带广播的逐元素相加，因为 ``x`` 是一维张量而 ``y`` 是二维张量。``x`` 在 ``y`` "
"的最后一个维度上进行广播，发出守护 ``s2 == s4``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``x1 = self.l(w)``: Calling ``nn.Linear()`` performs a matrix multiplication"
" with model parameters. In export, parameters, buffers, and constants are "
"considered program state, which is considered static, and so this is a "
"matmul between a dynamic input (``w: [s0, s1]``), and a statically-shaped "
"tensor. This emits the guard ``s1 == 5``."
msgstr ""
"``x1 = self.l(w)``: 调用 ``nn.Linear()`` "
"进行矩阵乘法，并使用模型参数。在导出时，参数、缓冲区和常量被视为程序状态，这被视为静态，因此这是动态输入（``w: [s0, "
"s1]``）和静态的张量之间的矩阵乘法。发出守护 ``s1 == 5``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``x2 = x0.flatten()``: This call actually doesn't emit any guards! (at least"
" none relevant to input shapes)"
msgstr "``x2 = x0.flatten()``: 实际上，这个调用未发出任何守护！（至少没有与输入形状相关的）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``x3 = x2 + z``: ``x2`` has shape ``[s3*s4]`` after flattening, and this "
"element-wise add emits ``s3 * s4 == s5``."
msgstr ""
"``x3 = x2 + z``: ``x2`` 在展平后具有形状 ``[s3*s4]``，此逐元素相加发出 ``s3 * s4 == s5``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Writing all of these guards down and summarizing is almost like a "
"mathematical proof, which is what the symbolic shapes subsystem tries to do!"
" In summary, we can conclude that the program must have the following input "
"shapes to be valid:"
msgstr "将所有这些守护写下来并总结几乎就像数学证明，而符号形状子系统试图实现这一点！总结一下，我们可以得出以下输入形状是程序有效时的必须："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``w: [s0, 5]``"
msgstr "``w: [s0, 5]``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``x: [s2]``"
msgstr "``x: [s2]``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``y: [s3, s2]``"
msgstr "``y: [s3, s2]``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``z: [s2*s3]``"
msgstr "``z: [s2*s3]``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And when we do finally print out the exported program to see our result, "
"those shapes are what we see annotated on the corresponding inputs:"
msgstr "当我们最后打印导出的程序以查看结果时，这些形状为对应的输入标注："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Another feature to notice is the range_constraints field above, which "
"contains a valid range for each symbol. This isn't so interesting currently,"
" since this export call doesn't emit any guards related to symbol bounds and"
" each base symbol has a generic bound, but this will come up later."
msgstr ""
"另一项需要注意的功能是上面的 range_constraints "
"字段，其中包含每个符号的有效范围。目前这不是很有趣，因为此导出调用未发出任何与符号界限相关的守护，每个基础符号都有通用界限，但稍后会涉及到。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So far, because we've been exporting this toy model, this experience has not"
" been representative of how hard it typically is to debug dynamic shapes "
"guards & issues. In most cases it isn't obvious what guards are being "
"emitted, and which operations and parts of user code are responsible. For "
"this toy model we pinpoint the exact lines, and the guards are rather "
"intuitive."
msgstr ""
"到目前为止，由于我们一直在导出这个玩具模型，这种体验并不代表调试动态形状守护和问题通常有多么困难。在大多数情况下，不明显哪些守护被发出，也不清楚是哪些操作和用户代码部分负责。对于这个玩具模型，我们明确指出了确切的代码行，守护相当直观。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In more complicated cases, a helpful first step is always to enable verbose "
"logging. This can be done either with the environment variable "
"``TORCH_LOGS=\"+dynamic\"``, or interactively with "
"``torch._logging.set_logs(dynamic=10)``:"
msgstr ""
"在更复杂的情况下，第一步是始终启用详细日志记录。这可通过环境变量 ``TORCH_LOGS=\"+dynamic\"`` 或交互方式用 "
"``torch._logging.set_logs(dynamic=10)`` 来完成："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This spits out quite a handful, even with this simple toy model. The log "
"lines here have been cut short at front and end to ignore unnecessary info, "
"but looking through the logs we can see the lines relevant to what we "
"described above; e.g. the allocation of symbols:"
msgstr ""
"即使是这个简单的玩具模型也会输出大量内容。这里的日志行前后已被截断以忽略不必要的信息，但通过查看日志，我们可以看到与上面描述相关的行；例如符号的分配："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The lines with `create_symbol` show when a new symbol has been allocated, "
"and the logs also identify the tensor variable names and dimensions they've "
"been allocated for. In other lines we can also see the guards emitted:"
msgstr ""
"`create_symbol` 行显示了何时分配了新符号，日志还标识了分配给它们的张量变量名称和维度。在其他行中，我们还可以看到发出的守护："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next to the ``[guard added]`` messages, we also see the responsible user "
"lines of code - luckily here the model is simple enough. In many real-world "
"cases it's not so straightforward: high-level torch operations can have "
"complicated fake-kernel implementations or operator decompositions that "
"complicate where and what guards are emitted. In such cases the best way to "
"dig deeper and investigate is to follow the logs' suggestion, and re-run "
"with environment variable "
"``TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"...\"``, to further attribute the"
" guard of interest."
msgstr ""
"在 ``[guard added]`` 消息旁边，我们还可以看到负责的用户代码行 - "
"幸运的是，这里模型足够简单。在许多实际案例中，情况并不那么直接：高级 torch "
"操作可能有复杂的伪内核实现或操作分解，这使得守护的发出位置和内容更加复杂。在这种情况下，深入调研和调查的最佳方式是遵循日志的建议，并重新运行环境变量 "
"``TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED=\"...\"``，进一步归因感兴趣的守护。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``Dim.AUTO`` is just one of the available options for interacting with "
"``dynamic_shapes``; as of writing this 2 other options are available: "
"``Dim.DYNAMIC``, and ``Dim.STATIC``. ``Dim.STATIC`` simply marks a dimension"
" static, while ``Dim.DYNAMIC`` is similar to ``Dim.AUTO`` in all ways except"
" one: it raises an error when specializing to a constant; this is designed "
"to maintain dynamism. See for example what happens when a static guard is "
"emitted on a dynamically-marked dimension:"
msgstr ""
"``Dim.AUTO`` 只是与 ``dynamic_shapes`` 交互的可选项之一；当前有另外两种选项：``Dim.DYNAMIC`` 和 "
"``Dim.STATIC``。``Dim.STATIC`` 简单地标记维度为静态，而 ``Dim.DYNAMIC`` 与 ``Dim.AUTO`` "
"在所有方面相似，唯一区别是当专门化为常量时会引发错误；这旨在保持动态性。看看当在动态标记的维度上发出静态守护时会发生什么："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Static guards also aren't always inherent to the model; they can also come "
"from user specifications. In fact, a common pitfall leading to shape "
"specializations is when the user specifies conflicting markers for "
"equivalent dimensions; one dynamic and another static. The same error type "
"is raised when this is the case for ``x.shape[0]`` and ``y.shape[1]``:"
msgstr ""
"静态守护并不总是模型固有的；它们也可能来自用户规范。实际上，一个常见的导致形状专门化的陷阱是当用户为等效维度指定了冲突的标记；一个是动态，另一个是静态。当对于"
" ``x.shape[0]`` 和 ``y.shape[1]`` 出现这种情况时会引发相同的错误类型："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here you might ask why export \"specializes\", i.e. why we resolve this "
"static/dynamic conflict by going with the static route. The answer is "
"because of the symbolic shapes system described above, of symbols and "
"guards. When ``x.shape[0]`` is marked static, we don't allocate a symbol, "
"and compile treating this shape as a concrete integer 4. A symbol is "
"allocated for ``y.shape[1]``, and so we finally emit the guard ``s3 == 4``, "
"leading to specialization."
msgstr ""
"在这里，您可能会问为什么导出会“专门化”，即为什么我们通过静态路径解决这种静态/动态冲突。答案是由于上面描述的符号形状系统，即符号和守护。当 "
"``x.shape[0]`` 被标记为静态时，我们不分配符号，并将此形状编译为具体的整数 4。为 ``y.shape[1]`` "
"分配了符号，因此我们最终发出守护 ``s3 == 4``，导致专门化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One feature of export is that during tracing, statements like asserts, "
"``torch._check()``, and ``if/else`` conditions will also emit guards. See "
"what happens when we augment the existing model with such statements:"
msgstr ""
"导出的一个功能是在追踪期间，诸如 assert、``torch._check()`` 和 ``if/else`` "
"条件等语句也会发出守护。看看我们如何在现有模型中加入这些语句："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Each of these statements emits an additional guard, and the exported program"
" shows the changes; ``s0`` is eliminated in favor of ``s2 + 2``, and ``s2`` "
"now contains lower and upper bounds, reflected in ``range_constraints``."
msgstr ""
"其中每条语句都发出了一个额外的守护，并且导出的程序显示了更改；``s0`` 被替换为 ``s2 + 2``，并且 ``s2`` "
"现在包含上限和下限，反映在 ``range_constraints`` 中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For the if/else condition, you might ask why the True branch was taken, and "
"why it wasn't the ``w.shape[0] != x.shape[0] + 2`` guard that got emitted "
"from tracing. The answer is that export is guided by the sample inputs "
"provided by tracing, and specializes on the branches taken. If different "
"sample input shapes were provided that fail the ``if`` condition, export "
"would trace and emit guards corresponding to the ``else`` branch. "
"Additionally, you might ask why we traced only the ``if`` branch, and if "
"it's possible to maintain control-flow in your program and keep both "
"branches alive. For that, refer to rewriting your model code following the "
"``Control Flow Ops`` section above."
msgstr ""
"对于 if/else 条件，您可能会问为什么选择了 True 分支，以及为什么没有发出 ``w.shape[0] != x.shape[0] + 2``"
" 的守护。答案是导出是由追踪提供的样本输入引导的，并专门化选中的分支。如果提供的不同样本输入形状未通过 ``if`` 条件，导出将追踪并发出对应于 "
"``else`` 分支的守护。此外，您可能会问为什么我们仅追踪了 ``if`` 分支，以及是否可以在程序中保持控制流并同时保留两个分支。为此，请参阅在 "
"“控制流操作” 部分中重写模型代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "0/1 specialization"
msgstr "0/1 专门化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since we're talking about guards and specializations, it's a good time to "
"talk about the 0/1 specialization issue we brought up earlier. The bottom "
"line is that export will specialize on sample input dimensions with value 0 "
"or 1, because these shapes have trace-time properties that don't generalize "
"to other shapes. For example, size 1 tensors can broadcast while other sizes"
" fail; and size 0 ... . This just means that you should specify 0/1 sample "
"inputs when you'd like your program to hardcode them, and non-0/1 sample "
"inputs when dynamic behavior is desirable. See what happens at runtime when "
"we export this linear layer:"
msgstr ""
"既然我们谈到了守护和专门化，是时候讨论我们之前提到的 0/1 专门化问题了。关键问题是导出会专门化值为 0 或 1 "
"的样本输入维度，因为这些形状在追踪时具有不适用于其他形状的特性。例如，大小为 1 的张量可以广播，而其他大小则会失败；大小为 0 "
"……这意味着您应该在希望程序硬编码这些形状时指定 0/1 样本输入，而在希望动态行为时指定非 0/1 "
"样本输入。看看我们导出这个线性层时在运行时会发生什么："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Named Dims"
msgstr "命名维度"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So far we've only been talking about 3 ways to specify dynamic shapes: "
"``Dim.AUTO``, ``Dim.DYNAMIC``, and ``Dim.STATIC``. The attraction of these "
"is the low-friction user experience; all the guards emitted during model "
"tracing are adhered to, and dynamic behavior like min/max ranges, relations,"
" and static/dynamic dimensions are automatically figured out underneath "
"export. The dynamic shapes subsystem essentially acts as a \"discovery\" "
"process, summarizing these guards and presenting what export believes is the"
" overall dynamic behavior of the program. The drawback of this design "
"appears once the user has stronger expectations or beliefs about the dynamic"
" behavior of these models - maybe there is a strong desire on dynamism and "
"specializations on particular dimensions are to be avoided at all costs, or "
"maybe we just want to catch changes in dynamic behavior with changes to the "
"original model code, or possibly underlying decompositions or meta-kernels. "
"These changes won't be detected and the ``export()`` call will most likely "
"succeed, unless tests are in place that check the resulting "
"``ExportedProgram`` representation."
msgstr ""
"到目前为止，我们只讨论了 3 种指定动态形状的方法：``Dim.AUTO``、``Dim.DYNAMIC`` 和 "
"``Dim.STATIC``。它们的吸引力在于用户体验的低摩擦性；模型追踪期间发出的所有守护均被遵守，以及导出会自动解决动态行为（例如最小/最大范围、关系以及静态/动态维度）。动态形状子系统本质上充当“发现”过程，总结这些守护并展示导出认为程序的总体动态行为。此设计的缺点出现在用户对这些模型的动态行为有更强的期望或看法时"
" - "
"可能用户对某些维度的动态性特别希望，因此绝不能专门化，或者只是希望通过检测对原模型代码进行更改来捕获动态行为的变化，或者可能是底层的分解或元内核。这些变化不会被检测到，并且"
" ``export()`` 调用很可能成功，除非有测试检查生成的 ``ExportedProgram`` 表示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For such cases, our stance is to recommend the \"traditional\" way of "
"specifying dynamic shapes, which longer-term users of export might be "
"familiar with: named ``Dims``:"
msgstr "对于这些情况，我们建议采用指定动态形状的“传统”方法，长期使用导出的用户可能熟悉这种方式：命名的 ``Dims``："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This style of dynamic shapes allows the user to specify what symbols are "
"allocated for input dimensions, min/max bounds on those symbols, and places "
"restrictions on the dynamic behavior of the ``ExportedProgram`` produced; "
"``ConstraintViolation`` errors will be raised if model tracing emits guards "
"that conflict with the relations or static/dynamic specifications given. For"
" example, in the above specification, the following is asserted:"
msgstr ""
"这种动态形状的风格允许用户指定为输入维度分配哪些符号、这些符号的最小/最大边界，并对生成的 ``ExportedProgram`` "
"的动态行为施加限制。如果模型追踪发出的守护与给定的关系或静态/动态规格冲突，则会引发 ``ConstraintViolation`` "
"错误。例如，在上述规范中，以下被断言："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``x.shape[0]`` is to have range ``[4, 256]``, and related to ``y.shape[0]`` "
"by ``y.shape[0] == 2 * x.shape[0]``."
msgstr ""
"``x.shape[0]`` 的范围为 ``[4, 256]``，并与 ``y.shape[0]`` 相关：``y.shape[0] == 2 * "
"x.shape[0]``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``x.shape[1]`` is static."
msgstr "``x.shape[1]`` 是静态的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``y.shape[1]`` has range ``[2, 512]``, and is unrelated to any other "
"dimension."
msgstr "``y.shape[1]`` 的范围为 ``[2, 512]``，并且与任何其他维度无关。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this design, we allow relations between dimensions to be specified with "
"univariate linear expressions: ``A * dim + B`` can be specified for any "
"dimension. This allows users to specify more complex constraints like "
"integer divisibility for dynamic dimensions:"
msgstr ""
"在这个设计中，我们允许通过单变量线性表达式指定维度之间的关系：`A * dim + B` "
"可以为任意维度指定。这使用户能够为动态维度指定更复杂的约束，例如整数可整除性："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Constraint violations, suggested fixes"
msgstr "约束违规，建议修复"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One common issue with this specification style (before ``Dim.AUTO`` was "
"introduced), is that the specification would often be mismatched with what "
"was produced by model tracing. That would lead to ``ConstraintViolation`` "
"errors and export suggested fixes - see for example with this model & "
"specification, where the model inherently requires equality between "
"dimensions 0 of ``x`` and ``y``, and requires dimension 1 to be static."
msgstr ""
"在使用这种规格定义样式时（`Dim.AUTO` 被引入之前），一个常见的问题是规格定义常常与模型跟踪生成的不匹配。这会导致 "
"`ConstraintViolation` 错误并导出建议修复。例如，在以下模型和规格中，模型本质上要求 `x` 和 `y` 的维度 0 相等，并且维度"
" 1 必须是静态的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The expectation with suggested fixes is that the user can interactively "
"copy-paste the changes into their dynamic shapes specification, and "
"successfully export afterwards."
msgstr "使用建议的修复方案的期望是，用户可以交互地将修复的改变复制粘贴到动态形状规范中，并随后成功导出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Lastly, there's couple nice-to-knows about the options for specification:"
msgstr "最后，这里有一些关于规格选项的有用信息："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``None`` is a good option for static behavior: - ``dynamic_shapes=None`` "
"(default) exports with the entire model being static. - specifying ``None`` "
"at an input-level exports with all tensor dimensions static, and is also "
"required for non-tensor inputs. - specifying ``None`` at a dimension-level "
"specializes that dimension, though this is deprecated in favor of "
"``Dim.STATIC``."
msgstr ""
"`None` 是一种静态行为的良好选项： - `dynamic_shapes=None`（默认）会将整个模型导出为静态。 - 在输入级别指定 "
"`None` 会将所有张量维度导出为静态，并且对于非张量输入也是必需的。 - 在维度级别指定 `None` 会专门化该维度，但这已被弃用，建议使用 "
"`Dim.STATIC`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"specifying per-dimension integer values also produces static behavior, and "
"will additionally check that the provided sample input matches the "
"specification."
msgstr "为每个维度指定整数值也会产生静态行为，并会额外检查提供的样本输入是否与规格匹配。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "These options are combined in the inputs & dynamic shapes spec below:"
msgstr "这些选项与输入和动态形状规范以下列方式结合："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Data-dependent errors"
msgstr "数据依赖的错误"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"While trying to export models, you have may have encountered errors like "
"\"Could not guard on data-dependent expression\", or Could not extract "
"specialized integer from data-dependent expression\". These errors exist "
"because ``torch.export()`` compiles programs using FakeTensors, which "
"symbolically represent their real tensor counterparts. While these have "
"equivalent symbolic properties (e.g. sizes, strides, dtypes), they diverge "
"in that FakeTensors do not contain any data values. While this avoids "
"unnecessary memory usage and expensive computation, it does mean that export"
" may be unable to out-of-the-box compile parts of user code where "
"compilation relies on data values. In short, if the compiler requires a "
"concrete, data-dependent value in order to proceed, it will error out, "
"complaining that the value is not available."
msgstr ""
"在尝试导出模型时，您可能遇到过类似“无法对数据依赖表达式进行保护”或“无法从数据依赖表达式中提取专门化的整数”这样的错误。出现这些错误的原因是 "
"`torch.export()` 使用 FakeTensors 来编译程序，FakeTensors "
"符号化地表示其真实的张量对应物。尽管它们具有等效的符号属性（例如大小、步幅、数据类型），但它们在不包含任何数据值这一点上不相同。虽然这避免了不必要的内存使用和昂贵的计算，但也意味着导出可能无法直接编译依赖于数据值的用户代码部分。简单来说，如果编译器需要一个具体的、数据依赖的值才能继续，它会报错并抱怨该值不可用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Data-dependent values appear in many places, and common sources are calls "
"like ``item()``, ``tolist()``, or ``torch.unbind()`` that extract scalar "
"values from tensors. How are these values represented in the exported "
"program? In the `Constraints/Dynamic Shapes "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-"
"dynamic-shapes>`_ section, we talked about allocating symbols to represent "
"dynamic input dimensions. The same happens here: we allocate symbols for "
"every data-dependent value that appears in the program. The important "
"distinction is that these are \"unbacked\" symbols, in contrast to the "
"\"backed\" symbols allocated for input dimensions. The `\"backed/unbacked\" "
"<https://pytorch.org/docs/main/export.programming_model.html#basics-of-"
"symbolic-shapes>`_ nomenclature refers to the presence/absence of a \"hint\""
" for the symbol: a concrete value backing the symbol, that can inform the "
"compiler on how to proceed."
msgstr ""
"数据依赖值出现在许多地方，常见的来源是诸如 `item()`、`tolist()` 或 `torch.unbind()` "
"之类的调用，这些调用从张量中提取标量值。这些值在导出的程序中是如何表示的？在 `约束/动态形状 "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-"
"dynamic-shapes>`_ "
"部分中，我们讨论了为动态输入维度分配符号。同样地，在这里我们为程序中出现的每个数据依赖值分配符号。重要的区别在于，这些是“无支持”的符号，与为输入维度分配的“有支持”符号相对。`“有支持/无支持”"
" <https://pytorch.org/docs/main/export.programming_model.html#basics-of-"
"symbolic-shapes>`_ 法术指的是是否存在“提示”：一个具体的值支持符号，这可以指导编译器如何进行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the input shape symbol case (backed symbols), these hints are simply the "
"sample input shapes provided, which explains why control-flow branching is "
"determined by the sample input properties. For data-dependent values, the "
"symbols are taken from FakeTensor \"data\" during tracing, and so the "
"compiler doesn't know the actual values (hints) that these symbols would "
"take on."
msgstr ""
"在输入形状符号情况下（有支持符号），这些提示就是提供的样本输入形状，这解释了为什么控制流分支是由样本输入属性确定的。对于数据依赖值，符号是在跟踪过程中从"
" FakeTensor “数据”中获取的，因此编译器不知道这些符号将取何值（提示）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's see how these show up in exported programs:"
msgstr "让我们看看这些在导出的程序中是如何表现的："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The result is that 3 unbacked symbols (notice they're prefixed with \"u\", "
"instead of the usual \"s\" for input shape/backed symbols) are allocated and"
" returned: 1 for the ``item()`` call, and 1 for each of the elements of "
"``y`` with the ``tolist()`` call. Note from the range constraints field that"
" these take on ranges of ``[-int_oo, int_oo]``, not the default ``[0, "
"int_oo]`` range allocated to input shape symbols, since we have no "
"information on what these values are - they don't represent sizes, so don't "
"necessarily have positive values."
msgstr ""
"结果是分配并返回了三个无支持符号（注意它们用“u”作为前缀，而不是输入形状/有支持符号的通常“s”前缀）：一个用于 `item()` 调用，两个用于 "
"`tolist()` 方法中 `y` 的每个元素。从范围约束字段可以看出，这些符号的范围是 `[-int_oo, "
"int_oo]`，而不是分配给输入形状符号的默认范围 `[0, int_oo]`，因为我们无法获得关于这些值的信息——它们不表示大小，因此不一定是正值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Guards, torch._check()"
msgstr "保护机制，torch._check()"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"But the case above is easy to export, because the concrete values of these "
"symbols aren't used in any compiler decision-making; all that's relevant is "
"that the return values are unbacked symbols. The data-dependent errors "
"highlighted in this section are cases like the following, where `data-"
"dependent guards "
"<https://pytorch.org/docs/main/export.programming_model.html#control-flow-"
"static-vs-dynamic>`_ are encountered:"
msgstr ""
"但上述情况容易导出，因为这些符号的具体值并未在任何编译器决策中使用；所有相关的只是返回值是无支持符号。本节讨论的数据依赖错误是以下情况之类的，当遇到 "
"`数据依赖保护 "
"<https://pytorch.org/docs/main/export.programming_model.html#control-flow-"
"static-vs-dynamic>`_ 时："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here we actually need the \"hint\", or the concrete value of ``a`` for the "
"compiler to decide whether to trace ``return y + 2`` or ``return y * 5`` as "
"the output. Because we trace with FakeTensors, we don't know what ``a // 2 "
">= 5`` actually evaluates to, and export errors out with \"Could not guard "
"on data-dependent expression ``u0 // 2 >= 5 (unhinted)``\"."
msgstr ""
"这里我们实际上需要“提示”或 `a` 的具体值，以便编译器决定是否跟踪并作为输出返回 `y + 2` 或者 `y * 5`。因为我们使用 "
"FakeTensors 进行跟踪，所以我们不知道 `a // 2 >= 5` 实际计算结果如何，因此导出失败并给出错误信息“无法对数据依赖表达式 `u0"
" // 2 >= 5 (未提示)` 进行保护”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So how do we export this toy model? Unlike ``torch.compile()``, export "
"requires full graph compilation, and we can't just graph break on this. Here"
" are some basic options:"
msgstr ""
"那么我们该如何导出这个简单模型呢？与 `torch.compile()` 不同，导出需要完整的图编译，不能只是进行图分段。以下是一些基本选项："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Manual specialization: we could intervene by selecting the branch to trace, "
"either by removing the control-flow code to contain only the specialized "
"branch, or using ``torch.compiler.is_compiling()`` to guard what's traced at"
" compile-time."
msgstr ""
"手动专门化：通过选择分支代码进行跟踪，可以介入解决问题，要么移除控制流代码仅保留特定分支，要么使用 "
"`torch.compiler.is_compiling()` 在编译时保护被跟踪的代码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.cond()``: we could rewrite the control-flow code to use "
"``torch.cond()`` so we don't specialize on a branch."
msgstr "`torch.cond()`：可以用 `torch.cond()` 重写控制流代码，从而避免专门化为某个分支。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"While these options are valid, they have their pitfalls. Option 1 sometimes "
"requires drastic, invasive rewrites of the model code to specialize, and "
"``torch.cond()`` is not a comprehensive system for handling data-dependent "
"errors. As we will see, there are data-dependent errors that do not involve "
"control-flow."
msgstr ""
"尽管这些选项有效，但它们有其局限性。选项 1 有时需要对模型代码进行大规模侵入性重写以实现专门化，而 `torch.cond()` "
"并不是处理数据依赖错误的综合系统。正如所见，也有不涉及控制流的数据依赖错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The generally recommended approach is to start with ``torch._check()`` "
"calls. While these give the impression of purely being assert statements, "
"they are in fact a system of informing the compiler on properties of "
"symbols. While a ``torch._check()`` call does act as an assertion at "
"runtime, when traced at compile-time, the checked expression is sent to the "
"symbolic shapes subsystem for reasoning, and any symbol properties that "
"follow from the expression being true, are stored as symbol properties "
"(provided it's smart enough to infer those properties). So even if unbacked "
"symbols don't have hints, if we're able to communicate properties that are "
"generally true for these symbols via ``torch._check()`` calls, we can "
"potentially bypass data-dependent guards without rewriting the offending "
"model code."
msgstr ""
"一般推荐的方法是从 `torch._check()` "
"调用开始。虽然这些调用的表面看起来只是断言语句，但实际上它们是一个告知编译器有关符号属性系统。在运行时，`torch._check()` "
"调用确实会作为一个断言执行，而在编译时被追踪时，检查表达式会被发送到符号形状子系统进行推理，并且任何从表达式为真而推导出来的符号属性都将被存储为属性（如果系统足够智能以推断这些属性）。因此，即使无支持符号没有提示，如果我们能通过"
" `torch._check()` 调用传递对这些符号普遍为真的属性，也可能不需要重写模型代码就能绕过数据依赖保护。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For example in the model above, inserting ``torch._check(a >= 10)`` would "
"tell the compiler that ``y + 2`` can always be returned, and "
"``torch._check(a == 4)`` tells it to return ``y * 5``. See what happens when"
" we re-export this model."
msgstr ""
"例如，在上述模型中，插入 `torch._check(a >= 10)` 将告知编译器 `y + 2` 始终可以返回，而 `torch._check(a"
" == 4)` 告诉它返回 `y * 5`。看看当我们重新导出这个模型时会发生什么。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Export succeeds, and note from the range constraints field that ``u0`` takes"
" on a range of ``[10, 60]``."
msgstr "导出成功，请注意在范围约束字段中 `u0` 的范围是 `[10, 60]`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So what information do ``torch._check()`` calls actually communicate? This "
"varies as the symbolic shapes subsystem gets smarter, but at a fundamental "
"level, these are generally true:"
msgstr ""
"那么 `torch._check()` 调用实际上传达了什么信息呢？随着符号形状子系统变得更智能，这些信息会有所不同，但从根本上讲，这些通常包括："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Equality with non-data-dependent expressions: ``torch._check()`` calls that "
"communicate equalities like ``u0 == s0 + 4`` or ``u0 == 5``."
msgstr "等同于非数据依赖表达式：传达等式的 `torch._check()` 调用，例如 `u0 == s0 + 4` 或 `u0 == 5`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Range refinement: calls that provide lower or upper bounds for symbols, like"
" the above."
msgstr "范围细化：提供符号上界或下界的调用，例如上面提到的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Some basic reasoning around more complicated expressions: inserting "
"``torch._check(a < 4)`` will typically tell the compiler that ``a >= 4`` is "
"false. Checks on complex expressions like ``torch._check(a ** 2 - 3 * a <= "
"10)`` will typically get you past identical guards."
msgstr ""
"对更复杂表达式的一些基本推理：插入 `torch._check(a < 4)` 通常会告诉编译器 `a >= 4` 为假。对像 "
"`torch._check(a ** 2 - 3 * a <= 10)` 这样复杂表达式的检查通常能绕过相同的保护。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As mentioned previously, ``torch._check()`` calls have applicability outside"
" of data-dependent control flow. For example, here's a model where "
"``torch._check()`` insertion prevails while manual specialization & "
"``torch.cond()`` do not:"
msgstr ""
"如前所述，`torch._check()` 调用在数据依赖控制流之外也有适用性。例如，这里有一个模型，其中 `torch._check()` "
"的插入起作用，而手动专门化和 `torch.cond()` 不起作用："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here is a scenario where ``torch._check()`` insertion is required simply to "
"prevent an operation from failing. The export call will fail with \"Could "
"not guard on data-dependent expression ``-u0 > 60``\", implying that the "
"compiler doesn't know if this is a valid indexing operation - if the value "
"of ``x`` is out-of-bounds for ``y`` or not. Here, manual specialization is "
"too prohibitive, and ``torch.cond()`` has no place. Instead, informing the "
"compiler of ``u0``'s range is sufficient:"
msgstr ""
"这是一个需要插入 `torch._check()` 来防止操作失败的场景。导出调用会失败，并出现错误信息“无法对数据依赖表达式 `-u0 > 60` "
"进行保护”，这表明编译器不知道这是否是一个有效的索引操作——`x` 的值是否超出 `y` "
"的边界。在这里，手动专门化过于繁琐，`torch.cond()` 没有用武之地。相反，告知编译器 `u0` 的范围就足够了："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Specialized values"
msgstr "专门化的值"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Another category of data-dependent error happens when the program attempts "
"to extract a concrete data-dependent integer/float value while tracing. This"
" looks something like \"Could not extract specialized integer from data-"
"dependent expression\", and is analogous to the previous class of errors - "
"if these occur when attempting to evaluate concrete integer/float values, "
"data-dependent guard errors arise with evaluating concrete boolean values."
msgstr ""
"当程序尝试在跟踪过程中提取具体数据依赖的整数或浮点值时，会发生另一类数据依赖错误。这种错误看起来像“无法从数据依赖表达式中提取专门化的整数”，类似于前一种错误类别——如果这些错误出现在尝试求具体整数/浮点值时，当尝试求具体布尔值会产生数据依赖保护错误。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This error typically occurs when there is an explicit or implicit ``int()`` "
"cast on a data-dependent expression. For example, this list comprehension "
"has a `range()` call that implicitly does an ``int()`` cast on the size of "
"the list:"
msgstr ""
"此错误通常发生在显式或隐式 `int()` 转换对数据依赖表达式进行操作时。例如，该列表解析中有一个 `range()` "
"调用，其隐式地对列表大小执行了一个 `int()` 转换："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For these errors, some basic options you have are:"
msgstr "对于这些错误，您可以采取的一些基本选项包括："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Avoid unnecessary ``int()`` cast calls, in this case the ``int(a)`` in the "
"return statement."
msgstr "避免不必要的 `int()` 转换调用，例如返回语句中的 `int(a)`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Use ``torch._check()`` calls; unfortunately all you may be able to do in "
"this case is specialize (with ``torch._check(a == 60)``)."
msgstr ""
"使用 `torch._check()` 调用；不幸的是，在此情况下您可能只能专门化（使用 `torch._check(a == 60)`）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Rewrite the offending code at a higher level. For example, the list "
"comprehension is semantically a ``repeat()`` op, which doesn't involve an "
"``int()`` cast. The following rewrite avoids data-dependent errors:"
msgstr ""
"在更高层次上重写有问题的代码。例如，列表解析语义上是一个 `repeat()` 操作，它不涉及 `int()` 转换。以下重写避免了数据依赖错误："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Data-dependent errors can be much more involved, and there are many more "
"options in your toolkit to deal with them: ``torch._check_is_size()``, "
"``guard_size_oblivious()``, or real-tensor tracing, as starters. For more "
"in-depth guides, please refer to the `Export Programming Model "
"<https://pytorch.org/docs/main/export.programming_model.html>`_, or `Dealing"
" with GuardOnDataDependentSymNode errors "
"<https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs>`_."
msgstr ""
"数据依赖错误可能会更复杂，您的工具集有更多选项来处理它们：如 "
"`torch._check_is_size()`、`guard_size_oblivious()` 或真实张量跟踪等入门工具。有关更深入的指南，请参考 "
"`导出编程模型 <https://pytorch.org/docs/main/export.programming_model.html>`_ 或 "
"`处理 GuardOnDataDependentSymNode 错误 "
"<https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Custom Ops"
msgstr "自定义操作"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.export`` can export PyTorch programs with custom operators. Please "
"refer to `this page "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html>`__ on "
"how to author a custom operator in either C++ or Python."
msgstr ""
"``torch.export`` 可以导出带有自定义操作符的 PyTorch 程序。有关如何使用 C++ 或 Python 编写自定义操作符，请参考 "
"`此页面 "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The following is an example of registering a custom operator in python to be"
" used by ``torch.export``. The important thing to note is that the custom op"
" must have a `FakeTensor kernel "
"<https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#heading=h.xvrg7clz290>`__."
msgstr ""
"以下是一个在 Python 中注册自定义操作符以供 ``torch.export`` 使用的示例。重要的是要注意，自定义操作符必须具有 "
"`FakeTensor 内核 "
"<https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#heading=h.xvrg7clz290>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Here is an example of exporting a program with the custom op."
msgstr "这是使用自定义操作符导出程序的示例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Note that in the ``ExportedProgram``, the custom operator is included in the"
" graph."
msgstr "注意，在``ExportedProgram``中，自定义操作符包含在图中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "IR/Decompositions"
msgstr "IR/分解"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The graph produced by ``torch.export`` returns a graph containing only `ATen"
" operators <https://pytorch.org/cppdocs/#aten>`__, which are the basic unit "
"of computation in PyTorch. As there are over 3000 ATen operators, export "
"provides a way to narrow down the operator set used in the graph based on "
"certain characteristics, creating different IRs."
msgstr ""
"``torch.export``生成的图仅包含`ATen操作符 <https://pytorch.org/cppdocs/#aten>`__，这是 "
"PyTorch 中的基本计算单位。由于 ATen 操作符超过 3000 个，导出提供了一种基于某些特性缩小图中使用操作符集合的方法，从而创建不同的 "
"IR。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By default, export produces the most generic IR which contains all ATen "
"operators, including both functional and non-functional operators. A "
"functional operator is one that does not contain any mutations or aliasing "
"of the inputs. You can find a list of all ATen operators `here "
"<https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml>`__"
" and you can inspect if an operator is functional by checking "
"``op._schema.is_mutable``, for example:"
msgstr ""
"默认情况下，导出生成最通用的 IR，其中包含所有 ATen 操作符，包括功能性和非功能性操作符。功能性操作符是不包含任何输入突变或别名的操作符。您可以在"
" `此处 "
"<https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml>`__"
" 找到所有 ATen 操作符的列表，并可以通过检查 ``op._schema.is_mutable`` 来确定操作符是否是功能性的，例如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This generic IR can be used to train in eager PyTorch Autograd. This IR can "
"be more explicitly reached through the API "
"``torch.export.export_for_training``, which was introduced in PyTorch 2.5, "
"but calling ``torch.export.export`` should produce the same graph as of "
"PyTorch 2.6."
msgstr ""
"此通用 IR 可用于在 PyTorch Autograd 中进行自适应训练。此 IR 可通过 API "
"``torch.export.export_for_training`` 更明确地获得，该 API 是在 PyTorch 2.5 中引入的，但调用 "
"``torch.export.export`` 应在 PyTorch 2.6 中生成相同的图。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can then lower this exported program to an operator set which only "
"contains functional ATen operators through the API ``run_decompositions``, "
"which decomposes the ATen operators into the ones specified in the "
"decomposition table, and functionalizes the graph. By specifying an empty "
"set, we're only performing functionalization, and does not do any additional"
" decompositions. This results in an IR which contains ~2000 operators "
"(instead of the 3000 operators above), and is ideal for inference cases."
msgstr ""
"我们可以通过 API ``run_decompositions`` 将已导出的程序降低到仅包含功能性 ATen 操作符的操作符集合中，该 API 会将 "
"ATen 操作符分解为分解表中指定的操作符，并使图功能化。通过指定一个空集，我们仅执行功能化，不进行任何额外的分解。这将生成一个包含大约 2000 "
"操作符（而不是上面提到的 3000 个操作符）的 IR，非常适合推理场景。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As we can see, the previously mutable operator, "
"``torch.ops.aten.add_.default`` has now been replaced with "
"``torch.ops.aten.add.default``, a l operator."
msgstr ""
"正如我们所看到的，先前的可变操作符 ``torch.ops.aten.add_.default`` 现在已替换为 "
"``torch.ops.aten.add.default``, 一个功能性操作符。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can also further lower this exported program to an operator set which "
"only contains the `Core ATen Operator Set "
"<https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>`__, "
"which is a collection of only ~180 operators. This IR is optimal for "
"backends who do not want to reimplement all ATen operators."
msgstr ""
"我们还可以将已导出的程序进一步降低到仅包含 `Core ATen 操作符集 "
"<https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>`__ "
"的操作符集合，该集合仅包含大约 180 个操作符。此 IR 对于不想重新实现所有 ATen 操作符的后端来说是最佳的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We now see that ``torch.ops.aten.conv2d.default`` has been decomposed into "
"``torch.ops.aten.convolution.default``. This is because ``convolution`` is a"
" more \"core\" operator, as operations like ``conv1d`` and ``conv2d`` can be"
" implemented using the same op."
msgstr ""
"我们现在看到 ``torch.ops.aten.conv2d.default`` 已被分解成 "
"``torch.ops.aten.convolution.default``。这是因为 ``convolution`` 是一个更“核心”的操作符，因为像"
" ``conv1d`` 和 ``conv2d`` 这样的操作可以用相同的操作符实现。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We can also specify our own decomposition behaviors:"
msgstr "我们还可以指定自己的分解行为："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Notice that instead of ``torch.ops.aten.conv2d.default`` being decomposed "
"into ``torch.ops.aten.convolution.default``, it is now decomposed into "
"``torch.ops.aten.convolution.default`` and ``torch.ops.aten.mul.Tensor``, "
"which matches our custom decomposition rule."
msgstr ""
"注意，与 ``torch.ops.aten.conv2d.default`` 被分解为 "
"``torch.ops.aten.convolution.default`` 不同，它现在被分解为 "
"``torch.ops.aten.convolution.default`` 和 "
"``torch.ops.aten.mul.Tensor``，这与我们的自定义分解规则匹配。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "ExportDB"
msgstr "ExportDB"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.export`` will only ever export a single computation graph from a "
"PyTorch program. Because of this requirement, there will be Python or "
"PyTorch features that are not compatible with ``torch.export``, which will "
"require users to rewrite parts of their model code. We have seen examples of"
" this earlier in the tutorial -- for example, rewriting if-statements using "
"``cond``."
msgstr ""
"``torch.export`` 只会从一个 PyTorch 程序中导出单个计算图。由于这种要求，有些 Python 或 PyTorch 的功能可能与 "
"``torch.export`` 不兼容，这需要用户重写部分模型代码。我们在前面的教程中已经看到了一些示例，例如使用 ``cond`` 重写 if "
"条件语句。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`ExportDB <https://pytorch.org/docs/main/generated/exportdb/index.html>`__ "
"is the standard reference that documents supported and unsupported "
"Python/PyTorch features for ``torch.export``. It is essentially a list a "
"program samples, each of which represents the usage of one particular "
"Python/PyTorch feature and its interaction with ``torch.export``. Examples "
"are also tagged by category so that they can be more easily searched."
msgstr ""
"`ExportDB <https://pytorch.org/docs/main/generated/exportdb/index.html>`__ "
"是记录 ``torch.export`` 所支持和不支持的 Python/PyTorch "
"功能的标准参考。它基本上是一系列程序示例，每个示例都代表一种特定 Python/PyTorch 功能及其与 ``torch.export`` "
"的交互。示例还可以按类别标记，以便更容易搜索。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For example, let's use ExportDB to get a better understanding of how the "
"predicate works in the ``cond`` operator. We can look at the example called "
"``cond_predicate``, which has a ``torch.cond`` tag. The example code looks "
"like:"
msgstr ""
"例如，让我们使用 ExportDB 来更好地理解 ``cond`` 操作符中的谓词是如何工作的。我们可以查看名为 ``cond_predicate`` "
"的示例，它有一个 ``torch.cond`` 标签。示例代码如："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"More generally, ExportDB can be used as a reference when one of the "
"following occurs:"
msgstr "更一般地说，在以下情况之一发生时，ExportDB 可以用作参考："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before attempting ``torch.export``, you know ahead of time that your model "
"uses some tricky Python/PyTorch features and you want to know if "
"``torch.export`` covers that feature."
msgstr ""
"在尝试 ``torch.export`` 之前，您已经知道您的模型使用了一些复杂的 Python/PyTorch 功能，并希望了解 "
"``torch.export`` 是否支持该功能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When attempting ``torch.export``, there is a failure and it's unclear how to"
" work around it."
msgstr "在尝试 ``torch.export`` 时出现了失败，并且不清楚如何解决。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"ExportDB is not exhaustive, but is intended to cover all use cases found in "
"typical PyTorch code. Feel free to reach out if there is an important "
"Python/PyTorch feature that should be added to ExportDB or supported by "
"``torch.export``."
msgstr ""
"ExportDB 不是详尽无遗的，但旨在覆盖典型 PyTorch 代码中发现的所有用例。如果有重要的 Python/PyTorch 功能需要添加到 "
"ExportDB 或由 ``torch.export`` 支持，请随时联系我们。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Running the Exported Program"
msgstr "运行导出的程序"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As ``torch.export`` is only a graph capturing mechanism, calling the "
"artifact produced by ``torch.export`` eagerly will be equivalent to running "
"the eager module. To optimize the execution of the Exported Program, we can "
"pass this exported artifact to backends such as Inductor through "
"``torch.compile``, `AOTInductor "
"<https://pytorch.org/docs/main/torch.compiler_aot_inductor.html>`__, or "
"`TensorRT <https://pytorch.org/TensorRT/dynamo/dynamo_export.html>`__."
msgstr ""
"由于 ``torch.export`` 只是一个图捕获机制，直接调用由 ``torch.export`` "
"生成的工件将在急性方式下等价于运行急性模块。为了优化导出程序的执行，我们可以将此导出的工件传递给诸如 Inductor 这样的后端，通过 "
"``torch.compile``、`AOTInductor "
"<https://pytorch.org/docs/main/torch.compiler_aot_inductor.html>`__ 或者 "
"`TensorRT <https://pytorch.org/TensorRT/dynamo/dynamo_export.html>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We introduced ``torch.export``, the new PyTorch 2.X way to export single "
"computation graphs from PyTorch programs. In particular, we demonstrate "
"several code modifications and considerations (control flow ops, "
"constraints, etc.) that need to be made in order to export a graph."
msgstr ""
"我们介绍了 ``torch.export``，这是 PyTorch 2.X 中用于从 PyTorch "
"程序中导出单个计算图的新方法。特别是，我们展示了为导出图需要进行的一些代码修改和注意事项（控制流操作符、约束等）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: torch_export_tutorial.py "
"<torch_export_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码：torch_export_tutorial.py "
"<torch_export_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: torch_export_tutorial.ipynb "
"<torch_export_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook：torch_export_tutorial.ipynb "
"<torch_export_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_torchrec_intro_tutorial.py>` to download the"
" full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_torchrec_intro_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introduction to TorchRec"
msgstr "TorchRec简介"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**TorchRec** is a PyTorch library tailored for building scalable and "
"efficient recommendation systems using embeddings. This tutorial guides you "
"through the installation process, introduces the concept of embeddings, and "
"highlights their importance in recommendation systems. It offers practical "
"demonstrations on implementing embeddings with PyTorch and TorchRec, "
"focusing on handling large embedding tables through distributed training and"
" advanced optimizations."
msgstr ""
"**TorchRec** 是一个 PyTorch "
"库，专为使用嵌入构建可扩展且高效的推荐系统而设计。此教程引导您完成安装过程，介绍嵌入的概念，并突出其在推荐系统中的重要性。它提供了使用 PyTorch "
"和 TorchRec 实现嵌入的实际演示，重点是通过分布式训练和高级优化处理大型嵌入表。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fundamentals of embeddings and their role in recommendation systems"
msgstr "嵌入的基础知识及其在推荐系统中的作用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"How to set up TorchRec to manage and implement embeddings in PyTorch "
"environments"
msgstr "如何设置 TorchRec 以在 PyTorch 环境中管理和实现嵌入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Explore advanced techniques for distributing large embedding tables across "
"multiple GPUs"
msgstr "探索将大型嵌入表分布到多个 GPU 上的高级技术"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch v2.5 or later with CUDA 11.8 or later"
msgstr "PyTorch v2.5 或更高版本支持 CUDA 11.8 或更高版本"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Python 3.9 or later"
msgstr "Python 3.9 或更高版本"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`FBGEMM <https://github.com/pytorch/fbgemm>`__"
msgstr "`FBGEMM <https://github.com/pytorch/fbgemm>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Install Dependencies"
msgstr "安装依赖项"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before running this tutorial in Google Colab or other environment, install "
"the following dependencies:"
msgstr "在 Google Colab 或其他环境中运行此教程之前，请安装以下依赖项："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are running this in Google Colab, make sure to switch to a GPU "
"runtime type. For more information, see `Enabling CUDA "
"<https://pytorch.org/tutorials/beginner/colab#enabling-cuda>`__"
msgstr ""
"如果您在 Google Colab 中运行此教程，请确保切换到 GPU 运行时类型。有关更多信息，请参见 `启用 CUDA "
"<https://pytorch.org/tutorials/beginner/colab#enabling-cuda>`__"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Embeddings"
msgstr "嵌入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When building recommendation systems, categorical features typically have "
"massive cardinality, posts, users, ads, and so on."
msgstr "在构建推荐系统时，类别特征通常具有较大的基数，比如帖子、用户、广告等。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to represent these entities and model these relationships, "
"**embeddings** are used. In machine learning, **embeddings are a vectors of "
"real numbers in a high-dimensional space used to represent meaning in "
"complex data like words, images, or users**."
msgstr ""
"为了表示这些实体并建模这些关系，使用了**嵌入**。在机器学习中，**嵌入是高维空间中代表复杂数据（如单词、图像或用户）意义的实数向量**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Embeddings in RecSys"
msgstr "嵌入在推荐系统中"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now you might wonder, how are these embeddings generated in the first place?"
" Well, embeddings are represented as individual rows in an **Embedding "
"Table**, also referred to as embedding weights. The reason for this is that "
"embeddings or embedding table weights are trained just like all of the other"
" weights of the model via gradient descent!"
msgstr ""
"现在您可能会问，这些嵌入是如何生成的呢？嵌入表示为**嵌入表**中的单独行，也称为嵌入权重。原因是嵌入或嵌入表权重通过梯度下降像模型的所有其他权重一样进行训练！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Embedding tables are simply a large matrix for storing embeddings, with two "
"dimensions (B, N), where:"
msgstr "嵌入表只是用于存储嵌入的大型矩阵，具有两个维度 (B, N)，其中："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "B is the number of embeddings stored by the table"
msgstr "B 是表存储的嵌入数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "N is the number of dimensions per embedding (N-dimensional embedding)."
msgstr "N 是每个嵌入的维度数 (N 维嵌入)。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The inputs to embedding tables represent embedding lookups to retrieve the "
"embedding for a specific index or row. In recommendation systems, such as "
"those used in many large systems, unique IDs are not only used for specific "
"users, but also across entities like posts and ads to serve as lookup "
"indices to respective embedding tables!"
msgstr ""
"嵌入表的输入代表嵌入查找，用于获取特定索引或行的嵌入。在许多大型系统中使用的推荐系统中，唯一 ID "
"不仅用于特定用户，还用于帖子和广告等实体，作为对各自嵌入表的查找索引！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Embeddings are trained in RecSys through the following process:"
msgstr "嵌入在推荐系统中通过以下过程进行训练："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Input/lookup indices are fed into the model, as unique IDs**. IDs are "
"hashed to the total size of the embedding table to prevent issues when the "
"ID > number of rows"
msgstr "**输入/查找索引作为唯一 ID 被馈入模型**。ID 被哈希到嵌入表的总大小，以防止出现 ID > 行数的问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Embeddings are then retrieved and **pooled, such as taking the sum or mean "
"of the embeddings**. This is required as there can be a variable number of "
"embeddings per example while the model expects consistent shapes."
msgstr "然后检索嵌入并进行**池化，如求和或求平均值**。这是必要的，因为每个示例可以有可变数量的嵌入，而模型期望一致的形状。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The **embeddings are used in conjunction with the rest of the model to "
"produce a prediction**, such as `Click-Through Rate (CTR) "
"<https://support.google.com/google-ads/answer/2615875?hl=en>`__ for an ad."
msgstr ""
"**嵌入与模型的其余部分一起生成预测结果**，例如广告的 `点击率 (CTR) <https://support.google.com/google-"
"ads/answer/2615875?hl=en>`__。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The loss is calculated with the prediction and the label for an example, and"
" **all weights of the model are updated through gradient descent and "
"backpropagation, including the embedding weights** that were associated with"
" the example."
msgstr "通过预测结果和示例的标签计算损失，并通过梯度下降和反向传播更新模型的所有权重，包括与示例相关的嵌入权重。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"These embeddings are crucial for representing categorical features, such as "
"users, posts, and ads, in order to capture relationships and make good "
"recommendations. The `Deep learning recommendation model "
"<https://arxiv.org/abs/1906.00091>`__ (DLRM) paper talks more about the "
"technical details of using embedding tables in RecSys."
msgstr ""
"这些嵌入对于表示类别特征（如用户、帖子和广告）至关重要，以便捕捉关系并提供良好的推荐。`深度学习推荐模型 "
"<https://arxiv.org/abs/1906.00091>`__ (DLRM) 的论文进一步讨论了在推荐系统中使用嵌入表的技术细节。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial introduces the concept of embeddings, showcase TorchRec "
"specific modules and data types, and depict how distributed training works "
"with TorchRec."
msgstr "本教程介绍了嵌入的概念，展示了 TorchRec 特定模块和数据类型，并描述了 TorchRec 的分布式训练是如何工作的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Embeddings in PyTorch"
msgstr "PyTorch 中的嵌入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In PyTorch, we have the following types of embeddings:"
msgstr "在 PyTorch 中，我们有以下类型的嵌入："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":class:`torch.nn.Embedding`: An embedding table where forward pass returns "
"the embeddings themselves as is."
msgstr ":class:`torch.nn.Embedding`: 一个嵌入表，前向遍历将直接返回嵌入本身。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":class:`torch.nn.EmbeddingBag`: Embedding table where forward pass returns "
"embeddings that are then pooled, for example, sum or mean, otherwise known "
"as **Pooled Embeddings**."
msgstr ""
":class:`torch.nn.EmbeddingBag`: "
"一个嵌入表，前向遍历将返回嵌入，然后进行池化，例如求和或求平均值，也被称为**池化嵌入**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we will go over a very brief introduction to performing "
"embedding lookups by passing in indices into the table."
msgstr "在本部分中，我们将简要介绍如何通过传递索引到表中来执行嵌入查找。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Congratulations! Now you have a basic understanding of how to use embedding "
"tables --- one of the foundations of modern recommendation systems! These "
"tables represent entities and their relationships. For example, the "
"relationship between a given user and the pages and posts they have liked."
msgstr ""
"恭喜！现在您已经对如何使用嵌入表有了基本的了解——这是现代推荐系统的基石之一！这些表表示实体及其关系。例如，给定用户与他们喜欢的页面和帖子的关系。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TorchRec Features Overview"
msgstr "TorchRec 特性概述"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the section above we've learned how to use embedding tables, one of the "
"foundations of modern recommendation systems! These tables represent "
"entities and relationships, such as users, pages, posts, etc. Given that "
"these entities are always increasing, a **hash** function is typically "
"applied to make sure the IDs are within the bounds of a certain embedding "
"table. However, in order to represent a vast amount of entities and reduce "
"hash collisions, these tables can become quite massive (think about the "
"number of ads for example). In fact, these tables can become so massive that"
" they won't be able to fit on 1 GPU, even with 80G of memory."
msgstr ""
"在上面的部分中，我们已经学习了如何使用嵌入表，这是现代推荐系统的基础之一！这些表表示实体和关系，例如用户、页面、帖子等。鉴于这些实体不断增加，通常会应用"
" **哈希** 函数以确保 ID "
"在某个嵌入表的范围内。然而，为了表示大量的实体并减少哈希冲突，这些表可能变得非常庞大（比如广告的数量）。事实上，这些表可能大到即使有 80G "
"的内存也无法适配到 1 个 GPU 上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to train models with massive embedding tables, sharding these "
"tables across GPUs is required, which then introduces a whole new set of "
"problems and opportunities in parallelism and optimization. Luckily, we have"
" the TorchRec library that has encountered, consolidated, and addressed many"
" of these concerns. TorchRec serves as a **library that provides primitives "
"for large scale distributed embeddings**."
msgstr ""
"为了训练具有庞大嵌入表的模型，需要在 GPU 之间分片这些表，这带来了并行和优化方面的一系列新问题和机遇。幸运的是，我们拥有 TorchRec "
"库，它汇总并解决了这些问题。TorchRec 作为一个**为大规模分布式嵌入提供原语的库**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, we will explore the major features of the TorchRec library. We will "
"start with ``torch.nn.Embedding`` and will extend that to custom TorchRec "
"modules, explore distributed training environment with generating a sharding"
" plan for embeddings, look at inherent TorchRec optimizations, and extend "
"the model to be ready for inference in C++. Below is a quick outline of what"
" this section consists of:"
msgstr ""
"接下来，我们将探索 TorchRec 库的主要特性。我们将从 ``torch.nn.Embedding`` 开始，并扩展到自定义 TorchRec "
"模块，探索分布式训练环境并生成嵌入的分片计划，查看固有的 TorchRec 优化，并将模型扩展为准备在 C++ 中进行推理。以下是本部分的简要概述："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TorchRec Modules and Data Types"
msgstr "TorchRec 模块和数据类型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed Training, Sharding, and Optimizations"
msgstr "分布式训练、分片和优化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's begin with importing TorchRec:"
msgstr "让我们从导入 TorchRec 开始："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This section goes over TorchRec Modules and data types including such "
"entities as ``EmbeddingCollection`` and ``EmbeddingBagCollection``, "
"``JaggedTensor``, ``KeyedJaggedTensor``, ``KeyedTensor`` and more."
msgstr ""
"本部分介绍了 TorchRec 模块和数据类型，其中包括 ``EmbeddingCollection`` 和 "
"``EmbeddingBagCollection``、``JaggedTensor``、``KeyedJaggedTensor``、``KeyedTensor``"
" 等实体。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "From ``EmbeddingBag`` to ``EmbeddingBagCollection``"
msgstr "从 ``EmbeddingBag`` 到 ``EmbeddingBagCollection``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have already explored :class:`torch.nn.Embedding` and "
":class:`torch.nn.EmbeddingBag`. TorchRec extends these modules by creating "
"collections of embeddings, in other words modules that can have multiple "
"embedding tables, with ``EmbeddingCollection`` and "
"``EmbeddingBagCollection`` We will use ``EmbeddingBagCollection`` to "
"represent a group of embedding bags."
msgstr ""
"我们已经探索了 :class:`torch.nn.Embedding` 和 "
":class:`torch.nn.EmbeddingBag`。TorchRec 通过创建嵌入集合扩展了这些模块，也就是可以包含多个嵌入表的模块，如 "
"``EmbeddingCollection`` 和 ``EmbeddingBagCollection``。我们将使用 "
"``EmbeddingBagCollection`` 来表示一组嵌入包。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example code below, we create an ``EmbeddingBagCollection`` (EBC) "
"with two embedding bags, 1 representing **products** and 1 representing "
"**users**. Each table, ``product_table`` and ``user_table``, is represented "
"by a 64 dimension embedding of size 4096."
msgstr ""
"在下面的示例代码中，我们创建了一个具有两个嵌入包的 ``EmbeddingBagCollection`` (EBC)，一个表示 **产品**，另一个表示"
" **用户**。每个表 ``product_table`` 和 ``user_table`` 都由大小为 4096 的 64 维嵌入表示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s inspect the forward method for ``EmbeddingBagCollection`` and the "
"module’s inputs and outputs:"
msgstr "让我们检查一下 ``EmbeddingBagCollection`` 的前向方法以及模块的输入和输出："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TorchRec Input/Output Data Types"
msgstr "TorchRec 输入/输出数据类型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRec has distinct data types for input and output of its modules: "
"``JaggedTensor``, ``KeyedJaggedTensor``, and ``KeyedTensor``. Now you might "
"ask, why create new data types to represent sparse features? To answer that "
"question, we must understand how sparse features are represented in code."
msgstr ""
"TorchRec 针对其模块的输入和输出具有独特的数据类型：``JaggedTensor``、``KeyedJaggedTensor`` 和 "
"``KeyedTensor``。您可能会问，为什么要创建新的数据类型来表示稀疏特征？要回答这个问题，我们必须了解稀疏特征在代码中的表示方式。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Sparse features are otherwise known as ``id_list_feature`` and "
"``id_score_list_feature``, and are the **IDs** that will be used as indices "
"to an embedding table to retrieve the embedding for that ID. To give a very "
"simple example, imagine a single sparse feature being Ads that a user "
"interacted with. The input itself would be a set of Ad IDs that a user "
"interacted with, and the embeddings retrieved would be a semantic "
"representation of those Ads. The tricky part of representing these features "
"in code is that in each input example, **the number of IDs is variable**. "
"One day a user might have interacted with only one ad while the next day "
"they interact with three."
msgstr ""
"稀疏特征也称为 ``id_list_feature`` 和 ``id_score_list_feature``，它们是将用作嵌入表索引的 "
"**IDs**，以检索该 ID 的嵌入。举个简单的例子，想象一个稀疏特征是用户互动过的广告。输入本身将是用户与之互动过的一组广告 "
"ID，检索到的嵌入将是这些广告的语义表示。在代码中表示这些特征的棘手部分在于每个输入示例中 IDs "
"的数量是可变的。一个用户某天可能只互动了一个广告，而第二天则可能互动了三个。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A simple representation is shown below, where we have a ``lengths`` tensor "
"denoting how many indices are in an example for a batch and a ``values`` "
"tensor containing the indices themselves."
msgstr ""
"一个简单的表示如下所示，其中我们有一个 ``lengths`` 张量表示批量示例中有多少个索引，以及一个 ``values`` 张量包含这些索引本身。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Next, let's look at the offsets as well as what is contained in each batch"
msgstr "接下来让我们来看看偏移量以及每个批次中包含的内容"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Congrats! You now understand TorchRec modules and data types. Give yourself "
"a pat on the back for making it this far. Next, we will learn about "
"distributed training and sharding."
msgstr "恭喜！您现在了解了 TorchRec 模块和数据类型。为自己鼓掌，为取得的进展感到骄傲。接下来，我们将学习分布式训练和分片。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed Training and Sharding"
msgstr "分布式训练和分片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we have a grasp on TorchRec modules and data types, it's time to "
"take it to the next level."
msgstr "现在我们对 TorchRec 模块和数据类型有了了解，是时候进入下一阶段了。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember, the main purpose of TorchRec is to provide primitives for "
"distributed embeddings. So far, we've only worked with embedding tables on a"
" single device. This has been possible given how small the embedding tables "
"have been, but in a production setting this isn't generally the case. "
"Embedding tables often get massive, where one table can't fit on a single "
"GPU, creating the requirement for multiple devices and a distributed "
"environment."
msgstr ""
"请记住，TorchRec "
"的主要目的是为分布式嵌入提供原语。到目前为止，我们只在单个设备上处理嵌入表。由于嵌入表很小，这一点是可行的，但在生产环境中通常并非如此。嵌入表通常非常庞大，其中一个表无法适配到单个"
" GPU 上，因此需要多设备和分布式环境。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we will explore setting up a distributed environment, "
"exactly how actual production training is done, and explore sharding "
"embedding tables, all with TorchRec."
msgstr "在本部分中，我们将探讨设置分布式环境的过程，了解实际生产训练是如何完成的，并探索嵌入表的分片，这些都是在 TorchRec 中完成的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**This section will also only use 1 GPU, though it will be treated in a "
"distributed fashion. This is only a limitation for training, as training has"
" a process per GPU. Inference does not run into this requirement**"
msgstr ""
"**本部分也仅使用 1 个 GPU，但它将在分布式环境中对待。这仅是训练的限制，因为训练每个 GPU 都需要一个进程。而推理则没有这一要求。**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the example code below, we set up our PyTorch distributed environment."
msgstr "在下面的示例代码中，我们设置了 PyTorch 分布式环境。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are running this in Google Colab, you can only call this cell once, "
"calling it again will cause an error as you can only initialize the process "
"group once."
msgstr "如果您正在 Google Colab 中运行此代码，您只能调用此代码块一次，再次调用将导致错误，因为进程组只能初始化一次。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Distributed Embeddings"
msgstr "分布式嵌入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have already worked with the main TorchRec module: "
"``EmbeddingBagCollection``. We have examined how it works along with how "
"data is represented in TorchRec. However, we have not yet explored one of "
"the main parts of TorchRec, which is **distributed embeddings**."
msgstr ""
"我们已经处理了主要的 TorchRec 模块：``EmbeddingBagCollection``。我们检查了它如何工作以及数据在 TorchRec "
"中的表示方式。然而，我们还没有探索 TorchRec 的主要部分之一，那就是 **分布式嵌入**。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"GPUs are the most popular choice for ML workloads by far today, as they are "
"able to do magnitudes more floating point operations/s (`FLOPs "
"<https://en.wikipedia.org/wiki/FLOPS>`__) than CPU. However, GPUs come with "
"the limitation of scarce fast memory (HBM which is analogous to RAM for "
"CPU), typically, ~10s of GBs."
msgstr ""
"如今，GPU 是最流行的 ML 工作负载选择，因为它们能够执行比 CPU 多得多的浮点运算 (FLOPs)。然而，GPU 存在快速内存（HBM，相当于 "
"CPU 的 RAM）稀缺的限制，通常只有几十 GB。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A RecSys model can contain embedding tables that far exceed the memory limit"
" for 1 GPU, hence the need for distribution of the embedding tables across "
"multiple GPUs, otherwise known as **model parallel**. On the other hand, "
"**data parallel** is where the entire model is replicated on each GPU, which"
" each GPU taking in a distinct batch of data for training, syncing gradients"
" on the backwards pass."
msgstr ""
"一个推荐系统模型可以包含远远超过 1 个 GPU 的内存限制的嵌入表，因此需要将嵌入表分布到多个 GPU 上，也就是所谓的 "
"**模型并行**。另一方面，**数据并行**是指整个模型在每个 GPU 上都被复制，每个 GPU 处理一批不同的数据进行训练，在反向传播过程中同步梯度。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Parts of the model that **require less compute but more memory (embeddings) "
"are distributed with model parallel** while parts that **require more "
"compute and less memory (dense layers, MLP, etc.) are distributed with data "
"parallel**."
msgstr ""
"**计算需求较低但内存需求较高的模型部分（如嵌入）通过模型并行分布，而计算需求较高但内存需求较低的模型部分（如密集层、MLP 等）通过数据并行分布。**"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sharding"
msgstr "分片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to distribute an embedding table, we split up the embedding table "
"into parts and place those parts onto different devices, also known as "
"“sharding”."
msgstr "为了分布一个嵌入表，我们将嵌入表拆分成多个部分并将这些部分放置到不同的设备上，也就是所谓的“分片”。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are many ways to shard embedding tables. The most common ways are:"
msgstr "分片嵌入表的方式有很多，最常见的方式有："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Table-Wise: the table is placed entirely onto one device"
msgstr "表级分片：表完全放置在一个设备上"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Column-Wise: columns of embedding tables are sharded"
msgstr "列级分片：嵌入表的列进行分片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Row-Wise: rows of embedding tables are sharded"
msgstr "行级分片：嵌入表的行进行分片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sharded Modules"
msgstr "分片模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"While all of this seems like a lot to deal with and implement, you're in "
"luck. **TorchRec provides all the primitives for easy distributed training "
"and inference**! In fact, TorchRec modules have two corresponding classes "
"for working with any TorchRec module in a distributed environment:"
msgstr ""
"虽然这些看起来需要解决和实现的任务很多，但您走运了。**TorchRec 提供了所有用于简单分布式训练和推理的原语！**事实上，TorchRec "
"模块在分布式环境中使用任何 TorchRec 模块时都有两个对应的类："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**The module sharder**: This class exposes a ``shard`` API that handles "
"sharding a TorchRec Module, producing a sharded module. * For "
"``EmbeddingBagCollection``, the sharder is `EmbeddingBagCollectionSharder "
"<https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.embeddingbag.EmbeddingBagCollectionSharder>`__"
msgstr ""
"**模块分片器**：此类提供一个 ``shard`` API，它处理 TorchRec 模块的分片，生成一个分片模块。 * 对于 "
"``EmbeddingBagCollection``，分片器是 `EmbeddingBagCollectionSharder`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Sharded module**: This class is a sharded variant of a TorchRec module. It"
" has the same input/output as a the regular TorchRec module, but much more "
"optimized and works in a distributed environment. * For "
"``EmbeddingBagCollection``, the sharded variant is "
"`ShardedEmbeddingBagCollection "
"<https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.embeddingbag.ShardedEmbeddingBagCollection>`__"
msgstr ""
"**分片模块**：此类是 TorchRec 模块的分片变体。其输入/输出与普通 TorchRec 模块相同，但经过大量优化并能在分布式环境中工作。 * "
"对于 ``EmbeddingBagCollection``，分片变体是 `ShardedEmbeddingBagCollection`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Every TorchRec module has an unsharded and sharded variant."
msgstr "每个 TorchRec 模块都有未分片和分片变体。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The unsharded version is meant to be prototyped and experimented with."
msgstr "未分片版本用于原型设计和实验。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The sharded version is meant to be used in a distributed environment for "
"distributed training and inference."
msgstr "分片版本用于分布式环境的分布式训练和推理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The sharded versions of TorchRec modules, for example "
"``EmbeddingBagCollection``, will handle everything that is needed for Model "
"Parallelism, such as communication between GPUs for distributing embeddings "
"to the correct GPUs."
msgstr ""
"TorchRec 模块的分片版本，例如 ``EmbeddingBagCollection``，将处理模型并行性所需的一切，例如在 GPU "
"之间通信以将嵌入分布到正确的 GPU 上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Refresher of our ``EmbeddingBagCollection`` module"
msgstr "回顾我们的 ``EmbeddingBagCollection`` 模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Planner"
msgstr "规划器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before we can show how sharding works, we must know about the **planner**, "
"which helps us determine the best sharding configuration."
msgstr "在展示分片如何工作之前，我们必须了解 **规划器**，它帮助我们确定最佳分片配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Given a number of embedding tables and a number of ranks, there are many "
"different sharding configurations that are possible. For example, given 2 "
"embedding tables and 2 GPUs, you can:"
msgstr "给定数量的嵌入表和 GPU 排列，有许多不同的分片配置是可能的。例如，给定两个嵌入表和两个 GPU，您可以："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Place 1 table on each GPU"
msgstr "将一个表放置在每个 GPU 上"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Place both tables on a single GPU and no tables on the other"
msgstr "将两个表都放到一个 GPU 上而另一个 GPU 不放置表"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Place certain rows and columns on each GPU"
msgstr "将某些行和列分片放到每个 GPU 上"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Given all of these possibilities, we typically want a sharding configuration"
" that is optimal for performance."
msgstr "鉴于所有这些可能性，我们通常需要一个性能最佳的分片配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That is where the planner comes in. The planner is able to determine given "
"the number of embedding tables and the number of GPUs, what is the optimal "
"configuration. Turns out, this is incredibly difficult to do manually, with "
"tons of factors that engineers have to consider to ensure an optimal "
"sharding plan. Luckily, TorchRec provides an auto planner when the planner "
"is used."
msgstr ""
"这就是规划器的作用。规划器可以根据嵌入表数量和 GPU "
"数量确定最佳配置。事实证明，手动完成这项任务非常困难，工程师需要考虑许多因素来确保优化的分片计划。幸运的是，当使用规划器时 TorchRec "
"提供了一个自动规划器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The TorchRec planner:"
msgstr "TorchRec 规划器能："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Assesses memory constraints of hardware"
msgstr "评估硬件的内存限制"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Estimates compute based on memory fetches as embedding lookups"
msgstr "根据嵌入查找作为内存提取估算计算量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Addresses data specific factors"
msgstr "处理数据特定因素"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Considers other hardware specifics like bandwidth to generate an optimal "
"sharding plan"
msgstr "考虑其他硬件特性，如带宽，以生成优化的分片计划"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In order to take into consideration all these variables, The TorchRec "
"planner can take in `various amounts of data for embedding tables, "
"constraints, hardware information, and topology "
"<https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/planner/planners.py#L147-L155>`__"
" to aid in generating the optimal sharding plan for a model, which is "
"routinely provided across stacks."
msgstr ""
"为了考虑到所有这些变量，TorchRec的规划器可以接受嵌入表的不同数据量、约束条件、硬件信息和拓扑结构，以帮助生成模型的最佳分片计划，这通常会在不同的栈之间提供。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"To learn more about sharding, see our `sharding tutorial "
"<https://pytorch.org/tutorials/advanced/sharding.html>`__."
msgstr "要了解更多关于分片的信息，请参阅我们的分片教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Planner Result"
msgstr "规划器结果"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As you can see above, when running the planner there is quite a bit of "
"output. We can see a lot of stats being calculated along with where our "
"tables end up being placed."
msgstr "正如您在上面看到的，运行规划器时会产生大量输出。我们可以看到许多统计数据的计算，以及嵌入表的最终放置位置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The result of running the planner is a static plan, which can be reused for "
"sharding! This allows sharding to be static for production models instead of"
" determining a new sharding plan everytime. Below, we use the sharding plan "
"to finally generate our ``ShardedEmbeddingBagCollection``."
msgstr ""
"运行规划器的结果是一个静态计划，可以重复用于分片！这允许生产模型的分片计划是静态的，而不是每次都重新生成新的分片计划。下面，我们使用分片计划最终生成了我们的``ShardedEmbeddingBagCollection``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "GPU Training with ``LazyAwaitable``"
msgstr "使用``LazyAwaitable``进行GPU训练"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember that TorchRec is a highly optimized library for distributed "
"embeddings. A concept that TorchRec introduces to enable higher performance "
"for training on GPU is a `LazyAwaitable "
"<https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.types.LazyAwaitable>`__."
" You will see ``LazyAwaitable`` types as outputs of various sharded TorchRec"
" modules. All a ``LazyAwaitable`` type does is delay calculating some result"
" as long as possible, and it does it by acting like an async type."
msgstr ""
"请记住，TorchRec是一个针对分布式嵌入高度优化的库。TorchRec引入的一个概念是`LazyAwaitable`，以实现GPU训练的更高性能。您将在各种分片的TorchRec模块的输出中看到``LazyAwaitable``类型。``LazyAwaitable``类型的作用就是尽可能延迟计算某些结果，它通过扮演类似异步类型来实现这一点。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Anatomy of Sharded TorchRec modules"
msgstr "分片的TorchRec模块结构"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have now successfully sharded an ``EmbeddingBagCollection`` given a "
"sharding plan that we generated! The sharded module has common APIs from "
"TorchRec which abstract away distributed communication/compute amongst "
"multiple GPUs. In fact, these APIs are highly optimized for performance in "
"training and inference. **Below are the three common APIs for distributed "
"training/inference** that are provided by TorchRec:"
msgstr ""
"我们已经成功分片了给定分片计划的``EmbeddingBagCollection``！分片模块具有TorchRec的通用API，抽象了多GPU间分布式通信和计算。事实上，这些API在训练和推断过程中进行了高度优化。以下是TorchRec为分布式训练和推断提供的三个通用API："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``input_dist``: Handles distributing inputs from GPU to GPU."
msgstr "``input_dist``：处理从GPU到GPU的输入分发。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``lookups``: Does the actual embedding lookup in an optimized, batched "
"manner using FBGEMM TBE (more on this later)."
msgstr "``lookups``：使用FBGEMM TBE以优化的、批处理的方式执行实际的嵌入查找（稍后详细介绍）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``output_dist``: Handles distributing outputs from GPU to GPU."
msgstr "``output_dist``：处理从GPU到GPU的输出分发。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The distribution of inputs and outputs is done through `NCCL Collectives "
"<https://docs.nvidia.com/deeplearning/nccl/user-"
"guide/docs/overview.html>`__, namely `All-to-Alls "
"<https://docs.nvidia.com/deeplearning/nccl/user-"
"guide/docs/usage/p2p.html#all-to-all>`__, which is where all GPUs send and "
"receive data to and from one another. TorchRec interfaces with PyTorch "
"distributed for collectives and provides clean abstractions to the end "
"users, removing the concern for the lower level details."
msgstr ""
"输入和输出的分发通过NCCL Collectives（例如All-to-"
"Alls）实现，这样所有GPU之间可发送和接收数据。TorchRec与PyTorch分布式进行接口连接，并为终端用户提供简洁抽象，去除了关注底层细节的需求。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The backwards pass does all of these collectives but in the reverse order "
"for distribution of gradients. ``input_dist``, ``lookup``, and "
"``output_dist`` all depend on the sharding scheme. Since we sharded in a "
"table-wise fashion, these APIs are modules that are constructed by "
"`TwPooledEmbeddingSharding "
"<https://pytorch.org/torchrec/torchrec.distributed.sharding.html#torchrec.distributed.sharding.tw_sharding.TwPooledEmbeddingSharding>`__."
msgstr ""
"反向传播过程会以相反的顺序执行所有这些收集操作，从而分发梯度。``input_dist``、``lookup``和``output_dist``全部依赖于分片方案。由于我们是以表级方式分片的，这些API是通过TwPooledEmbeddingSharding模块构造的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Optimizing Embedding Lookups"
msgstr "优化嵌入查找"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In performing lookups for a collection of embedding tables, a trivial "
"solution would be to iterate through all the ``nn.EmbeddingBags`` and do a "
"lookup per table. This is exactly what the standard, unsharded "
"``EmbeddingBagCollection`` does. However, while this solution is simple, it "
"is extremely slow."
msgstr ""
"在为嵌入表集合执行查找时，一个简单的解决方案是迭代所有的``nn.EmbeddingBags``并逐表进行查找。这正是标准未分片的``EmbeddingBagCollection``所做的。然而，虽然此解决方案简单，但非常慢。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`FBGEMM <https://github.com/pytorch/FBGEMM/tree/main/fbgemm_gpu>`__ is a "
"library that provides GPU operators (otherwise known as kernels) that are "
"very optimized. One of these operators is known as **Table Batched "
"Embedding** (TBE), provides two major optimizations:"
msgstr "FBGEMM是一个提供GPU操作符（即内核）的库，这些操作符经过高度优化。其中之一被称为表批处理嵌入（TBE），它提供了两个主要优化："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Table batching, which allows you to look up multiple embeddings with one "
"kernel call."
msgstr "表批处理，允许通过一次内核调用查找多个嵌入。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimizer Fusion, which allows the module to update itself given the "
"canonical pytorch optimizers and arguments."
msgstr "优化器融合，允许模块根据PyTorch的优化器和参数更新自身。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ``ShardedEmbeddingBagCollection`` uses the FBGEMM TBE as the lookup "
"instead of traditional ``nn.EmbeddingBags`` for optimized embedding lookups."
msgstr ""
"``ShardedEmbeddingBagCollection``使用FBGEMM "
"TBE作为查找方式，而非传统的``nn.EmbeddingBags``，以优化嵌入查找。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``DistributedModelParallel``"
msgstr "``DistributedModelParallel``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have now explored sharding a single ``EmbeddingBagCollection``! We were "
"able to take the ``EmbeddingBagCollectionSharder`` and use the unsharded "
"``EmbeddingBagCollection`` to generate a ``ShardedEmbeddingBagCollection`` "
"module. This workflow is fine, but typically when implementing model "
"parallel, `DistributedModelParallel "
"<https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel>`__"
" (DMP) is used as the standard interface. When wrapping your model (in our "
"case ``ebc``), with DMP, the following will occur:"
msgstr ""
"我们已经探索了一个``EmbeddingBagCollection``的分片！我们能够使用``EmbeddingBagCollectionSharder``和未分片的``EmbeddingBagCollection``生成``ShardedEmbeddingBagCollection``模块。这种工作流是可以接受的，但通常在实现模型并行时，使用DistributedModelParallel（DMP）作为标准接口。当用DMP包装您的模型（在我们的例子中是``ebc``），将发生以下过程："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Decide how to shard the model. DMP will collect the available sharders and "
"come up with a plan of the optimal way to shard the embedding table(s) (for "
"example, ``EmbeddingBagCollection``)"
msgstr "决定如何分片模型。DMP会收集可用的分片器，并制定嵌入表（例如，``EmbeddingBagCollection``）分片的最佳计划。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Actually shard the model. This includes allocating memory for each embedding"
" table on the appropriate device(s)."
msgstr "实际上进行模型分片。这包括在适当的设备上为每个嵌入表分配内存。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"DMP takes in everything that we've just experimented with, like a static "
"sharding plan, a list of sharders, etc. However, it also has some nice "
"defaults to seamlessly shard a TorchRec model. In this toy example, since we"
" have two embedding tables and one GPU, TorchRec will place both on the "
"single GPU."
msgstr ""
"DMP接收我们刚刚尝试过的所有内容，例如静态分片计划、分片器列表等。然而，它还具有一些很好的默认设置，可以无缝地分片TorchRec模型。在这个实例中，由于我们有两个嵌入表和一个GPU，TorchRec会将两者都放在单个GPU上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sharding Best Practices"
msgstr "分片最佳实践"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Currently, our configuration is only sharding on 1 GPU (or rank), which is "
"trivial: just place all the tables on 1 GPUs memory. However, in real "
"production use cases, embedding tables are **typically sharded on hundreds "
"of GPUs**, with different sharding methods such as table-wise, row-wise, and"
" column-wise. It is incredibly important to determine a proper sharding "
"configuration (to prevent out of memory issues) while keeping it balanced "
"not only in terms of memory but also compute for optimal performance."
msgstr ""
"目前，我们的配置仅在一个GPU（或rank）上进行分片，这很简单：只需将所有表放在一个GPU的内存中。然而，在实际的生产场景中，嵌入表通常**分片到数百个GPU上**，使用不同的分片方法，如表级、行级和列级。确定适当的分片配置（防止内存不足问题）并在内存和计算方面保持平衡以优化性能，这非常重要。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Adding in the Optimizer"
msgstr "添加优化器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember that TorchRec modules are hyperoptimized for large scale "
"distributed training. An important optimization is in regards to the "
"optimizer."
msgstr "请记住，TorchRec模块针对大规模分布式训练进行了超强优化。一个重要的优化与优化器相关。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRec modules provide a seamless API to fuse the backwards pass and "
"optimize step in training, providing a significant optimization in "
"performance and decreasing the memory used, alongside granularity in "
"assigning distinct optimizers to distinct model parameters."
msgstr ""
"TorchRec模块提供了无缝的API，将反向传播和优化步骤合并到训练中，这显著提高了性能，减少了内存使用，并增加了为不同的模型参数分配不同优化器的细粒度控制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Optimizer Classes"
msgstr "优化器类"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRec uses ``CombinedOptimizer``, which contains a collection of "
"``KeyedOptimizers``. A ``CombinedOptimizer`` effectively makes it easy to "
"handle multiple optimizers for various sub groups in the model. A "
"``KeyedOptimizer`` extends the ``torch.optim.Optimizer`` and is initialized "
"through a dictionary of parameters exposes the parameters. Each ``TBE`` "
"module in a ``EmbeddingBagCollection`` will have it's own ``KeyedOptimizer``"
" which combines into one ``CombinedOptimizer``."
msgstr ""
"TorchRec使用``CombinedOptimizer``，它包含一组``KeyedOptimizers``。``CombinedOptimizer``实际上可以轻松地处理模型中各子组的多个优化器。``KeyedOptimizer``扩展了``torch.optim.Optimizer``，通过参数字典初始化并公开参数。``EmbeddingBagCollection``中的每个``TBE``模块都会有它自己的``KeyedOptimizer``，然后这些优化器被组合成一个``CombinedOptimizer``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fused optimizer in TorchRec"
msgstr "TorchRec中的融合优化器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using ``DistributedModelParallel``, the **optimizer is fused, which means "
"that the optimizer update is done in the backward**. This is an optimization"
" in TorchRec and FBGEMM, where the optimizer embedding gradients are not "
"materialized and applied directly to the parameters. This brings significant"
" memory savings as embedding gradients are typically size of the parameters "
"themselves."
msgstr ""
"在使用``DistributedModelParallel``时，**优化器是融合的，这意味着优化器更新是在反向传播中完成的**。这是TorchRec和FBGEMM中的一个优化，优化器的嵌入梯度不会被物化，而是直接应用于参数。这带来了显著的内存节省，因为嵌入梯度通常与参数本身的大小相当。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"You can, however, choose to make the optimizer ``dense`` which does not "
"apply this optimization and let's you inspect the embedding gradients or "
"apply computations to it as you wish. A dense optimizer in this case would "
"be your `canonical PyTorch model training loop with optimizer. "
"<https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html>`__"
msgstr ""
"但是，您可以选择让优化器变为``dense``，这不会应用此优化，并让您检查嵌入梯度或对其进行计算。对于这种情况，``dense``优化器将是您在PyTorch中进行模型训练的常规优化器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Once the optimizer is created through ``DistributedModelParallel``, you "
"still need to manage an optimizer for the other parameters not associated "
"with TorchRec embedding modules. To find the other parameters, use "
"``in_backward_optimizer_filter(model.named_parameters())``. Apply an "
"optimizer to those parameters as you would a normal Torch optimizer and "
"combine this and the ``model.fused_optimizer`` into one "
"``CombinedOptimizer`` that you can use in your training loop to "
"``zero_grad`` and ``step`` through."
msgstr ""
"一旦通过``DistributedModelParallel``创建优化器，您仍需要为与TorchRec嵌入模块无关的其他参数管理一个优化器。要找到这些其他参数，请使用``in_backward_optimizer_filter(model.named_parameters())``。像正常的Torch优化器一样为这些参数应用优化器，并将这部分优化器与``model.fused_optimizer``合并到一个``CombinedOptimizer``中，您可以在训练循环中用它来进行``zero_grad``和``step``操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Adding an Optimizer to ``EmbeddingBagCollection``"
msgstr "向``EmbeddingBagCollection``添加优化器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will do this in two ways, which are equivalent, but give you options "
"depending on your preferences:"
msgstr "我们将通过两个方式这样做，它们是等效的，但可以根据您的偏好提供选择："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Passing optimizer kwargs through ``fused_params`` in sharder."
msgstr "通过分片器中的``fused_params``的优化器kwargs。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Through ``apply_optimizer_in_backward``, which converts the optimizer "
"parameters to ``fused_params`` to pass to the ``TBE`` in the "
"``EmbeddingBagCollection`` or ``EmbeddingCollection``."
msgstr ""
"通过``apply_optimizer_in_backward``，将优化器参数转换为``fused_params``以传递给``EmbeddingBagCollection``或``EmbeddingCollection``中的``TBE``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we are able to train distributed embeddings, how can we take the "
"trained model and optimize it for inference? Inference is typically very "
"sensitive to **performance and size of the model**. Running just the trained"
" model in a Python environment is incredibly inefficient. There are two key "
"differences between inference and training environments:"
msgstr ""
"现在我们能够训练分布式嵌入了，那么如何将训练好的模型优化用于推断呢？推断通常对**模型性能和大小**非常敏感。在Python环境中仅运行训练好的模型是非常低效的。训练环境和推断环境之间有两个关键区别："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Quantization**: Inference models are typically quantized, where model "
"parameters lose precision for lower latency in predictions and reduced model"
" size. For example FP32 (4 bytes) in trained model to INT8 (1 byte) for each"
" embedding weight. This is also necessary given the vast scale of embedding "
"tables, as we want to use as few devices as possible for inference to "
"minimize latency."
msgstr ""
"**量化**：推断模型通常会被量化，将模型参数的精度降低以获得更低的预测延迟和更小的模型大小。例如，将训练模型中的FP32（每权重4字节）转换为INT8（每权重1字节）。这是必要的，因为嵌入表的规模非常巨大，我们希望在推断中使用尽可能少的设备以最小化延迟。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**C++ environment**: Inference latency is very important, so in order to "
"ensure ample performance, the model is typically ran in a C++ environment, "
"along with the situations where we don't have a Python runtime, like on "
"device."
msgstr "**C++环境**：推断延迟非常重要，为了确保足够的性能，模型通常运行在C++环境中，或者在某些没有Python运行时的设备上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchRec provides primitives for converting a TorchRec model into being "
"inference ready with:"
msgstr "TorchRec提供了将TorchRec模型转换为推断准备状态的基本措施："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"APIs for quantizing the model, introducing optimizations automatically with "
"FBGEMM TBE"
msgstr "用于量化模型的API，自动使用FBGEMM TBE进行优化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sharding embeddings for distributed inference"
msgstr "为分布式推断分片嵌入"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compiling the model to `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`__ (compatible in C++)"
msgstr "将模型编译为TorchScript（兼容C++）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "In this section, we will go over this entire workflow of:"
msgstr "在本节中，我们将介绍以下工作流程："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Quantizing the model"
msgstr "量化模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sharding the quantized model"
msgstr "对量化模型进行分片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compiling the sharded quantized model into TorchScript"
msgstr "将分片量化的模型编译为TorchScript"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Quantization"
msgstr "量化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As you can see above, the normal EBC contains embedding table weights as "
"FP32 precision (32 bits for each weight). Here, we will use the TorchRec "
"inference library to quantize the embedding weights of the model to INT8"
msgstr ""
"正如您在上面看到的，普通EBC包含嵌入表权重作为FP32精度（每权重32位）。在这里，我们将使用TorchRec推断库，将模型的嵌入权重量化为INT8。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Shard"
msgstr "分片"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here we perform sharding of the TorchRec quantized model. This is to ensure "
"we are using the performant module through FBGEMM TBE. Here we are using one"
" device to be consistent with training (1 TBE)."
msgstr ""
"这里我们对TorchRec量化模型进行分片。这样可以确保我们使用通过FBGEMM TBE的高性能模块。为了与训练保持一致，这里我们使用一个设备（1 "
"TBE）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Compilation"
msgstr "编译"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now we have the optimized eager TorchRec inference model. The next step is "
"to ensure that this model is loadable in C++, as currently it is only "
"runnable in a Python runtime."
msgstr "现在我们有了优化的急切TorchRec推理模型。下一步是确保此模型可以在C++中加载，因为目前它只能在Python运行时中运行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The recommended method of compilation at Meta is two fold: `torch.fx tracing"
" <https://pytorch.org/docs/stable/fx.html>`__ (generate intermediate "
"representation of model) and converting the result to TorchScript, where "
"TorchScript is C++ compatible."
msgstr ""
"Meta推荐的编译方法有两种：`torch.fx tracing "
"<https://pytorch.org/docs/stable/fx.html>`__（生成模型中间表示）并将结果转换为TorchScript，TorchScript与C++兼容。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, you have gone from training a distributed RecSys model all"
" the way to making it inference ready. The `TorchRec repo "
"<https://github.com/pytorch/torchrec/tree/main/torchrec/inference>`__ has a "
"full example of how to load a TorchRec TorchScript model into C++ for "
"inference."
msgstr ""
"在本教程中，您从训练一个分布式RecSys模型到使其准备推理。此外，`TorchRec仓库 "
"<https://github.com/pytorch/torchrec/tree/main/torchrec/inference>`__ "
"有完整示例，说明如何将TorchRec TorchScript模型加载到C++中进行推理。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For more information, please see our `dlrm "
"<https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm/>`__ "
"example, which includes multinode training on the Criteo 1TB dataset using "
"the methods described in `Deep Learning Recommendation Model for "
"Personalization and Recommendation Systems "
"<https://arxiv.org/abs/1906.00091>`__."
msgstr ""
"了解更多信息，请参阅我们的 `dlrm "
"<https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm/>`__ "
"示例，其中包括使用 `Deep Learning Recommendation Model for Personalization and "
"Recommendation Systems <https://arxiv.org/abs/1906.00091>`__ 描述的方法在Criteo "
"1TB数据集上进行多节点训练。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: torchrec_intro_tutorial.py "
"<torchrec_intro_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: torchrec_intro_tutorial.py "
"<torchrec_intro_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: torchrec_intro_tutorial.ipynb "
"<torchrec_intro_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: torchrec_intro_tutorial.ipynb "
"<torchrec_intro_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "There is a newer tutorial on this topic."
msgstr "已有关于此主题的较新教程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Redirecting..."
msgstr "正在重定向..."

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Grokking PyTorch Intel CPU performance from first principles"
msgstr "从头理解PyTorch在Intel CPU上的性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A case study on the TorchServe inference framework optimized with `Intel® "
"Extension for PyTorch* <https://github.com/intel/intel-extension-for-"
"pytorch>`_."
msgstr ""
"关于TorchServe推理框架在 `Intel® Extension for PyTorch* "
"<https://github.com/intel/intel-extension-for-pytorch>`_ 优化下的案例研究。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Authors: Min Jean Cho, Mark Saroufim"
msgstr "作者：Min Jean Cho, Mark Saroufim"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Reviewers: Ashok Emani, Jiong Gong"
msgstr "审阅者：Ashok Emani, Jiong Gong"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Getting a strong out-of-box performance for deep learning on CPUs can be "
"tricky but it’s much easier if you’re aware of the main problems that affect"
" performance, how to measure them and how to solve them."
msgstr "在CPU上获得较强的深度学习性能可能很难，但如果了解影响性能的主要问题、如何测量它们以及如何解决它们，就会容易得多。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TL;DR"
msgstr "概述"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Problem"
msgstr "问题"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "How to measure it"
msgstr "如何测量"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Solution"
msgstr "解决方案"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Bottlenecked GEMM execution units"
msgstr "受限的GEMM执行单元"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Imbalance or Serial Spinning "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/spin-time/imbalance-or-serial-"
"spinning-1.html>`_"
msgstr ""
"`不平衡或串行自旋 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/spin-time/imbalance-or-serial-"
"spinning-1.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Front-End Bound "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/front-end-bound.html>`_"
msgstr ""
"`前端受限 <https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/front-end-bound.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Core Bound "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/back-end-bound.html>`_"
msgstr ""
"`核心受限 <https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/back-end-bound.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Avoid using logical cores by setting thread affinity to physical cores via "
"core pinning"
msgstr "通过核心绑定将线程亲和性设置为物理核心，避免使用逻辑核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Non Uniform Memory Access (NUMA)"
msgstr "非统一内存访问（NUMA）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Local vs. remote memory access"
msgstr "本地与远程内存访问"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`UPI Utilization "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/memory-bound/dram-bound/upi-"
"utilization-bound.html>`_"
msgstr ""
"`UPI使用率 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/reference/cpu-metrics-reference/memory-bound/dram-bound/upi-"
"utilization-bound.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Latency in memory accesses"
msgstr "内存访问延迟"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Thread migration"
msgstr "线程迁移"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Avoid cross-socket computation by setting thread affinity to a specific "
"socket via core pinning"
msgstr "通过核心绑定将线程亲和性设置为特定插槽，避免跨插槽计算。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"*GEMM (General Matrix Multiply)* run on fused-multiply-add (FMA) or dot-"
"product (DP) execution units which will be bottlenecked and cause delays in "
"thread waiting/*spinning at synchronization* barrier when *hyperthreading* "
"is enabled - because using logical cores causes insufficient concurrency for"
" all working threads as each logical thread *contends for the same core "
"resources*. Instead, if we use 1 thread per physical core, we avoid this "
"contention. So we generally recommend *avoiding logical cores* by setting "
"CPU *thread affinity* to physical cores via *core pinning*."
msgstr ""
"*GEMM（一般矩阵乘法）*运行在融合乘加（FMA）或点积（DP）执行单元上，可能因启用超线程导致线程等待/*同步点旋转*障碍而受限，因为使用逻辑核心会导致所有工作线程之间的并发性不足，因为每个逻辑线程*争用同一核心资源*。相反，如果我们每个物理核心使用1个线程，我们就避免了这种争用。因此我们通常建议通过*核心绑定*将CPU线程亲和性设置为物理核心，*避免逻辑核心*的使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Multi-socket systems have *Non-Uniform Memory Access (NUMA)* which is a "
"shared memory architecture that describes the placement of main memory "
"modules with respect to processors. But if a process is not NUMA-aware, slow"
" *remote memory* is frequently accessed when *threads migrate* cross socket "
"via *Intel Ultra Path Interconnect (UPI)* during run time. We address this "
"problem by setting CPU *thread affinity* to a specific socket via *core "
"pinning*."
msgstr ""
"多插槽系统具有*非统一内存访问（NUMA）*，它是一种共享内存架构，描述了主内存模块相对于处理器的位置。但如果一个进程不是NUMA感知的，当运行时线程通过*Intel超路径互连（UPI）*跨插槽迁移时会频繁访问较慢的*远程内存*。我们通过将CPU线程亲和性设置为特定插槽来解决该问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Knowing these principles in mind, proper CPU runtime configuration can "
"significantly boost out-of-box performance."
msgstr "牢记这些原则，正确的CPU运行时配置可以显著提升开箱即用的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this blog, we'll walk you through the important runtime configurations "
"you should be aware of from `CPU Performance Tuning Guide "
"<https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-"
"specific-optimizations>`_, explain how they work, how to profile them and "
"how to integrate them within a model serving framework like `TorchServe "
"<https://github.com/pytorch/serve>`_ via an easy to use `launch script "
"<https://github.com/intel/intel-extension-for-"
"pytorch/blob/master/docs/tutorials/performance_tuning/launch_script.md>`_ "
"which we’ve `integrated <https://github.com/pytorch/serve/pull/1354>`_ "
":superscript:`1` natively."
msgstr ""
"在这篇博客中，我们将带领您了解 `CPU性能调优指南 "
"<https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-"
"specific-optimizations>`_ 中应该注意的重要运行时配置，解释它们的工作原理、如何剖析它们以及如何将它们集成到诸如 "
"`TorchServe <https://github.com/pytorch/serve>`_ 这样的模型服务框架中，通过一个易于使用的 `启动脚本 "
"<https://github.com/intel/intel-extension-for-"
"pytorch/blob/master/docs/tutorials/performance_tuning/launch_script.md>`_，我们已经"
" `原生集成 <https://github.com/pytorch/serve/pull/1354>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We’ll explain all of these ideas :strong:`visually` from :strong:`first "
"principles` with lots of :strong:`profiles` and show you how we applied our "
"learnings to make out of the box CPU performance on TorchServe better."
msgstr ""
"我们将从 :strong:`第一原理` 通过 :strong:`可视化` 和大量 :strong:`分析结果` "
"来解释所有这些想法，并展示如何应用我们的学习成果以改进TorchServe上的开箱即用CPU性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The feature has to be explicitly enabled by setting "
"*cpu_launcher_enable=true* in *config.properties*."
msgstr "必须在 *config.properties* 文件中通过设置 *cpu_launcher_enable=true* 显式启用此功能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Avoid logical cores for deep learning"
msgstr "避免在深度学习中使用逻辑核心"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Avoiding logical cores for deep learning workloads generally improves "
"performance. To understand this, let us take a step back to GEMM."
msgstr "避免在深度学习工作负载中使用逻辑核心通常可以提高性能。为了解这一点，让我们先回到GEMM展开。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ":strong:`Optimizing GEMM optimizes deep learning`"
msgstr ":strong:`优化GEMM即优化深度学习`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The majority of time in deep learning training or inference is spent on "
"millions of repeated operations of GEMM which is at the core of fully "
"connected layers. Fully connected layers have been used for decades since "
"multi-layer perceptrons (MLP) `proved to be a universal approximator of any "
"continuous function "
"<https://en.wikipedia.org/wiki/Universal_approximation_theorem>`_. Any MLP "
"can be entirely represented as GEMM. And even a convolution can be "
"represented as a GEMM by using a `Toepliz matrix "
"<https://en.wikipedia.org/wiki/Toeplitz_matrix>`_."
msgstr ""
"在深度学习的训练或推理中，大部分时间花在了数百万次的GEMM操作上，它是全连接层的核心。全连接层几十年来一直被使用，因为多层感知机（MLP）被证明是任何连续函数的通用逼近器。`<https://en.wikipedia.org/wiki/Universal_approximation_theorem>`_"
" 任何MLP都可以完全表示为GEMM。甚至卷积也可以通过使用 `Toeplitz矩阵 "
"<https://en.wikipedia.org/wiki/Toeplitz_matrix>`_ 表示为GEMM。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Returning to the original topic, most GEMM operators benefit from using non-"
"hyperthreading, because the majority of time in deep learning training or "
"inference is spent on millions of repeated operations of GEMM running on "
"fused-multiply-add (FMA) or dot-product (DP) execution units shared by "
"hyperthreading cores. With hyperthreading enabled, OpenMP threads will "
"contend for the same GEMM execution units."
msgstr ""
"回到原话题，大多数GEMM操作在使用非超线程时受益，因为深度学习训练或推理的绝大部分时间都在数百万次的GEMM操作上，这些操作运行于由超线程核心共享的融合乘加（FMA）或点积（DP）执行单元上。如果启用了超线程，OpenMP线程会争用同一GEMM执行单元。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And if 2 logical threads run GEMM at the same time, they will be sharing the"
" same core resources causing front end bound, such that the overhead from "
"this front end bound is greater than the gain from running both logical "
"threads at the same time."
msgstr "如果两个逻辑线程同时运行GEMM，它们会共享同一核心资源，导致前端受限，这种前端受限的开销会大于两个线程同时运行的收益。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Therefore we generally recommend avoiding using logical cores for deep "
"learning workloads to achieve good performance. The launch script by default"
" uses physical cores only; however, users can easily experiment with logical"
" vs. physical cores by simply toggling the ``--use_logical_core`` launch "
"script knob."
msgstr ""
"因此我们通常建议避免在深度学习工作负载中使用逻辑核心以实现良好的性能。启动脚本默认仅使用物理核心；但是，用户可以轻松地通过简单切换 "
"``--use_logical_core`` 启动脚本选项来试验逻辑核心与物理核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ":strong:`Exercise`"
msgstr ":strong:`练习`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We'll use the following example of feeding ResNet50 dummy tensor:"
msgstr "我们将使用以下ResNet50虚拟张量传输的示例："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Throughout the blog, we'll use `Intel® VTune™ Profiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-"
"profiler.html#gs.v4egjg>`_ to profile and verify optimizations. And we'll "
"run all exercises on a machine with two Intel(R) Xeon(R) Platinum 8180M "
"CPUs. The CPU information is shown in Figure 2.1."
msgstr ""
"在整个博客中，我们将使用 `Intel® VTune™ Profiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-"
"profiler.html#gs.v4egjg>`_ 剖析和验证优化。我们将在配备两个Intel(R) Xeon(R) Platinum 8180M "
"CPU的机器上进行所有练习。CPU信息显示在图2.1中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Environment variable ``OMP_NUM_THREADS`` is used to set the number of "
"threads for parallel region. We'll compare ``OMP_NUM_THREADS=2`` with (1) "
"use of logical cores and (2) use of physical cores only."
msgstr ""
"环境变量 ``OMP_NUM_THREADS`` 用于设置并行区域的线程数量。我们将比较 ``OMP_NUM_THREADS=2`` "
"的以下两种情况：(1) 使用逻辑核心 和 (2) 仅使用物理核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Both OpenMP threads trying to utilize the same GEMM execution units shared "
"by hyperthreading cores (0, 56)"
msgstr "两个OpenMP线程试图利用由超线程核心共享的同一GEMM执行单元（0,56）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can visualize this by running ``htop`` command on Linux as shown below."
msgstr "我们可以通过在Linux上运行 ``htop`` 命令进行可视化，如下所示。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We notice that the Spin Time is flagged, and Imbalance or Serial Spinning "
"contributed to the majority of it - 4.980 seconds out of the 8.982 seconds "
"total. The Imbalance or Serial Spinning when using logical cores is due to "
"insufficient concurrency of working threads as each logical thread contends "
"for the same core resources."
msgstr ""
"我们注意到自旋时间被标记出来，不平衡或串行自旋占多数——8.982秒总时间中的4.980秒。使用逻辑核心时，由于工作线程的并发性不足，每个逻辑线程争用同一核心资源，这导致不平衡或串行自旋。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The Top Hotspots section of the execution summary indicates that "
"``__kmp_fork_barrier`` took 4.589 seconds of CPU time - during 9.33% of the "
"CPU execution time, threads were just spinning at this barrier due to thread"
" synchronization."
msgstr ""
"执行摘要的热点部分表明 ``__kmp_fork_barrier`` "
"使用了4.589秒CPU时间——CPU执行时间的9.33%期间，线程仅在此障碍处自旋以同步线程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Each OpenMP thread utilizing GEMM execution units in respective physical "
"cores (0,1)"
msgstr "每个OpenMP线程在各自的物理核心（0,1）中利用GEMM执行单元"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We first note that the execution time dropped from 32 seconds to 23 seconds "
"by avoiding logical cores. While there's still some non-negligible Imbalance"
" or Serial Spinning, we note relative improvement from 4.980 seconds to "
"3.887 seconds."
msgstr ""
"我们首先注意到，通过避免逻辑核心，执行时间从32秒下降到23秒。尽管仍有一些不可忽略的不平衡或串行自旋，但我们注意到相对的改进，从4.980秒减少到3.887秒。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"By not using logical threads (instead, using 1 thread per physical core), we"
" avoid logical threads contending for the same core resources. The Top "
"Hotspots section also indicates relative improvement of "
"``__kmp_fork_barrier`` time from 4.589 seconds to 3.530 seconds."
msgstr ""
"通过不使用逻辑线程（而是每个物理核心使用1个线程），我们避免了逻辑线程争用同一核心资源。热点部分还表明 ``__kmp_f오k_barrier`` "
"时间从4.589秒相对改进到3.530秒。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Local memory access is always faster than remote memory access"
msgstr "本地内存访问总是比远程内存访问快"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We generally recommend binding a process to a local socket such that the "
"process does not migrate across sockets. Generally the goal of doing so is "
"to utilize high speed cache on local memory and to avoid remote memory "
"access which can be ~2x slower."
msgstr "我们通常建议将进程绑定到本地插槽，以确保进程不会在插槽之间迁移。这样做的目的是利用本地内存上的高速缓存，并避免~2倍缓慢的远程内存访问。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Figure 1. Two-socket configuration"
msgstr "图1. 双插槽配置"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 1. shows a typical two-socket configuration. Notice that each socket "
"has its own local memory. Sockets are connected to each other via Intel "
"Ultra Path Interconnect (UPI) which allows each socket to access the local "
"memory of another socket called remote memory. Local memory access is always"
" faster than remote memory access."
msgstr ""
"图1显示了典型的双插槽配置。注意，每个插槽都有自己的本地内存。插槽通过Intel超路径互连（UPI）相互连接，允许每个插槽访问另一个插槽的本地内存（称为远程内存）。本地内存访问总是比远程内存访问快。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Figure 2.1. CPU information"
msgstr "图2.1. CPU信息"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Users can get their CPU information by running ``lscpu`` command on their "
"Linux machine. Figure 2.1. shows an example of ``lscpu``  execution on a "
"machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. Notice that there are"
" 28 cores per socket, and 2 threads per core (i.e., hyperthreading is "
"enabled). In other words, there are 28 logical cores in addition to 28 "
"physical cores, giving a total of 56 cores per socket. And there are 2 "
"sockets, giving a total of 112 cores (``Thread(s) per core`` x ``Core(s) per"
" socket`` x ``Socket(s)``)."
msgstr ""
"用户可以通过在其Linux机上运行 ``lscpu`` 命令获得其CPU信息。图2.1显示了在具有两个Intel(R) Xeon(R) Platinum"
" 8180M CPU的机器上的``lscpu`` "
"执行示例。注意，每个插槽有28个核心，每个核心有2个线程（即启用了超线程）。换句话说，除了28个物理核心外，还有28个逻辑核心，总共有56个核心每插槽。而且有两个插槽，总共112个核心（``每个核心线程数``"
" x ``每插槽核心数`` x ``插槽数``）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Figure 2.2. CPU information"
msgstr "图2.2. CPU信息"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The 2 sockets are mapped to 2 NUMA nodes (NUMA node 0, NUMA node 1) "
"respectively.  Physical cores are indexed prior to logical cores. As shown "
"in Figure 2.2., the first 28 physical cores (0-27) and the first 28 logical "
"cores (56-83) on the first socket are on NUMA node 0. And the second 28 "
"physical cores (28-55) and the second 28 logical cores (84-111) on the "
"second socket are on NUMA node 1. Cores on the same socket share local "
"memory and last level cache (LLC) which is much faster than cross-socket "
"communication via Intel UPI."
msgstr ""
"这两个插槽分别映射到两个NUMA节点（NUMA节点0，NUMA节点1）。物理核心的索引优先于逻辑核心。如图2.2所示，第一个插槽上的前28个物理核心（0-27）和前28个逻辑核心（56-83）位于NUMA节点0。而第二个插槽上的后28个物理核心（28-55）和后28个逻辑核心（84-111）位于NUMA节点1。同一插槽上的核心共享本地内存和最后一级缓存（LLC），其速度明显快于通过Intel"
" UPI进行的跨插槽通信。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Now that we understand NUMA, cross-socket (UPI) traffic, local vs. remote "
"memory access in multi-processor systems, let's profile and verify our "
"understanding."
msgstr "如今我们已经了解了NUMA、多处理器系统中的跨插槽（UPI）流量、本地和远程内存访问的概念，让我们对其进行分析并验证我们的理解。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We'll reuse the ResNet50 example above."
msgstr "我们将重用上面的ResNet50示例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As we did not pin threads to processor cores of a specific socket, the "
"operating system periodically schedules threads on processor cores located "
"in different sockets."
msgstr "由于没有将线程绑定到特定插槽的处理器核心，操作系统会定期安排线程在不同插槽的处理器核心上运行。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 3. CPU usage of non NUMA-aware application. 1 main worker thread was "
"launched, then it launched a physical core number (56) of threads on all "
"cores, including logical cores."
msgstr "图3：非NUMA感知型应用程序的CPU使用情况。启动了1个主工作线程，然后在所有核心（包括逻辑核心）上启动了等于物理核心数（56）的线程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"(Aside: If the number of threads is not set by `torch.set_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.set_num_threads.html>`_, "
"the default number of threads is the number of physical cores in a "
"hyperthreading enabled system. This can be verified by "
"`torch.get_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.get_num_threads.html>`_. "
"Hence we see above about half of the cores busy running the example script.)"
msgstr ""
"（旁注：如果未通过`torch.set_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.set_num_threads.html>`_设置线程数量，则默认线程数量是启用了超线程系统中物理核心的数量。这可以通过`torch.get_num_threads"
" "
"<https://pytorch.org/docs/stable/generated/torch.get_num_threads.html>`_验证。因此，我们在上面的示例脚本中看到大约一半的核心处于忙碌状态。）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Figure 4. Non-Uniform Memory Access Analysis graph"
msgstr "图4：非统一内存访问分析图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Figure 4. compares local vs. remote memory access over time. We verify usage"
" of remote memory which could result in sub-optimal performance."
msgstr "图4比较了本地内存访问和远程内存访问的时间变化。我们验证了远程内存的使用，这可能导致性能次优。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":strong:`Set thread affinity to reduce remote memory access and cross-socket"
" (UPI) traffic`"
msgstr ":strong:`设置线程亲和性以减少远程内存访问和跨插槽（UPI）流量`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Pinning threads to cores on the same socket helps maintain locality of "
"memory access. In this example, we'll pin to the physical cores on the first"
" NUMA node (0-27). With the launch script, users can easily experiment with "
"NUMA nodes configuration by simply toggling the ``--node_id`` launch script "
"knob."
msgstr ""
"将线程绑定到同一插槽上的核心可以帮助保持内存访问的本地性。在本例中，我们将线程绑定到第一个NUMA节点（0-27）的物理核心上。通过启动脚本，用户可以轻松地通过切换``--node_id``脚本选项调整NUMA节点配置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's visualize the CPU usage now."
msgstr "现在让我们可视化CPU使用情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Figure 5. CPU usage of NUMA-aware application"
msgstr "图5：NUMA感知应用程序的CPU使用情况"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"1 main worker thread was launched, then it launched threads on all physical "
"cores on the first numa node."
msgstr "启动了1个主工作线程，然后在第一个NUMA节点上的所有物理核心上启动了线程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Figure 6. Non-Uniform Memory Access Analysis graph"
msgstr "图6：非统一内存访问分析图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As shown in Figure 6., now almost all memory accesses are local accesses."
msgstr "如图6所示，现在几乎所有的内存访问都是本地访问。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Efficient CPU usage with core pinning for multi-worker inference"
msgstr "通过核心绑定实现多工作者推理的高效CPU使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When running multi-worker inference, cores are overlapped (or shared) "
"between workers causing inefficient CPU usage. To address this problem, the "
"launch script equally divides the number of available cores by the number of"
" workers such that each worker is pinned to assigned cores during runtime."
msgstr ""
"运行多工作者推理时，工作者之间核心会重叠（或共享），导致CPU使用效率低下。为解决此问题，启动脚本将可用核心的数量平均分配给工作者，并将其绑定到指定核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ":strong:`Exercise with TorchServe`"
msgstr ":strong:`使用TorchServe的练习`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For this exercise, let's apply the CPU performance tuning principles and "
"recommendations that we have discussed so far to `TorchServe apache-bench "
"benchmarking "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-"
"apache-bench>`_."
msgstr ""
"在此练习中，让我们将之前讨论的CPU性能调优原则和建议应用于`TorchServe apache-bench性能测试 "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-"
"apache-bench>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll use ResNet50 with 4 workers, concurrency 100, requests 10,000. All "
"other parameters (e.g., batch_size, input, etc) are the same as the `default"
" parameters "
"<https://github.com/pytorch/serve/blob/master/benchmarks/benchmark-"
"ab.py#L18>`_."
msgstr ""
"我们将使用ResNet50与4个工作者，100的并发性，10,000的请求。所有其他参数（例如batch_size、输入等）与`默认参数 "
"<https://github.com/pytorch/serve/blob/master/benchmarks/benchmark-"
"ab.py#L18>`_相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We'll compare the following three configurations:"
msgstr "我们将比较以下三种配置："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "default TorchServe setting (no core pinning)"
msgstr "默认TorchServe设置（无核心绑定）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`torch.set_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.set_num_threads.html>`_ = "
"``number of physical cores / number of workers`` (no core pinning)"
msgstr ""
"`torch.set_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.set_num_threads.html>`_ = "
"``物理核心数量 / 工作者数量``（无核心绑定）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "core pinning via the launch script (Required Torchserve>=0.6.1)"
msgstr "通过启动脚本实现核心绑定（需要Torchserve>=0.6.1）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"After this exercise, we'll have verified that we prefer avoiding logical "
"cores and prefer local memory access via core pinning with a real TorchServe"
" use case."
msgstr "在此练习结束时，我们将验证我们更倾向于避开逻辑核心，并通过核心绑定实现本地内存访问的实际TorchServe用例。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1. Default TorchServe setting (no core pinning)"
msgstr "1. 默认TorchServe设置（无核心绑定）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The `base_handler "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py>`_"
" doesn't explicitly set `torch.set_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.set_num_threads.html>`_. "
"Hence the default number of threads is the number of physical CPU cores as "
"described `here "
"<https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#runtime-"
"api>`_. Users can check the number of threads by `torch.get_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.get_num_threads.html>`_ in "
"the base_handler. Each of the 4 main worker threads launches a physical core"
" number (56) of threads, launching a total of 56x4 = 224 threads, which is "
"more than the total number of cores 112.  Therefore cores are guaranteed to "
"be heavily overlapped with high logical core utilization- multiple workers "
"using multiple cores at the same time. Furthermore, because threads are not "
"affinitized to specific CPU cores, the operating system periodically "
"schedules threads to cores located in different sockets."
msgstr ""
"`base_handler "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py>`_不会显式设置`torch.set_num_threads"
" "
"<https://pytorch.org/docs/stable/generated/torch.set_num_threads.html>`_。因此，默认线程数量是物理CPU核心的数量，如`此处"
" "
"<https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#runtime-"
"api>`_所述。用户可以通过base_handler中的`torch.get_num_threads "
"<https://pytorch.org/docs/stable/generated/torch.get_num_threads.html>`_检查线程数量。每个4个主工作线程启动了等于物理核心数（56）的线程，总共启动了56x4"
" = "
"224个线程，超过了核心总数112。因此，核心重叠程度很高，逻辑核心使用率也很高——多个工作者同时使用多个核心。此外，由于线程未绑定到指定的CPU核心，操作系统会定期将线程调度到不同插槽的核心上。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "CPU usage"
msgstr "CPU使用"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"4 main worker threads were launched, then each launched a physical core "
"number (56) of threads on all cores, including logical cores."
msgstr "启动了4个主工作线程，然后每个线程在所有核心（包括逻辑核心）上启动了等于物理核心数（56）的线程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Core Bound stalls"
msgstr "核心受限停滞"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We observe a very high Core Bound stall of 88.4%, decreasing pipeline "
"efficiency. Core Bound stalls indicate sub-optimal use of available "
"execution units in the CPU. For example, several GEMM instructions in a row "
"competing for fused-multiply-add (FMA) or dot-product (DP) execution units "
"shared by hyperthreading cores could cause Core Bound stalls. And as "
"described in the previous section, use of logical cores amplifies this "
"problem."
msgstr ""
"我们观察到非常高的核心受限停滞率为88.4%，导致流水线效率降低。核心受限停滞指示CPU中可用执行单元的次优使用。例如，一系列连续的GEMM指令在超线程核心共享的融合乘加（FMA）或点积（DP）执行单元上竞争可能导致核心受限停滞。正如前一节所述，使用逻辑核心会放大这个问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"An empty pipeline slot not filled with micro-ops (uOps) is attributed to a "
"stall. For example, without core pinning CPU usage may not effectively be on"
" compute but on other operations like thread scheduling from Linux kernel. "
"We see above that ``__sched_yield`` contributed to the majority of the Spin "
"Time."
msgstr ""
"一个没有填充微操作（uOps）的流水线插槽会被归因于停滞。例如，没有核心绑定的情况下，CPU使用可能不会有效地用于计算，而是用于其他操作，例如线程从Linux内核中的计划调度。我们在上面看到``__sched_yield``占用了大部分的旋转时间。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Thread Migration"
msgstr "线程迁移"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Without core pinning, scheduler may migrate thread executing on a core to a "
"different core. Thread migration can disassociate the thread from data that "
"has already been fetched into the caches resulting in longer data access "
"latencies. This problem is exacerbated in NUMA systems when thread migrates "
"across sockets. Data that has been fetched to high speed cache on local "
"memory now becomes remote memory, which is much slower."
msgstr ""
"没有核心绑定时，调度器可能会将一个正在核心上执行的线程迁移到另一个核心。线程迁移会导致线程与已经提取到缓存中的数据解除关联，从而导致更长的数据访问延迟。在NUMA系统中，当线程跨插槽迁移时，这个问题会加剧。已经提取到本地内存高速缓存中的数据现在成为远程内存，速度大大降低。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Generally the total number of threads should be less than or equal to the "
"total number of threads supported by the core. In the above example, we "
"notice a large number of threads executing on core_51 instead of the "
"expected 2 threads (since hyperthreading is enabled in Intel(R) Xeon(R) "
"Platinum 8180 CPUs) . This indicates thread migration."
msgstr ""
"通常，总线程数应小于或等于核心支持的总线程数。在上述示例中，我们注意到大量线程在core_51上执行，而不是预期的2个线程（因为Intel(R) "
"Xeon(R) Platinum 8180 CPU中启用了超线程）。这表明线程迁移。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Additionally, notice that thread (TID:97097) was executing on a large number"
" of CPU cores, indicating CPU migration. For example, this thread was "
"executing on cpu_81, then migrated to cpu_14, then migrated to cpu_5, and so"
" on. Furthermore, note that this thread migrated cross socket back and forth"
" many times, resulting in very inefficient memory access. For example, this "
"thread executed on cpu_70 (NUMA node 0), then migrated to cpu_100 (NUMA node"
" 1), then migrated to cpu_24 (NUMA node 0)."
msgstr ""
"此外，注意线程（TID:97097）在大量CPU核心上执行，表明CPU迁移。例如，该线程先在cpu_81上执行，然后迁移到cpu_14，再迁移到cpu_5，如此往复。此外，请注意该线程多次跨插槽迁移，导致非常低效的内存访问。例如，该线程在cpu_70（NUMA节点0）上执行，然后迁移到cpu_100（NUMA节点1），再迁移到cpu_24（NUMA节点0）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Non Uniform Memory Access Analysis"
msgstr "非统一内存访问分析"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Compare local vs. remote memory access over time. We observe that about "
"half, 51.09%, of the memory accesses were remote accesses, indicating sub-"
"optimal NUMA configuration."
msgstr "比较本地和远程内存访问随时间的变化。我们观察到大约一半（51.09%）的内存访问是远程访问，这表明NUMA配置次优。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"2. torch.set_num_threads = ``number of physical cores / number of workers`` "
"(no core pinning)"
msgstr "2. torch.set_num_threads = ``物理核心数 / 工作者数``（无核心绑定）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For an apple-to-apple comparison with launcher's core pinning, we'll set the"
" number of threads to the number of cores divided by the number of workers "
"(launcher does this internally). Add the following code snippet in the "
"`base_handler "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py>`_:"
msgstr ""
"为了和启动器的核心绑定进行公平比较，我们将线程数设置为核心数除以工作者数（启动器内部会执行此操作）。在`base_handler "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py>`_中添加以下代码片段："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As before without core pinning, these threads are not affinitized to "
"specific CPU cores, causing the operating system to periodically schedule "
"threads on cores located in different sockets."
msgstr "和之前一样，没有核心绑定，所以这些线程未绑定到指定的CPU核心，导致操作系统会定期将线程调度到不同插槽的核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"4 main worker threads were launched, then each launched a "
"``num_physical_cores/num_workers`` number (14) of threads on all cores, "
"including logical cores."
msgstr "启动了4个主工作线程，然后每个线程在所有核心（包括逻辑核心）上启动了``物理核心数/工作者数``（14）的线程。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Although the percentage of Core Bound stalls has decreased from 88.4% to "
"73.5%, the Core Bound is still very high."
msgstr "虽然核心受限停滞比例从88.4%下降到73.5%，但核心受限比例仍然很高。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Similar as before, without core pinning thread (TID:94290) was executing on "
"a large number of CPU cores, indicating CPU migration. We notice again "
"cross-socket thread migration, resulting in very inefficient memory access. "
"For example, this thread executed on cpu_78 (NUMA node 0), then migrated to "
"cpu_108 (NUMA node 1)."
msgstr ""
"与之前类似，没有核心绑定时，线程（TID:94290）在大量CPU核心上执行，表明CPU迁移。我们再次注意到跨插槽线程迁移，导致非常低效的内存访问。例如，该线程首先在cpu_78（NUMA节点0）上执行，然后迁移到cpu_108（NUMA节点1）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Although an improvement from the original 51.09%, still 40.45% of memory "
"access is remote, indicating sub-optimal NUMA configuration."
msgstr "尽管比最初的51.09%有了改进，但仍然有40.45%的内存访问是远程访问，表明NUMA配置次优。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "3. launcher core pinning"
msgstr "3. 启动器核心绑定"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Launcher will internally equally distribute physical cores to workers, and "
"bind them to each worker. As a reminder, launcher by default uses physical "
"cores only. In this example, launcher will bind worker 0 to cores 0-13 (NUMA"
" node 0), worker 1 to cores 14-27 (NUMA node 0), worker 2 to cores 28-41 "
"(NUMA node 1), and worker 3 to cores 42-55 (NUMA node 1). Doing so ensures "
"that cores are not overlapped among workers and avoids logical core usage."
msgstr ""
"启动器将在内部将物理核心平均分配给工作者，并绑定它们到每个工作者。提醒一下，启动器默认只使用物理核心。在本示例中，启动器将把工作者0绑定到核心0-13（NUMA节点0），工作者1绑定到核心14-27（NUMA节点0），工作者2绑定到核心28-41（NUMA节点1），工作者3绑定到核心42-55（NUMA节点1）。这样可以确保工作者之间的核心不重叠，并避免使用逻辑核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"4 main worker threads were launched, then each launched a "
"``num_physical_cores/num_workers`` number (14) of threads affinitized to the"
" assigned physical cores."
msgstr "启动了4个主工作线程，然后每个线程以``物理核心数/工作者数``（14）的线程数绑定到分配的物理核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Core Bound stalls has decreased significantly from the original 88.4% to "
"46.2% - almost a 2x improvement."
msgstr "核心受限停滞比例显著降低，从最初的88.4%下降到46.2%，几乎提高了2倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We verify that with core binding, most CPU time is effectively used on "
"compute - Spin Time of 0.256s."
msgstr "我们验证了通过核心绑定，大多数CPU时间被有效地用于计算——旋转时间仅为0.256秒。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We verify that `OMP Primary Thread #0` was bound to assigned physical cores "
"(42-55), and did not migrate cross-socket."
msgstr "我们验证了`OMP主线程#0`绑定到指定的物理核心（42-55），并且没有跨插槽迁移。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Now almost all, 89.52%, memory accesses are local accesses."
msgstr "现在几乎所有的内存访问（89.52%）都是本地访问。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this blog, we've showcased that properly setting your CPU runtime "
"configuration can significantly boost out-of-box CPU performance."
msgstr "在这篇博客中，我们展示了正确设置CPU运行时配置如何显著提升开箱即用的CPU性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have walked through some general CPU performance tuning principles and "
"recommendations:"
msgstr "我们讨论了一些通用的CPU性能调优原则和建议："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In a hyperthreading enabled system, avoid logical cores by setting thread "
"affinity to physical cores only via core pinning."
msgstr "在启用了超线程的系统中，通过核心绑定仅将线程设置为物理核心，避免使用逻辑核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In a multi-socket system with NUMA, avoid cross-socket remote memory access "
"by setting thread affinity to a specific socket via core pinning."
msgstr "在具有NUMA的多插槽系统中，通过核心绑定将线程限制在特定插槽内，避免跨插槽的远程内存访问。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We have visually explained these ideas from first principles and have "
"verified the performance boost with profiling. And finally, we have applied "
"all of our learnings to TorchServe to boost out-of-box TorchServe CPU "
"performance."
msgstr ""
"我们从第一原则出发直观地解释了这些理念，并通过性能分析验证了性能提升。最后，我们将所有学习成果应用于TorchServe，大幅提升了开箱即用的TorchServe"
" CPU性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"These principles can be automatically configured via an easy to use launch "
"script which has already been integrated into TorchServe."
msgstr "这些原则可以通过一个易于使用的启动脚本自动配置，该脚本已集成到TorchServe中。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For interested readers, please check out the following documents:"
msgstr "对于感兴趣的读者，请查看以下文档："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`CPU specific optimizations "
"<https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-"
"specific-optimizations>`_"
msgstr ""
"`CPU 特定优化 "
"<https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-"
"specific-optimizations>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Maximize Performance of Intel® Software Optimization for PyTorch* on CPU "
"<https://www.intel.com/content/www/us/en/developer/articles/technical/how-"
"to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html>`_"
msgstr ""
"`最大化 Intel® 软件优化 PyTorch* 在 CPU 上性能 "
"<https://www.intel.com/content/www/us/en/developer/articles/technical/how-"
"to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Performance Tuning Guide <https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html>`_"
msgstr ""
"`性能调优指南 <https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Launch Script Usage Guide <https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/launch_script.html>`_"
msgstr ""
"`启动脚本使用指南 <https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/launch_script.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Top-down Microarchitecture Analysis Method "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"cookbook/top/methodologies/top-down-microarchitecture-analysis-"
"method.html>`_"
msgstr ""
"`自上而下微架构分析方法 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"cookbook/top/methodologies/top-down-microarchitecture-analysis-"
"method.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Configuring oneDNN for Benchmarking <https://oneapi-"
"src.github.io/oneDNN/dev_guide_performance_settings.html#benchmarking-"
"settings>`_"
msgstr ""
"`为基准测试配置 oneDNN <https://oneapi-"
"src.github.io/oneDNN/dev_guide_performance_settings.html#benchmarking-"
"settings>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Intel® VTune™ Profiler "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-"
"profiler.html#gs.tcbgpa>`_"
msgstr ""
"`Intel® VTune™ 分析器 "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-"
"profiler.html#gs.tcbgpa>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Intel® VTune™ Profiler User Guide "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top.html>`_"
msgstr ""
"`Intel® VTune™ 分析器用户指南 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"And stay tuned for a follow-up posts on optimized kernels on CPU via `Intel®"
" Extension for PyTorch* <https://github.com/intel/intel-extension-for-"
"pytorch>`_ and advanced launcher configurations such as memory allocator."
msgstr ""
"敬请关注后续博客，关于通过 `Intel® Extension for PyTorch* "
"<https://github.com/intel/intel-extension-for-pytorch>`_ 在 CPU "
"上优化内核和高级启动器配置，例如内存分配器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Acknowledgement"
msgstr "致谢"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their "
"immense guidance and support, and thorough feedback and reviews throughout "
"many steps of this blog. We would also like to thank Hamid Shojanazeri "
"(Meta), Li Ning (AWS) and Jing Xu (Intel) for helpful feedback in code "
"review. And Suraj Subramanian (Meta) and Geeta Chauhan (Meta) for helpful "
"feedback on the blog."
msgstr ""
"我们感谢 Ashok Emani（Intel）和 Jiong "
"Gong（Intel）在博客的多个步骤中提供了巨大的指导和支持，以及详细的反馈和审查。我们还感谢 Hamid Shojanazeri（Meta）、Li "
"Ning（AWS）和 Jing Xu（Intel）在代码审查中的有益反馈。以及 Suraj Subramanian（Meta）和 Geeta "
"Chauhan（Meta）对博客的有益反馈。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Grokking PyTorch Intel CPU performance from first principles (Part 2)"
msgstr "从基本原理理解 PyTorch Intel CPU 性能（第 2 部分）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Authors: `Min Jean Cho <https://github.com/min-jean-cho>`_, `Jing Xu "
"<https://github.com/jingxu10>`_, `Mark Saroufim "
"<https://github.com/msaroufim>`_"
msgstr ""
"作者：`Min Jean Cho <https://github.com/min-jean-cho>`_, `Jing Xu "
"<https://github.com/jingxu10>`_, `Mark Saroufim "
"<https://github.com/msaroufim>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In the `Grokking PyTorch Intel CPU Performance From First Principles "
"<https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html>`_ "
"tutorial , we have introduced how to tune CPU runtime configurations, how to"
" profile them, and how to integrate them into `TorchServe "
"<https://github.com/pytorch/serve>`_ for optimized CPU performance."
msgstr ""
"在 `从基本原理理解 PyTorch Intel CPU 性能 "
"<https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html>`_ "
"教程中，我们介绍了如何调整 CPU 运行时配置、如何进行性能分析，以及如何将它们集成到 `TorchServe "
"<https://github.com/pytorch/serve>`_ 中以优化 CPU 性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will demonstrate boosting performance with memory "
"allocator via the `Intel® Extension for PyTorch* Launcher "
"<https://github.com/intel/intel-extension-for-"
"pytorch/blob/master/docs/tutorials/performance_tuning/launch_script.md>`_ , "
"and optimized kernels on CPU via `Intel® Extension for PyTorch* "
"<https://github.com/intel/intel-extension-for-pytorch>`_ , and apply them to"
" TorchServe showcasing 7.71x throughput speedup for ResNet50 and 2.20x "
"throughput speedup for BERT."
msgstr ""
"在本教程中，我们将演示如何使用更高效的内存分配器通过 `Intel® Extension for PyTorch* 启动器 "
"<https://github.com/intel/intel-extension-for-"
"pytorch/blob/master/docs/tutorials/performance_tuning/launch_script.md>`_ "
"提升性能，并通过 `Intel® Extension for PyTorch* <https://github.com/intel/intel-"
"extension-for-pytorch>`_ 优化 CPU 内核，然后将其应用于 TorchServe 展示 ResNet50 的 7.71 "
"倍吞吐量加速和 BERT 的 2.20 倍吞吐量加速。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Throughout this tutorial, we will use `Top-down Microarchitecture Analysis "
"(TMA) <https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"cookbook/top/methodologies/top-down-microarchitecture-analysis-"
"method.html>`_ to profile and show that the Back End Bound (Memory Bound, "
"Core Bound) is often the primary bottleneck for under-optimized or under-"
"tuned deep learning workloads, and demonstrate optimization techniques via "
"Intel® Extension for PyTorch* for improving Back End Bound. We will use  "
"`toplev <https://github.com/andikleen/pmu-tools/wiki/toplev-manual>`_, a "
"tool part of `pmu-tools <https://github.com/andikleen/pmu-tools>`_ built on "
"top of `Linux perf <https://man7.org/linux/man-pages/man1/perf.1.html>`_, "
"for TMA."
msgstr ""
"在本教程中，我们将使用 `自上而下微架构分析（TMA） "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"cookbook/top/methodologies/top-down-microarchitecture-analysis-"
"method.html>`_ 进行性能分析并展示后端瓶颈（Memory Bound，Core "
"Bound）通常是未优化或未调优的深度学习工作负载的主要瓶颈，并演示通过 Intel® Extension for PyTorch* "
"优化后端瓶颈的技术。我们将使用 `toplev <https://github.com/andikleen/pmu-tools/wiki/toplev-"
"manual>`_（`pmu-tools <https://github.com/andikleen/pmu-tools>`_ 中的工具）以及基于 "
"`Linux perf <https://man7.org/linux/man-pages/man1/perf.1.html>`_ 创建的 TMA "
"工具。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will also use `Intel® VTune™ Profiler's Instrumentation and Tracing "
"Technology (ITT) <https://github.com/pytorch/pytorch/issues/41001>`__ to "
"profile at finer granularity."
msgstr ""
"我们还将使用 `Intel® VTune™ 分析器的仪器化和追踪技术 (ITT) "
"<https://github.com/pytorch/pytorch/issues/41001>`__ 进行更细粒度的性能分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Top-down Microarchitecture Analysis Method (TMA)"
msgstr "自上而下微架构分析方法 (TMA)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When tuning CPU for optimal performance, it's useful to know where the "
"bottleneck is. Most CPU cores have on-chip Performance Monitoring Units "
"(PMUs). PMUs are dedicated pieces of logic within a CPU core that count "
"specific hardware events as they occur on the system. Examples of these "
"events may be Cache Misses or Branch Mispredictions. PMUs are used for Top-"
"down Microarchitecture Analysis (TMA) to identify the bottlenecks. TMA "
"consists of hierarchical levels as shown:"
msgstr ""
"在调优 CPU 以获得最佳性能时，了解瓶颈位置非常有用。大多数 CPU 核心都具有芯片上的性能监测单元 (PMU)。PMU 是 CPU "
"核心内专门的逻辑在系统发生硬件事件时进行计数。示例事件包括缓存未命中或分支预测错误。PMU 用于自上而下微架构分析 (TMA) 以识别瓶颈。TMA "
"包括分层级别，如所示："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The top level, level-1, metrics collect *Retiring*, *Bad Speculation*, "
"*Front End Bound*, *Back End Bound*. The pipeline of CPU can conceptually be"
" simplified and divided into two: the frontend and the backend. The "
"*frontend* is responsible for fetching the program code and decoding them "
"into low-level hardware operations called micro-ops (uOps). The uOps are "
"then fed to the *backend* in a process called allocation. Once allocated, "
"the backend is responsible for executing the uOp in an available execution "
"unit. A completion of uOp's execution is called *retirement*. In contrast, a"
" *bad speculation* is when speculatively fetched uOps are canceled before "
"retiring such as in the case of mispredicted branches. Each of these metrics"
" can further be broken down in the subsequent levels to pinpoint the "
"bottleneck."
msgstr ""
"第一级指标收集 *退休操作 (Retiring)*、*错误推测 (Bad Speculation)*、*前端瓶颈 (Front End Bound)* "
"和 *后端瓶颈 (Back End Bound)*。CPU 的管道可以从概念上简化为两部分：前端和后端。*前端* "
"负责获取程序代码并将其解码为低级硬件操作（称为微操作或者 uOps）。然后将 uOps 在一个分配过程被发送到 "
"*后端*。分配后，后端负责在可用执行单元执行 uOp。完成 uOp 的执行称为 *退休 (Retirement)*。而 *错误推测* "
"是指在退休前取消预测性获取的 uOps。例如分支预测错误的情况。每个指标可以进一步分解到后续级别以找到瓶颈的位置。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Tune for the Back End Bound"
msgstr "针对后端瓶颈调优"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The majority of untuned deep learning workloads will be Back End Bound. "
"Resolving Back End bound is often resolving sources of latency causing "
"retirement to take longer than necessary. As shown above, Back End Bound has"
" two sub-metrics – Core Bound and Memory Bound."
msgstr ""
"大多数未调优的深度学习工作负载都会受到后端瓶颈的影响。解决后端瓶颈通常意味着解决导致退休操作比必要时间更长的延迟原因。如上所述，后端瓶颈有两个子指标—核心瓶颈"
" (Core Bound) 和内存瓶颈 (Memory Bound)。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Memory Bound stalls have causes related to the memory subsystem. For "
"example, last-level cache (LLC or L3 cache) miss causing access to DRAM. "
"Scaling deep learning models often requires significant compute. And high "
"compute utilization requires that data is available when the execution units"
" need it to execute the uOps. This requires prefetching the data and reusing"
" the data in cache instead of fetching that same data multiple times from "
"main memory which causes execution units to be starved while data is being "
"returned. Throughout this tutorial, we wll show that a more efficient memory"
" allocator, operator fusion, memory layout format optimization reduce "
"overhead on Memory Bound with better cache locality."
msgstr ""
"内存瓶颈的停滞通常与内存子系统有关。例如，末级缓存 (LLC 或 L3 缓存) 未命中导致访问 "
"DRAM。扩展深度学习模型通常需要显著的计算能力。而高计算利用率要求在执行单元需要执行 uOps "
"时数据是可用的。这需要提前获取数据并重复使用缓存中的数据，而不是从主内存多次获取相同的数据，这会导致数据在返回时执行单元被饿死。整个教程中，我们将展示更高效的内存分配器、操作符融合和内存布局格式优化，如何通过缓存局部性减少内存瓶颈的开销。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Core Bound stalls indicate sub-optimal use of available execution units "
"while there are no uncompleted memory accesses. For example, several general"
" matrix-matrix multiplication (GEMM) instructions in a row competing for "
"fused-multiply-add (FMA) or dot-product (DP) execution units could cause "
"Core Bound stalls. Key deep learning kernels, including the DP kernels, have"
" been well optimized by `oneDNN library <https://github.com/oneapi-"
"src/oneDNN>`_ (oneAPI Deep Neural Network Library), reducing overhead on "
"Core Bound."
msgstr ""
"核心瓶颈指示在没有未完成的内存访问时对可用执行单元的使用不优化。例如，一组连续的通用矩阵乘法 (GEMM) 指令竞争融合乘加 (FMA) 或点积 "
"(DP) 执行单元可能会导致核心瓶颈。关键的深度学习内核，包括 DP 内核，已通过 `oneDNN 库 "
"<https://github.com/oneapi-src/oneDNN>`_（oneAPI 深度神经网络库）进行了优化，从而减少核心瓶颈的开销。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Operations like GEMM, convolution, deconvolution are compute-intensive. "
"While operations like pooling, batch normalization, activation functions "
"like ReLU are memory-bound."
msgstr "诸如 GEMM、卷积、反卷积等操作是计算密集型的。而诸如池化、批归一化、ReLU 等激活函数则是内存密集型的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Intel® VTune™ Profiler's Instrumentation and Tracing Technology (ITT)"
msgstr "Intel® VTune™ 分析器的仪器化和追踪技术 (ITT)"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The ITT APIs of Intel® VTune Profiler is a useful tool to annotate a region "
"of your workload for tracing to profile and visualize at a finer granularity"
" of your annotation – OP/function/sub-function granularity. By annotating at"
" the granularity of your PyTorch model's OPs, Intel® VTune Profiler's ITT "
"enables op-level profiling. Intel® VTune Profiler's ITT has been integrated "
"into `PyTorch Autograd Profiler "
"<https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#autograd-"
"profiler>`_. :superscript:`1`"
msgstr ""
"Intel® VTune 分析器的 ITT API 是一个有用的工具，可以标注工作负载中的区域，进行追踪以在更细粒度的标注级别—操作 "
"(OP)/函数/子函数粒度进行可视化和分析。通过在 PyTorch 模型操作级别进行标注，Intel® VTune 分析器的 ITT "
"支持操作级分析。Intel® VTune 分析器的 ITT 已集成到 `PyTorch Autograd 分析器 "
"<https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#autograd-"
"profiler>`_ 中。 :superscript:`1`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The feature has to be explicitly enabled by *with "
"torch.autograd.profiler.emit_itt()*."
msgstr "该功能需要显式启用，通过 *with torch.autograd.profiler.emit_itt()*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TorchServe with Intel® Extension for PyTorch*"
msgstr "结合 Intel® Extension for PyTorch* 使用 TorchServe"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Intel® Extension for PyTorch* <https://github.com/intel/intel-extension-"
"for-pytorch>`__ is a Python package to extend PyTorch with optimizations for"
" extra performance boost on Intel hardware."
msgstr ""
"`Intel® Extension for PyTorch* <https://github.com/intel/intel-extension-"
"for-pytorch>`__ 是一个扩展 PyTorch 的 Python 包，针对 Intel 硬件进行性能优化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Intel® Extension for PyTorch* has already been integrated into TorchServe to"
" improve the performance out-of-box. :superscript:`2` For custom handler "
"scripts, we recommend adding the *intel_extension_for_pytorch* package in."
msgstr ""
"Intel® Extension for PyTorch* 已被集成到 TorchServe 中，实现开箱即用的性能提升。 "
":superscript:`2` 对于定制的处理器脚本，我们建议添加 *intel_extension_for_pytorch* 包。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The feature has to be explicitly enabled by setting *ipex_enable=true* in "
"*config.properties*."
msgstr "该功能需要在 *config.properties* 中显式启用，通过设置 *ipex_enable=true*。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Throughout this section, we will show that Back End Bound is often the "
"primary bottleneck for under-optimized or under-tuned deep learning "
"workloads, and demonstrate optimization techniques via Intel® Extension for "
"PyTorch* for improving Back End Bound, which has two submetrics - Memory "
"Bound, and Core Bound. A more efficient memory allocator, operator fusion, "
"memory layout format optimization improve Memory Bound. Ideally, Memory "
"Bound can be improved to Core Bound by optimized operators and better cache "
"locality. And key deep learning primitives, such as convolution, matrix "
"multiplication, dot-product, have been well optimized by Intel® Extension "
"for PyTorch* and oneDNN library, improving Core Bound."
msgstr ""
"在本节中，我们将展示后端瓶颈通常是未优化或未调优的深度学习工作负载的主要瓶颈，并通过 Intel® Extension for PyTorch* "
"展示优化后端瓶颈的技术，这包括两个子指标—内存瓶颈和核心瓶颈。一个更高效的内存分配器、操作符融合、内存布局格式优化能够改善内存瓶颈。理想情况下，内存瓶颈可以通过优化操作符和更好的缓存局部性改善为核心瓶颈。而关键的深度学习主要操作，如卷积、矩阵乘法、点积，已通过"
" Intel® Extension for PyTorch* 和 oneDNN 库进行了充分优化，从而改善核心瓶颈。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Leveraging Advanced Launcher Configuration: Memory Allocator"
msgstr "利用高级启动器配置：内存分配器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Memory allocator plays an important role from performance perspective. A "
"more efficient memory usage reduces overhead on unnecessary memory "
"allocations or destructions, and thus faster execution. For deep learning "
"workloads in practice, especially those running on large multi-core systems "
"or servers like TorchServe, TCMalloc, or JeMalloc can generally get better "
"memory usage than the default PyTorch memory allocator, PTMalloc."
msgstr ""
"从性能角度来看，内存分配器起着重要作用。更高效的内存使用减少了不必要的内存分配或销毁的开销，从而更快地执行。对于实践中的深度学习任务，特别是运行在大型多核系统或服务器（如"
" TorchServe）上的任务，与默认 PyTorch 内存分配器 PTMalloc 相比，TCMalloc 或 JeMalloc "
"通常能够获得更好的内存使用效果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TCMalloc, JeMalloc, PTMalloc"
msgstr "TCMalloc、JeMalloc、PTMalloc"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Both TCMalloc and JeMalloc use thread-local caches to reduce overhead on "
"thread synchronization, and lock contention by using spinlocks and per-"
"thread arenas respectively. TCMalloc and JeMalloc reduce overhead on "
"unnecessary memory allocation and deallocation. Both allocators categorize "
"memory allocations by sizes to reduce overhead on memory fragmentation."
msgstr ""
"TCMalloc 和 JeMalloc 都使用线程本地缓存，通过使用自旋锁和线程专属内存池分别减少线程同步和锁竞争的开销。TCMalloc 和 "
"JeMalloc 减少了不必要的内存分配和释放的开销。两种分配器都通过对内存分配按大小分类来减少内存碎片的开销。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"With the launcher, users can easily experiment with different memory "
"allocators by choosing one of the three launcher knobs *--enable_tcmalloc* "
"(TCMalloc), *--enable_jemalloc* (JeMalloc), *--use_default_allocator* "
"(PTMalloc)."
msgstr ""
"用户可以通过启动器轻松尝试不同的内存分配器，选择三个启动器选项之一 "
"*--enable_tcmalloc*（TCMalloc）、*--enable_jemalloc*（JeMalloc）、*--use_default_allocator*（PTMalloc）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Exercise"
msgstr "练习"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's profile PTMalloc vs. JeMalloc."
msgstr "让我们分析 PTMalloc 与 JeMalloc 的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will use the launcher to designate the memory allocator, and to bind the "
"workload to physical cores of the first socket to avoid any NUMA "
"complication – to profile the effect of memory allocator only."
msgstr "我们将使用启动器指定内存分配器，并将工作负载绑定到第一个插槽的物理核心，以避免任何 NUMA 相关问题—仅分析内存分配器的效果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The following example measures the average inference time of ResNet50:"
msgstr "以下示例测量了 ResNet50 的平均推理时间："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's collect level-1 TMA metrics."
msgstr "让我们收集一级 TMA 指标。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Level-1 TMA shows that both PTMalloc and JeMalloc are bounded by the "
"backend. More than half of the execution time was stalled by the backend. "
"Let's go one level deeper."
msgstr "一级 TMA 显示，PTMalloc 和 JeMalloc 都受到后端的限制。超过一半的执行时间被后端阻塞了。让我们深入一级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Level-2 TMA shows that the Back End Bound was caused by Memory Bound. Let's "
"go one level deeper."
msgstr "二级 TMA 显示，后端限制是由内存限制引起的。让我们深入一级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Most of the metrics under the Memory Bound identify which level of the "
"memory hierarchy from the L1 cache to main memory is the bottleneck. A "
"hotspot bounded at a given level indicates that most of the data was being "
"retrieved from that cache or memory-level. Optimizations should focus on "
"moving data closer to the core. Level-3 TMA shows that PTMalloc was "
"bottlenecked by DRAM Bound. On the other hand, JeMalloc was bottlenecked by "
"L1 Bound – JeMalloc moved data closer to the core, and thus faster "
"execution."
msgstr ""
"“内存限制”下的大多数指标可以鉴定从 L1 "
"缓存到主内存的内存层级中哪个是瓶颈。在给定层级受限的热点表明大多数数据是从该缓存或内存层级中检索的。优化应着重于将数据移至核心更近。三级 TMA "
"显示，PTMalloc 由于 DRAM 限制而成为瓶颈。而另一方面，JeMalloc 由于 L1 限制而成为瓶颈——JeMalloc "
"将数据移至核心更近，因此执行更快。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's look at Intel® VTune Profiler ITT trace. In the example script, we "
"have annotated each *step_x* of the inference loop."
msgstr "让我们看看 Intel® VTune Profiler ITT 跟踪。在示例脚本中，我们为推理循环的每个 *step_x* 进行了注释。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Each step is traced in the timeline graph. The duration of model inference "
"on the last step (step_99) decreased from 304.308 ms to 261.843 ms."
msgstr "每一步都在时间线图中被记录。最后一步（step_99）中模型推理的持续时间从 304.308 毫秒减少到 261.843 毫秒。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Exercise with TorchServe"
msgstr "与 TorchServe 的练习"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's profile PTMalloc vs. JeMalloc with TorchServe."
msgstr "让我们对 PTMalloc 和 JeMalloc 与 TorchServe 进行性能分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will use `TorchServe apache-bench benchmarking "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-"
"apache-bench>`_ with ResNet50 FP32, batch size 32, concurrency 32, requests "
"8960. All other parameters are the same as the `default parameters "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-"
"parameters>`_."
msgstr ""
"我们将使用 ResNet50 FP32、批量大小 32、并发性 32、请求数 8960 的 `TorchServe apache-bench 基准测试 "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-"
"apache-bench>`_。所有其他参数与 `默认参数 "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-"
"parameters>`_ 相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As in the previous exercise, we will use the launcher to designate the "
"memory allocator, and to bind the workload to physical cores of the first "
"socket. To do so, user simply needs to add a few lines in `config.properties"
" <https://pytorch.org/serve/configuration.html#config-properties-file>`__:"
msgstr ""
"在前面的练习中，我们将使用启动器指定内存分配器，并将工作负载绑定至第一个插槽的物理核心。为此，用户只需在 `config.properties "
"<https://pytorch.org/serve/configuration.html#config-properties-file>`__ "
"中添加几行代码即可："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PTMalloc"
msgstr "PTMalloc"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "JeMalloc"
msgstr "JeMalloc"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's go one level deeper."
msgstr "让我们深入一级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's use Intel® VTune Profiler ITT to annotate `TorchServe inference scope "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py#L188>`_"
" to profile at inference-level granularity. As `TorchServe Architecture "
"<https://github.com/pytorch/serve/blob/master/docs/internals.md#torchserve-"
"architecture>`_ consists of several sub-components, including the Java "
"frontend for handling request/response, and the Python backend for running "
"the actual inference on the models, it is helpful to use Intel® VTune "
"Profiler ITT to limit the collection of trace data at inference-level."
msgstr ""
"让我们使用 Intel® VTune Profiler ITT 标注 `TorchServe 推理范围 "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py#L188>`_，以便以推理级别的粒度进行性能分析。由于"
" `TorchServe 架构 "
"<https://github.com/pytorch/serve/blob/master/docs/internals.md#torchserve-"
"architecture>`_ 包括几个子组件，例如用于处理请求/响应的 Java 前端，以及用于实际模型推理的 Python 后端，使用 Intel®"
" VTune Profiler ITT 限制推理级别的跟踪数据收集是非常有帮助的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Each inference call is traced in the timeline graph. The duration of the "
"last model inference decreased from 561.688 ms to 251.287 ms - 2.2x speedup."
msgstr "每个推理调用都在时间线图中被记录。最后一次模型推理的持续时间从 561.688 毫秒减少到 251.287 毫秒——加速了 2.2 倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The timeline graph can be expanded to see op-level profiling results. The "
"duration of *aten::conv2d* decreased from 16.401 ms to 6.392 ms - 2.6x "
"speedup."
msgstr ""
"可以展开时间线图以查看操作级别的性能分析结果。*aten::conv2d* 的持续时间从 16.401 毫秒减少到 6.392 毫秒——加速了 2.6 "
"倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we have demonstrated that JeMalloc can give better "
"performance than the default PyTorch memory allocator, PTMalloc, with "
"efficient thread-local caches improving Back-End-Bound."
msgstr ""
"在本节中，我们已证明 JeMalloc 可以比默认的 PyTorch 内存分配器 PTMalloc "
"提供更好的性能，借助高效的线程局部缓存提升了后端限制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Intel® Extension for PyTorch*"
msgstr "Intel® PyTorch 扩展*"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The three major `Intel® Extension for PyTorch* "
"<https://github.com/intel/intel-extension-for-pytorch>`__ optimization "
"techniques, Operator, Graph, Runtime, are as shown:"
msgstr ""
"Intel® PyTorch 扩展* 的三个主要 `优化技术 <https://github.com/intel/intel-extension-"
"for-pytorch>`__，运算符、图、运行时，如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Intel® Extension for PyTorch* Optimization Techniques"
msgstr "Intel® PyTorch 扩展*优化技术"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Operator"
msgstr "运算符"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Graph"
msgstr "图"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Runtime"
msgstr "运行时"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Vectorization and Multi-threading"
msgstr "向量化和多线程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Low-precision BF16/INT8 compute"
msgstr "低精度 BF16/INT8 计算"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Data layout optimization for better cache locality"
msgstr "数据布局优化以改善缓存局部性"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Constant folding to reduce compute"
msgstr "常数折叠以减少计算"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Op fusion for better cache locality"
msgstr "操作融合以改善缓存局部性"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Thread affinitization"
msgstr "线程绑定"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Memory buffer pooling"
msgstr "内存缓冲池化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "GPU runtime"
msgstr "GPU 运行时"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Launcher"
msgstr "启动器"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Operator Optimization"
msgstr "运算符优化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Optimized operators and kernels are registered through PyTorch dispatching "
"mechanism. These operators and kernels are accelerated from native "
"vectorization feature and matrix calculation feature of Intel hardware. "
"During execution, Intel® Extension for PyTorch* intercepts invocation of "
"ATen operators, and replaces the original ones with these optimized ones. "
"Popular operators like Convolution, Linear have been optimized in Intel® "
"Extension for PyTorch*."
msgstr ""
"优化的运算符和内核通过 PyTorch 的分派机制注册。这些运算符和内核通过 Intel "
"硬件的本地向量化功能和矩阵计算功能进行了加速。在执行期间，Intel® PyTorch 扩展*拦截 ATen "
"运算符的调用，并用这些优化的运算符替代原始运算符。Intel® PyTorch 扩展* 已对卷积、线性等流行运算符进行了优化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's profile optimized operator with Intel® Extension for PyTorch*. We will"
" compare with and without the lines in code changes."
msgstr "让我们使用 Intel® PyTorch 扩展*进行优化运算符分析。我们将比较代码变更前后的效果。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As in the previous exercises, we will bind the workload to physical cores of"
" the first socket."
msgstr "如前面的练习中，我们将工作负载绑定至第一个插槽的物理核心。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The model consists of two operations—Conv2d and ReLU. By printing the model "
"object, we get the following output."
msgstr "模型由两个操作组成——Conv2d 和 ReLU。通过打印模型对象，我们得到以下输出。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Notice the Back End Bound reduced from 68.9 to 38.5 – 1.8x speedup."
msgstr "注意后端限制从 68.9 减少到 38.5——加速了 1.8 倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Additionally, let's profile with PyTorch Profiler."
msgstr "此外，让我们使用 PyTorch Profiler 进行性能分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Notice the CPU time reduced from 851 us to 310 us – 2.7X speedup."
msgstr "注意 CPU 时间从 851 微秒减少到 310 微秒——加速了 2.7 倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Graph Optimization"
msgstr "图优化"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"It is highly recommended for users to take advantage of Intel® Extension for"
" PyTorch* with `TorchScript <https://pytorch.org/docs/stable/jit.html>`_ for"
" further graph optimizations. To optimize performance further with "
"TorchScript, Intel® Extension for PyTorch* supports oneDNN fusion of "
"frequently used FP32/BF16 operator patterns, like Conv2D+ReLU, Linear+ReLU, "
"and more to reduce operator/kernel invocation overheads, and for better "
"cache locality. Some operator fusions allow to maintain temporary "
"calculations, data type conversions, data layouts for better cache locality."
" As well as for INT8, Intel® Extension for PyTorch* has built-in "
"quantization recipes to deliver good statistical accuracy for popular DL "
"workloads including CNN, NLP and recommendation models. The quantized model "
"is then optimized with oneDNN fusion support."
msgstr ""
"强烈建议用户利用 Intel® PyTorch 扩展*与 `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`_ 进行进一步图优化。为了进一步优化性能，Intel® "
"PyTorch 扩展*支持一DNN融合常用的 FP32/BF16 运算符模式，如 Conv2D+ReLU、Linear+ReLU "
"等，以减少运算符/内核调用开销，并改善缓存局部性。一些运算符融合允许保持临时计算、数据类型转换、数据布局以改善缓存局部性。对于 INT8，Intel® "
"PyTorch 扩展*还具有内置量化方案，为流行的深度学习工作负载（包括 CNN、NLP "
"和推荐模型）提供良好的统计准确性。量化模型随后利用一DNN融合支持进行优化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's profile FP32 graph optimization with TorchScript."
msgstr "让我们为 FP32 图优化进行 TorchScript 性能分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Notice the Back End Bound reduced from 67.1 to 37.5 – 1.8x speedup."
msgstr "注意后端限制从 67.1 减少到 37.5——加速了 1.8 倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Notice that with Intel® Extension for PyTorch*  Conv + ReLU operators are "
"fused, and the CPU time reduced from 803 us to 248 us – 3.2X speedup. The "
"oneDNN eltwise post-op enables fusing a primitive with an elementwise "
"primitive. This is one of the most popular kinds of fusion: an eltwise "
"(typically an activation function such as ReLU) with preceding convolution "
"or inner product. Have a look at the oneDNN verbose log shown in the next "
"section."
msgstr ""
"注意使用 Intel® PyTorch 扩展*后，Conv + ReLU 运算符被融合，CPU 时间从 803 微秒减少到 248 微秒——加速了 "
"3.2 倍。一DNN eltwise 后续操作支持将一个基元与元素操作基元融合。这是最流行的融合类型之一：一个元素操作（通常是一个激活函数，例如 "
"ReLU）与前面的卷积或内积结合。请查看下一节中的一DNN详细日志。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Channels Last Memory Format"
msgstr "频道最后内存格式"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When invoking *ipex.optimize* on model, Intel® Extension for PyTorch* "
"automatically converts the model to optimized memory format, channels last. "
"Channels last is a memory format that is more friendly to Intel "
"Architecture. Compared to PyTorch default channels first NCHW (batch, "
"channels, height, width) memory format, channels last NHWC (batch, height, "
"width, channels) memory format generally accelerates convolutional neural "
"networks with better cache locality."
msgstr ""
"调用 *ipex.optimize* 模型时，Intel® PyTorch 扩展*会自动将模型转换为优化的内存格式——频道最后。频道最后是一种对 "
"Intel 架构更友好的内存格式。相比 PyTorch 默认的频道优先 NCHW（批次、频道、高度、宽度）内存格式，频道最后 "
"NHWC（批次、高度、宽度、频道）内存格式通常通过更好的缓存局部性加速卷积神经网络。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One thing to note is that it is expensive to convert memory format. So it's "
"better to convert the memory format prior to deployment once, and keep the "
"memory format conversion minimum during deployment. As the data propagates "
"through model's layers the channels last memory format is preserved through "
"consecutive channels last supported layers (for example, Conv2d -> ReLU -> "
"Conv2d) and conversions are only made in between channels last unsupported "
"layers. See `Memory Format Propagation "
"<https://www.intel.com/content/www/us/en/develop/documentation/onednn-"
"developer-guide-and-reference/top/programming-model/memory-format-"
"propagation.html>`_ for more details."
msgstr ""
"需要注意的一点是，转换内存格式是昂贵的。因此，最好在部署前一次转换内存格式，并在部署过程中保持最低的内存格式转换。当数据通过模型的各层传播时，频道最后内存格式在连续频道最后支持层（例如，Conv2d"
" -> ReLU -> Conv2d）中保持，并且转换仅在频道最后不支持的层之间进行。有关详细信息，请参阅 `内存格式传播 "
"<https://www.intel.com/content/www/us/en/develop/documentation/onednn-"
"developer-guide-and-reference/top/programming-model/memory-format-"
"propagation.html>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let's demonstrate channels last optimization."
msgstr "让我们演示频道最后的优化。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will use `oneDNN verbose mode <https://oneapi-"
"src.github.io/oneDNN/dev_guide_verbose.html>`_, a tool to help collect "
"information at oneDNN graph level such as operator fusions, kernel execution"
" time spent on executing oneDNN primitives. For more information, refer to "
"the `oneDNN Documentation <https://oneapi-"
"src.github.io/oneDNN/index.html>`_."
msgstr ""
"我们将使用 `一DNN详细模式 <https://oneapi-"
"src.github.io/oneDNN/dev_guide_verbose.html>`_，它是一个工具，可帮助收集一DNN图层级信息，例如运算符融合、内核执行时间等。更多信息，请参考"
" `一DNN文档 <https://oneapi-src.github.io/oneDNN/index.html>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Above is oneDNN verbose from channels first. We can verify that there are "
"reorders from weight and data, then do computation, and finally reorder "
"output back."
msgstr "上述是来自频道优先的一DNN详细信息。我们可以验证从权重和数据中进行重新排序，然后进行计算，最后将输出重新排序回去。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Above is oneDNN verbose from channels last. We can verify that channels last"
" memory format avoids unnecessary reorders."
msgstr "上述是来自频道最后的一DNN详细信息。我们可以验证频道最后内存格式避免了不必要的重新排序。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Performance Boost with Intel® Extension for PyTorch*"
msgstr "使用 Intel® PyTorch 扩展*的性能提升"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Below summarizes performance boost of TorchServe with Intel® Extension for "
"PyTorch* for ResNet50 and BERT-base-uncased."
msgstr ""
"以下总结了 TorchServe 使用 Intel® PyTorch 扩展*为 ResNet50 和 BERT-base-uncased的性能提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let's profile Intel® Extension for PyTorch* optimizations with TorchServe."
msgstr "让我们为 TorchServe 使用 Intel® PyTorch 扩展*优化进行性能分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will use `TorchServe apache-bench benchmarking "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-"
"apache-bench>`_ with ResNet50 FP32 TorchScript, batch size 32, concurrency "
"32, requests 8960. All other parameters are the same as the `default "
"parameters "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-"
"parameters>`_."
msgstr ""
"我们将使用 ResNet50 FP32 TorchScript、批量大小 32、并发性 32、请求数 8960 的 `TorchServe "
"apache-bench 基准测试 "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-"
"apache-bench>`_。所有其他参数与 `默认参数 "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-"
"parameters>`_ 相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As in the previous exercise, we will use the launcher to bind the workload "
"to physical cores of the first socket. To do so, user simply needs to add a "
"few lines in `config.properties "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-"
"parameters>`__:"
msgstr ""
"如前面的练习中，我们将使用启动器将工作负载绑定至第一个插槽的物理核心。为此，用户只需在 `config.properties "
"<https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-"
"parameters>`__ 中添加几行代码即可："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Level-1 TMA shows that both are bounded by the backend. As discussed "
"earlier, the majority of untuned deep learning workloads will be Back End "
"Bound. Notice the Back End Bound reduced from 70.0 to 54.1. Let's go one "
"level deeper."
msgstr ""
"一级 TMA 显示两者都受到后端的限制。如前所述，未经优化的深度学习工作负载中的大多数往往会受到后端限制。注意后端限制从 70.0 减少到 "
"54.1。让我们深入一级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As discussed earlier, Back End Bound has two submetrics – Memory Bound and "
"Core Bound. Memory Bound indicates the workload is under-optimized or under-"
"utilized, and ideally memory-bound operations can be improved to core-bound "
"by optimizing the OPs and improving cache locality. Level-2 TMA shows that "
"the Back End Bound improved from Memory Bound to Core Bound. Let's go one "
"level deeper."
msgstr ""
"如前所述，后端限制有两个子指标——内存限制和核心限制。内存限制指示工作负载未优化或未充分利用，理想情况下可以通过优化操作和改善缓存局部性将内存限制的操作改进为核心限制的操作。二级"
" TMA 显示后端限制从内存限制改进为核心限制。让我们深入一级。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Scaling deep learning models for production on a model serving framework "
"like TorchServe requires high compute utilization. This requires that data "
"is available through prefetching and reusing the data in cache when the "
"execution units need it to execute the uOps. Level-3 TMA shows that the Back"
" End Memory Bound improved from DRAM Bound to Core Bound."
msgstr ""
"将深度学习模型扩展到像 TorchServe "
"这样的模型服务框架用于生产需要高计算利用率。这需要数据通过预取在执行单元需要时可用，并在缓存中重复使用数据以执行 uOps。三级 TMA "
"显示后端内存限制从 DRAM 限制改进至核心限制。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As in the previous exercise with TorchServe, let's use Intel® VTune Profiler"
" ITT to annotate `TorchServe inference scope "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py#L188>`_"
" to profile at inference-level granularity."
msgstr ""
"如使用 TorchServe 的前一练习中，让我们使用 Intel® VTune Profiler ITT 标注 `TorchServe 推理范围 "
"<https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py#L188>`_，以便以推理级别的粒度进行性能分析。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Each inference call is traced in the timeline graph. The duration of the "
"last inference call decreased from 215.731 ms to 95.634 ms - 2.3x speedup."
msgstr "每个推理调用都在时间线图中被记录。最后一次推理调用的持续时间从 215.731 毫秒减少到 95.634 毫秒——加速了 2.3 倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The timeline graph can be expanded to see op-level profiling results. Notice"
" that Conv + ReLU has been fused, and the duration decreased from 6.393 ms +"
" 1.731 ms to 3.408 ms - 2.4x speedup."
msgstr ""
"时间线图可以展开查看操作级性能分析结果。注意到卷积加ReLU已经被融合，持续时间从6.393毫秒加1.731毫秒减少到3.408毫秒 - "
"提升了2.4倍性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we have used Top-down Microarchitecture Analysis (TMA) and"
" Intel® VTune™ Profiler's Instrumentation and Tracing Technology (ITT) to "
"demonstrate that"
msgstr "在本教程中，我们使用了顶层微架构分析 (TMA) 和 Intel® VTune™ Profiler 的仪器和跟踪技术 (ITT) 来演示:"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Often the primary bottleneck of under-optimized or under-tuned deep learning"
" workloads are Back End Bound, which has two submetrics, Memory Bound and "
"Core Bound."
msgstr "通常，未优化或未调优的深度学习工作负载的主要瓶颈是后端受限，其中包括两个子指标：内存受限和核心受限。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A more efficient memory allocator, operator fusion, memory layout format "
"optimization by Intel® Extension for PyTorch* improve Memory Bound."
msgstr "通过 Intel® PyTorch* 扩展提供更高效的内存分配器、操作融合和内存布局格式优化，可改善内存受限问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Key deep learning primitives, such as convolution, matrix multiplication, "
"dot-product, etc have been well optimized by Intel® Extension for PyTorch* "
"and oneDNN library, improving Core Bound."
msgstr "Intel® PyTorch* 扩展和 oneDNN 库已经对关键深度学习原语，如卷积、矩阵乘法、点积等进行了优化，提升了核心受限问题。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Intel® Extension for PyTorch* has been integrated into TorchServe with an "
"ease-of-use API."
msgstr "Intel® PyTorch* 扩展已整合到 TorchServe 中，并提供了易于使用的 API。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"TorchServe with Intel® Extension for PyTorch* shows 7.71x throughput speedup"
" for ResNet50, and 2.20x throughput speedup for BERT."
msgstr ""
"使用 Intel® PyTorch* 扩展的 TorchServe 对 ResNet50 提供了 7.71 倍吞吐量加速，对 BERT 提供了 2.20"
" 倍吞吐量加速。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Related Readings"
msgstr "相关阅读"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Top-Down performance analysis methodology "
"<https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-"
"methodology>`_"
msgstr ""
"`顶层性能分析方法 <https://easyperf.net/blog/2019/02/09/Top-Down-performance-"
"analysis-methodology>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`Accelerating PyTorch with Intel® Extension for PyTorch* "
"<https://medium.com/pytorch/accelerating-pytorch-with-intel-extension-for-"
"pytorch-3aef51ea3722>`_"
msgstr ""
"`使用 Intel® PyTorch 扩展加速 PyTorch <https://medium.com/pytorch/accelerating-"
"pytorch-with-intel-extension-for-pytorch-3aef51ea3722>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their "
"immense guidance and support, and thorough feedback and reviews throughout "
"many steps of this tutorial. We would also like to thank Hamid Shojanazeri "
"(Meta) and Li Ning (AWS) for their helpful feedback in code review and the "
"tutorial."
msgstr ""
"我们感谢 Ashok Emani (Intel) 和 Jiong Gong (Intel) "
"在本教程的多个阶段提供了巨大的指导和支持，以及全面的反馈和审阅。同时感谢 Hamid Shojanazeri (Meta) 和 Li Ning "
"(AWS) 在代码评审和教程中的有益反馈。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here <sphx_glr_download_intermediate_torchvision_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_intermediate_torchvision_tutorial.py>` "
"下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "TorchVision Object Detection Finetuning Tutorial"
msgstr "TorchVision 对象检测微调教程"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For this tutorial, we will be finetuning a pre-trained `Mask R-CNN "
"<https://arxiv.org/abs/1703.06870>`_ model on the `Penn-Fudan Database for "
"Pedestrian Detection and Segmentation "
"<https://www.cis.upenn.edu/~jshi/ped_html/>`_. It contains 170 images with "
"345 instances of pedestrians, and we will use it to illustrate how to use "
"the new features in torchvision in order to train an object detection and "
"instance segmentation model on a custom dataset."
msgstr ""
"在本教程中，我们将对预训练的 `Mask R-CNN <https://arxiv.org/abs/1703.06870>`_ 模型在 `Penn-"
"Fudan 行人检测和分割数据库 <https://www.cis.upenn.edu/~jshi/ped_html/>`_ "
"上进行微调。该数据库包含170张图片和345个行人实例，我们将使用它来演示如何利用 torchvision "
"的新功能来训练针对自定义数据集的对象检测和实例分割模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial works only with torchvision version >=0.16 or nightly. If "
"you're using torchvision<=0.15, please follow `this tutorial instead "
"<https://github.com/pytorch/tutorials/blob/d686b662932a380a58b7683425faa00c06bcf502/intermediate_source/torchvision_tutorial.rst>`_."
msgstr ""
"本教程仅适用于 torchvision 版本 >=0.16 或 nightly。如果您使用的是 torchvision<=0.15，请改为按照 "
"`这篇教程 "
"<https://github.com/pytorch/tutorials/blob/d686b662932a380a58b7683425faa00c06bcf502/intermediate_source/torchvision_tutorial.rst>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Defining the Dataset"
msgstr "定义数据集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The reference scripts for training object detection, instance segmentation "
"and person keypoint detection allows for easily supporting adding new custom"
" datasets. The dataset should inherit from the standard "
":class:`torch.utils.data.Dataset` class, and implement ``__len__`` and "
"``__getitem__``."
msgstr ""
"对象检测、实例分割和人物关键点检测的参考脚本允许轻松添加新的自定义数据集。数据集应继承标准类 "
":class:`torch.utils.data.Dataset`，并实现 ``__len__`` 和 ``__getitem__``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The only specificity that we require is that the dataset ``__getitem__`` "
"should return a tuple:"
msgstr "我们要求数据集 ``__getitem__`` 返回一个元组:"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"image: :class:`torchvision.tv_tensors.Image` of shape ``[3, H, W]``, a pure "
"tensor, or a PIL Image of size ``(H, W)``"
msgstr ""
"image: 形状为 ``[3, H, W]`` 的 :class:`torchvision.tv_tensors.Image`（纯张量）或大小为 "
"``(H, W)`` 的 PIL 图像"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "target: a dict containing the following fields"
msgstr "target: 包含以下字段的字典"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``boxes``, :class:`torchvision.tv_tensors.BoundingBoxes` of shape ``[N, "
"4]``: the coordinates of the ``N`` bounding boxes in ``[x0, y0, x1, y1]`` "
"format, ranging from ``0`` to ``W`` and ``0`` to ``H``"
msgstr ""
"``boxes``, 形状为 ``[N, 4]`` 的 :class:`torchvision.tv_tensors.BoundingBoxes`：以 "
"``[x0, y0, x1, y1]`` 格式定义的 ``N`` 个边界框的坐标，范围从 ``0`` 到 ``W`` 和 ``0`` 到 ``H``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``labels``, integer :class:`torch.Tensor` of shape ``[N]``: the label for "
"each bounding box. ``0`` represents always the background class."
msgstr ""
"``labels``, 形状为 ``[N]`` 的整数 :class:`torch.Tensor`：每个边界框的标签。标签 ``0`` 始终代表背景类。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``image_id``, int: an image identifier. It should be unique between all the "
"images in the dataset, and is used during evaluation"
msgstr "``image_id``, int: 图像标识符。在数据集中所有图像中应该是唯一的，用于评估期间"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``area``, float :class:`torch.Tensor` of shape ``[N]``: the area of the "
"bounding box. This is used during evaluation with the COCO metric, to "
"separate the metric scores between small, medium and large boxes."
msgstr ""
"``area``, 形状为 ``[N]`` 的 float :class:`torch.Tensor`: 边界框的面积。在评估 COCO "
"指标时，用于将指标分数划分为小型、中型和大型框。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``iscrowd``, uint8 :class:`torch.Tensor` of shape ``[N]``: instances with "
"``iscrowd=True`` will be ignored during evaluation."
msgstr ""
"``iscrowd``, 形状为 ``[N]`` 的 uint8 :class:`torch.Tensor`: 实例其中 "
"``iscrowd=True`` 将在评估期间被忽略。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"(optionally) ``masks``, :class:`torchvision.tv_tensors.Mask` of shape ``[N, "
"H, W]``: the segmentation masks for each one of the objects"
msgstr ""
"(可选) ``masks``, 形状为 ``[N, H, W]`` 的 :class:`torchvision.tv_tensors.Mask`: "
"每个对象的分割掩码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If your dataset is compliant with above requirements then it will work for "
"both training and evaluation codes from the reference script. Evaluation "
"code will use scripts from ``pycocotools`` which can be installed with ``pip"
" install pycocotools``."
msgstr ""
"如果您的数据集符合上述要求，则可以支持参考脚本中的训练和评估代码。评估代码将使用 ``pycocotools`` 库中的脚本，该库可通过 ``pip "
"install pycocotools`` 安装。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For Windows, please install ``pycocotools`` from `gautamchitnis "
"<https://github.com/gautamchitnis/cocoapi>`_ with command"
msgstr ""
"对于 Windows，请从 `gautamchitnis <https://github.com/gautamchitnis/cocoapi>`_ 安装"
" ``pycocotools`` ，命令如下"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-"
"master#subdirectory=PythonAPI``"
msgstr ""
"``pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-"
"master#subdirectory=PythonAPI``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One note on the ``labels``. The model considers class ``0`` as background. "
"If your dataset does not contain the background class, you should not have "
"``0`` in your ``labels``. For example, assuming you have just two classes, "
"*cat* and *dog*, you can define ``1`` (not ``0``) to represent *cats* and "
"``2`` to represent *dogs*. So, for instance, if one of the images has both "
"classes, your ``labels`` tensor should look like ``[1, 2]``."
msgstr ""
"关于 ``labels`` 的一点注意事项。模型将类 ``0`` 视为背景。如果您的数据集中不包含背景类，则您的 ``labels`` 中不应有 "
"``0``。例如，假设您只有两个类，*猫* 和 *狗*，您可以定义 ``1``（而不是 ``0``）来表示 *猫*，定义 ``2`` 来表示 "
"*狗*。因此，假如有一个图片同时包含这两个类，那么您的 ``labels`` 张量应为 ``[1, 2]``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Additionally, if you want to use aspect ratio grouping during training (so "
"that each batch only contains images with similar aspect ratios), then it is"
" recommended to also implement a ``get_height_and_width`` method, which "
"returns the height and the width of the image. If this method is not "
"provided, we query all elements of the dataset via ``__getitem__`` , which "
"loads the image in memory and is slower than if a custom method is provided."
msgstr ""
"此外，如果您希望在训练期间使用纵横比分组（以使每个批次仅包含具有相似纵横比的图像），建议实现一个 ``get_height_and_width`` "
"方法以返回图像的高度和宽度。如果未提供此方法，我们通过 ``__getitem__`` "
"查询数据集的所有元素，这会加载图像到内存，速度比提供自定义方法慢。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Writing a custom dataset for PennFudan"
msgstr "为 PennFudan 编写自定义数据集"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s write a dataset for the PennFudan dataset. First, let's download the "
"dataset and extract the `zip file "
"<https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip>`_:"
msgstr ""
"让我们为 PennFudan 数据集编写一个数据集。首先下载数据集并提取 `ZIP 文件 "
"<https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip>`_:"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We have the following folder structure:"
msgstr "我们有以下文件夹结构:"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Here is one example of a pair of images and segmentation masks"
msgstr "以下是一个图片和分割掩码的示例配对"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So each image has a corresponding segmentation mask, where each color "
"correspond to a different instance. Let’s write a "
":class:`torch.utils.data.Dataset` class for this dataset. In the code below,"
" we are wrapping images, bounding boxes and masks into "
":class:`torchvision.tv_tensors.TVTensor` classes so that we will be able to "
"apply torchvision built-in transformations (`new Transforms API "
"<https://pytorch.org/vision/stable/transforms.html>`_) for the given object "
"detection and segmentation task. Namely, image tensors will be wrapped by "
":class:`torchvision.tv_tensors.Image`, bounding boxes into "
":class:`torchvision.tv_tensors.BoundingBoxes` and masks into "
":class:`torchvision.tv_tensors.Mask`. As "
":class:`torchvision.tv_tensors.TVTensor` are :class:`torch.Tensor` "
"subclasses, wrapped objects are also tensors and inherit the plain "
":class:`torch.Tensor` API. For more information about torchvision "
"``tv_tensors`` see `this documentation "
"<https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-"
"are-tvtensors>`_."
msgstr ""
"因此，每个图片都有一个对应的分割掩码，其中每种颜色对应一个实例。让我们为这个数据集编写一个 "
":class:`torch.utils.data.Dataset` 类。在以下代码中，我们将图片、边界框和掩码包装到 "
":class:`torchvision.tv_tensors.TVTensor` 类中，以便能为给定的对象检测和分割任务应用 torchvision "
"内置变换（`新变换 API "
"<https://pytorch.org/vision/stable/transforms.html>`_）。具体来说，图片张量将由 "
":class:`torchvision.tv_tensors.Image` 包装，边界框由 "
":class:`torchvision.tv_tensors.BoundingBoxes` 包装，掩码由 "
":class:`torchvision.tv_tensors.Mask` 包装。由于 "
":class:`torchvision.tv_tensors.TVTensor` 是 :class:`torch.Tensor` "
"的子类，包装的对象也是张量并继承了普通 :class:`torch.Tensor` 的 API。关于 torchvision "
"``tv_tensors`` 的更多信息，请参阅 `这份文档 "
"<https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-"
"are-tvtensors>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That’s all for the dataset. Now let’s define a model that can perform "
"predictions on this dataset."
msgstr "数据集就此定义完成。现在让我们定义一个能够在这个数据集上进行预测的模型。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Defining your model"
msgstr "定义您的模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we will be using `Mask R-CNN "
"<https://arxiv.org/abs/1703.06870>`_, which is based on top of `Faster R-CNN"
" <https://arxiv.org/abs/1506.01497>`_. Faster R-CNN is a model that predicts"
" both bounding boxes and class scores for potential objects in the image."
msgstr ""
"在本教程中，我们将使用 `Mask R-CNN <https://arxiv.org/abs/1703.06870>`_，它基于 `Faster "
"R-CNN <https://arxiv.org/abs/1506.01497>`_。 Faster R-CNN "
"是一种模型，可预测图片中潜在对象的边界框和类别分数。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts "
"segmentation masks for each instance."
msgstr "Mask R-CNN 在 Faster R-CNN 的基础上增加了一个分支，还预测每个实例的分割掩码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There are two common situations where one might want to modify one of the "
"available models in TorchVision Model Zoo. The first is when we want to "
"start from a pre-trained model, and just finetune the last layer. The other "
"is when we want to replace the backbone of the model with a different one "
"(for faster predictions, for example)."
msgstr ""
"TorchVision "
"模型库中有两种常见的情况，一个是我们希望从一个预训练模型开始，只微调最后的一层。另一个是我们希望替换模型的主干，使用不同的主干（例如，以获得更快的预测）。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Let’s go see how we would do one or another in the following sections."
msgstr "接下来我们将演示如何实现上述两种情况。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "1 - Finetuning from a pretrained model"
msgstr "1 - 从预训练模型进行微调"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s suppose that you want to start from a model pre-trained on COCO and "
"want to finetune it for your particular classes. Here is a possible way of "
"doing it:"
msgstr "假设您希望从一个在 COCO 上预训练的模型开始，并只对其进行微调以适配您的特定类别。可以按以下方式实现："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "2 - Modifying the model to add a different backbone"
msgstr "2 - 修改模型以添加不同的主干"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Object detection and instance segmentation model for PennFudan Dataset"
msgstr "为 PennFudan 数据集编写对象检测和实例分割模型"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In our case, we want to finetune from a pre-trained model, given that our "
"dataset is very small, so we will be following approach number 1."
msgstr "在我们的案例中，由于数据集非常小，我们希望从预训练模型开始微调，因此我们将采用第一种方法。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Here we want to also compute the instance segmentation masks, so we will be "
"using Mask R-CNN:"
msgstr "在这里，我们希望还能够计算实例分割掩码，因此将使用 Mask R-CNN："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"That’s it, this will make ``model`` be ready to be trained and evaluated on "
"your custom dataset."
msgstr "完成后，``model`` 就可以准备在您的自定义数据集上进行训练和评估了。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Putting everything together"
msgstr "将所有部分整合到一起"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In ``references/detection/``, we have a number of helper functions to "
"simplify training and evaluating detection models. Here, we will use "
"``references/detection/engine.py`` and ``references/detection/utils.py``. "
"Just download everything under ``references/detection`` to your folder and "
"use them here. On Linux if you have ``wget``, you can download them using "
"below commands:"
msgstr ""
"在 ``references/detection/`` 文件夹中，我们有一些辅助函数可以简化训练和评估检测模型的过程。在这里我们将使用 "
"``references/detection/engine.py`` 和 ``references/detection/utils.py``。您只需将 "
"``references/detection`` 文件夹中的所有内容都下载到您的文件夹中并在此使用。在 Linux 上如果有 "
"``wget``，可以使用以下命令下载它们："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Since v0.15.0 torchvision provides `new Transforms API "
"<https://pytorch.org/vision/stable/transforms.html>`_ to easily write data "
"augmentation pipelines for Object Detection and Segmentation tasks."
msgstr ""
"自版本 v0.15.0 起，torchvision 提供了 `新变换 API "
"<https://pytorch.org/vision/stable/transforms.html>`_ "
"，可以轻松编写用于对象检测和分割任务的数据增强管道。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s write some helper functions for data augmentation / transformation:"
msgstr "让我们为数据增强/变换编写一些辅助函数："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Testing ``forward()`` method (Optional)"
msgstr "测试 ``forward()`` 方法（可选）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Before iterating over the dataset, it's good to see what the model expects "
"during training and inference time on sample data."
msgstr "在遍历数据集之前，最好查看模型在训练和推断时对样本数据的预期。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let’s now write the main function which performs the training and the "
"validation:"
msgstr "现在让我们编写一个主函数来执行训练和验证："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So after one epoch of training, we obtain a COCO-style mAP > 50, and a mask "
"mAP of 65."
msgstr "经过一个训练周期后，我们获得了 COCO 风格的 mAP > 50 和分割掩码 mAP 为 65。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"But what do the predictions look like? Let’s take one image in the dataset "
"and verify"
msgstr "但预测结果是什么样的呢？让我们选一个数据集中的图片并查看一下"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The results look good!"
msgstr "结果看起来不错！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Wrapping up"
msgstr "总结"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, you have learned how to create your own training pipeline "
"for object detection models on a custom dataset. For that, you wrote a "
":class:`torch.utils.data.Dataset` class that returns the images and the "
"ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN "
"model pre-trained on COCO train2017 in order to perform transfer learning on"
" this new dataset."
msgstr ""
"在本教程中，您已经学习如何为自定义数据集上的对象检测模型创建自己的训练管道。为此，您编写了一个 "
":class:`torch.utils.data.Dataset` 类来返回图片以及真实边界框和分割掩码。您还利用了一个在 COCO train2017"
" 上预训练的 Mask R-CNN 模型，以便在这个新数据集上进行迁移学习。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"For a more complete example, which includes multi-machine / multi-GPU "
"training, check ``references/detection/train.py``, which is present in the "
"torchvision repository."
msgstr ""
"若需更完整的示例，包括多机/多GPU训练，请查看 torchvision 仓库中提供的 "
"``references/detection/train.py``。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: torchvision_tutorial.py "
"<torchvision_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: torchvision_tutorial.py <torchvision_tutorial.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: torchvision_tutorial.ipynb "
"<torchvision_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: torchvision_tutorial.ipynb "
"<torchvision_tutorial.ipynb>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Click :ref:`here "
"<sphx_glr_download_intermediate_transformer_building_blocks.py>` to download"
" the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_intermediate_transformer_building_blocks.py>`"
" 下载完整示例代码"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Learn how to optimize transformer models by replacing nn.Transformer with "
"Nested Tensors and torch.compile() for significant performance gains in "
"PyTorch."
msgstr ""
"学习如何通过使用嵌套张量和 torch.compile() 替换 nn.Transformer 来优化 Transformer 模型，以显著提高 "
"PyTorch 的性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Accelerating PyTorch Transformers by replacing ``nn.Transformer`` with "
"Nested Tensors and ``torch.compile()``"
msgstr ""
"通过使用嵌套张量和 torch.compile() 替换 ``nn.Transformer`` 来加速 PyTorch Transformer"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "**Author:** `Mikayla Gawarecki <https://github.com/mikaylagawarecki>`_"
msgstr "**作者:** `Mikayla Gawarecki <https://github.com/mikaylagawarecki>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Learn about the low-level building blocks PyTorch provides to build custom "
"transformer layers ( nested tensors, ``scaled_dot_product_attention``, "
"``torch.compile()``, and ``FlexAttention``)"
msgstr ""
"了解 PyTorch 提供的构建自定义 Transformer "
"层的底层模块（嵌套张量、``scaled_dot_product_attention``、``torch.compile()`` 和 "
"``FlexAttention``）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Discover how the above improve memory usage and performance using "
"MultiHeadAttention as an example"
msgstr "以多头注意力为例，探索上述模块如何提高内存使用和性能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Explore advanced customizations using the aforementioned building blocks"
msgstr "使用上述构建模块探索高级定制功能"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "PyTorch v.2.6.0 or later"
msgstr "PyTorch v.2.6.0 或更高版本"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Over the past few years, the PyTorch team has developed various lower level "
"features that, when composed, can create a variety of transformer variants. "
"These include:"
msgstr "过去几年中，PyTorch 团队开发了各种底层功能，这些功能可以组合成各种 Transformer 变体，包括以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Nested Tensors with the ``torch.jagged`` layout (AKA NJTs)"
msgstr "具有 ``torch.jagged`` 布局的嵌套张量（即 NJTs）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``scaled_dot_product_attention``"
msgstr "``scaled_dot_product_attention``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``torch.compile()``"
msgstr "``torch.compile()``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "``FlexAttention``"
msgstr "``FlexAttention``"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"This tutorial will give a brief overview of the above technologies and "
"demonstrate how they can be composed to yield flexible and performant "
"transformer layers with improved user experience."
msgstr "本教程将简要介绍上述技术，并演示如何组合它们以生成具有更好用户体验的灵活且高性能的 Transformer 层。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One may observe that the ``torch.nn`` module currently provides various "
"``Transformer``-related layers. In particular, it includes "
"``TransformerEncoderLayer``, ``TransformerEncoder``, "
"``TransformerDecoderLayer``, ``TransformerDecoder``, ``Transformer`` and "
"``MultiheadAttention``. This family of layers was initially implemented "
"following the `Attention is All You Need "
"<https://arxiv.org/abs/1706.03762>`_ paper. The components discussed in this"
" tutorial provide improved user experience, flexibility and performance over"
" the existing ``nn`` layers."
msgstr ""
"我们可能会注意到 ``torch.nn`` 模块目前提供各种与 ``Transformer`` 相关的层。特别是，它包括 "
"``TransformerEncoderLayer``、``TransformerEncoder``、``TransformerDecoderLayer``、``TransformerDecoder``、``Transformer``"
" 和 ``MultiheadAttention``。这些层系列最初是基于《Attention is All You Need "
"<https://arxiv.org/abs/1706.03762>`_》论文实现的。本教程中讨论的组件在现有 ``nn`` "
"层的基础上提供了更好的用户体验、灵活性和性能。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Is this tutorial for me?"
msgstr "这个教程适合我吗？"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are wondering about what building blocks the ``torch`` library "
"provides for writing your own transformer layers and best practices, you are"
" in the right place. Please keep reading!"
msgstr ""
"如果您想知道 ``torch`` 库提供了哪些构建模块用于编写您自己的 Transformer 层以及最佳实践，那么您来对地方了。请继续阅读！"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are looking for an out-of-the-box implementation of a popular "
"transformer architecture, note that there are many open-source libraries "
"that provide them, including:"
msgstr "如果您正在寻找一种现成的流行 Transformer 架构实现，请注意有许多开源库提供这些架构，包括："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`HuggingFace transformers <https://github.com/huggingface/transformers>`_"
msgstr ""
"`HuggingFace transformers <https://github.com/huggingface/transformers>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`xformers <https://github.com/facebookresearch/xformers>`_"
msgstr "`xformers <https://github.com/facebookresearch/xformers>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`torchtune <https://github.com/pytorch/torchtune>`_"
msgstr "`torchtune <https://github.com/pytorch/torchtune>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"If you are only interested in performant attention score modifications, "
"please check out the `FlexAttention blog "
"<https://pytorch.org/blog/flexattention/>`_ that contains a `gym of masks "
"<https://github.com/pytorch-labs/attention-gym>`_."
msgstr ""
"如果您只对性能优良的注意力得分修改感兴趣，请查看 `FlexAttention 博客 "
"<https://pytorch.org/blog/flexattention/>`_，其中包含一个 `mask 多样化 gym "
"<https://github.com/pytorch-labs/attention-gym>`_。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Introducing the Building Blocks"
msgstr "介绍构建模块"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"First, we will briefly introduce the four technologies mentioned in the "
"introduction"
msgstr "首先，我们将简要介绍介绍部分提到的四项技术"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`torch.nested <https://pytorch.org/tutorials/prototype/nestedtensor.html>`_"
msgstr ""
"`torch.nested <https://pytorch.org/tutorials/prototype/nestedtensor.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Nested tensors generalize the shape of regular dense tensors, allowing for "
"representation of ragged-sized data with the same tensor UX. In the context "
"of transformers, we can think of nested tensors as a tool for representing "
"variable sequence lengths. They eliminate the need for the bug-prone "
"practices of explicit padding and masking (think ``key_padding_mask`` in "
"``nn.MultiHeadAttention``)."
msgstr ""
"嵌套张量推广了常规密集张量的形状，可以使用相同的张量 UX 表示大小不规则的数据。在 Transformer "
"的上下文中，我们可以将嵌套张量视为表示可变序列长度的工具。它们消除了显式填充和掩码（例如 ``nn.MultiHeadAttention`` 中的 "
"``key_padding_mask``）的易出错操作。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`scaled_dot_product_attention "
"<https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html>`_"
msgstr ""
"`scaled_dot_product_attention "
"<https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``scaled_dot_product_attention`` is a primitive for "
":math:`\\text{softmax}(\\frac{QK^T}{\\sqrt{E}} + B)V` that dispatches into "
"either fused implementations of the operator or a fallback implementation. "
"It works out of the box in eager mode (i.e. the default mode of using "
"PyTorch where operations are executed on the fly as they are encountered) "
"and also integrates seamlessly with ``torch.compile()``. As of 2.6, it will "
"also offer grouped query attention natively."
msgstr ""
"``scaled_dot_product_attention`` 是一个原语，用于 "
":math:`\\text{softmax}(\\frac{QK^T}{\\sqrt{E}} + B)V`，它可以调度到该操作的融合实现或后备实现。它在"
" eager 模式（即使用 PyTorch 的默认模式，操作会在遇到时立即执行）下开箱即用，并且可以无缝集成到 ``torch.compile()`` "
"中。从 2.6 开始，它还将原生支持分组查询注意力。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`torch.compile() "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"
msgstr ""
"`torch.compile() "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``torch.compile()`` is a compiler introduced in version 2.0 that is able to "
"capture a graph of PyTorch code and perform various optimizations on it, "
"such as fusing together sequences of ops. Nested tensors with the "
"``torch.jagged`` layout and ``scaled_dot_product_attention`` work seamlessly"
" with compile. In the context of transformers, the value add of using "
"compile with nested tensor and SDPA is that compile can remove framework "
"overhead ones sees in eager mode and fuse sequences of ops in transformers "
"together, such as projection and activation."
msgstr ""
"``torch.compile()`` 是一个在 2.0 版中引入的编译器，它能够捕获 PyTorch "
"代码的图并执行各种优化，例如将操作序列融合在一起。具有 ``torch.jagged`` 布局的嵌套张量和 "
"``scaled_dot_product_attention`` 可以与编译器无缝结合。在 Transformer 的环境中，结合嵌套张量和 SDPA "
"使用编译器的价值在于，它可以消除 eager 模式中看到的框架开销，并融合 Transformers 中的操作序列，例如投影和激活。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`FlexAttention <https://pytorch.org/blog/flexattention/>`_"
msgstr "`FlexAttention <https://pytorch.org/blog/flexattention/>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"``FlexAttention`` is a primitive that allows users to modify attention "
"scores prior to the softmax operation. It generalizes the additive ``B`` "
"term above for ``scaled_dot_product_attention``, allowing for arbitrary "
"calculation. It requires compile to achieve good performance."
msgstr ""
"``FlexAttention`` 是一个原语，允许用户在执行软最大化操作之前修改注意力分数。它广义化了上面用于 "
"``scaled_dot_product_attention`` 的加性 ``B`` 项，允许进行任意计算。为了实现良好的性能，它需要编译器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "The above building blocks are \"All You Need\" (as of October 2024)"
msgstr "上述构建模块是“全都需要”（截至 2024 年 10 月）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The main premise in this section is that most transformer variations are "
"GPT-style, consisting of layers like Embedding, Positional Encoding, "
"Attention Blocks and Feed Forward networks. If we were to try to classify "
"the differences in this space, we might land on something like:"
msgstr ""
"本节的主要前提是，大多数 Transformer 变体都是 GPT "
"风格的，由嵌入层、位置编码、注意力块和前馈网络等层组成。如果我们尝试对这一领域的差异进行分类，可能会得到如下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Layer type (activation functions such as ``SwiGLU`` and others, "
"normalization functions such as ``RMSNorm`` and others, positional "
"encodings, such as Sinusoidal, Rotary.)"
msgstr "层类型（如 ``SwiGLU`` 等激活函数、``RMSNorm`` 等归一化函数、Sinusoidal 和 Rotary 等位置编码）"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Layer ordering, such as where to apply norms and positional encoding."
msgstr "层顺序，例如在何处应用归一化和位置编码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Modifications to attention score, such as ``ALiBi``, Relative Positional "
"Bias and so on."
msgstr "对注意力分数的修改，例如 ``ALiBi``、相对位置偏差等。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In a pre-compiler environment, you might write a custom transformer and "
"notice that it functions correctly but is slow. To address this, you might "
"develop a custom fused kernel for the specific series of operations. In a "
"compiler environment, you can simply perform the initial step and then "
"compile and benefit from improved performance."
msgstr ""
"在预编译器环境中，您可能会编写一个自定义 "
"Transformer，并注意到它功能正常但速度很慢。为了解决这个问题，您可能会为特定操作序列开发一个自定义融合内核。在编译器环境中，您可以简单地执行初始步骤，然后进行编译并受益于性能提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "MultiheadAttention"
msgstr "多头注意力"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Remember that MultiheadAttention takes in a query, key, and value, and "
"consists of an input projection, a ``scaled_dot_product_attention`` operator"
" and an output projection. The main takeaway we want to demonstrate here is "
"the improvement yielded when we replaced padded/masked inputs with nested "
"tensors. The improvements are threefold:"
msgstr ""
"请记住，多头注意力接收一个查询、一个键和一个值，包括输入投影、``scaled_dot_product_attention`` "
"操作和输出投影。这里我们希望展示的主要观点是，当我们使用嵌套张量替换填充/掩码输入时的改进。改进分为三点："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**User Experience** Remember that ``nn.MultiheadAttention`` requires "
"``query``, ``key``, and ``value`` to be dense ``torch.Tensors``. It also "
"provides a ``key_padding_mask`` that is used to mask out padding tokens in "
"the ``key`` that arise due to different sequence lengths within a batch. "
"Since there is no ``query_padding_mask`` in ``nn.MHA``, users have to take "
"care to mask/slice the outputs appropriately to account for query sequence "
"lengths. ``NestedTensor`` cleanly removes the need for this sort of error-"
"prone padding masks."
msgstr ""
"**用户体验** 请记住，``nn.MultiheadAttention`` 要求 ``query``、``key`` 和 ``value`` 为密集的"
" ``torch.Tensor``。它还提供了一个 ``key_padding_mask``，用于掩盖由于批次中序列长度不同而出现在 ``key`` "
"中的填充标记。由于 ``nn.MHA`` 中没有 "
"``query_padding_mask``，用户必须注意适当地掩盖/切片输出数据，以确保查询序列长度被正确考虑。``NestedTensor`` "
"干净地消除了此类容易出错的填充掩码的需求。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Memory** Instead of materializing a dense ``[B, S, D]`` tensor with a "
"``[B, S]`` padding mask (where ``B`` is batch size, ``S`` is max sequence "
"length in the batch and ``D`` is embedding size), nested tensors allow you "
"to cleanly represent the batch of varying sequence lengths. As a result, the"
" input and intermediate activations will use less memory."
msgstr ""
"**内存** 与显式生成具有 ``[B, S, D]`` 张量的密集和 ``[B, S]`` 填充掩码（其中 ``B`` 表示批次大小，``S`` "
"表示批次中的最大序列长度，``D`` 表示嵌入大小）不同，嵌套张量能清晰地表示批次中具有不同序列长度的数据。因此，输入和中间激活将使用更少的内存。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"**Performance** Since padding is not materialized and unnecessary "
"computation on padding is skipped, performance and memory usage improve."
msgstr "**性能** 由于填充没有被显式生成，并且未对填充数据进行不必要的计算，性能和内存使用得以提升。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We'll demonstrate the above by building upon the ``MultiheadAttention`` "
"layer in the `Nested Tensor tutorial "
"<https://pytorch.org/tutorials/prototype/nestedtensor.html>`_ and comparing "
"it to the ``nn.MultiheadAttention`` layer."
msgstr ""
"我们将通过基于 ` Nested Tensor 教程 "
"<https://pytorch.org/tutorials/prototype/nestedtensor.html>`_ 中的 "
"``MultiheadAttention`` 层构建内容，与传统 ``nn.MultiheadAttention`` 层进行比较。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Utilities"
msgstr "工具函数"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we include a utility to generate semi-realistic data using "
"``Zipf`` distribution for sentence lengths. This is used to generate the "
"nested query, key, and value tensors. We also include a benchmark utility."
msgstr ""
"在本节中，我们提供一个使用 ``Zipf`` 分布为句子长度生成半真实数据的工具函数。这用于生成嵌套查询、键和值张量。我们还提供了一个性能基准工具。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We will now demonstrate the performance improvements of using nested tensors"
" in the ``MultiheadAttention`` layer + compile for self attention. We "
"compare this against the traditional ``nn.MultiheadAttention`` + compile "
"with padding and masking."
msgstr ""
"现在，我们将在 ``MultiheadAttention`` 层中使用嵌套张量 + 编译器进行自注意力，并演示其性能改进，与传统的填充和掩码结合编译器的"
" ``nn.MultiheadAttention`` 进行对比。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "For reference, here are some sample outputs on A100:"
msgstr "参考来说，此处是一些 A100 的输出实例："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "We can also see the same for backward pass"
msgstr "我们也可以在反向传播中看到相同的改进"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Sample outputs on A100:"
msgstr "A100 的输出示例："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "GPT-style layer"
msgstr "GPT 风格的层"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"A basic GPT-style transformer layer consists of a causal self-attention "
"layer followed by a feed-forward network (FFN) with skip connections. "
"Implementing this is fairly straightforward using the ``MultiheadAttention``"
" layer above and gives equivalent results to an "
"``nn.TransformerEncoderLayer`` with ``is_causal=True``."
msgstr ""
"一个基本的 GPT 风格 Transformer 层由一个因果自注意力层随后接一个跳跃连接的前馈网络（FFN）组成。使用上述 "
"``MultiheadAttention`` 层实现这一点是非常直接的，并且生成的结果与 ``nn.TransformerEncoderLayer`` "
"的 ``is_causal=True`` 相同。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We  demonstrate examples of implementing the rest of the ``nn`` layers `here"
" <https://github.com/mikaylagawarecki/transformer_tutorial_accompaniment>`_ "
"but omit that from this tutorial for brevity."
msgstr ""
"我们 `在这里 "
"<https://github.com/mikaylagawarecki/transformer_tutorial_accompaniment>`_ "
"演示实现其他 ``nn`` 层的示例，但为了简洁起见，这在本教程中被略去。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Going one step further"
msgstr "更进一步"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"So far, we have demonstrated how to implement a performant "
"``MultiheadAttention`` layer that follows the traditional "
"``nn.MultiheadAttention``. Going back to our classification of modifications"
" to the transformer architecture, remember that we classified the "
"modifications into layer type, layer ordering, and modifications to the "
"attention score. We trust that changing layer type and layer ordering (such "
"as swapping ``LayerNorm`` for ``RMSNorm``) is fairly straightforward."
msgstr ""
"到目前为止，我们已经展示了如何实现一个性能优异的 ``MultiheadAttention`` 层，该层遵循传统的 "
"``nn.MultiheadAttention``。回到我们对变换器架构修改的分类，请记住我们将这些修改分类为层类型、层顺序和对注意力分数的修改。我们相信更改层类型和层顺序（如将"
" ``LayerNorm`` 替换为 ``RMSNorm``）是相对简单的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this section, we will discuss various functionalities using the "
"aforementioned building blocks, including the following:"
msgstr "在本节中，我们将讨论使用上述构建模块的各种功能，包括以下内容："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Cross Attention"
msgstr "交叉注意力"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Fully masked rows no longer cause NaNs"
msgstr "完全遮蔽的行不再导致 NaN"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Modifying attention score: ALiBi with FlexAttention and NJT"
msgstr "修改注意力分数：使用 FlexAttention 和 NJT 实现 ALiBi"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Packed Projection"
msgstr "嵌套投影"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Cross attention is a form of attention where the query and key/value tensors"
" are from different sequences."
msgstr "交叉注意力是一种注意力形式，其中查询和键/值张量来自不同的序列。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"One example of this is in ``nn.TransformerDecoderLayer`` where the query "
"comes from the decoder and the key/value come from the encoder."
msgstr "一个示例是 ``nn.TransformerDecoderLayer`` 中，查询来自解码器，键/值来自编码器。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"The above MultiheadAttention layer nicely generalizes to this case with "
"nested tensors for both query and key/value."
msgstr "上述 MultiheadAttention 层很好地将嵌套张量扩展到这种情况下的查询和键/值。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"As above, we can compare this against the vanilla compiled "
"``nn.MultiheadAttention``."
msgstr "与上述一样，我们可以对比其与传统的编译 `nn.MultiheadAttention`。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"There has been a long standing issue with ``nn.MultiheadAttention`` and "
"``scaled_dot_product_attention`` where if a row was fully masked out, the "
"output of the attention layer would be NaN. See `issue "
"<https://github.com/pytorch/pytorch/issues/41508>`_. This is because the "
"softmax over an empty set is undefined."
msgstr ""
"长期以来，``nn.MultiheadAttention`` 和 ``scaled_dot_product_attention`` "
"一直存在一个问题，即如果某行完全被遮蔽，则注意力层的输出将是 NaN。请参见 `问题 "
"<https://github.com/pytorch/pytorch/issues/41508>`_。这是因为对空集进行软最大化是未定义的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Thanks to `this PR <https://github.com/pytorch/pytorch/pull/133882>`_ this "
"is no longer the case. Instead, the output corresponding to fully masked "
"rows in ``scaled_dot_product_attention`` will be 0. For cases where "
"``nn.MHA`` does not employ the \"fast-path\", this will also apply."
msgstr ""
"得益于`这个PR "
"<https://github.com/pytorch/pytorch/pull/133882>`_，情况不再是这样。现在，在``scaled_dot_product_attention``中完全被掩盖的行对应的输出将为0。而对于``nn.MHA``不使用“快速路径”的情况，这也同样适用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Using a custom MHA layer with NJTs is strongly recommended over the existing"
" \"fast-path\" in ``nn.MultiheadAttention`` as NJT's ability to model "
"raggedness appropriately makes it possible to properly express empty "
"sequences."
msgstr ""
"强烈建议在使用自定义的MHA层时搭配NJT，而不是使用现有的``nn.MultiheadAttention``中的“快速路径”，因为NJT能够合理地表示不规则性，从而可以正确表达空序列。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "FlexAttention + NJT"
msgstr "FlexAttention + NJT"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"NJT also composes with the ``FlexAttention`` module. This is a "
"generalization of the ``MultiheadAttention`` layer that allows for arbitrary"
" modifications to the attention score. The example below takes the "
"``alibi_mod`` that implements `ALiBi <https://arxiv.org/abs/2108.12409>`_ "
"from `attention gym <https://github.com/pytorch-labs/attention-gym>`_ and "
"uses it with nested input tensors."
msgstr ""
"NJT还可以与``FlexAttention``模块组合使用。``FlexAttention``模块是``MultiheadAttention``层的通用化版本，允许对注意力分数进行任意修改。下面的示例展示了如何使用`attention"
" gym <https://github.com/pytorch-labs/attention-"
"gym>`_中的``alibi_mod``实现`ALiBi "
"<https://arxiv.org/abs/2108.12409>`_，并将其与嵌套输入张量结合使用。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In addition, one can also use the ``block_mask`` utility of "
"``FlexAttention`` with NJTs via the ``create_nested_block_mask`` function. "
"This is useful for taking advantage of the sparsity of the mask to speed up "
"the attention computation. In particular, the function creates a sparse "
"block mask for a \"stacked sequence\" of all the variable length sequences "
"in the NJT combined into one, while properly masking out inter-sequence "
"attention. In the following example, we show how to create a causal block "
"mask using this utility."
msgstr ""
"此外，还可以通过``create_nested_block_mask``函数将``FlexAttention``的``block_mask``实用程序与NJT结合使用。这对于利用掩码的稀疏性来加速注意力计算非常有用。具体来说，该函数为所有NJT中可变长度序列合并为一个“堆叠序列”创建稀疏块掩码，同时正确屏蔽序列间的注意。在以下示例中，我们展示了如何使用此工具创建因果块掩码。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Packed projection is a technique that makes use of the fact that when the "
"input for projection (matrix multiplications) are the same (self-attention),"
" we can pack the projection weights and biases into single tensors. It is "
"especially useful when the individual projections are memory bound rather "
"than compute bound. There are two examples that we will demonstrate here:"
msgstr ""
"打包投影是一种技术，它利用了当用于投影（矩阵乘法）的输入相同时（自注意），我们可以将投影权重和偏置打包为单个张量的特点。它尤其适用于单个投影受限于内存而不是计算的情况。我们将在此演示两个例子："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Input projection for MultiheadAttention"
msgstr "MultiheadAttention的输入投影"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "SwiGLU activation in feed-forward network of Transformer Layer"
msgstr "Transformer层中前馈网络的SwiGLU激活"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"When doing self-attention, the ``query``, ``key``, and ``value`` are the "
"same tensor. Each of these tensors is projected with a ``Linear(E_q, "
"E_total)`` layer. Instead, we can pack this into one layer, which is what we"
" do in the MultiheadAttention layer above."
msgstr ""
"在执行自注意时，``query``、``key``和``value``是同一个张量。每个张量都通过``Linear(E_q, "
"E_total)``层进行投影。相反，我们可以将这些投影打包为一个层，这就是我们在上述MultiheadAttention层中所做的。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Let us compare the performance of the packed projection against the usual "
"method:"
msgstr "让我们比较打包投影与通常方法的性能："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "SwiGLU feed forward network of Transformer Layer"
msgstr "Transformer层中的SwiGLU前馈网络"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"Swish-Gated Linear Unit (SwiGLU) is a non-linear activation function that is"
" increasingly popular in the feed-forward network of the transformer layer "
"(e.g. Llama). A feed-forward network with SwiGLU activation is defined as:"
msgstr ""
"Swish门控线性单元（SwiGLU）是一种非线性激活函数，在transformer层的前馈网络中越来越受欢迎（例如Llama）。使用SwiGLU激活的前馈网络定义为："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "An alternative way of implementing this that uses packed projection is"
msgstr "使用打包投影实现该网路的另一种方法是"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We can compare the performance of the two implementations as follows "
"Depending on your hardware, you might see different results. On an A100 I "
"see 1.12x speedup for D=128."
msgstr "我们可以比较这两种实现的性能，如下所示。根据您的硬件，您可能会看到不同的结果。在A100上，我观察到D=128时的速度提高了1.12倍。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "Extended examples"
msgstr "扩展示例"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"We intend to update this tutorial to demonstrate more examples of how to use"
" the various performant building blocks such as KV-Caching, Grouped Query "
"Attention etc. Further, there are several good examples of using various "
"performant building blocks to implement various transformer architectures. "
"Some examples include"
msgstr ""
"我们计划更新本教程，以展示如何使用各种高性能构建模块（例如KV缓存、分组查询注意力等）的更多示例。此外，还有一些使用各种高性能构建模块来实现不同transformer架构的优秀示例。一些示例如下："

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid "`gpt-fast <https://github.com/pytorch-labs/gpt-fast>`_"
msgstr "`gpt-fast <https://github.com/pytorch-labs/gpt-fast>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`segment-anything-fast <https://github.com/pytorch-labs/segment-anything-"
"fast>`_"
msgstr ""
"`segment-anything-fast <https://github.com/pytorch-labs/segment-anything-"
"fast>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`lucidrains implementation of NaViT with nested tensors "
"<https://github.com/lucidrains/vit-"
"pytorch/blob/73199ab486e0fad9eced2e3350a11681db08b61b/vit_pytorch/na_vit_nested_tensor.py>`_"
msgstr ""
"`lucidrains带嵌套张量的NaViT实现 <https://github.com/lucidrains/vit-"
"pytorch/blob/73199ab486e0fad9eced2e3350a11681db08b61b/vit_pytorch/na_vit_nested_tensor.py>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"`torchtune's implementation of VisionTransformer "
"<https://github.com/pytorch/torchtune/blob/a8a64ec6a99a6ea2be4fdaf0cd5797b03a2567cf/torchtune/modules/vision_transformer.py#L16>`_"
msgstr ""
"`torchtune&apos;s VisionTransformer实现 "
"<https://github.com/pytorch/torchtune/blob/a8a64ec6a99a6ea2be4fdaf0cd5797b03a2567cf/torchtune/modules/vision_transformer.py#L16>`_"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
"In this tutorial, we have introduced the low level building blocks PyTorch "
"provides for writing transformer layers and demonstrated examples how to "
"compose them. It is our hope that this tutorial has educated the reader on "
"the ease with which flexible and performant transformer layers can be "
"implemented by users of PyTorch."
msgstr ""
"在本教程中，我们介绍了PyTorch提供的用于编写transformer层的低级构建块，并展示了如何组合它们的示例。我们希望本教程能让读者了解在PyTorch中实现灵活且高性能的transformer层是多么容易。"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Python source code: transformer_building_blocks.py "
"<transformer_building_blocks.py>`"
msgstr ""
":download:`下载Python源代码: transformer_building_blocks.py "
"<transformer_building_blocks.py>`"

#: ../../intermediate/transformer_building_blocks.rst:1033
msgid ""
":download:`Download Jupyter notebook: transformer_building_blocks.ipynb "
"<transformer_building_blocks.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: transformer_building_blocks.ipynb "
"<transformer_building_blocks.ipynb>`"
