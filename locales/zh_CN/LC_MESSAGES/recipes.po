#
msgid ""
msgstr ""

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Leverage Intel® Advanced Matrix Extensions"
msgstr "利用英特尔®高级矩阵扩展"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Introduction"
msgstr "简介"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Advanced Matrix Extensions (AMX), also known as Intel® Advanced Matrix "
"Extensions (Intel® AMX), is an x86 extension, which introduce two new "
"components: a 2-dimensional register file called 'tiles' and an accelerator "
"of Tile Matrix Multiplication (TMUL) that is able to operate on those tiles."
" AMX is designed to work on matrices to accelerate deep-learning training "
"and inference on the CPU and is ideal for workloads like natural-language "
"processing, recommendation systems and image recognition."
msgstr ""
"高级矩阵扩展（AMX），也称为英特尔®高级矩阵扩展（Intel® "
"AMX），是一种x86扩展，它引入了两个新组件：一个称为“tiles”的二维寄存器文件和一个可以操作这些tiles的矩阵乘加加速器（TMUL）。AMX旨在处理矩阵操作，以加速CPU上的深度学习训练和推理，非常适合自然语言处理、推荐系统和图像识别等工作负载。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel advances AI capabilities with 4th Gen Intel® Xeon® Scalable processors"
" and Intel® AMX, delivering 3x to 10x higher inference and training "
"performance versus the previous generation, see `Accelerate AI Workloads "
"with Intel® AMX`_. Compared to 3rd Gen Intel Xeon Scalable processors "
"running Intel® Advanced Vector Extensions 512 Neural Network Instructions "
"(Intel® AVX-512 VNNI), 4th Gen Intel Xeon Scalable processors running Intel "
"AMX can perform 2,048 INT8 operations per cycle, rather than 256 INT8 "
"operations per cycle. They can also perform 1,024 BF16 operations per cycle,"
" as compared to 64 FP32 operations per cycle, see page 4 of `Accelerate AI "
"Workloads with Intel® AMX`_. For more detailed information of AMX, see "
"`Intel® AMX Overview`_."
msgstr ""
"英特尔通过第4代英特尔®至强®可扩展处理器和英特尔®AMX，提供比前一代高3到10倍的推理和训练性能，详见`通过英特尔®AMX加速AI工作负载`_。与运行英特尔®高级矢量扩展512神经网络指令（Intel®"
" AVX-512 "
"VNNI）的第3代英特尔至强可扩展处理器相比，运行英特尔AMX的第4代至强可扩展处理器每周期可执行2,048次INT8操作，而非256次INT8操作。它们每周期还可执行1,024次BF16操作，而非64次FP32操作，详见`通过英特尔®AMX加速AI工作负载`_的第4页。有关AMX的更多详细信息，参见`英特尔®AMX概述`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "AMX in PyTorch"
msgstr "PyTorch中的AMX"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch leverages AMX for computing intensive operators with BFloat16 and "
"quantization with INT8 by its backend oneDNN to get higher performance out-"
"of-box on x86 CPUs with AMX support. For more detailed information of "
"oneDNN, see `oneDNN`_."
msgstr ""
"PyTorch利用AMX通过其后端oneDNN对使用BFloat16和INT8量化的计算密集型操作符实现开箱即用的高性能支持x86 "
"CPU的AMX功能。有关oneDNN的更多详细信息，请参见`oneDNN`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The operation is fully handled by oneDNN according to the execution code "
"path generated. For example, when a supported operation gets executed into "
"oneDNN implementation on a hardware platform with AMX support, AMX "
"instructions will be invoked automatically inside oneDNN. Since oneDNN is "
"the default acceleration library for PyTorch CPU, no manual operations are "
"required to enable the AMX support."
msgstr ""
"操作的处理完全由oneDNN根据生成的执行代码路径进行。例如，当支持的操作在支持AMX的硬件平台上执行到oneDNN实现时，AMX指令将在oneDNN内部自动调用。由于oneDNN是PyTorch"
" CPU的默认加速库，因此无需手动操作来启用AMX支持。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Guidelines of leveraging AMX with workloads"
msgstr "利用AMX处理工作负载的指南"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This section provides guidelines on how to leverage AMX with various "
"workloads."
msgstr "本节提供有关如何利用AMX处理各种工作负载的指南。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "BFloat16 data type:"
msgstr "BFloat16数据类型："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Using ``torch.cpu.amp`` or ``torch.autocast(\"cpu\")`` would utilize AMX "
"acceleration for supported operators."
msgstr "使用``torch.cpu.amp``或``torch.autocast(\"cpu\")``将为支持的操作符利用AMX加速。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Use ``torch.channels_last`` memory format to get better performance."
msgstr "使用``torch.channels_last``内存格式以获得更好的性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Quantization:"
msgstr "量化："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Applying quantization would utilize AMX acceleration for supported "
"operators."
msgstr "应用量化将为支持的操作符利用AMX加速。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "torch.compile:"
msgstr "torch.compile："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When the generated graph model runs into oneDNN implementations with the "
"supported operators, AMX accelerations will be activated."
msgstr "当生成的图形模型运行到带有支持操作符的oneDNN实现时，将激活AMX加速。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When using PyTorch on CPUs that support AMX, the framework will "
"automatically enable AMX usage by default. This means that PyTorch will "
"attempt to leverage the AMX feature whenever possible to speed up matrix "
"multiplication operations. However, it's important to note that the decision"
" to dispatch to the AMX kernel ultimately depends on the internal "
"optimization strategy of the oneDNN library and the quantization backend, "
"which PyTorch relies on for performance enhancements. The specific details "
"of how AMX utilization is handled internally by PyTorch and the oneDNN "
"library may be subject to change with updates and improvements to the "
"framework."
msgstr ""
"使用支持AMX的CPU运行PyTorch时，框架将默认自动启用AMX功能。这意味着PyTorch将尽可能利用AMX功能来加速矩阵乘法操作。但需要注意的是，最终是否调度至AMX内核取决于oneDNN库和量化后端的内部优化策略，PyTorch依赖它们进行性能提升。PyTorch和oneDNN库如何在内部处理AMX的具体细节可能会随着框架的更新和改进而改变。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "CPU operators that can leverage AMX:"
msgstr "可利用AMX的CPU操作符："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "BF16 CPU ops that can leverage AMX:"
msgstr "可利用AMX的BF16 CPU操作符："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``conv1d``"
msgstr "``conv1d``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``conv2d``"
msgstr "``conv2d``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``conv3d``"
msgstr "``conv3d``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``conv_transpose1d``"
msgstr "``conv_transpose1d``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``conv_transpose2d``"
msgstr "``conv_transpose2d``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``conv_transpose3d``"
msgstr "``conv_transpose3d``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``bmm``"
msgstr "``bmm``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``mm``"
msgstr "``mm``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``baddbmm``"
msgstr "``baddbmm``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``addmm``"
msgstr "``addmm``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``addbmm``"
msgstr "``addbmm``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``linear``"
msgstr "``linear``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``matmul``"
msgstr "``matmul``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Quantization CPU ops that can leverage AMX:"
msgstr "可利用AMX的量化CPU操作符："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Confirm AMX is being utilized"
msgstr "确认AMX是否被利用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Set environment variable ``export ONEDNN_VERBOSE=1``, or use "
"``torch.backends.mkldnn.verbose`` to enable oneDNN to dump verbose messages."
msgstr ""
"设置环境变量``export "
"ONEDNN_VERBOSE=1``，或使用``torch.backends.mkldnn.verbose``启用oneDNN输出详细消息。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "For example, get oneDNN verbose:"
msgstr "例如，获取oneDNN详细信息："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you get the verbose of ``avx512_core_amx_bf16`` for BFloat16 or "
"``avx512_core_amx_int8`` for quantization with INT8, it indicates that AMX "
"is activated."
msgstr ""
"如果您获取到``avx512_core_amx_bf16``的BFloat16或``avx512_core_amx_int8``的INT8量化的详细信息，这表明AMX已激活。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Conclusion"
msgstr "结论"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we briefly introduced AMX, how to utilize AMX in PyTorch "
"to accelerate workloads, and how to confirm that AMX is being utilized."
msgstr "在本教程中，我们简要介绍了AMX，如何在PyTorch中使用AMX加速工作负载，以及如何确认AMX是否被利用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"With the improvements and updates of PyTorch and oneDNN, the utilization of "
"AMX may be subject to change accordingly."
msgstr "随着PyTorch和oneDNN的改进和更新，AMX的利用可能会相应发生变化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As always, if you run into any problems or have any questions, you can use "
"`forum <https://discuss.pytorch.org/>`_ or `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_ to get in touch."
msgstr ""
"一如既往，如果您遇到任何问题或有任何疑问，可以通过`论坛 <https://discuss.pytorch.org/>`_或`GitHub问题 "
"<https://github.com/pytorch/pytorch/issues>`_联系我们。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "edit"
msgstr "编辑"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Making Native Android Application that uses PyTorch prebuilt libraries"
msgstr "制作使用PyTorch预构建库的原生Android应用程序"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch Mobile is no longer actively supported. Please check out `ExecuTorch"
" <https://github.com/pytorch/executorch>`__."
msgstr ""
"PyTorch Mobile不再获得积极支持。请查看`ExecuTorch "
"<https://github.com/pytorch/executorch>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Redirecting in 3 seconds..."
msgstr "3秒后重定向..."

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(beta) Bundling inputs to PyTorch Models"
msgstr "(测试版) 将输入打包到PyTorch模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author**: `Jacob Szwejbka <https://github.com/JacobSzwejbka>`_"
msgstr "**作者**：`Jacob Szwejbka <https://github.com/JacobSzwejbka>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This tutorial introduces the steps to use PyTorch's utility to bundle "
"example or trivial inputs directly into your TorchScript Module."
msgstr "本教程介绍了使用PyTorch工具将示例或简单输入直接打包到TorchScript模块中的步骤。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The interface of the model remains unchanged (other than adding a few "
"methods), so it can still be safely deployed to production. The advantage of"
" this standardized interface is that tools that run models can use it "
"instead of having some sort of external file (or worse, document) that tells"
" you how to run the model properly."
msgstr ""
"模型的接口保持不变（除了添加了一些方法），因此仍然可以安全地部署到生产环境。标准化接口的优势在于，运行模型的工具可以使用它，而不需要依赖某种外部文件（或者更差的是文档）来说明如何正确运行模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Common case"
msgstr "常见情况"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One of the common cases—bundling an input to a model that only uses "
"'forward' for inference."
msgstr "一种常见情况——为仅使用“forward”进行推理的模型打包输入。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Prepare model**: Convert your model to TorchScript through either tracing "
"or scripting"
msgstr "**准备模型**：通过追踪或脚本将模型转换为TorchScript"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Create example input and attach to model**"
msgstr "**创建示例输入并附加到模型**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Run model with input as arguments**"
msgstr "**使用输入作为参数运行模型**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Uncommon case"
msgstr "非典型情况"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"An uncommon case would be bundling and retrieving inputs for functions "
"beyond 'forward'."
msgstr "一种非典型情况是为除“forward”之外的函数打包和检索输入。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Retrieve inputs and run model on them**"
msgstr "**检索输入并运行模型**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inflatable args"
msgstr "可膨胀的参数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Attaching inputs to models can result in nontrivial size increases. "
"Inflatable args are a way to compress and decompress inputs to minimize this"
" impact."
msgstr "将输入附加到模型可能会导致非小的体积增加。可膨胀参数是一种压缩和解压输入以最小化这种影响的方法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Any automatic compression, or parsing of inflatable args only happens to top"
" level arguments in the input tuple."
msgstr "对输入元组中的顶层参数，任何自动的压缩或可膨胀参数解析都会发生。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"ie if your model takes in a List type of inputs you would need to create an "
"inflatable arg that returned a list not create a list of inflatable args."
msgstr "例如，如果模型接受一个列表类型的输入，您需要创建一个返回列表的可膨胀参数，而不是创建一个可膨胀参数的列表。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Existing Inflatable args**"
msgstr "**现有的可膨胀参数**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The following input types are compressed automatically without requiring an "
"explicit inflatable arg:"
msgstr "以下输入类型无需显式可膨胀参数即可自动压缩："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Small contiguous tensors are cloned to have small storage."
msgstr "小的连续张量被克隆以具有小存储。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Inputs from torch.zeros, torch.ones, or torch.full are moved to their "
"compact representations."
msgstr "来自torch.zeros、torch.ones或torch.full的输入被移至其紧凑表示形式。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Creating your own**"
msgstr "**创建您自己的**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Inflatable args are composed of 2 parts, the deflated (compressed) argument,"
" and an expression or function definition to inflate them."
msgstr "可膨胀参数由两个部分组成，压缩后的参数和一个用于解压的表达式或函数定义。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Using a function instead**"
msgstr "**使用函数代替**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you need to create a more complicated input providing a function is an "
"easy alternative"
msgstr "如果需要创建更复杂的输入，提供一个函数是一个简单的替代方法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Learn More"
msgstr "了解更多"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To learn more about PyTorch Mobile, please refer to `PyTorch Mobile Home "
"Page <https://pytorch.org/mobile/home/>`_"
msgstr ""
"要了解有关PyTorch Mobile的更多内容，请参见`PyTorch Mobile主页 "
"<https://pytorch.org/mobile/home/>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(beta) Compiling the optimizer with torch.compile"
msgstr "(测试版) 使用torch.compile编译优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author:** `Michael Lazos <https://github.com/mlazos>`_"
msgstr "**作者**：`Michael Lazos <https://github.com/mlazos>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The optimizer is a key algorithm for training any deep learning model. Since"
" it is responsible for updating every model parameter, it can often become "
"the bottleneck in training performance for large models. In this recipe, we "
"will apply ``torch.compile`` to the optimizer to observe the GPU performance"
" improvement."
msgstr ""
"优化器是训练任何深度学习模型的关键算法。由于它负责更新每个模型参数，因此在大型模型的训练性能中，这常常会成为瓶颈。在此示例中，我们将对优化器应用``torch.compile``以观察其在GPU上的性能改进。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This tutorial requires PyTorch 2.2.0 or later."
msgstr "本教程需要PyTorch 2.2.0或更高版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Setup"
msgstr "模型设置"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For this example, we'll use a simple sequence of linear layers. Since we are"
" only benchmarking the optimizer, the choice of model doesn't matter because"
" optimizer performance is a function of the number of parameters."
msgstr "在此示例中，我们将使用一系列简单的线性层。由于我们仅对优化器进行基准测试，模型的选择并不重要，因为优化器性能取决于参数的数量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Depending on what machine you are using, your exact results may vary."
msgstr "取决于您使用的机器，确切结果可能会有所不同。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Setting up and running the optimizer benchmark"
msgstr "设置并运行优化器基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this example, we'll use the Adam optimizer and create a helper function "
"to wrap the step() in ``torch.compile()``."
msgstr "在此示例中，我们将使用Adam优化器并创建一个辅助函数将``step()``包装在``torch.compile()``中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compile`` is only supported on cuda devices with compute capability "
">= 7.0"
msgstr "``torch.compile``仅支持计算能力>=7.0的CUDA设备。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Sample Results:"
msgstr "示例结果："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Eager runtime: 747.2437149845064us"
msgstr "Eager运行时：747.2437149845064微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Compiled runtime: 392.07384741178us"
msgstr "编译运行时：392.07384741178微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "See Also"
msgstr "另请参见"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "For an in-depth technical overview, see"
msgstr "有关深入的技术概述，请参见"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Compiling the optimizer with PT2 <https://dev-"
"discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669>`__"
msgstr ""
"`使用PT2编译优化器 <https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-"
"pt2/1669>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_compiling_optimizer_lr_scheduler.py>` to download"
" the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_recipes_compiling_optimizer_lr_scheduler.py>`"
" 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(beta) Running the compiled optimizer with an LR Scheduler"
msgstr "(测试版) 使用学习率调度器运行已编译优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The optimizer is a key algorithm for training any deep learning model. In "
"this example, we will show how to pair the optimizer, which has been "
"compiled using ``torch.compile``, with the LR schedulers to accelerate "
"training convergence."
msgstr ""
"优化器是训练任何深度学习模型的关键算法。在此示例中，我们将展示如何将通过``torch.compile``编译的优化器与学习率调度器配对以加速训练收敛。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This tutorial requires PyTorch 2.3.0 or later."
msgstr "本教程需要PyTorch 2.3.0或更高版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "For this example, we'll use a simple sequence of linear layers."
msgstr "在此示例中，我们将使用一系列简单的线性层。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Setting up and running the compiled optimizer with LR Scheduler"
msgstr "设置并运行带学习率调度器的已编译优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this section, we'll use the Adam optimizer with LinearLR Scheduler and "
"create a helper function to wrap the ``step()`` call for each of them in "
"``torch.compile()``."
msgstr ""
"在本节中，我们将使用Adam优化器和LinearLR调度器，并创建一个辅助函数为它们分别的``step()``调用包装``torch.compile()``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compile`` is only supported on CUDA devices that have a compute "
"capability of 7.0 or higher."
msgstr "``torch.compile``仅支持计算能力>=7.0的CUDA设备。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Extension: What happens with a non-tensor LR?"
msgstr "扩展：非张量学习率会发生什么？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For the curious, we will show how to peek into what happens with "
"``torch.compile`` when we don't wrap the LR in a tensor."
msgstr "对于好奇者，我们将展示当不将学习率包装在张量中时，``torch.compile``会发生什么。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"With this example, we can see that we recompile the optimizer a few times "
"due to the guard failure on the ``lr`` in ``param_groups[0]``."
msgstr "通过此示例，我们可以看到由于学习率``param_groups[0]``中的守护失败，我们多次重新编译了优化器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial we showed how to pair the optimizer compiled with "
"``torch.compile`` with an LR Scheduler to accelerate training convergence. "
"We used a model consisting of a simple sequence of linear layers with the "
"Adam optimizer paired with a LinearLR scheduler to demonstrate the LR "
"changing across iterations."
msgstr ""
"在本教程中，我们展示了如何将使用``torch.compile``编译的优化器与学习率调度器配对，以加速训练收敛。我们使用了由简单线性层组成的模型，其中Adam优化器与LinearLR调度器配对，演示学习率在迭代中变化的情况。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "See also:"
msgstr "另请参见："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Compiled optimizer tutorial "
"<https://pytorch.org/tutorials/recipes/compiling_optimizer.html>`__ - an "
"intro into the compiled optimizer."
msgstr ""
"`编译优化器教程 <https://pytorch.org/tutorials/recipes/compiling_optimizer.html>`__"
" - 一个关于编译优化器的介绍。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Compiling the optimizer with PT2 <https://dev-"
"discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669>`__ - deeper "
"technical details on the compiled optimizer."
msgstr ""
"`使用PT2编译优化器 <https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-"
"pt2/1669>`__ - 有关编译优化器的更深入技术细节。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Total running time of the script:** ( 0 minutes  0.000 seconds)"
msgstr "**脚本的总运行时间：** (0分钟 0.000秒)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: compiling_optimizer_lr_scheduler.py "
"<compiling_optimizer_lr_scheduler.py>`"
msgstr ""
":download:`下载Python源代码：compiling_optimizer_lr_scheduler.py "
"<compiling_optimizer_lr_scheduler.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: compiling_optimizer_lr_scheduler.ipynb"
" <compiling_optimizer_lr_scheduler.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：compiling_optimizer_lr_scheduler.ipynb "
"<compiling_optimizer_lr_scheduler.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`由Sphinx-Gallery生成的图集 <https://sphinx-gallery.github.io>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Direct Device-to-Device Communication with TensorPipe CUDA RPC"
msgstr "使用 TensorPipe CUDA RPC 的直接设备间通信"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Direct device-to-device RPC (CUDA RPC) is introduced in PyTorch 1.8 as a "
"prototype feature. This API is subject to change."
msgstr "设备间直接 RPC（CUDA RPC）在 PyTorch 1.8 中作为原型特性引入。此 API 可能会发生变化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In this recipe, you will learn:"
msgstr "在此教程中，您将学习："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The high-level idea of CUDA RPC."
msgstr "CUDA RPC 的高级概念。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use CUDA RPC."
msgstr "如何使用 CUDA RPC。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Requirements"
msgstr "要求"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 1.8+"
msgstr "PyTorch 1.8+"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Getting Started With Distributed RPC Framework "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`_"
msgstr ""
"`分布式 RPC 框架入门教程 "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is CUDA RPC?"
msgstr "什么是 CUDA RPC?"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"CUDA RPC supports directly sending Tensors from local CUDA memory to remote "
"CUDA memory. Prior to v1.8 release, PyTorch RPC only accepts CPU Tensors. As"
" a result, when an application needs to send a CUDA Tensor through RPC, it "
"has to first move the Tensor to CPU on the caller, send it via RPC, and then"
" move it to the destination device on the callee, which incurs both "
"unnecessary synchronizations and D2H and H2D copies. Since v1.8, RPC allows "
"users to configure a per-process global device map using the `set_device_map"
" "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map>`_"
" API, specifying how to map local devices to remote devices. More "
"specifically, if ``worker0``'s device map has an entry ``\"worker1\" : "
"{\"cuda:0\" : \"cuda:1\"}``, all RPC arguments on ``\"cuda:0\"`` from "
"``worker0`` will be directly sent to ``\"cuda:1\"`` on ``worker1``. The "
"response of an RPC will use the inverse of the caller device map, i.e., if "
"``worker1`` returns a Tensor on ``\"cuda:1\"``, it will be directly sent to "
"``\"cuda:0\"`` on ``worker0``. All intended device-to-device direct "
"communication must be specified in the per-process device map. Otherwise, "
"only CPU tensors are allowed."
msgstr ""
"CUDA RPC 支持直接将张量从本地 CUDA 内存发送到远程 CUDA 内存。在 v1.8 版本之前，PyTorch RPC 仅接受 CPU "
"张量。因此，当应用程序需要通过 RPC 发送 CUDA 张量时，它必须先将张量从调用方的 GPU 移动到 CPU，再通过 RPC "
"发送，然后在被调用方将其移回目标设备，这会导致不必要的同步以及 D2H 和 H2D 拷贝。从 v1.8 开始，RPC 允许用户使用 "
"`set_device_map "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map>`_"
" API 配置每个进程的全局设备映射，指定如何将本地设备映射到远程设备。更具体地说，如果 ``worker0`` 的设备映射条目为 "
"``\"worker1\" : {\"cuda:0\" : \"cuda:1\"}``，则来自 ``worker0`` 的所有 "
"``\"cuda:0\"`` 上的 RPC 参数将直接发送到 ``worker1`` 的 ``\"cuda:1\"`` 上。RPC "
"的响应将使用调用方设备映射的逆映射，即如果 ``worker1`` 返回 ``\"cuda:1\"`` 上的张量，它将直接发送到 ``worker0``"
" 的 ``\"cuda:0\"`` 上。所有计划进行的设备间直接通信必须在每个进程的设备映射中指定。否则，只允许使用 CPU 张量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Under the hood, PyTorch RPC relies on `TensorPipe "
"<https://github.com/pytorch/tensorpipe>`_ as the communication backend. "
"PyTorch RPC extracts all Tensors from each request or response into a list "
"and packs everything else into a binary payload. Then, TensorPipe will "
"automatically choose a communication channel for each Tensor based on Tensor"
" device type and channel availability on both the caller and the callee. "
"Existing TensorPipe channels cover NVLink, InfiniBand, SHM, CMA, TCP, etc."
msgstr ""
"在底层，PyTorch RPC 依赖 `TensorPipe <https://github.com/pytorch/tensorpipe>`_ "
"作为通信后端。PyTorch RPC 将每个请求或响应中的所有张量提取到一个列表中，并将其他内容打包到二进制数据载荷中。然后，TensorPipe "
"将根据张量设备类型以及调用方和被调用方的通信通道可用性自动选择适合每个张量的通信通道。现有的 TensorPipe 通道包括 "
"NVLink、InfiniBand、SHM、CMA、TCP 等。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use CUDA RPC?"
msgstr "如何使用 CUDA RPC?"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code below shows how to use CUDA RPC. The model contains two linear "
"layers and is split into two shards. The two shards are placed on "
"``worker0`` and ``worker1`` respectively, and ``worker0`` serves as the "
"master that drives the forward and backward passes. Note that we "
"intentionally skipped `DistributedOptimizer "
"<https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim>`_ "
"to highlight the performance improvements when using CUDA RPC. The "
"experiment repeats the forward and backward passes 10 times and measures the"
" total execution time. It compares using CUDA RPC against manually staging "
"to CPU memory and using CPU RPC."
msgstr ""
"下面的代码展示了如何使用 CUDA RPC。模型包含两个线性层，并分成两个分片。这两个分片分别放置在 ``worker0`` 和 ``worker1``"
" 上，而 ``worker0`` 作为主节点驱动前向和后向过程。请注意，我们有意略过了 `DistributedOptimizer "
"<https://pytorch.org/docs/master/rpc.html#module-"
"torch.distributed.optim>`_，以突出使用 CUDA RPC 时的性能提升。实验重复前向和后向过程 10 "
"次，并测量总执行时间。它比较了使用 CUDA RPC 与手动移动到 CPU 内存并使用 CPU RPC 的效果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Outputs are displayed below, which shows that CUDA RPC can help to achieve "
"34X speed up compared to CPU RPC in this experiment."
msgstr "如下显示的输出表明，在本实验中使用 CUDA RPC 与 CPU RPC 比较可以实现 34 倍的速度提升。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Deploying with Flask"
msgstr "使用 Flask 部署"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to wrap your trained PyTorch model in a Flask container to expose it via"
" a web API"
msgstr "如何将训练后的 PyTorch 模型封装在 Flask 容器中并通过 Web API 进行暴露"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to translate incoming web requests into PyTorch tensors for your model"
msgstr "如何将 Web 请求解析为供模型使用的 PyTorch 张量"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to package your model’s output for an HTTP response"
msgstr "如何将模型的输出封装为 HTTP 响应"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You will need a Python 3 environment with the following packages (and their "
"dependencies) installed:"
msgstr "您需要一个安装了以下包（及其依赖项）的 Python 3 环境："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 1.5"
msgstr "PyTorch 1.5"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TorchVision 0.6.0"
msgstr "TorchVision 0.6.0"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Flask 1.1"
msgstr "Flask 1.1"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Optionally, to get some of the supporting files, you'll need git."
msgstr "可选：若需获得一些支持性文件，您需要安装 git。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The instructions for installing PyTorch and TorchVision are available at "
"`pytorch.org`_. Instructions for installing Flask are available on `the "
"Flask site`_."
msgstr ""
"安装 PyTorch 和 TorchVision 的指南可在 `pytorch.org`_ 上获得。安装 Flask 的指南可在 `Flask 站点`_"
" 上获得。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is Flask?"
msgstr "什么是 Flask?"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Flask is a lightweight web server written in Python. It provides a "
"convenient way for you to quickly set up a web API for predictions from your"
" trained PyTorch model, either for direct use, or as a web service within a "
"larger system."
msgstr ""
"Flask 是一个用 Python 编写的轻量级 Web 服务器。它为您提供了一种方便的方法，能够快速设置 Web API，以从训练后的 PyTorch"
" 模型进行预测，无论是直接使用还是作为较大系统中的 Web 服务。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Setup and Supporting Files"
msgstr "设置和支持文件"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We're going to create a web service that takes in images, and maps them to "
"one of the 1000 classes of the ImageNet dataset. To do this, you'll need an "
"image file for testing. Optionally, you can also get a file that will map "
"the class index output by the model to a human-readable class name."
msgstr ""
"我们将创建一个 Web 服务，它接收图像并将其映射到 ImageNet 数据集的 1000 "
"个类别之一。为此，您需要一个用于测试的图像文件。可选地，您还可以获得一个文件，该文件可将模型输出的类别索引映射为人类可读的类别名称。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Option 1: To Get Both Files Quickly"
msgstr "选项 1：快速获取两个文件"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You can pull both of the supporting files quickly by checking out the "
"TorchServe repository and copying them to your working folder. *(NB: There "
"is no dependency on TorchServe for this tutorial - it's just a quick way to "
"get the files.)* Issue the following commands from your shell prompt:"
msgstr ""
"您可以通过检出 TorchServe 仓库并将它们复制到您的工作目录中快速获取两个支持文件。（注意：此教程与 TorchServe "
"没有依赖关系——这只是获取文件的快捷方式。）从您的终端提示符发出以下命令："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "And you've got them!"
msgstr "然后您就能获得它们！"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Option 2: Bring Your Own Image"
msgstr "选项 2：使用您自己的图像"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The ``index_to_name.json`` file is optional in the Flask service below. You "
"can test your service with your own image - just make sure it's a 3-color "
"JPEG."
msgstr ""
"Flask 服务中的 ``index_to_name.json`` 文件是可选项。您可以使用自己的图像测试服务——只需确保它是 3 色 JPEG。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Building Your Flask Service"
msgstr "构建您的 Flask 服务"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The full Python script for the Flask service is shown at the end of this "
"recipe; you can copy and paste that into your own ``app.py`` file. Below "
"we'll look at individual sections to make their functions clear."
msgstr ""
"此教程最后展示了 Flask 服务的完整 Python 脚本；您可以将其复制粘贴到自己的 ``app.py`` "
"文件中。下面我们将逐步查看个别部分，以明确它们的功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Imports"
msgstr "导入"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In order:"
msgstr "依次为："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We'll be using a pre-trained DenseNet model from ``torchvision.models``"
msgstr "我们将使用来自 ``torchvision.models`` 的预训练 DenseNet 模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torchvision.transforms`` contains tools for manipulating your image data"
msgstr "``torchvision.transforms`` 包含用于操作图像数据的工具"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Pillow (``PIL``) is what we'll use to load the image file initially"
msgstr "Pillow（``PIL``）用于首次加载图像文件"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "And of course we'll need classes from ``flask``"
msgstr "当然还需要 ``flask`` 的类"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Pre-Processing"
msgstr "预处理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The web request gave us an image file, but our model expects a PyTorch "
"tensor of shape (N, 3, 224, 224) where *N* is the number of items in the "
"input batch. (We will just have a batch size of 1.) The first thing we do is"
" compose a set of TorchVision transforms that resize and crop the image, "
"convert it to a tensor, then normalize the values in the tensor. (For more "
"information on this normalization, see the documentation for "
"``torchvision.models_``.)"
msgstr ""
"Web 请求提供了一个图像文件，但我们模型需要一个形状为 (N, 3, 224, 224) 的 PyTorch 张量，其中 *N* "
"是输入批次的项数。（我们只会有一个批次大小为 1。）首先，我们创建了一组 TorchVision "
"变换，这些变换可重设图像尺寸并裁剪图像、将图像转换为张量，然后归一化张量中的值。（有关此归一化的更多信息，请参阅 "
"``torchvision.models_`` 的文档。）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"After that, we open the file and apply the transforms. The transforms return"
" a tensor of shape (3, 224, 224) - the 3 color channels of a 224x224 image. "
"Because we need to make this single image a batch, we use the "
"``unsqueeze_(0)`` call to modify the tensor in place by adding a new first "
"dimension. The tensor contains the same data, but now has shape (1, 3, 224, "
"224)."
msgstr ""
"之后，我们打开该文件并应用这些变换。变换返回了一个形状为 (3, 224, 224) 的张量——224x224 图像的 3 "
"个颜色通道。因为我们需要将这张单独的图像变成一个批次，所以我们使用 ``unsqueeze_(0)`` "
"来在原地修改张量，并添加一个新的第一个维度。张量包含相同的数据，但现在形状为 (1, 3, 224, 224)。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In general, even if you're not working with image data, you will need to "
"transform the input from your HTTP request into a tensor that PyTorch can "
"consume."
msgstr "一般来说，即使您未处理图像数据，也需要将 HTTP 请求中的输入转换为 PyTorch 可以处理的张量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inference"
msgstr "推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The inference itself is the simplest part: When we pass the input tensor to "
"them model, we get back a tensor of values that represent the model's "
"estimated likelihood that the image belongs to a particular class. The "
"``max()`` call finds the class with the maximum likelihood value, and "
"returns that value with the ImageNet class index. Finally, we extract that "
"class index from the tensor containing it with the ``item()`` call, and "
"return it."
msgstr ""
"推理本身是最简单的部分：当我们将输入张量传递给模型时，它会返回一个值张量，这些值代表模型估算的图像属于特定类别的可能性。``max()`` "
"调用找到可能性值最大的类别，并返回该值及其对应的 ImageNet 类索引。最后，我们使用 ``item()`` "
"调用从包含类索引的张量中提取该索引并返回。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Post-Processing"
msgstr "后处理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The ``render_prediction()`` method maps the predicted class index to a "
"human-readable class label. It's typical, after getting the prediction from "
"your model, to perform post-processing to make the prediction ready for "
"either human consumption, or for another piece of software."
msgstr ""
"``render_prediction()`` "
"方法将预测的类索引映射为人类可读的类标签。在从模型获得预测后，通常会进行后处理，以使预测适合人类使用或适合由另一个软件使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Running The Full Flask App"
msgstr "运行完整的 Flask 应用程序"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Paste the following into a file called ``app.py``:"
msgstr "将以下内容粘贴到一个名为 ``app.py`` 的文件中："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To start the server from your shell prompt, issue the following command:"
msgstr "从终端提示符启动服务器，请发出以下命令："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default, your Flask server is listening on port 5000. Once the server is "
"running, open another terminal window, and test your new inference server:"
msgstr "默认情况下，您的 Flask 服务器在端口 5000 上监听。当服务器正在运行时，打开另一个终端窗口，并测试您的新推理服务器："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If everything is set up correctly, you should recevie a response similar to "
"the following:"
msgstr "如果一切设置正确，您应该收到类似以下的响应："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Important Resources"
msgstr "重要资源"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`pytorch.org`_ for installation instructions, and more documentation and "
"tutorials"
msgstr "`pytorch.org`_ 提供安装说明和更多文档以及教程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The `Flask site`_ has a `Quick Start guide`_ that goes into more detail on "
"setting up a simple Flask service"
msgstr "`Flask 站点`_ 提供了一个 `快速入门指南`_，详细介绍了设置简单 Flask 服务的过程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Asynchronous Saving with Distributed Checkpoint (DCP)"
msgstr "使用分布式检查点 (DCP) 的异步保存"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Author:** `Lucas Pasqualin <https://github.com/lucasllc>`__, `Iris Zhang "
"<https://github.com/wz337>`__, `Rodrigo Kumpera "
"<https://github.com/kumpera>`__, `Chien-Chin Huang "
"<https://github.com/fegin>`__"
msgstr ""
"**作者：** `Lucas Pasqualin <https://github.com/lucasllc>`__, `Iris Zhang "
"<https://github.com/wz337>`__, `Rodrigo Kumpera "
"<https://github.com/kumpera>`__, `Chien-Chin Huang "
"<https://github.com/fegin>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Checkpointing is often a bottle-neck in the critical path for distributed "
"training workloads, incurring larger and larger costs as both model and "
"world sizes grow. One excellent strategy for offsetting this cost is to "
"checkpoint in parallel, asynchronously. Below, we expand the save example "
"from the `Getting Started with Distributed Checkpoint Tutorial "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst>`__"
" to show how this can be integrated quite easily with "
"``torch.distributed.checkpoint.async_save``."
msgstr ""
"检查点通常是分布式训练任务关键路径中的瓶颈，随着模型和世界规模的增长，其成本也越来越高。一种优秀的策略是异步并行检查点。下面，我们扩展了 "
"`分布式检查点教程入门 "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst>`__"
" 中保存的示例，以展示如何非常轻松地与 ``torch.distributed.checkpoint.async_save`` 集成。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"What you will learn"
msgstr ""
"您将学到什么"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use DCP to generate checkpoints in parallel"
msgstr "如何使用 DCP 并行生成检查点"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Effective strategies to optimize performance"
msgstr "优化性能的有效策略"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Prerequisites"
msgstr ""
"前提条件"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch v2.4.0 or later"
msgstr "PyTorch v2.4.0 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Getting Started with Distributed Checkpoint Tutorial "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst>`__"
msgstr ""
"`分布式检查点教程入门 "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Asynchronous Checkpointing Overview"
msgstr "异步检查点概述"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Before getting started with Asynchronous Checkpointing, it's important to "
"understand it's differences and limitations as compared to synchronous "
"checkpointing. Specifically:"
msgstr "在开始使用异步检查点之前，了解它与同步检查点之间的差异和限制很重要。具体来说："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Memory requirements - Asynchronous checkpointing works by first copying "
"models into internal CPU-buffers."
msgstr "内存需求 - 异步检查点通过将模型首先复制到内部CPU缓冲区来工作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is helpful since it ensures model and optimizer weights are not "
"changing while the model is still checkpointing, but does raise CPU memory "
"by a factor of ``checkpoint_size_per_rank X number_of_ranks``. Additionally,"
" users should take care to understand the memory constraints of their "
"systems. Specifically, pinned memory implies the usage of ``page-lock`` "
"memory, which can be scarce as compared to ``pageable`` memory."
msgstr ""
"这非常有帮助，因为它确保模型和优化器权重在检查点过程中不会改变，但会将CPU内存提高至“checkpoint_size_per_rank X "
"number_of_ranks”的倍数。此外，用户应注意了解其系统的内存限制。特别是，固定内存在使用“页面锁定”内存，这种内存相比于“可分页”内存更加稀缺。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Checkpoint Management - Since checkpointing is asynchronous, it is up to the"
" user to manage concurrently run checkpoints. In general, users can"
msgstr "检查点管理 - 由于检查点是异步的，用户需要自行管理同时运行的检查点。通常，用户可以"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"employ their own management strategies by handling the future object "
"returned form ``async_save``. For most users, we recommend limiting "
"checkpoints to one asynchronous request at a time, avoiding additional "
"memory pressure per request."
msgstr ""
"通过处理 `async_save` "
"返回的未来对象来采用自己的管理策略。对于大多数用户，我们建议将检查点限制为一次一个异步请求，以避免因每次请求带来的额外内存压力。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Even more performance with Pinned Memory"
msgstr "使用固定内存获得更高性能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If the above optimization is still not performant enough, you can take "
"advantage of an additional optimization for GPU models which utilizes a "
"pinned memory buffer for checkpoint staging. Specifically, this optimization"
" attacks the main overhead of asynchronous checkpointing, which is the in-"
"memory copying to checkpointing buffers. By maintaining a pinned memory "
"buffer between checkpoint requests users can take advantage of direct memory"
" access to speed up this copy."
msgstr ""
"如果上述优化仍然不足以满足性能需求，您可以通过使用额外优化来利用固定内存缓冲区对GPU模型进行检查点处理。特别地，这种优化针对异步检查点的主要开销，即将内存复制到检查点缓冲区。通过在检查点请求之间保持一个固定内存缓冲区，用户可以利用直接内存访问来加速复制。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The main drawback of this optimization is the persistence of the buffer in "
"between checkpointing steps. Without the pinned memory optimization (as "
"demonstrated above), any checkpointing buffers are released as soon as "
"checkpointing is finished. With the pinned memory implementation, this "
"buffer is maintained between steps, leading to the same peak memory pressure"
" being sustained through the application life."
msgstr ""
"这种优化的主要缺点是在检查点步骤之间保持缓冲区的持久性。没有固定内存优化（如上所述），任何检查点缓冲区在检查点完成后都会被释放。使用固定内存实现时，此缓冲区在步骤之间保持存在，导致应用生命周期内持续承受同样的峰值内存压力。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In conclusion, we have learned how to use DCP's :func:`async_save` API to "
"generate checkpoints off the critical training path. We've also learned "
"about the additional memory and concurrency overhead introduced by using "
"this API, as well as additional optimizations which utilize pinned memory to"
" speed things up even further."
msgstr ""
"总之，我们学习了如何使用DCP的 `async_save` API "
"在关键训练路径之外生成检查点。我们还了解了使用此API引入的额外内存和并发开销，以及利用固定内存进一步加速的额外优化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Saving and loading models tutorial "
"<https://pytorch.org/tutorials/beginner/saving_loading_models.html>`__"
msgstr ""
"`保存和加载模型教程 "
"<https://pytorch.org/tutorials/beginner/saving_loading_models.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Getting started with FullyShardedDataParallel tutorial "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__"
msgstr ""
"`FullyShardedDataParallel教程介绍 "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Getting Started with Distributed Checkpoint (DCP)"
msgstr "分布式检查点(DCP)入门"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Author**: `Iris Zhang <https://github.com/wz337>`__, `Rodrigo Kumpera "
"<https://github.com/kumpera>`__, `Chien-Chin Huang "
"<https://github.com/fegin>`__, `Lucas Pasqualin "
"<https://github.com/lucasllc>`__"
msgstr ""
"**作者**：`Iris Zhang <https://github.com/wz337>`__, `Rodrigo Kumpera "
"<https://github.com/kumpera>`__, `Chien-Chin Huang "
"<https://github.com/fegin>`__, `Lucas Pasqualin "
"<https://github.com/lucasllc>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst>`__."
msgstr ""
"|编辑| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst>`__"
" 中查看和编辑此教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Prerequisites:"
msgstr "先决条件："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`FullyShardedDataParallel API documents "
"<https://pytorch.org/docs/master/fsdp.html>`__"
msgstr ""
"`FullyShardedDataParallel API文档 "
"<https://pytorch.org/docs/master/fsdp.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.load API documents "
"<https://pytorch.org/docs/stable/generated/torch.load.html>`__"
msgstr ""
"`torch.load API文档 "
"<https://pytorch.org/docs/stable/generated/torch.load.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Checkpointing AI models during distributed training could be challenging, as"
" parameters and gradients are partitioned across trainers and the number of "
"trainers available could change when you resume training. Pytorch "
"Distributed Checkpointing (DCP) can help make this process easier."
msgstr ""
"在分布式训练期间检查点AI模型可能会很困难，因为参数和梯度在训练器之间分区，而当您恢复训练时可用的训练器数量可能会改变。Pytorch分布式检查点(DCP)可以帮助简化这个过程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we show how to use DCP APIs with a simple FSDP wrapped "
"model."
msgstr "在本教程中，我们展示了如何使用DCP API处理一个简单的FSDP封装模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How DCP works"
msgstr "DCP如何工作"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":func:`torch.distributed.checkpoint` enables saving and loading models from "
"multiple ranks in parallel. You can use this module to save on any number of"
" ranks in parallel, and then re-shard across differing cluster topologies at"
" load time."
msgstr ""
":func:`torch.distributed.checkpoint` "
"支持并行从多个进程保存和加载模型。您可以使用该模块在任意数量的进程中进行并行保存，然后在加载时重新划分到不同的集群拓扑中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Addditionally, through the use of modules in "
":func:`torch.distributed.checkpoint.state_dict`, DCP offers support for "
"gracefully handling ``state_dict`` generation and loading in distributed "
"settings. This includes managing fully-qualified-name (FQN) mappings across "
"models and optimizers, and setting default parameters for PyTorch provided "
"parallelisms."
msgstr ""
"此外，通过使用 :func:`torch.distributed.checkpoint.state_dict` 模块，DCP支持在分布式环境中优雅地处理"
" `state_dict` 的生成和加载。这包括管理跨模型和优化器的完全限定名称(FQN)映射，并设置PyTorch提供的并行默认参数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"DCP is different from :func:`torch.save` and :func:`torch.load` in a few "
"significant ways:"
msgstr "DCP与 :func:`torch.save` 和 :func:`torch.load` 在以下几个重要方面不同："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "It produces multiple files per checkpoint, with at least one per rank."
msgstr "它对每个检查点生成多个文件，至少每个进程一个。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"It operates in place, meaning that the model should allocate its data first "
"and DCP uses that storage instead."
msgstr "它在原地操作，意味着模型首先需要分配数据，然后DCP使用该存储空间。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"DCP offers special handling of Stateful objects (formally defined in "
"`torch.distributed.checkpoint.stateful`), automatically calling both "
"`state_dict` and `load_state_dict` methods if they are defined."
msgstr ""
"DCP对状态对象提供特殊处理（在 `torch.distributed.checkpoint.stateful` 中正式定义），如果定义了 "
"`state_dict` 和 `load_state_dict` 方法，会自动调用它们。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code in this tutorial runs on an 8-GPU server, but it can be easily "
"generalized to other environments."
msgstr "本教程中的代码运行在一个8-GPU服务器上，但可以轻松推广到其他环境。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use DCP"
msgstr "如何使用DCP"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Here we use a toy model wrapped with FSDP for demonstration purposes. "
"Similarly, the APIs and logic can be applied to larger models for "
"checkpointing."
msgstr "在这里我们使用一个封装了FSDP的玩具模型进行演示。类似地，这些API和逻辑可以应用于更大的模型检查点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Saving"
msgstr "保存"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Now, let's create a toy module, wrap it with FSDP, feed it with some dummy "
"input data, and save it."
msgstr "现在，让我们创建一个玩具模块，用FSDP封装它，用一些虚拟输入数据来喂养它并保存它。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Please go ahead and check the `checkpoint` directory. You should see 8 "
"checkpoint files as shown below."
msgstr "请继续检查 `checkpoint` 目录。您应该会看到如下所示的8个检查点文件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Distributed Checkpoint"
msgstr "分布式检查点"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Loading"
msgstr "加载"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"After saving, let’s create the same FSDP-wrapped model, and load the saved "
"state dict from storage into the model. You can load in the same world size "
"or different world size."
msgstr "保存后，我们创建一个相同的FSDP封装模型，并从存储中加载保存的状态字典到模型中。您可以在相同的世界大小或不同的世界大小加载数据。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Please note that you will have to call :func:`model.state_dict` prior to "
"loading and pass it to DCP's :func:`load_state_dict` API. This is "
"fundamentally different from :func:`torch.load`, as :func:`torch.load` "
"simply requires the path to the checkpoint prior for loading. The reason "
"that we need the ``state_dict`` prior to loading is:"
msgstr ""
"请注意，您需要在加载之前调用 :func:`model.state_dict` 并将其传递给DCP的 :func:`load_state_dict` "
"API。这与 :func:`torch.load` 基本不同，因为 :func:`torch.load` "
"仅要求加载之前提供检查点的路径。我们需要在加载之前的 `state_dict` 的原因是："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"DCP uses the pre-allocated storage from model state_dict to load from the "
"checkpoint directory. During loading, the state_dict passed in will be "
"updated in place."
msgstr "DCP使用模型状态字典中预分配的存储加载检查点目录数据。在加载期间，传入的状态字典将被原位更新。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"DCP requires the sharding information from the model prior to loading to "
"support resharding."
msgstr "DCP需要模型的分片信息支持加载时的重新分片。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you would like to load the saved checkpoint into a non-FSDP wrapped model"
" in a non-distributed setup, perhaps for inference, you can also do that "
"with DCP. By default, DCP saves and loads a distributed ``state_dict`` in "
"Single Program Multiple Data(SPMD) style. However if no process group is "
"initialized, DCP infers the intent is to save or load in \"non-distributed\""
" style, meaning entirely in the current process."
msgstr ""
"如果您想在非分布式设置中将已保存的检查点加载到非FSDP封装模型中，可能用于推理，您也可以使用DCP。默认情况下，DCP以单程序多数据(SPMD)风格保存和加载分布式"
" `state_dict`。但是如果未初始化任何进程组，则DCP会推断出意图是在\"非分布式\"风格中进行保存或加载，即完全在当前进程内。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Distributed checkpoint support for Multi-Program Multi-Data is still under "
"development."
msgstr "对多程序多数据格式的分布式检查点支持仍在开发中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Formats"
msgstr "格式"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One drawback not yet mentioned is that DCP saves checkpoints in a format "
"which is inherently different then those generated using torch.save. Since "
"this can be an issue when users wish to share models with users used to the "
"torch.save format, or in general just want to add format flexibility to "
"their applications. For this case, we provide the ``format_utils`` module in"
" ``torch.distributed.checkpoint.format_utils``."
msgstr ""
"尚未提到的一个缺点是，DCP以与使用torch.save生成的检查点格式本质上不同的格式保存检查点。当用户希望与习惯torch.save格式的其他用户分享模型时，或者在一般情况下想为应用增加格式灵活性时，这可能会成为一个问题。对于这种情况，我们提供了"
" `torch.distributed.checkpoint.format_utils` 中的 `format_utils` 模块。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A command line utility is provided for the users convenience, which follows "
"the following format:"
msgstr "提供了一个命令行工具供用户使用，其格式如下："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the command above, ``mode`` is one of ``torch_to_dcp`` or "
"``dcp_to_torch``."
msgstr "在上述命令中，`mode` 是 `torch_to_dcp` 或 `dcp_to_torch` 之一。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Alternatively, methods are also provided for users who may wish to convert "
"checkpoints directly."
msgstr "或者，对于希望直接转换检查点的用户，也提供了相应的方法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In conclusion, we have learned how to use DCP's :func:`save` and "
":func:`load` APIs, as well as how they are different form :func:`torch.save`"
" and :func:`torch.load`. Additionally, we've learned how to use "
":func:`get_state_dict` and :func:`set_state_dict` to automatically manage "
"parallelism-specific FQN's and defaults during state dict generation and "
"loading."
msgstr ""
"总之，我们学习了如何使用DCP的 `save` 和 `load` API，以及它们与 `torch.save` 和 `torch.load` "
"的不同。此外，我们还学习了如何使用 `get_state_dict` 和 `set_state_dict` "
"在状态字典生成和加载期间自动管理特定于并行的FQN和默认值。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "For more information, please see the following:"
msgstr "欲了解更多信息，请参见以下内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Getting Started with ``CommDebugMode``"
msgstr "``CommDebugMode`` 入门"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author**: `Anshul Sinha <https://github.com/sinhaanshul>`__"
msgstr "**作者**：`Anshul Sinha <https://github.com/sinhaanshul>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we will explore how to use ``CommDebugMode`` with "
"PyTorch's DistributedTensor (DTensor) for debugging by tracking collective "
"operations in distributed training environments."
msgstr ""
"在本教程中，我们将探讨如何使用PyTorch的DistributedTensor（DTensor）进行 ``CommDebugMode`` "
"的调试，并跟踪分布式训练环境中的集体操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Prerequisites"
msgstr "先决条件"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Python 3.8 - 3.11"
msgstr "Python 3.8 - 3.11"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 2.2 or later"
msgstr "PyTorch 2.2 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is ``CommDebugMode`` and why is it useful"
msgstr "什么是 ``CommDebugMode`` 以及它为何有用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As the size of models continues to increase, users are seeking to leverage "
"various combinations of parallel strategies to scale up distributed "
"training. However, the lack of interoperability between existing solutions "
"poses a significant challenge, primarily due to the absence of a unified "
"abstraction that can bridge these different parallelism strategies. To "
"address this issue, PyTorch has proposed `DistributedTensor(DTensor) "
"<https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/examples/comm_mode_features_example.py>`_"
" which abstracts away the complexities of tensor communication in "
"distributed training, providing a seamless user experience. However, when "
"dealing with existing parallelism solutions and developing parallelism "
"solutions using the unified abstraction like DTensor, the lack of "
"transparency about what and when the collective communications happens under"
" the hood could make it challenging for advanced users to identify and "
"resolve issues. To address this challenge, ``CommDebugMode``, a Python "
"context manager will serve as one of the primary debugging tools for "
"DTensors, enabling users to view when and why collective operations are "
"happening when using DTensors, effectively addressing this issue."
msgstr ""
"随着模型规模不断扩大，用户寻求利用各种并行策略组合来扩展分布式训练。然而，现有解决方案之间缺乏互操作性构成了一个显著挑战，主要是因为缺乏能够桥接这些不同并行策略的统一抽象。为解决这一问题，PyTorch提出了"
" `DistributedTensor(DTensor) "
"<https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/examples/comm_mode_features_example.py>`_，它抽象了分布式训练中张量通信的复杂性，提供了无缝的用户体验。然而，在使用现有并行解决方案以及利用像DTensor这样的统一抽象开发并行解决方案时，缺乏关于底层集体通信的透明性可能会使高级用户难以识别和解决问题。为解决这一问题，``CommDebugMode``，一个Python上下文管理器，将成为DTensor的主要调试工具之一，使用户能够查看使用DTensor进行操作时的集体通信发生的时间和原因，从而有效解决这一问题。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using ``CommDebugMode``"
msgstr "使用 ``CommDebugMode``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Here is how you can use ``CommDebugMode``:"
msgstr "以下是使用 ``CommDebugMode`` 的方法："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This is what the output looks like for a MLPModule at noise level 0:"
msgstr "以下是一个噪声级别为0时的MLPModule输出示例："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To use ``CommDebugMode``, you must wrap the code running the model in "
"``CommDebugMode`` and call the API that you want to use to display the data."
" You can also use a ``noise_level`` argument to control the verbosity level "
"of displayed information. Here is what each noise level displays:"
msgstr ""
"要使用 ``CommDebugMode``，您需要将运行模型的代码包装在 ``CommDebugMode`` "
"中，并调用您希望用来显示数据的API。您还可以使用噪声级别参数来控制显示信息的详细程度。以下是各个噪声级别的显示内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0. Prints module-level collective counts"
msgstr "0. 显示模块级集体操作计数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"1. Prints DTensor operations (not including trivial operations), module "
"sharding information"
msgstr "1. 显示DTensor操作（不包括简单操作），模块分片信息"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Prints tensor operations (not including trivial operations)"
msgstr "2. 显示张量操作（不包括简单操作）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Prints all operations"
msgstr "3. 显示所有操作"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the example above, you can see that the collective operation, all_reduce,"
" occurs once in the forward pass of the ``MLPModule``. Furthermore, you can "
"use ``CommDebugMode`` to pinpoint that the all-reduce operation happens in "
"the second linear layer of the ``MLPModule``."
msgstr ""
"在上面的示例中，您可以看到在 ``MLPModule`` 的前向传递中发生了一次集体操作 all_reduce。此外，您可以使用 "
"``CommDebugMode`` 来定位all-reduce操作发生在 ``MLPModule`` 的第二个线性层。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Below is the interactive module tree visualization that you can use to "
"upload your own JSON dump:"
msgstr "以下是您可以使用JSON导出上传的交互式模块树可视化："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we have learned how to use ``CommDebugMode`` to debug "
"Distributed Tensors and parallelism solutions that uses communication "
"collectives with PyTorch. You can use your own JSON outputs in the embedded "
"visual browser."
msgstr ""
"在这个教程中，我们学习了如何使用 ``CommDebugMode`` "
"来调试分布式张量和使用通信集成的并行解决方案。你可以在嵌入式可视化浏览器中使用自己的 JSON 输出。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For more detailed information about ``CommDebugMode``, see "
"`comm_mode_features_example.py "
"<https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/examples/comm_mode_features_example.py>`_"
msgstr ""
"有关 ``CommDebugMode`` 的详细信息，请参见 `comm_mode_features_example.py "
"<https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/examples/comm_mode_features_example.py>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Getting Started with DeviceMesh"
msgstr "设备网格入门"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Author**: `Iris Zhang <https://github.com/wz337>`__, `Wanchao Liang "
"<https://github.com/wanchaol>`__"
msgstr ""
"**作者**: `Iris Zhang <https://github.com/wz337>`__, `Wanchao Liang "
"<https://github.com/wanchaol>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst>`__."
msgstr ""
"|编辑| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst>`__"
" 查看和编辑此教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Distributed Communication Package - torch.distributed "
"<https://pytorch.org/docs/stable/distributed.html>`__"
msgstr ""
"`分布式通信包 - torch.distributed "
"<https://pytorch.org/docs/stable/distributed.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 2.2"
msgstr "PyTorch 2.2"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Setting up distributed communicators, i.e. NVIDIA Collective Communication "
"Library (NCCL) communicators, for distributed training can pose a "
"significant challenge. For workloads where users need to compose different "
"parallelisms, users would need to manually set up and manage NCCL "
"communicators (for example, :class:`ProcessGroup`) for each parallelism "
"solution. This process could be complicated and susceptible to errors. "
":class:`DeviceMesh` can simplify this process, making it more manageable and"
" less prone to errors."
msgstr ""
"为分布式训练设置分布式通信器，即 NVIDIA 集体通信库 (NCCL) "
"通信器，可能会带来显著的挑战。对于需要组合不同并行解决方案的工作负载，用户需要为每个并行解决方案手动设置和管理 NCCL 通信器（例如 "
":class:`ProcessGroup`）。这个过程可能会很复杂并且容易出错。:class:`DeviceMesh` "
"可以简化这个过程，使其更加易于管理且不易出错。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is DeviceMesh"
msgstr "什么是 DeviceMesh"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":class:`DeviceMesh` is a higher level abstraction that manages "
":class:`ProcessGroup`. It allows users to effortlessly create inter-node and"
" intra-node process groups without worrying about how to set up ranks "
"correctly for different sub process groups. Users can also easily manage the"
" underlying process_groups/devices for multi-dimensional parallelism via "
":class:`DeviceMesh`."
msgstr ""
":class:`DeviceMesh` 是一个管理 :class:`ProcessGroup` "
"的高级抽象。它允许用户从容创建节点间和节点内的进程组，而无需担心如何为不同的子进程组正确设置排名。用户还可以通过 :class:`DeviceMesh`"
" 轻松管理用于多维并行的底层进程组/设备。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch DeviceMesh"
msgstr "PyTorch DeviceMesh"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Why DeviceMesh is Useful"
msgstr "DeviceMesh 的作用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"DeviceMesh is useful when working with multi-dimensional parallelism (i.e. "
"3-D parallel) where parallelism composability is required. For example, when"
" your parallelism solutions require both communication across hosts and "
"within each host. The image above shows that we can create a 2D mesh that "
"connects the devices within each host, and connects each device with its "
"counterpart on the other hosts in a homogeneous setup."
msgstr ""
"在工作中需要多维并行（即 3D 并行），并且要求并行组合时，DeviceMesh "
"非常有用。例如，当你的并行解决方案需要既跨主机通信又在每个主机内通信时。上图显示我们可以创建一个 2D "
"网格连接每个主机内的设备，并在同一设置下连接其他主机设备的对应部分。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Without DeviceMesh, users would need to manually set up NCCL communicators, "
"cuda devices on each process before applying any parallelism, which could be"
" quite complicated. The following code snippet illustrates a hybrid sharding"
" 2-D Parallel pattern setup without :class:`DeviceMesh`. First, we need to "
"manually calculate the shard group and replicate group. Then, we need to "
"assign the correct shard and replicate group to each rank."
msgstr ""
"没有 DeviceMesh，用户需要手动设置 NCCL 通信器、每个进程上的 CUDA "
"设备，然后才能应用任何并行解决方案，这可能非常复杂。以下代码片段展示了没有使用 :class:`DeviceMesh` 的混合分片 2D "
"并行模式设置。首先，我们需要手动计算分片组和复制组。然后，我们需要将正确的分片组和复制组分配给每个排名。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To run the above code snippet, we can leverage PyTorch Elastic. Let's create"
" a file named ``2d_setup.py``. Then, run the following `torch "
"elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ command."
msgstr ""
"为了运行上述代码片段，我们可以利用 PyTorch Elastic。创建一个名为 ``2d_setup.py`` 的文件。然后运行以下 `torch "
"elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ 命令。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For simplicity of demonstration, we are simulating 2D parallel using only "
"one node. Note that this code snippet can also be used when running on multi"
" hosts setup."
msgstr "为了简化演示，我们仅使用一个节点模拟 2D 并行。请注意，此代码片段也适用于运行在多主机设置。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"With the help of :func:`init_device_mesh`, we can accomplish the above 2D "
"setup in just two lines, and we can still access the underlying "
":class:`ProcessGroup` if needed."
msgstr ""
"借助 :func:`init_device_mesh`，我们可以仅用两行完成上述的 2D 设置，并且如果需要，我们仍可以访问底层的 "
":class:`ProcessGroup`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let's create a file named ``2d_setup_with_device_mesh.py``. Then, run the "
"following `torch elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ command."
msgstr ""
"创建一个名为 ``2d_setup_with_device_mesh.py`` 的文件。然后运行以下 `torch elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ 命令。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use DeviceMesh with HSDP"
msgstr "如何将 DeviceMesh 与 HSDP 一起使用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Hybrid Sharding Data Parallel(HSDP) is 2D strategy to perform FSDP within a "
"host and DDP across hosts."
msgstr "混合分片数据并行 (HSDP) 是一种在主机内执行 FSDP 并在主机间执行 DDP 的 2D 策略。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let's see an example of how DeviceMesh can assist with applying HSDP to your"
" model with a simple setup. With DeviceMesh, users would not need to "
"manually create and manage shard group and replicate group."
msgstr ""
"让我们看一个示例，了解 DeviceMesh 如何帮助应用 HSDP 到你的模型上的简单设置。有了 "
"DeviceMesh，用户无需手动创建和管理分片组和复制组。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let's create a file named ``hsdp.py``. Then, run the following `torch "
"elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ command."
msgstr ""
"创建一个名为 ``hsdp.py`` 的文件。然后运行以下 `torch elastic/torchrun "
"<https://pytorch.org/docs/stable/elastic/quickstart.html>`__ 命令。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use DeviceMesh for your custom parallel solutions"
msgstr "如何将 DeviceMesh 应用于自定义并行解决方案"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When working with large scale training, you might have more complex custom "
"parallel training composition. For example, you may need to slice out sub-"
"meshes for different parallelism solutions. DeviceMesh allows users to slice"
" child mesh from the parent mesh and re-use the NCCL communicators already "
"created when the parent mesh is initialized."
msgstr ""
"在处理大规模训练时，你可能有更复杂的自定义并行训练组合。例如，你可能需要为不同的并行解决方案切片子网格。DeviceMesh "
"允许用户从父网格中切出子网格并重用在初始化父网格时已经创建的 NCCL 通信器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In conclusion, we have learned about :class:`DeviceMesh` and "
":func:`init_device_mesh`, as well as how they can be used to describe the "
"layout of devices across the cluster."
msgstr ""
"总结来说，我们学习了 :class:`DeviceMesh` 和 :func:`init_device_mesh` 及其如何用于描述跨集群设备的布局。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`2D parallel combining Tensor/Sequence Parallel with FSDP "
"<https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/fsdp_tp_example.py>`__"
msgstr ""
"`结合张量/序列并行与 FSDP 的 2D 并行方案 "
"<https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/fsdp_tp_example.py>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Composable PyTorch Distributed with PT2 "
"<https://static.sched.com/hosted_files/pytorch2023/d1/%5BPTC%2023%5D%20Composable%20PyTorch%20Distributed%20with%20PT2.pdf>`__"
msgstr ""
"`使用 PT2 的可组合 PyTorch 分布式 "
"<https://static.sched.com/hosted_files/pytorch2023/d1/%5BPTC%2023%5D%20Composable%20PyTorch%20Distributed%20with%20PT2.pdf>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Distributed Optimizer with TorchScript support"
msgstr "支持 TorchScript 的分布式优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TorchScript is no longer in active development."
msgstr "TorchScript 不再被积极开发。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The high-level idea of distributed optimizer with TorchScript support and "
"what this feature brings"
msgstr "关于支持 TorchScript 的分布式优化器的高层理念及此功能的优点"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to write customized distributed optimizer that enables TorchScript "
"support"
msgstr "如何编写自定义的支持 TorchScript 的分布式优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is Distributed Optimizer?"
msgstr "什么是分布式优化器？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`DistributedOptimizer <https://pytorch.org/docs/master/rpc.html#module-"
"torch.distributed.optim>`_ takes a list of remote parameters (RRef) and runs"
" the optimizer locally on the workers where the parameters live, which is "
"commonly used together with Distributed RPC/Autograd to do model parallel "
"training. It could use any of the local optimizer algorithms (either pre-"
"defined algorithms provided in ``torch.optim`` or custom defined ones) to "
"apply the gradients on each worker."
msgstr ""
"`DistributedOptimizer <https://pytorch.org/docs/master/rpc.html#module-"
"torch.distributed.optim>`_ 接受一组远程参数 (RRef) 并在参数所在的工作节点上本地运行优化器，这通常与分布式 "
"RPC/Autograd 一起使用以进行模型并行训练。它可以使用任何本地的优化器算法（无论是 `torch.optim` "
"提供的预定义算法，还是用户自定义的算法）在各个工作节点上应用梯度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is Distributed Optimizer with TorchScript support?"
msgstr "支持 TorchScript 的分布式优化器是什么？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Distributed Optimizer are widely used in distributed model parallel "
"training, and in some common use cases, training need to be done in "
"multithreaded manner instead of multiprocess due to performance concern and "
"resource utilizations (or at least partially multithreaded, i.e. Parameter "
"Server hosting part of the model and parameters, with new thread updating "
"the parameters per request). PyTorch itself does not support multithreaded "
"training natively as it suffers from the Python's Global Interpreter Lock "
"(GIL), but it could leverage `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`_ to get rid of GIL and run the "
"model in a multithreaded way."
msgstr ""
"分布式优化器广泛用于分布式模型并行训练，并且在一些常见情况下，由于性能问题和资源利用需要，训练需要以多线程而不是多进程的方式进行（或者至少部分多线程，例如参数服务器托管部分模型和参数，并启动新的线程按请求更新参数）。PyTorch"
" 本身由于受到 Python 的全局解释器锁 (GIL) 限制，无法原生支持多线程训练，但可以利用 `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`_ 摆脱 GIL，从而以多线程方式运行模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For critical model training workloads, improving the training performance is"
" an important topic. Researchers often would like to implement different "
"optimization strategies with the graph representation (i.e. via operator "
"fusion) or implement custom operator kernels in order to speed up training."
msgstr ""
"对于关键的模型训练任务，提高训练性能是一个重要的主题。研究人员通常希望通过图形表示（例如操作符融合）或实现自定义的操作符内核来实现不同的优化策略，以加快训练速度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Distributed Optimizer with TorchScript support could help getting rid of "
"GIL, thus improve PyTorch's training performance in the multithreaded "
"environment, it also unlocks the potential to further enhance the "
"performance by using advanced compiler technologies that TorchScript offers "
"(i.e. CPU/GPU fusion)."
msgstr ""
"支持 TorchScript 的分布式优化器可以帮助摆脱 GIL，从而在多线程环境下提高 PyTorch 的训练性能，同时也解锁了使用 "
"TorchScript 提供的先进编译器技术（例如 CPU/GPU 融合）进一步提升性能的潜力。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to write a customized distributed optimizer with TorchScript support?"
msgstr "如何编写支持 TorchScript 的自定义分布式优化器？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code below shows how to write a customized distributed optimizer given "
"an existing local optimizer implementation, which unlocks the TorchScript "
"benefits including GIL removal and performance improvement opportunities."
msgstr ""
"下面的代码展示了如何基于现有的本地优化器实现编写一个支持 TorchScript 的自定义分布式优化器，这解锁了包括去除 GIL 和性能提升机会在内的 "
"TorchScript 优势。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Suppose that you already have a local optimizer that is currently used "
"during training, In this case we will use `quasi-hyperbolic momentum (QHM) "
"<https://github.com/facebookresearch/qhoptim/blob/e81dea3f2765780cf4fbb90b87b22ba7604b8625/qhoptim/pyt/qhm.py#L12>`_"
" as an example to show how to enable the TorchScript support, note that it "
"also applies to any custom optimizers that inherits from "
"``torch.optim.Optimizer``."
msgstr ""
"假设你已经有了一个本地优化器，它当前在训练中被使用。在这种情况下，我们将使用 `准双曲动量 (QHM) "
"<https://github.com/facebookresearch/qhoptim/blob/e81dea3f2765780cf4fbb90b87b22ba7604b8625/qhoptim/pyt/qhm.py#L12>`_"
" 作为示例来展示如何启用 TorchScript 支持，请注意这也适用于任何继承自 ``torch.optim.Optimizer`` 的自定义优化器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"First, we need to separate the computation and state management from the "
"optimizer implementation, this is so that we could extract the computation "
"part and make it a free function, which is TorchScript friendly. It has two "
"benefits: 1. The computation logic becomes easier to inspect, it allows us "
"to quickly turn the parameter update/computation part into TorchScript, and "
"utilize TorchScript IR to do further optimizations (operator fusion, etc.) "
"2. Distributed Optimizer underlying is using a different mechanisms to get "
"gradients and update parameters (we store gradients separately instead of "
"directly populating the ``param.grad`` field during backward). Separating "
"the computation allows distributed optimizer to enable the possibility of "
"optimizer update in multithreaded mode, as it eliminates the possible race "
"condition to ``param.grad``."
msgstr ""
"首先，我们需要将计算和状态管理从优化器实现中分离出来，这样我们就可以提取计算部分并将其变成一个自由函数，这是 TorchScript "
"友好的。这有两个好处：1. 计算逻辑更容易检查，允许我们快速将参数更新/计算部分转换为 TorchScript，并利用 TorchScript IR "
"进一步做优化（操作符融合等）2. 分布式优化器的底层是通过不同的机制获取梯度并更新参数（我们单独存储梯度而不是在后向传播中直接填充 "
"``param.grad`` 字段）。分离计算允许分布式优化器在多线程模式下启用优化器更新的可能性，因为它消除了对 ``param.grad`` "
"的潜在竞态条件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Next we will define a distributed functional optimizer with TorchScript "
"compatability to manage the optimizer states and calls into the TorchScript "
"compatible update function we defined above. Note that a few conventions are"
" different from normal custom optimizers: 1. We don't inherit "
"``torch.optim.Optimizer`` as TorchScript does not support polymorphism 2. "
"``step`` takes gradients list instead of the loss closure."
msgstr ""
"接下来我们将定义具有 TorchScript 兼容性的分布式功能优化器，用于管理优化器状态并调用我们上面定义的 TorchScript "
"兼容更新函数。请注意，与普通自定义优化器相比，有几个约定不同：1. 我们不继承 ``torch.optim.Optimizer``，因为 "
"TorchScript 不支持多态性 2. ``step`` 接受梯度列表而不是损失闭包。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Finally, we register our newly defined distributed functional optimizer into"
" the ``functional_optim_map`` This is so that the ``DistributedOptimizer`` "
"will try to pick up our custom implementation instead of the pre-defined "
"default ones."
msgstr ""
"最后，我们将新定义的分布式功能优化器注册到 ``functional_optim_map`` 中，这样 ``DistributedOptimizer``"
" 就会尝试选择我们的自定义实现，而不是预定义的默认实现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Now you can use the ``QHM`` optimizer as normal in distributed training by "
"passing it to `DistributedOptimizer "
"<https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim>`_"
msgstr ""
"现在，你可以像正常使用优化器一样在分布式训练中使用 ``QHM``，通过将它传递给 `DistributedOptimizer "
"<https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"DistributedOptimizer will automatically transform the QHM optimizer into the"
" ``FunctionalQHM`` under the hood, and enable the TorchScript support. This "
"will unlock the performance that boosted by multithreaded training and also "
"give more potentials for further improvements (i.e. TorchScript fusion, "
"etc.)"
msgstr ""
"DistributedOptimizer 将自动将 QHM 优化器转换为底层的 ``FunctionalQHM``，并启用 TorchScript "
"支持。这将解锁通过多线程训练而加速的性能，同时还提供进一步改进的更多潜力（例如 TorchScript 融合等）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note that majority of PyTorch built-in optimizers are already using this "
"methodology to speed up distributed training. If you see warning about some "
"optimizers haven't been converted yet, you can write your own conversion by "
"following this recipe."
msgstr ""
"请注意，大部分 PyTorch 内置优化器已经采用这种方法来加速分布式训练。如果你看到某些优化器尚未转换的警告，你可以按照这个教程编写自己的转换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Profiling PyTorch RPC-Based Workloads"
msgstr "PyTorch 基于 RPC 的工作负载分析"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This tutorial has been deprecated."
msgstr "本教程已废弃。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Redirecting to home page."
msgstr "重定向至主页。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_foreach_map.py>` to download the"
" full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_recipes_foreach_map.py>` 下载完整的示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Explicit horizontal fusion with foreach_map and torch.compile"
msgstr "使用 foreach_map 和 torch.compile 显式水平融合"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Horizontal fusion is a key optimization in ML compilers. In eager,"
msgstr "水平融合是 ML 编译器中的一个关键优化。在急切模式下，"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"this is typically expressed using the torch._foreach* ops which parallelizes"
" operations across a list of tensors. However, supporting all possible "
"permutations of arguments is quite difficult (e.g. mixtures of scalars and "
"lists). Foreach_map allows conversion of any pointwise op in ``torch`` to a "
"horiztonally fused foreach variant. In this tutorial, we will demonstrate "
"how to implement the Adam optimizer with ``foreach_map`` to generate a fully"
" fused kernel."
msgstr ""
"这通常通过使用 torch._foreach* "
"操作来实现，这些操作可以并行处理张量列表中的操作。然而，支持所有可能的参数排列组合是相当困难的（例如标量和列表的混合）。Foreach_map 允许将 "
"Torch 中的任何逐点操作转换为水平融合的 foreach 版本。在本教程中，我们将演示如何使用 foreach_map 来实现 Adam "
"优化器，从而生成一个完全融合的内核。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe describes a prototype feature. Prototype features are typically "
"at an early stage for feedback and testing and are subject to change."
msgstr "本教程描述的是一个原型功能。原型功能通常处于反馈和测试的早期阶段，随时可能发生变化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch v2.7.0 or later"
msgstr "需要 PyTorch v2.7.0 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For this example, we'll use a simple sequence of linear layers. We "
"instantiate an independent copy to compare the two optimizer "
"implementations."
msgstr "在本例中，我们将使用一个简单的线性层序列。我们会实例化一个独立的副本来比较两种优化器实现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Helper functions for foreach_map implementation"
msgstr "用于 foreach_map 实现的辅助函数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In this section, we'll begin our implementation of the Adam optimizer."
msgstr "在本节中，我们将开始实现 Adam 优化器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Setting up and running the compiled kernel"
msgstr "设置和运行编译的内核"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In this section, we'll run our Adam optimizer and compare the results"
msgstr "在本节中，我们将运行我们的 Adam 优化器并比较结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we successfully implemented a custom fully-fused Adam "
"optimizer using foreach_map. By leveraging the power of foreach_map and "
"torch.compile, we were able to create an optimized version of the Adam "
"optimizer that can be used in various machine learning applications. This "
"tutorial provides a comprehensive guide on how to use foreach_map and "
"torch.compile to optimize machine learning models, and serves as a valuable "
"resource for developers looking to improve the performance of their models "
"with horizontal fusion."
msgstr ""
"在本教程中，我们成功使用 foreach_map 实现了一个自定义的完全融合的 Adam 优化器。通过利用 foreach_map 和 "
"torch.compile 的强大功能，我们能够创建适用于各种机器学习应用的优化版 Adam 优化器。本教程提供了如何使用 foreach_map 和 "
"torch.compile 优化机器学习模型的全面指南，对希望通过水平融合提高模型性能的开发者来说是一个有价值的资源。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: foreach_map.py <foreach_map.py>`"
msgstr ":download:`下载 Python 源代码: foreach_map.py <foreach_map.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: foreach_map.ipynb <foreach_map.ipynb>`"
msgstr ":download:`下载 Jupyter 笔记本: foreach_map.ipynb <foreach_map.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Fuse Modules Recipe"
msgstr "模块融合教程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe demonstrates how to fuse a list of PyTorch modules into a single"
" module and how to do the performance test to compare the fused model with "
"its non-fused version."
msgstr "本教程展示了如何将 PyTorch 模块列表融合成一个单一模块，以及如何进行性能测试以比较融合的模型与其未融合版本的性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Before quantization is applied to a model to reduce its size and memory "
"footprint (see `Quantization Recipe <quantization.html>`_ for details on "
"quantization), the list of modules in the model may be fused first into a "
"single module. Fusion is optional, but it may save on memory access, make "
"the model run faster, and improve its accuracy."
msgstr ""
"在对模型进行量化以减小其大小和内存占用之前（有关量化的详细信息，请参阅 `量化教程 "
"<quantization.html>`_），模型中的模块列表可以首先融合到一个单一模块中。融合是可选的，但它可能节省内存访问，提高模型运行速度，并改善其准确性。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Pre-requisites"
msgstr "前置要求"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 1.6.0 or 1.7.0"
msgstr "需要 PyTorch 1.6.0 或 1.7.0"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Steps"
msgstr "步骤"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Follow the steps below to fuse an example model, quantize it, script it, "
"optimize it for mobile, save it and test it with the Android benchmark tool."
msgstr "按照以下步骤，融合一个示例模型，对其进行量化、脚本化，为移动端优化，保存并使用 Android 基准测试工具测试其性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1. Define the Example Model"
msgstr "1. 定义示例模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Use the same example model defined in the `PyTorch Mobile Performance "
"Recipes <https://pytorch.org/tutorials/recipes/mobile_perf.html>`_:"
msgstr ""
"使用 `PyTorch 移动端性能教程 "
"<https://pytorch.org/tutorials/recipes/mobile_perf.html>`_ 中定义的相同示例模型:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Generate Two Models with and without `fuse_modules`"
msgstr "2. 生成带有和不带 `fuse_modules` 的两个模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Add the following code below the model definition above and run the script:"
msgstr "在上述模型定义下面添加以下代码并运行脚本:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The graphs of the original model and its fused version will be printed as "
"follows:"
msgstr "原始模型及其融合版本的图形将如下所示打印:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the second fused model output, the first item `bn` in the list is "
"replaced with the fused module, and the rest of the modules (`relu` in this "
"example) is replaced with identity. In addition, the non-fused and fused "
"versions of the model `model.pt` and `model_fused.pt` are generated."
msgstr ""
"在第二个融合模型的输出中，列表中的第一个项 `bn` 将被替换为融合模块，其余模块（在本例中是 `relu`）将被替换为 "
"identity。此外，还会生成模型的未融合和融合版本 `model.pt` 和 `model_fused.pt`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Build the Android benchmark Tool"
msgstr "3. 构建 Android 基准测试工具"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Get the PyTorch source and build the Android benchmark tool as follows:"
msgstr "获取 PyTorch 源代码并按以下方式构建 Android 基准测试工具:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This will generate the Android benchmark binary `speed_benchmark_torch` in "
"the `build_android/bin` folder."
msgstr ""
"这将在 `build_android/bin` 文件夹中生成 Android 基准测试二进制文件 `speed_benchmark_torch`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Test Compare the Fused and Non-Fused Models"
msgstr "4. 测试比较融合和未融合模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Connect your Android device, then copy `speed_benchmark_torch` and the model"
" files and run the benchmark tool on them:"
msgstr "连接您的 Android 设备，然后复制 `speed_benchmark_torch` 和模型文件，并使用基准测试工具运行它们:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The results from the last two commands should be like:"
msgstr "最后两个命令的结果应如下所示:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "and"
msgstr "和"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For this example model, there is no much performance difference between the "
"fused and non-fused models. But the similar steps can be used to fuse and "
"prepare a real deep model and test to see the performance improvement. Keep "
"in mind that currently `torch.quantization.fuse_modules` only fuses the "
"following sequence of modules:"
msgstr ""
"对于本示例模型，融合和未融合模型之间的性能差异不大。但可以使用类似步骤来融合和准备一个真实的深度学习模型，并测试性能是否有所改进。请记住，目前 "
"`torch.quantization.fuse_modules` 仅融合以下模块序列:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "conv, bn"
msgstr "conv, bn"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "conv, bn, relu"
msgstr "conv, bn, relu"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "conv, relu"
msgstr "conv, relu"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "linear, relu"
msgstr "linear, relu"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "bn, relu"
msgstr "bn, relu"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If any other sequence list is provided to the `fuse_modules` call, it will "
"simply be ignored."
msgstr "如果向 `fuse_modules` 调用提供了任何其他序列列表，它将被简单地忽略。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"See `here <https://pytorch.org/docs/stable/quantization.html#preparing-"
"model-for-quantization>`_ for the official documentation of "
"`torch.quantization.fuse_modules`."
msgstr ""
"请参阅 `此处 <https://pytorch.org/docs/stable/quantization.html#preparing-model-"
"for-quantization>`_ 获取 `torch.quantization.fuse_modules` 的官方文档。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(Beta) PyTorch Inference Performance Tuning on AWS Graviton Processors"
msgstr "(Beta) 基于 AWS Graviton 处理器的 PyTorch 推理性能优化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author**: `Sunita Nadampalli <https://github.com/snadampal>`_"
msgstr "**作者**: `Sunita Nadampalli <https://github.com/snadampal>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`AWS Graviton <https://aws.amazon.com/ec2/graviton/>`_ is a series of ARM-"
"based processors designed by AWS. AWS Graviton3 processors are optimized for"
" Machine Learning (ML) workloads, including support for ``bfloat16``, "
"Scalable Vector Extension (SVE) and twice the Single Instruction Multiple "
"Data (SIMD) bandwidth compared to Graviton2."
msgstr ""
"`AWS Graviton <https://aws.amazon.com/ec2/graviton/>`_ 是 AWS 设计的一系列基于 ARM "
"的处理器。AWS Graviton3 处理器专为机器学习（ML）工作负载优化，包括支持 ``bfloat16``、可扩展矢量扩展（SVE）和比 "
"Graviton2 更高的单指令多数据（SIMD）带宽。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch provides native reference ATen kernels for the machine learning "
"operators like convolutions, matmul, relu, etc. These operators can be "
"accelerated with platform specific kernel implementations from Basic Linear "
"Algebra (BLAS) libraries. On AWS Graviton CPUs, MKLDNN with Arm Compute "
"Library (`ACL <https://github.com/ARM-software/ComputeLibrary>`_) and "
"`OpenBLAS <https://github.com/OpenMathLib/OpenBLAS>`_ libraries provide "
"optimized implementations for a subset of the operators. Both these "
"libraries are integrated into PyTorch with PyTorch 2.0 version."
msgstr ""
"PyTorch 为机器学习操作符（如卷积、矩阵乘法和 ReLU 等）提供了本地的 ATen "
"内核。这些操作符可以通过来自基础线性代数库（BLAS）的特定平台内核实现来加速。在 AWS Graviton CPU 上，MKLDNN 与 Arm "
"计算库 (`ACL <https://github.com/ARM-software/ComputeLibrary>`_) 和 `OpenBLAS "
"<https://github.com/OpenMathLib/OpenBLAS>`_ 提供了优化的部分操作符实现。这两个库自 PyTorch 2.0 "
"版本起已集成到 PyTorch 中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial we will cover how to achieve the best inference performance"
" for linear layer neural network on AWS Graviton3 CPUs (`AWS c7g instance "
"<https://aws.amazon.com/ec2/instance-types/c7g/>`_) with ``bfloa16`` kernels"
" and with the right backend selection."
msgstr ""
"在本教程中，我们将讨论如何在 AWS Graviton3 CPU（`AWS c7g 实例 "
"<https://aws.amazon.com/ec2/instance-types/c7g/>`_）上使用 ``bfloa16`` "
"内核和正确的后端选择，以实现线性层神经网络的最佳推理性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Contents"
msgstr "内容"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Basic Usage"
msgstr "基本用法"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Speed up inference with Bfloat16 fast math kernels"
msgstr "使用 Bfloat16 快速数学内核加速推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Improve inference performance with OpenBLAS for smaller batch dimensions"
msgstr "通过 OpenBLAS 改善小批量尺寸的推理性能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Optimize memory allocation overhead with Linux Transparent huge pages"
msgstr "使用 Linux 的透明大页优化内存分配开销"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To successfully run this tutorial and reproduce the speedup numbers shown "
"below, you need an instance from the Graviton3 family (``c7g/r7g/m7g``) of "
"hardware. For this tutorial, we used the `c7g.xl (4vcpu) instance "
"<https://aws.amazon.com/ec2/instance-types/c7g/>`_ ."
msgstr ""
"要成功运行本教程并重现下面显示的加速结果，您需要一个 Graviton3 家族（``c7g/r7g/m7g``）的硬件实例。对于本教程，我们使用了 "
"`c7g.xl (4vcpu) 实例 <https://aws.amazon.com/ec2/instance-types/c7g/>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch natively supports AWS Graviton3 optimizations starting with PyTorch "
"2.0 version. Please refer to this `blog <https://pytorch.org/blog/optimized-"
"pytorch-w-graviton/>`_ for more details on the optimizations."
msgstr ""
"从 PyTorch 2.0 版本开始，PyTorch 本地支持 AWS Graviton3 优化。请参阅这篇 `博客 "
"<https://pytorch.org/blog/optimized-pytorch-w-graviton/>`_ 获取更多优化详情。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Install PyTorch by running the following command:"
msgstr "运行以下命令来安装 PyTorch:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We will start by importing the required dependencies and defining the device"
" will run on:"
msgstr "我们将从导入所需依赖库和定义运行设备开始:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Given linear layers are at the heart of several neural networks, including "
"transformers, we take a linear layer for this demo. We define our neural "
"network by subclassing ``nn.Module``, and initializing the layers in "
"``__init__``. We construct the network with a typical large language model "
"parameters to match the real world scenario:"
msgstr ""
"鉴于线性层是多个神经网络（包括 transformer）的核心，我们在本演示中选用了一个线性层。我们通过继承 ``nn.Module`` "
"定义我们的神经网络，并在 ``__init__`` 中初始化各层。为匹配真实场景，我们使用了典型的大型语言模型参数来构建网络:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let's create an instance of ``MyNeuralNetwork``, and move it to the device:"
msgstr "让我们创建 ``MyNeuralNetwork`` 的一个实例，并将其移至设备上运行:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Next, let's get the prediction probabilities by passing them through an "
"instance of the ``nn.Softmax`` module:"
msgstr "接下来，让我们通过一个 ``nn.Softmax`` 模块的实例来获取预测概率:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "output:"
msgstr "输出:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Our network functionality is verified. Next, we will profile the "
"performance. Lets' check two different scenarios: small and large batch "
"dimensions."
msgstr "我们的网络功能已验证完成。接下来我们将进行性能分析。让我们检查两个不同的场景：小批量和大批量尺寸。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Scenario 1:** A larger batch dimension, for example 256:"
msgstr "**场景一:** 较大的批量尺寸，例如 256:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Following is the profiler output with the default PyTorch configuration:"
msgstr "以下是默认配置下 PyTorch 的性能分析输出:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Name"
msgstr "名称"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Self CPU %"
msgstr "自身 CPU %"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Self CPU"
msgstr "自身 CPU 时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "CPU total %"
msgstr "总 CPU %"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "CPU total"
msgstr "总 CPU 时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "CPU time avg"
msgstr "平均 CPU 时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "# of Calls"
msgstr "调用次数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "aten::addmm"
msgstr "aten::addmm"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.61%"
msgstr "97.61%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "15.813s"
msgstr "15.813s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "98.61%"
msgstr "98.61%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "15.977s"
msgstr "15.977s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "53.255ms"
msgstr "53.255ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "300"
msgstr "300"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "aten::clamp_min"
msgstr "aten::clamp_min"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.09%"
msgstr "1.09%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "177.032ms"
msgstr "177.032ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "885.160us"
msgstr "885.160us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "200"
msgstr "200"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "aten::copy"
msgstr "aten::copy"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.00%"
msgstr "1.00%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "162.054ms"
msgstr "162.054ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "540.180us"
msgstr "540.180us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "mymodel_inference"
msgstr "mymodel_inference"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.22%"
msgstr "0.22%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "35.738ms"
msgstr "35.738ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "100.00%"
msgstr "100.00%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "16.201s"
msgstr "16.201s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1"
msgstr "1"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "aten::linear"
msgstr "aten::linear"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.02%"
msgstr "0.02%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.955ms"
msgstr "2.955ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "98.66%"
msgstr "98.66%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "15.985s"
msgstr "15.985s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "53.282ms"
msgstr "53.282ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "aten::t"
msgstr "aten::t"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.01%"
msgstr "0.01%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.421ms"
msgstr "2.421ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.03%"
msgstr "0.03%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5.043ms"
msgstr "5.043ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "16.810us"
msgstr "16.810us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "aten::relu"
msgstr "aten::relu"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.356ms"
msgstr "2.356ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.11%"
msgstr "1.11%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "179.388ms"
msgstr "179.388ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "896.940us"
msgstr "896.940us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 16.201s"
msgstr "**自身 CPU 时间总计:** 16.201s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Speed up Inference with ``bfloat16`` Fast Math Kernels"
msgstr "使用 ``bfloat16`` 快速数学内核加速推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"AWS Graviton3 processors support `bfloat16 MMLA instructions "
"<https://developer.arm.com/documentation/ddi0596/2020-12/SVE-"
"Instructions/BFMMLA--BFloat16-floating-point-matrix-multiply-accumulate->`_."
" Arm Compute Library (`ACL <https://github.com/ARM-"
"software/ComputeLibrary>`_) provides optimized ``bfloat16`` General Matrix "
"Multiplication (GEMM) kernels for AWS Graviton processors, and are "
"integrated into PyTorch via MKLDNN backend starting with PyTorch 2.0.  The "
"inference performance can be optimized with the fast math GEMM kernels. The "
"fast math mode is not enabled by default because these kernels perform GEMM "
"in ``bfloat16`` precision instead of ``float``, and hence results in a "
"slight drop in the model inference accuracy. However, the accuracy drop is "
"within the ``cosine similarity`` threshold defined for ``bfloat16`` backend "
"in ``torchbench`` test suite, and hence acceptable for majority of the "
"applications. To enable the fast math GEMM kernels, set the following "
"environment variable:"
msgstr ""
"AWS Graviton3 处理器支持 `bfloat16 MMLA 指令 "
"<https://developer.arm.com/documentation/ddi0596/2020-12/SVE-"
"Instructions/BFMMLA--BFloat16-floating-point-matrix-multiply-"
"accumulate->`_。Arm 计算库 (`ACL <https://github.com/ARM-"
"software/ComputeLibrary>`_) 提供了针对 AWS Graviton 处理器优化的 ``bfloat16`` "
"通用矩阵乘法（GEMM）内核，并从 PyTorch 2.0 开始通过 MKLDNN 后端集成到 PyTorch 中。可以通过快速数学 GEMM "
"内核优化推理性能。快速数学模式默认未启用，因为这些内核以 ``bfloat16`` 精度而非 ``float`` 进行 GEMM "
"操作，因此会导致模型推理精度略有下降。然而，该精度下降在 ``torchbench`` 测试套件中 `bfloat16` "
"后端定义的余弦相似性阈值范围内，因此对大多数应用程序而言是可以接受的。要启用快速数学 GEMM 内核，可以设置以下环境变量:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When you run the above inference script, you should see the following "
"profiler output with the MKLDNN fast math mode enabled:"
msgstr "运行上述推理脚本时，您应该会看到启用了 MKLDNN 快速数学模式的以下性能分析输出:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.61%"
msgstr "95.61%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.943s"
msgstr "6.943s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.10%"
msgstr "97.10%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7.052s"
msgstr "7.052s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "23.507ms"
msgstr "23.507ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.31%"
msgstr "2.31%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "167.653ms"
msgstr "167.653ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "838.265us"
msgstr "838.265us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.48%"
msgstr "1.48%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "107.593ms"
msgstr "107.593ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "358.643us"
msgstr "358.643us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.43%"
msgstr "0.43%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "31.167ms"
msgstr "31.167ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7.262s"
msgstr "7.262s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.04%"
msgstr "0.04%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.911ms"
msgstr "2.911ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.21%"
msgstr "97.21%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7.060s"
msgstr "7.060s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "23.533ms"
msgstr "23.533ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.414ms"
msgstr "2.414ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.07%"
msgstr "0.07%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.892ms"
msgstr "4.892ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "16.307us"
msgstr "16.307us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.281ms"
msgstr "2.281ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.34%"
msgstr "2.34%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "169.934ms"
msgstr "169.934ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "849.670us"
msgstr "849.670us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 7.262s"
msgstr "**自身 CPU 时间总计:** 7.262s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is around ``2x (7.262s vs 16.201s)`` performance improvement with the "
"``bfloat16`` fastmath kernels. Next, let’s look at the smaller batch "
"dimension scenario."
msgstr ""
"这是约 ``2 倍 (7.262s vs 16.201s)`` 的性能提升，得益于 ``bfloat16`` "
"快速数学内核。然而，让我们看看小批量尺寸场景。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Scenario 2:** A smaller batch dimension, for example, 32:"
msgstr "**场景二:** 较小的批量尺寸，例如 32:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You should see the following profiler output when the above script is run "
"with the PyTorch default configuration:"
msgstr "运行上述脚本时，使用 PyTorch 默认配置您应该会看到以下性能分析输出:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.51%"
msgstr "95.51%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5.821s"
msgstr "5.821s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.04%"
msgstr "97.04%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5.914s"
msgstr "5.914s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "19.713ms"
msgstr "19.713ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.33%"
msgstr "2.33%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "142.244ms"
msgstr "142.244ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "711.220us"
msgstr "711.220us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.51%"
msgstr "1.51%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "92.322ms"
msgstr "92.322ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "307.740us"
msgstr "307.740us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.45%"
msgstr "0.45%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "27.713ms"
msgstr "27.713ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.094s"
msgstr "6.094s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.495ms"
msgstr "2.495ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.16%"
msgstr "97.16%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5.921s"
msgstr "5.921s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "19.736ms"
msgstr "19.736ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.131ms"
msgstr "2.131ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.441ms"
msgstr "4.441ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "14.803us"
msgstr "14.803us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.942ms"
msgstr "1.942ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.37%"
msgstr "2.37%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "144.186ms"
msgstr "144.186ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "720.930us"
msgstr "720.930us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 6.094s"
msgstr "**自身 CPU 时间总计:** 6.094s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The following output is the profiler output when run with the MKLDNN fast "
"math mode enabled:"
msgstr "以下是启用了 MKLDNN 快速数学模式的性能分析输出:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "93.31%"
msgstr "93.31%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.848s"
msgstr "3.848s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.66%"
msgstr "95.66%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.944s"
msgstr "3.944s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "13.148ms"
msgstr "13.148ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.43%"
msgstr "3.43%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "141.309ms"
msgstr "141.309ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "706.545us"
msgstr "706.545us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.916ms"
msgstr "95.916ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "319.720us"
msgstr "319.720us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.67%"
msgstr "0.67%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "27.431ms"
msgstr "27.431ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.123s"
msgstr "4.123s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.06%"
msgstr "0.06%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.471ms"
msgstr "2.471ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.83%"
msgstr "95.83%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.951s"
msgstr "3.951s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "13.170ms"
msgstr "13.170ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.05%"
msgstr "0.05%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.027ms"
msgstr "2.027ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.10%"
msgstr "0.10%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.243ms"
msgstr "4.243ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "14.143us"
msgstr "14.143us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.928ms"
msgstr "1.928ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.47%"
msgstr "3.47%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "143.237ms"
msgstr "143.237ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "716.185us"
msgstr "716.185us"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 4.123s"
msgstr "**自身 CPU 时间总计:** 4.123s"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The MKLDNN fast math mode yields approximately a **1.47x  (4.123s vs "
"6.094s)**  performance improvement for smaller batch dimensions. Although "
"this improvement is noteworthy, the overall performance still leaves room "
"for improvement. This is because of the runtime overhead (weights reorders "
"and kernel launch time) from oneDNN and ACL backend outweighing the compute "
"benefits from the ACL GEMM kernels for the smaller batch compute."
msgstr ""
"启用 MKLDNN 快速数学模式后，小批量尺寸下的性能提升约为 **1.47 倍 (4.123s vs "
"6.094s)**。虽然这一提升值得注意，但总的来说，性能仍有提高的空间。这是因为 oneDNN 和 ACL "
"后端的运行时开销（权重重排和内核启动时间）抵消了 ACL GEMM 内核在小批量计算中的计算收益。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Improve Inference Performance with OpenBLAS for Smaller Batch Dimensions"
msgstr "通过 OpenBLAS 改善小批量尺寸的推理性能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The inference performance for smaller batch dimensions can be improved by "
"offloading the smaller shapes from MKLDNN to OpenBLAS backend. We are "
"working on making the backend selection automatic, with robust heuristics, "
"for the future releases. Till the heuristics are implemented, the smaller "
"shapes can be offloaded to OpenBLAS by increasing the threshold for MKLDNN "
"backend selection. In the following example, we use ``64`` as the threshold,"
" so that input with ``batch dimension of 32`` is not dispatched to MKLDNN. "
"Instead, it is dispatched to OpenBLAS."
msgstr ""
"通过将小尺寸张量从MKLDNN后端卸载到OpenBLAS后端，可以提高较小批次尺寸的推理性能。我们正在研究在未来的发布中通过强大的启发式方法使后端选择自动化。在启发式方法实现之前，可以通过提高MKLDNN后端选择的阈值，将较小的张量卸载到OpenBLAS。在下例中，我们使用"
" ``64`` 作为阈值，这样 ``批次尺寸为32`` 的输入将不会分派给MKLDNN，而是分派给OpenBLAS。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Here is the profiler output with OpenBLAS backend:"
msgstr "以下是使用OpenBLAS后端的性能分析器输出："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "96.25%"
msgstr "96.25%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.958s"
msgstr "1.958秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.51%"
msgstr "97.51%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.984s"
msgstr "1.984秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.612ms"
msgstr "6.612毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.28%"
msgstr "1.28%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "26.124ms"
msgstr "26.124毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "130.620us"
msgstr "130.620微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.23%"
msgstr "1.23%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "24.951ms"
msgstr "24.951毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "83.170us"
msgstr "83.170微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.86%"
msgstr "0.86%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "17.423ms"
msgstr "17.423毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.034s"
msgstr "2.034秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.08%"
msgstr "0.08%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.691ms"
msgstr "1.691毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "97.74%"
msgstr "97.74%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.988s"
msgstr "1.988秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.628ms"
msgstr "6.628毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.520ms"
msgstr "1.520毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.14%"
msgstr "0.14%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.945ms"
msgstr "2.945毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "9.817us"
msgstr "9.817微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.258ms"
msgstr "1.258毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.35%"
msgstr "1.35%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "27.382ms"
msgstr "27.382毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "136.910us"
msgstr "136.910微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 2.034s"
msgstr "**自CPU时间总计：** 2.034秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As you can see above, switching to OpenBLAS doubled the performance "
"**(2.034s vs 4.123s)** compared to the default MKLDNN backend configuration."
" This becomes significant for even smaller batch dimensions, for example, "
"for a batch dimension of 10:"
msgstr ""
"如上所示，与默认的MKLDNN后端配置相比，切换到OpenBLAS使性能提高了一倍 **（2.034秒 vs "
"4.123秒）**。这种提升对更小的批次尺寸会更加显著，例如，批次尺寸为10时："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The following is the profiler output with MKLDNN fast math mode:"
msgstr "以下是使用MKLDNN快速数学模式的性能分析器输出："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "87.81%"
msgstr "87.81%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.613s"
msgstr "3.613秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "91.90%"
msgstr "91.90%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.781s"
msgstr "3.781秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "12.604ms"
msgstr "12.604毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7.18%"
msgstr "7.18%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "295.437ms"
msgstr "295.437毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.477ms"
msgstr "1.477毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.07%"
msgstr "4.07%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "167.516ms"
msgstr "167.516毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "558.387us"
msgstr "558.387微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "27.708ms"
msgstr "27.708毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.115s"
msgstr "4.115秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.499ms"
msgstr "2.499毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "92.06%"
msgstr "92.06%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.788s"
msgstr "3.788秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "12.627ms"
msgstr "12.627毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.982ms"
msgstr "1.982毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.11%"
msgstr "0.11%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.385ms"
msgstr "4.385毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "14.617us"
msgstr "14.617微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.932ms"
msgstr "1.932毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7.23%"
msgstr "7.23%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "297.369ms"
msgstr "297.369毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.487ms"
msgstr "1.487毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 4.115s"
msgstr "**自CPU时间总计：** 4.115秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "and the following is the profiler output with the OpenBLAS backend:"
msgstr "以下是使用OpenBLAS后端的性能分析器输出："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "92.66%"
msgstr "92.66%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.179s"
msgstr "1.179秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.23%"
msgstr "95.23%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.211s"
msgstr "1.211秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.038ms"
msgstr "4.038毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.83%"
msgstr "2.83%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "36.060ms"
msgstr "36.060毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "180.300us"
msgstr "180.300微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.52%"
msgstr "2.52%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "32.013ms"
msgstr "32.013毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "106.710us"
msgstr "106.710微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.38%"
msgstr "1.38%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "17.521ms"
msgstr "17.521毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.272s"
msgstr "1.272秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.750ms"
msgstr "1.750毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "95.60%"
msgstr "95.60%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.216s"
msgstr "1.216秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.054ms"
msgstr "4.054毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.12%"
msgstr "0.12%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.475ms"
msgstr "1.475毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.24%"
msgstr "0.24%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.033ms"
msgstr "3.033毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "10.110us"
msgstr "10.110微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.285ms"
msgstr "1.285毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.94%"
msgstr "2.94%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "37.345ms"
msgstr "37.345毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "186.725us"
msgstr "186.725微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 1.272s"
msgstr "**自CPU时间总计：** 1.272秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Here we observed **3.2x (1.272s vs 4.115s)** performance improvement by "
"tuning the backend thresholds appropriately."
msgstr "通过适当地调整后端阈值，我们观察到**3.2倍（1.272秒 vs 4.115秒）**的性能提升。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Optimize Memory Allocation Overhead with Linux Transparent Huge Pages (THP)"
msgstr "使用Linux透明大页面（THP）优化内存分配开销"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We also observed that for these larger networks, tensor memory allocations "
"take significant portion of the inference latency. This can be optimized by "
"enabling Linux transparent huge page allocations from PyTorch C10 memory "
"allocator. Currently the feature is not enabled by default because it will "
"increase the memory footprint marginally. Set the following environment "
"variable to enable it:"
msgstr ""
"我们还观察到，对于这些较大的网络，张量内存分配占据了推理延迟的很大一部分。通过从PyTorch "
"C10内存分配器启用Linux透明巨页分配，可以优化这一点。目前该功能默认未启用，因为它会略微增加内存占用。通过设置以下环境变量来启用它："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "For the batch dimension of 256 and with MKLDNN fast math mode:"
msgstr "对于批次尺寸为256且使用MKLDNN快速数学模式："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The following is the profiler output with THP memory allocations enabled:"
msgstr "以下是启用THP内存分配的性能分析器输出："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "91.31%"
msgstr "91.31%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.115s"
msgstr "6.115秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "94.39%"
msgstr "94.39%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.321s"
msgstr "6.321秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "21.069ms"
msgstr "21.069毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.82%"
msgstr "4.82%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "322.568ms"
msgstr "322.568毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.613ms"
msgstr "1.613毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.06%"
msgstr "3.06%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "204.602ms"
msgstr "204.602毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "682.007us"
msgstr "682.007微秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0.61%"
msgstr "0.61%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "40.777ms"
msgstr "40.777毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.697s"
msgstr "6.697秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3.082ms"
msgstr "3.082毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "94.51%"
msgstr "94.51%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6.329s"
msgstr "6.329秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "21.097ms"
msgstr "21.097毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2.547ms"
msgstr "2.547毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4.85%"
msgstr "4.85%"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "325.115ms"
msgstr "325.115毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1.626ms"
msgstr "1.626毫秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Self CPU time total:** 6.697s"
msgstr "**自CPU时间总计：** 6.697秒"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is an additional **1.08x or 8% (6.697s vs 7.262s)** improvement on top "
"of the already optimized MKLDNN fast math mode measured above."
msgstr "在已经优化的MKLDNN快速数学模式基础上，这又是一项**1.08倍或8%（6.697秒 vs 7.262秒）**的改进。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we covered PyTorch inference on AWS Graviton3 instances by"
" covering the basic usage, demonstrating speedups with fast math kernels, "
"comparing different backends for different batch dimensions, and how to "
"optimize tensor memory allocation latencies with Linux transparent huge "
"pages. The recommendation is to use MKLDNN backend with Bfloat16 fastmath "
"mode and THP memory allocations for larger tensor shapes and to use OpenBLAS"
" backend for smaller tensor shapes. We hope that you will give it a try!"
msgstr ""
"在本教程中，我们通过介绍AWS "
"Graviton3实例上的PyTorch推理的基本用法、演示快速数学核的加速、对比不同批次尺寸的不同后端以及如何通过Linux透明巨页优化张量内存分配延迟，全面探讨了PyTorch推理。推荐对于更大的张量形状使用Bfloat16快速数学模式和THP内存分配的MKLDNN后端，而对于较小的张量形状使用OpenBLAS后端。希望您能尝试一下！"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Intel® Extension for PyTorch*"
msgstr "Intel® 用于PyTorch的扩展"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Extension for PyTorch* extends PyTorch* with up-to-date features "
"optimizations for an extra performance boost on Intel hardware. "
"Optimizations take advantage of AVX-512 Vector Neural Network Instructions "
"(AVX512 VNNI) and Intel® Advanced Matrix Extensions (Intel® AMX) on Intel "
"CPUs as well as Intel X\\ :sup:`e`\\  Matrix Extensions (XMX) AI engines on "
"Intel discrete GPUs. Moreover, through PyTorch* `xpu` device, Intel® "
"Extension for PyTorch* provides easy GPU acceleration for Intel discrete "
"GPUs with PyTorch*."
msgstr ""
"Intel® "
"用于PyTorch的扩展通过提供最新的性能优化功能，进一步提升了在Intel硬件上的性能。这些优化利用了AVX-512向量神经网络指令（AVX512 "
"VNNI）和Intel®高级矩阵扩展（Intel® AMX）以及Intel离散GPU上的Intel X\\ :sup:`e`\\ "
"矩阵扩展（XMX）AI引擎。此外，通过PyTorch `xpu` 设备，Intel® "
"用于PyTorch的扩展为Intel离散GPU的PyTorch加速提供了便捷。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Extension for PyTorch* has been released as an open–source project at"
" `Github <https://github.com/intel/intel-extension-for-pytorch>`_."
msgstr ""
"Intel® 用于PyTorch的扩展已作为开源项目发布在 `Github <https://github.com/intel/intel-"
"extension-for-pytorch>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Source code for CPU is available at `main branch "
"<https://github.com/intel/intel-extension-for-pytorch/tree/main>`_."
msgstr ""
"CPU的源码可在`main分支 <https://github.com/intel/intel-extension-for-"
"pytorch/tree/main>`_ 上获取。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Source code for GPU is available at `xpu-main branch "
"<https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main>`_."
msgstr ""
"GPU的源码可在`xpu-main分支 <https://github.com/intel/intel-extension-for-"
"pytorch/tree/xpu-main>`_ 上获取。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Features"
msgstr "功能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Intel® Extension for PyTorch* shares most of features for CPU and GPU."
msgstr "Intel® 用于PyTorch的扩展在CPU和GPU之间共享大部分功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Ease-of-use Python API:** Intel® Extension for PyTorch* provides simple "
"frontend Python APIs and utilities for users to get performance "
"optimizations such as graph optimization and operator optimization with "
"minor code changes. Typically, only 2 to 3 clauses are required to be added "
"to the original code."
msgstr ""
"**易用的Python API：** Intel® 用于PyTorch的扩展通过简单的前端Python "
"API和实用工具，为用户提供了通过少量代码更改获得性能优化（如图优化和操作符优化）的可能性。通常，仅需在原始代码中添加2至3行代码。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Channels Last:** Comparing to the default NCHW memory format, "
"channels_last (NHWC) memory format could further accelerate convolutional "
"neural networks. In Intel® Extension for PyTorch*, NHWC memory format has "
"been enabled for most key CPU operators, though not all of them have been "
"merged to PyTorch master branch yet. They are expected to be fully landed in"
" PyTorch upstream soon."
msgstr ""
"**Channels Last：** "
"与默认的NCHW内存格式相比，channels_last（NHWC）内存格式可以进一步加速卷积神经网络。在Intel® "
"用于PyTorch的扩展中，大多数CPU关键操作符已经启用了NHWC内存格式，尽管其中的一部分尚未合并到PyTorch主分支中。预计这些功能很快会完全合并到PyTorch上游。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Auto Mixed Precision (AMP):** Low precision data type BFloat16 has been "
"natively supported on the 3rd Generation Xeon scalable Servers (aka Cooper "
"Lake) with AVX512 instruction set and will be supported on the next "
"generation of Intel® Xeon® Scalable Processors with Intel® Advanced Matrix "
"Extensions (Intel® AMX) instruction set with further boosted performance. "
"The support of Auto Mixed Precision (AMP) with BFloat16 for CPU and BFloat16"
" optimization of operators have been massively enabled in Intel® Extension "
"for PyTorch*, and partially upstreamed to PyTorch master branch. Most of "
"these optimizations will be landed in PyTorch master through PRs that are "
"being submitted and reviewed. Auto Mixed Precision (AMP) with both BFloat16 "
"and Float16 have been enabled for Intel discrete GPUs."
msgstr ""
"**自动混合精度（AMP）：** 低精度数据类型BFloat16已经在第3代Xeon可扩展服务器（又称Cooper "
"Lake）中原生支持，并将被支持Intel® Xeon®可扩展处理器的新一代中（配备Intel®高级矩阵扩展（Intel® "
"AMX）指令集），其性能将进一步提高。在Intel® "
"用于PyTorch的扩展中，已经广泛支持了用BFloat16进行的自动混合精度（AMP）以及操作符优化。这些功能的一部分已被部分上游合并到PyTorch主分支中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Graph Optimization:** To optimize performance further with torchscript, "
"Intel® Extension for PyTorch* supports fusion of frequently used operator "
"patterns, like Conv2D+ReLU, Linear+ReLU, etc. The benefit of the fusions are"
" delivered to users in a transparent fashion. Detailed fusion patterns "
"supported can be found `here <https://github.com/intel/intel-extension-for-"
"pytorch>`_. The graph optimization will be up-streamed to PyTorch with the "
"introduction of oneDNN Graph API."
msgstr ""
"**图优化：** 为了通过torchscript进一步优化性能，Intel® "
"用于PyTorch的扩展支持常用的操作符模式的融合，例如Conv2D+ReLU、Linear+ReLU等。这些优化以透明的方式传递给用户。支持的详细融合模式可在"
" `这里 <https://github.com/intel/intel-extension-for-pytorch>`_ 找到。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Operator Optimization:** Intel® Extension for PyTorch* also optimizes "
"operators and implements several customized operators for performance. A few"
" ATen operators are replaced by their optimized counterparts in Intel® "
"Extension for PyTorch* via ATen registration mechanism. Moreover, some "
"customized operators are implemented for several popular topologies. For "
"instance, ROIAlign and NMS are defined in Mask R-CNN. To improve performance"
" of these topologies, Intel® Extension for PyTorch* also optimized these "
"customized operators."
msgstr ""
"**操作符优化：** Intel® 用于PyTorch的扩展优化了操作符，并为性能实现了一些定制化操作符。例如，在Mask "
"R-CNN模型中定义的ROIAlign和NMS。为提升这些模型的性能，Intel® 用于PyTorch的扩展还优化了这些定制操作符。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Getting Started"
msgstr "快速上手"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Minor code changes are required for users to get start with Intel® Extension"
" for PyTorch*. Both PyTorch imperative mode and TorchScript mode are "
"supported. This section introduces usage of Intel® Extension for PyTorch* "
"API functions for both imperative mode and TorchScript mode, covering data "
"type Float32 and BFloat16. C++ usage will also be introduced at the end."
msgstr ""
"用户需要对代码进行少量更改即可开始使用Intel® "
"用于PyTorch的扩展。支持PyTorch命令式模式和TorchScript模式。本节介绍了如何在命令式模式和TorchScript模式下使用Intel®"
" 用于PyTorch的扩展API功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You just need to import Intel® Extension for PyTorch* package and apply its "
"optimize function against the model object. If it is a training workload, "
"the optimize function also needs to be applied against the optimizer object."
msgstr "您只需要导入Intel® 用于PyTorch的扩展包并将其优化功能应用到模型对象上。如果是训练工作量，还需要对优化器对象应用优化功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For training and inference with BFloat16 data type, `torch.cpu.amp` has been"
" enabled in PyTorch upstream to support mixed precision with convenience. "
"BFloat16 datatype has been enabled excessively for CPU operators in PyTorch "
"upstream and Intel® Extension for PyTorch*. Meanwhile `torch.xpu.amp`, "
"registered by Intel® Extension for PyTorch*, enables easy usage of BFloat16 "
"and Float16 data types on Intel discrete GPUs. Either `torch.cpu.amp` or "
"`torch.xpu.amp` matches each operator to its appropriate datatype "
"automatically and returns the best possible performance."
msgstr ""
"对于使用BFloat16数据类型进行训练和推理，PyTorch上游已启用`torch.cpu.amp`，支持便捷的混合精度。此外，Intel® "
"用于PyTorch的扩展注册的`torch.xpu.amp`，允许在Intel离散GPU上轻松使用BFloat16和Float16数据类型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Examples -- CPU"
msgstr "示例 - CPU"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This section shows examples of training and inference on CPU with Intel® "
"Extension for PyTorch*"
msgstr "本节展示了在CPU上使用Intel® 用于PyTorch的扩展进行训练和推理的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code changes that are required for Intel® Extension for PyTorch* are "
"highlighted."
msgstr "使用Intel® 用于PyTorch的扩展所需的代码更改已被突出显示。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Training"
msgstr "训练"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Float32"
msgstr "Float32"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "BFloat16"
msgstr "BFloat16"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inference - Imperative Mode"
msgstr "推理 - 命令式模式"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inference - TorchScript Mode"
msgstr "推理 - TorchScript模式"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"TorchScript mode makes graph optimization possible, hence improves "
"performance for some topologies. Intel® Extension for PyTorch* enables most "
"commonly used operator pattern fusion, and users can get the performance "
"benefit without additional code changes."
msgstr ""
"TorchScript模式使图优化成为可能，因此可以提升一些模型的性能。Intel® "
"用于PyTorch的扩展启用了大多数常用的操作符模式融合，用户无需额外代码更改即可获得性能收益。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Examples -- GPU"
msgstr "示例 - GPU"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This section shows examples of training and inference on GPU with Intel® "
"Extension for PyTorch*"
msgstr "本节展示了在GPU上使用Intel® 用于PyTorch的扩展进行训练和推理的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code changes that are required for Intel® Extension for PyTorch* are "
"highlighted with comments in a line above."
msgstr "需要更改的代码在上一行用注释突出显示。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Float16"
msgstr "Float16"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "C++ (CPU only)"
msgstr "C++（仅限CPU）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To work with libtorch, C++ library of PyTorch, Intel® Extension for PyTorch*"
" provides its C++ dynamic library as well. The C++ library is supposed to "
"handle inference workload only, such as service deployment. For regular "
"development, please use Python interface. Comparing to usage of libtorch, no"
" specific code changes are required, except for converting input data into "
"channels last data format. Compilation follows the recommended methodology "
"with CMake. Detailed instructions can be found in `PyTorch tutorial "
"<https://pytorch.org/tutorials/advanced/cpp_export.html#depending-on-"
"libtorch-and-building-the-application>`_. During compilation, Intel "
"optimizations will be activated automatically once C++ dynamic library of "
"Intel® Extension for PyTorch* is linked."
msgstr ""
"为了支持libtorch（PyTorch的C++库），Intel® "
"用于PyTorch的扩展还提供了其C++动态库。C++库主要用于处理推理工作，例如服务部署。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**example-app.cpp**"
msgstr "**示例程序文件（example-app.cpp）**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**CMakeLists.txt**"
msgstr "**构建脚本（CMakeLists.txt）**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Command for compilation**"
msgstr "**编译命令**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If `Found INTEL_EXT_PT_CPU` is shown as `TRUE`, the extension had been "
"linked into the binary. This can be verified with the Linux command `ldd`."
msgstr "若 `Found INTEL_EXT_PT_CPU` 显示为 `TRUE`，则说明扩展已链接到二进制文件中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Zoo (CPU only)"
msgstr "模型库（仅限CPU）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Use cases that had already been optimized by Intel engineers are available "
"at `Model Zoo for Intel® Architecture <https://github.com/IntelAI/models/>`_"
" (with the branch name in format of `pytorch-r<version>-models`). Many "
"PyTorch use cases for benchmarking are also available on the GitHub page. "
"You can get performance benefits out-of-the-box by simply running scripts in"
" the Model Zoo."
msgstr ""
"已由Intel工程师优化的用例可在 `Intel® 架构下的Model Zoo "
"<https://github.com/IntelAI/models/>`_ 中找到。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Tutorials"
msgstr "教程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"More detailed tutorials are available in the official Intel® Extension for "
"PyTorch* Documentation:"
msgstr "官方Intel® 用于PyTorch的扩展文档中提供了更加详细的教程："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`CPU <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/>`_"
msgstr ""
"`CPU <https://intel.github.io/intel-extension-for-pytorch/cpu/latest/>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`GPU <https://intel.github.io/intel-extension-for-pytorch/xpu/latest/>`_"
msgstr ""
"`GPU <https://intel.github.io/intel-extension-for-pytorch/xpu/latest/>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Ease-of-use quantization for PyTorch with Intel® Neural Compressor"
msgstr "使用Intel® 神经压缩器为PyTorch实现易用的量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Overview"
msgstr "概述"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Most deep learning applications are using 32-bits of floating-point "
"precision for inference. But low precision data types, especially int8, are "
"getting more focus due to significant performance boost. One of the "
"essential concerns on adopting low precision is how to easily mitigate the "
"possible accuracy loss and reach predefined accuracy requirement."
msgstr ""
"大多数深度学习应用都使用32位浮点数精度进行推理。但低精度数据类型，尤其是int8，由于其显著的性能提升而越来越受到关注。在采用低精度时，一个重要的问题是如何轻松缓解可能的精度损失并满足预定义的精度要求。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Neural Compressor aims to address the aforementioned concern by "
"extending PyTorch with accuracy-driven automatic tuning strategies to help "
"user quickly find out the best quantized model on Intel hardware, including "
"Intel Deep Learning Boost (`Intel DL Boost "
"<https://www.intel.com/content/www/us/en/artificial-intelligence/deep-"
"learning-boost.html>`_) and Intel Advanced Matrix Extensions (`Intel AMX "
"<https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-"
"developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-"
"for-amx-instructions/intrinsics-for-amx-tile-instructions.html>`_)."
msgstr ""
"英特尔®神经压缩器旨在通过扩展PyTorch的基于精度的自动调优策略来解决上述问题，帮助用户快速找到在英特尔硬件（包括英特尔深度学习加速（`Intel "
"DL Boost <https://www.intel.com/content/www/us/en/artificial-"
"intelligence/deep-learning-boost.html>`_）和英特尔高级矩阵扩展（`Intel AMX "
"<https://www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-"
"developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-"
"for-amx-instructions/intrinsics-for-amx-tile-instructions.html>`_）上）的最佳量化模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Neural Compressor has been released as an open-source project at "
"`Github <https://github.com/intel/neural-compressor>`_."
msgstr ""
"英特尔®神经压缩器已作为一个开源项目在`Github <https://github.com/intel/neural-"
"compressor>`_上发布。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Ease-of-use Python API:** Intel® Neural Compressor provides simple "
"frontend Python APIs and utilities for users to do neural network "
"compression with few line code changes. Typically, only 5 to 6 clauses are "
"required to be added to the original code."
msgstr ""
"**易用的Python API:** 英特尔®神经压缩器提供了简单的前端Python "
"API和实用工具，使用户可以通过少量代码更改完成神经网络压缩。通常情况下，仅需添加5到6段代码即可。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Quantization:** Intel® Neural Compressor supports accuracy-driven "
"automatic tuning process on post-training static quantization, post-training"
" dynamic quantization, and quantization-aware training on PyTorch fx graph "
"mode and eager model."
msgstr ""
"**量化:** 英特尔®神经压缩器支持基于精确度的自动调优过程，包括后训练静态量化、后训练动态量化以及基于PyTorch "
"fx图模式和eager模型的量化感知训练。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"*This tutorial mainly focuses on the quantization part. As for how to use "
"Intel® Neural Compressor to do pruning and distillation, please refer to "
"corresponding documents in the Intel® Neural Compressor github repo.*"
msgstr "*本教程主要关注量化部分。至于如何使用英特尔®神经压缩器进行剪枝和蒸馏，请参考英特尔®神经压缩器github库中的相应文档。*"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Installation"
msgstr "安装"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "*Supported python versions are 3.6 or 3.7 or 3.8 or 3.9*"
msgstr "*支持的Python版本为3.6、3.7、3.8或3.9*"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Usages"
msgstr "使用方法"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Minor code changes are required for users to get started with Intel® Neural "
"Compressor quantization API. Both PyTorch fx graph mode and eager mode are "
"supported."
msgstr "用户仅需少量代码更改即可开始使用英特尔®神经压缩器的量化API。支持PyTorch fx图模式和eager模式。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Neural Compressor takes a FP32 model and a yaml configuration file as"
" inputs. To construct the quantization process, users can either specify the"
" below settings via the yaml configuration file or python APIs:"
msgstr ""
"英特尔®神经压缩器将FP32模型和一个yaml配置文件作为输入。要构建量化过程，用户可以通过yaml配置文件或Python API指定以下设置："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Calibration Dataloader (Needed for static quantization)"
msgstr "校准数据加载器（用于静态量化）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Evaluation Dataloader"
msgstr "评估数据加载器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Evaluation Metric"
msgstr "评估指标"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Neural Compressor supports some popular dataloaders and evaluation "
"metrics. For how to configure them in yaml configuration file, user could "
"refer to `Built-in Datasets <https://github.com/intel/neural-"
"compressor/blob/master/docs/dataset.md>`_."
msgstr ""
"英特尔®神经压缩器支持一些流行的数据加载器和评估指标。有关如何在yaml配置文件中配置这些内容，用户可以参考`内建数据集 "
"<https://github.com/intel/neural-compressor/blob/master/docs/dataset.md>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If users want to use a self-developed dataloader or evaluation metric, "
"Intel® Neural Compressor supports this by the registration of customized "
"dataloader/metric using python code."
msgstr "如果用户想使用自定义的数据加载器或评估指标，英特尔®神经压缩器也通过Python代码注册的方式支持这些定制。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For the yaml configuration file format please refer to `yaml template "
"<https://github.com/intel/neural-"
"compressor/blob/master/neural_compressor/template/ptq.yaml>`_."
msgstr ""
"有关yaml配置文件格式，请参考`yaml模板 <https://github.com/intel/neural-"
"compressor/blob/master/neural_compressor/template/ptq.yaml>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code changes that are required for *Intel® Neural Compressor* are "
"highlighted with comments in the line above."
msgstr "所需的代码更改在代码行上方以注释的形式标注。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model"
msgstr "模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, the LeNet model is used to demonstrate how to deal with "
"*Intel® Neural Compressor*."
msgstr "在本教程中，LeNet模型被用来演示如何使用英特尔®神经压缩器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The pretrained model weight `lenet_mnist_model.pth` comes from `here "
"<https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing>`_."
msgstr ""
"预训练模型权重`lenet_mnist_model.pth`来源于`这里 "
"<https://drive.google.com/drive/folders/1fn83DF14tWmit0RTKWRhPq5uVXt73e0h?usp=sharing>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Accuracy driven quantization"
msgstr "基于精度的量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Neural Compressor supports accuracy-driven automatic tuning to "
"generate the optimal int8 model which meets a predefined accuracy goal."
msgstr "英特尔®神经压缩器支持基于精度的自动调优以生成满足预定义精度目标的最佳int8模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Below is an example of how to quantize a simple network on PyTorch `FX graph"
" mode <https://pytorch.org/docs/stable/fx.html>`_ by auto-tuning."
msgstr ""
"以下是一个通过自动调优在PyTorch `FX图模式 "
"<https://pytorch.org/docs/stable/fx.html>`_中量化简单网络的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the `conf.yaml` file, the built-in metric `top1` of Intel® Neural "
"Compressor is specified as the evaluation method, and `1%` relative accuracy"
" loss is set as the accuracy target for auto-tuning. Intel® Neural "
"Compressor will traverse all possible quantization config combinations on "
"per-op level to find out the optimal int8 model that reaches the predefined "
"accuracy target."
msgstr ""
"在`conf.yaml`文件中，英特尔®神经压缩器内置的评估方法`top1`被指定为评估指标，并将`1%`相对精度损失设置为自动调优的精度目标。英特尔®神经压缩器将遍历每个操作级别上所有可能的量化配置组合，以找到满足预定义精度目标的最佳int8模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Besides those built-in metrics, Intel® Neural Compressor also supports "
"customized metric through python code:"
msgstr "除了这些内置指标，英特尔®神经压缩器还通过Python代码支持自定义指标："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the above example, a `class` which contains `update()` and `result()` "
"function is implemented to record per mini-batch result and calculate final "
"accuracy at the end."
msgstr "在上述示例中，实现了一个包含`update()`和`result()`函数的`类`，以记录每个小批量的结果并在最后计算最终精度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Quantization aware training"
msgstr "量化感知训练"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Besides post-training static quantization and post-training dynamic "
"quantization, Intel® Neural Compressor supports quantization-aware training "
"with an accuracy-driven automatic tuning mechanism."
msgstr "除了后训练静态量化和后训练动态量化，英特尔®神经压缩器还支持具有基于精度的自动调优机制的量化感知训练。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Below is an example of how to do quantization aware training on a simple "
"network on PyTorch `FX graph mode "
"<https://pytorch.org/docs/stable/fx.html>`_."
msgstr ""
"以下是在PyTorch `FX图模式 "
"<https://pytorch.org/docs/stable/fx.html>`_上对简单网络进行量化感知训练的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Performance only quantization"
msgstr "仅以性能为目的的量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® Neural Compressor supports directly yielding int8 model with dummy "
"dataset for the performance benchmarking purpose."
msgstr "英特尔®神经压缩器支持直接使用虚拟数据集生成int8模型，以进行性能基准测试。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Below is an example of how to quantize a simple network on PyTorch `FX graph"
" mode <https://pytorch.org/docs/stable/fx.html>`_ with a dummy dataset."
msgstr ""
"以下是使用虚拟数据集在PyTorch `FX图模式 "
"<https://pytorch.org/docs/stable/fx.html>`_中量化简单网络的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Quantization outputs"
msgstr "量化输出"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Users could know how many ops get quantized from log printed by Intel® "
"Neural Compressor like below:"
msgstr "用户可以通过英特尔®神经压缩器打印的日志了解多少操作被量化，例如："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The quantized model will be generated under `./output` directory, in which "
"there are two files: 1. best_configure.yaml 2. best_model_weights.pt"
msgstr ""
"量化后的模型将生成在`./output`目录中，其中包含两个文件：1. best_configure.yaml  2. "
"best_model_weights.pt"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The first file contains the quantization configurations of each op, the "
"second file contains int8 weights and zero point and scale info of "
"activations."
msgstr "第一个文件包含每个操作的量化配置，第二个文件包含int8权重以及激活值的零点和缩放信息。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Deployment"
msgstr "部署"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Users could use the below code to load quantized model and then do inference"
" or performance benchmark."
msgstr "用户可以使用以下代码加载量化后的模型，然后进行推理或性能基准测试。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Please visit `Intel® Neural Compressor Github repo "
"<https://github.com/intel/neural-compressor>`_ for more tutorials."
msgstr ""
"请访问`英特尔®神经压缩器Github库 <https://github.com/intel/neural-compressor>`_以获取更多教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Loading data in PyTorch"
msgstr "在PyTorch中加载数据"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The content is deprecated. See `Datasets & DataLoaders "
"<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>`__ "
"instead."
msgstr ""
"内容已废弃。请参考`数据集和数据加载器 "
"<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(beta) Efficient mobile interpreter in Android and iOS"
msgstr "(测试版)在Android和iOS中的高效移动解释器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Pytorch Mobile Performance Recipes"
msgstr "PyTorch移动性能技巧"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Preparation for Android Recipe"
msgstr "为Android调优准备模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Preparation for iOS Recipe"
msgstr "为iOS调优准备模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Profiling PyTorch workloads with The Instrumentation and Tracing Technology "
"(ITT) API"
msgstr "使用仪器和跟踪技术（ITT）API分析PyTorch工作负载"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is Intel® VTune™ Profiler"
msgstr "什么是英特尔®VTune™分析器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is Instrumentation and Tracing Technology (ITT) API"
msgstr "什么是仪器和跟踪技术（ITT）API"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to visualize PyTorch model hierarchy in Intel® VTune™ Profiler"
msgstr "如何在英特尔®VTune™分析器中可视化PyTorch模型层次结构"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "A short sample code showcasing how to use PyTorch ITT APIs"
msgstr "一个展示如何使用PyTorch ITT API的短代码示例"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 1.13 or later"
msgstr "PyTorch 1.13或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Intel® VTune™ Profiler"
msgstr "英特尔®VTune™分析器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The instructions for installing PyTorch are available at `pytorch.org "
"<https://pytorch.org/get-started/locally/>`__."
msgstr ""
"PyTorch的安装说明可在`pytorch.org <https://pytorch.org/get-started/locally/>`__上找到。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Intel® VTune™ Profiler is a performance analysis tool for serial and "
"multithreaded applications. For those who are familiar with Intel "
"Architecture, Intel® VTune™ Profiler provides a rich set of metrics to help "
"users understand how the application executed on Intel platforms, and thus "
"have an idea where the performance bottleneck is."
msgstr ""
"英特尔®VTune™分析器是一个用于串行和多线程应用的性能分析工具。对于熟悉英特尔架构的开发者，英特尔®VTune™分析器提供了一套丰富的指标，帮助用户了解应用在英特尔平台上的执行情况，从而定位性能瓶颈。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"More detailed information, including a Getting Started guide, are available "
"`on the Intel website "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-"
"profiler.html>`__."
msgstr ""
"更多详细信息，包括入门指南，请参考`英特尔官方网站 "
"<https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-"
"profiler.html>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`The Instrumentation and Tracing Technology API (ITT API) "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/api-support/instrumentation-and-tracing-technology-apis.html>`_ "
"provided by the Intel® VTune™ Profiler enables target application to "
"generate and control the collection of trace data during its execution."
msgstr ""
"`仪器和跟踪技术API（ITT API） "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/api-support/instrumentation-and-tracing-technology-"
"apis.html>`_由英特尔®VTune™分析器提供，用于在目标应用执行过程中生成和控制跟踪数据的收集。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The advantage of ITT feature is to label time span of individual PyTorch "
"operators, as well as customized regions, on Intel® VTune™ Profiler GUI. "
"When users find anything abnormal, it will be very helpful to locate which "
"operator behaved unexpectedly."
msgstr ""
"ITT功能的优势在于可以在英特尔®VTune™分析器GUI中标记单个PyTorch操作以及自定义区域的时间跨度。当用户发现任何异常时，这对于定位哪个操作行为异常非常有帮助。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The ITT API had been integrated into PyTorch since 1.13. Users don't need to"
" invoke the original ITT C/C++ APIs, but only need to invoke the Python APIs"
" in PyTorch. More detailed information can be found at `PyTorch Docs "
"<https://pytorch.org/docs/stable/profiler.html#intel-instrumentation-and-"
"tracing-technology-apis>`__."
msgstr ""
"ITT API自PyTorch 1.13版本起已集成。用户无需调用原始的ITT C/C++ API，只需调用PyTorch中的Python "
"API即可。更多详细信息可以参考`PyTorch文档 "
"<https://pytorch.org/docs/stable/profiler.html#intel-instrumentation-and-"
"tracing-technology-apis>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Two types of usage are provided in PyTorch:"
msgstr "PyTorch提供了两种使用方法："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Implicit invocation: By default, all operators that are registered by "
"following the PyTorch operator registration mechanism will be labeled by ITT"
" feature automatically when its feature is enabled."
msgstr "隐式调用：默认情况下，所有通过PyTorch操作注册机制注册的操作在启用ITT功能时都会被自动标记。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Explicit invocation: If customized labeling is needed, users can use APIs "
"mentioned at `PyTorch Docs "
"<https://pytorch.org/docs/stable/profiler.html#intel-instrumentation-and-"
"tracing-technology-apis>`__ explicitly to label a desired range."
msgstr ""
"显式调用：如果需要自定义标记，用户可以在`PyTorch文档 "
"<https://pytorch.org/docs/stable/profiler.html#intel-instrumentation-and-"
"tracing-technology-apis>`__中提到的API中显式标记所需的范围。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To enable explicit invocation, code which are expected to be labeled should "
"be invoked under a `torch.autograd.profiler.emit_itt()` scope. For example:"
msgstr "要启用显式调用，预期被标记的代码应在`torch.autograd.profiler.emit_itt()`范围内调用。例如："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Launch Intel® VTune™ Profiler"
msgstr "启动英特尔®VTune™分析器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To verify the functionality, you need to start an Intel® VTune™ Profiler "
"instance. Please check the `Intel® VTune™ Profiler User Guide "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/launch.html>`__ for steps to launch Intel® VTune™ Profiler."
msgstr ""
"要验证功能，需要启动一个英特尔®VTune™分析器实例。请查看`英特尔®VTune™分析器用户指南 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/launch.html>`__了解启动英特尔®VTune™分析器的步骤。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Users can also use web-server-ui by following  `Intel® VTune™ Profiler Web "
"Server UI Guide <https://www.intel.com/content/www/us/en/docs/vtune-"
"profiler/user-guide/2024-1/web-server-ui.html>`__ ex :  vtune-backend --web-"
"port=8080  --allow-remote-access --enable-server-profiling"
msgstr ""
"用户还可以通过`英特尔®VTune™分析器Web服务器UI指南 "
"<https://www.intel.com/content/www/us/en/docs/vtune-profiler/user-"
"guide/2024-1/web-server-ui.html>`__使用web服务器UI，例如：vtune-backend --web-"
"port=8080 --allow-remote-access --enable-server-profiling"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Once you get the Intel® VTune™ Profiler GUI launched, you should see a user "
"interface as below:"
msgstr "一旦英特尔®VTune™分析器GUI启动，您应该会看到如下的用户界面："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Three sample results are available on the left side navigation bar under "
"`sample (matrix)` project. If you do not want profiling results appear in "
"this default sample project, you can create a new project via the button "
"`New Project...` under the blue `Configure Analysis...` button. To start a "
"new profiling, click the blue `Configure Analysis...` button to initiate "
"configuration of the profiling."
msgstr ""
"左侧导航栏下的`sample "
"(matrix)`项目中可以看到三个示例结果。如果您不希望分析结果出现在默认的示例项目中，可以通过蓝色的`配置分析...`按钮下的`新建项目...`按钮创建新项目。要开始新的分析，请点击蓝色的`配置分析...`按钮启动分析配置。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Configure Profiling for CPU"
msgstr "配置CPU性能分析"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Once you click the `Configure Analysis...` button, you should see the screen"
" below:"
msgstr "点击`配置分析...`按钮后，您应该看到以下界面："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The right side of the windows is split into 3 parts: `WHERE` (top left), "
"`WHAT` (bottom left), and `HOW` (right). With `WHERE`, you can assign a "
"machine where you want to run the profiling on. With `WHAT`, you can set the"
" path of the application that you want to profile. To profile a PyTorch "
"script, it is recommended to wrap all manual steps, including activating a "
"Python environment and setting required environment variables, into a bash "
"script, then profile this bash script. In the screenshot above, we wrapped "
"all steps into the `launch.sh` bash script and profile `bash` with the "
"parameter to be `<path_of_launch.sh>`. On the right side `HOW`, you can "
"choose whatever type that you would like to profile. Intel® VTune™ Profiler "
"provides a bunch of profiling types that you can choose from. Details can be"
" found at `Intel® VTune™ Profiler User Guide "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/analyze-performance.html>`__."
msgstr ""
"窗口的右侧分为三部分：`WHERE`（左上），`WHAT`（左下）和`HOW`（右侧）。通过`WHERE`，您可以分配用于运行性能分析的机器。通过`WHAT`，您可以设置要分析的应用程序的路径。为了分析PyTorch脚本，建议将所有手动步骤（包括激活Python环境和设置所需的环境变量）封装到一个bash脚本中，然后分析此bash脚本。在上方的截图中，我们将所有步骤封装到`launch.sh`"
" bash脚本中，并使用参数`<path_of_launch.sh>`分析`bash`。在右侧的`HOW`，您可以选择要分析的类型。Intel® "
"VTune™ Profiler 提供了许多分析类型可供选择。可以在`Intel® VTune™ Profiler用户指南 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/analyze-performance.html>`__找到详细信息。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Configure Profiling for XPU"
msgstr "为XPU配置性能分析"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Pick GPU Offload Profiling Type instead of Hotspots, and follow the same "
"instructions as CPU to Launch the Application."
msgstr "选择GPU卸载性能分析类型，而不是Hotspots，并按照CPU相同的说明启动应用程序。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Read Profiling Result"
msgstr "读取性能分析结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"With a successful profiling with ITT, you can open `Platform` tab of the "
"profiling result to see labels in the Intel® VTune™ Profiler timeline."
msgstr "成功使用ITT分析后，您可以打开分析结果的`Platform`选项卡以在Intel® VTune™ Profiler时间线上查看标签。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The timeline shows the main thread as a `python` thread on the top, and "
"individual OpenMP threads below. Labeled PyTorch operators and customized "
"regions are shown in the main thread row. All operators starting with "
"`aten::` are operators labeled implicitly by the ITT feature in PyTorch. "
"Labels `iteration_N` are explicitly labeled with specific APIs "
"`torch.profiler.itt.range_push()`, `torch.profiler.itt.range_pop()` or "
"`torch.profiler.itt.range()` scope. Please check the sample code in the next"
" section for details."
msgstr ""
"时间线显示主线程为顶部的`python`线程，下面是单独的OpenMP线程。标记的PyTorch操作符和自定义区域显示在主线程的一行中。所有以`aten::`开头的操作符都是由PyTorch中的ITT功能隐式标记的。标记为`iteration_N`的标签使用特定的API显式标记，例如`torch.profiler.itt.range_push()`，`torch.profiler.itt.range_pop()`或`torch.profiler.itt.range()`作用域。有关详细信息，请查看下一节的示例代码。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Red boxes marked with `convolution` and `reorder` are labeled from Intel® "
"oneAPI Deep Neural Network Library (oneDNN)."
msgstr "用红色框标注的`convolution`和`reorder`来自Intel® oneAPI深度神经网络库（oneDNN）的标签。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As illustrated on the right side navigation bar, brown portions in the "
"timeline rows show CPU usage of individual threads. The percerntage of "
"height of a thread row that the brown portion occupies at a timestamp aligns"
" with that of the CPU usage in that thread at that timestamp. Thus, it is "
"intuitive from this timeline to understand the followings:"
msgstr ""
"如右侧导航栏所示，时间线行中的棕色部分表示各线程的CPU使用情况。线程行高度中棕色部分在某个时间点的占比与该时间点线程的CPU使用率一致。因此，直观地来看这条时间线可以了解以下内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How well CPU cores are utilized on each thread."
msgstr "每个线程上的CPU核心利用率如何。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How balance CPU cores are utilized on all threads. Do all threads have good "
"CPU usage?"
msgstr "所有线程上的CPU核心利用是否均衡。所有线程的CPU使用情况是否良好？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How well OpenMP threads are synchronized. Are there jitters when starting "
"OpenMP threads or OpenMP threads finish."
msgstr "OpenMP线程同步的情况如何。当OpenMP线程启动或结束时是否有抖动。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Of course there are much more enriched sets of profiling features that "
"Intel® VTune™ Profiler provides to help you understand a performance issue. "
"When you understand the root cause of a performance issue, you can get it "
"fixed. More detailed usage instructions are available at `Intel® VTune™ "
"Profiler User Guide "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/analyze-performance.html>`__."
msgstr ""
"当然，Intel® VTune™ "
"Profiler还提供了更丰富的性能分析功能，帮助您了解性能问题。当了解性能问题的根本原因后，您可以解决它。可以在`Intel® VTune™ "
"Profiler用户指南 "
"<https://www.intel.com/content/www/us/en/develop/documentation/vtune-"
"help/top/analyze-performance.html>`__找到更多详细的使用说明。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Read XPU Profiling Result"
msgstr "读取XPU性能分析结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The timeline shows the main thread as a `python` thread on the top. Labeled "
"PyTorch operators and customized regions are shown in the main thread row. "
"All operators starting with `aten::` are operators labeled implicitly by the"
" ITT feature in PyTorch. The timeline also shows the GPU Computing Queue on "
"the top, and users could see different XPU Kernels dispatched into GPU "
"Queue."
msgstr ""
"时间线显示主线程为顶部的`python`线程。标记的PyTorch操作符和自定义区域显示在主线程的一行中。所有以`aten::`开头的操作符都是由PyTorch的ITT功能隐式标记的。时间线还显示顶部的GPU计算队列，用户可以看到不同的XPU内核被分派到GPU队列。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The sample code below is the script that was used for profiling in the "
"screenshots above."
msgstr "以下示例代码是上方截图中用于性能分析的脚本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The topology is formed by two operators, `Conv2d` and `Linear`. Three "
"iterations of inference were performed. Each iteration was labeled by "
"PyTorch ITT APIs as text string `iteration_N`. Either pair of "
"`torch.profile.itt.range_push` and `torch.profile.itt.range_pop` or "
"`torch.profile.itt.range` scope does the customized labeling feature."
msgstr ""
"拓扑由两个操作符`Conv2d`和`Linear`组成。进行了三次推理迭代。每次迭代都使用PyTorch ITT "
"API标记为字符串`iteration_N`。`torch.profile.itt.range_push`和`torch.profile.itt.range_pop`或`torch.profile.itt.range`作用域都可以完成自定义标签功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The `launch.sh` bash script, mentioned in the Intel® VTune™ Profiler GUI "
"screenshot, to wrap all manual steps is shown below."
msgstr "截图中Intel® VTune™ Profiler GUI中提到的`launch.sh` bash脚本，用于封装所有手动步骤，见下文。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Summary of PyTorch Mobile Recipes"
msgstr "PyTorch Mobile 配方摘要"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Quantization Recipe"
msgstr "量化配方"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe demonstrates how to quantize a PyTorch model so it can run with "
"reduced size and faster inference speed with about the same accuracy as the "
"original model. Quantization can be applied to both server and mobile model "
"deployment, but it can be especially important or even critical on mobile, "
"because a non-quantized model's size may exceed the limit that an iOS or "
"Android app allows for, cause the deployment or OTA update to take too much "
"time, and make the inference too slow for a good user experience."
msgstr ""
"本配方演示如何对PyTorch模型进行量化，以便以减小的尺寸和更快的推理速度运行，同时保持与原始模型大致相同的准确性。量化可应用于服务器和移动模型部署，但在移动端尤为重要甚至关键，因为非量化模型的大小可能超出iOS或Android应用程序允许的限制，导致部署或OTA更新耗时过长，并且推理速度过慢，影响用户体验。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Quantization is a technique that converts 32-bit floating numbers in the "
"model parameters to 8-bit integers. With quantization, the model size and "
"memory footprint can be reduced to 1/4 of its original size, and the "
"inference can be made about 2-4 times faster, while the accuracy stays about"
" the same."
msgstr ""
"量化是一种技术，将模型参数中的32位浮点数转为8位整数。通过量化，模型大小和内存占用可减少到原来的1/4，并使推理速度提高约2-4倍，同时准确性几乎未受到影响。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are overall three approaches or workflows to quantize a model: post "
"training dynamic quantization, post training static quantization, and "
"quantization aware training. But if the model you want to use already has a "
"quantized version, you can use it directly without going through any of the "
"three workflows above. For example, the `torchvision` library already "
"includes quantized versions for models MobileNet v2, ResNet 18, ResNet 50, "
"Inception v3, GoogleNet, among others. So we will make the last approach "
"another workflow, albeit a simple one."
msgstr ""
"总体来说，有三种方法或工作流程可以量化模型：训练后动态量化、训练后静态量化、量化感知训练。但是，如果您要使用的模型已经有量化版本，则可以直接使用，而无需经过以上三种工作流程。例如，`torchvision`库已经为MobileNet"
" v2、ResNet 18、ResNet 50、Inception "
"v3、GoogleNet等模型包含量化版本。因此，我们将最后一种方法视为一种简单的工作流程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The quantization support is available for a limited set of operators. See "
"`this <https://pytorch.org/blog/introduction-to-quantization-on-"
"pytorch/#device-and-operator-support>`_ for more information."
msgstr ""
"量化支持的操作符集有限。请参阅`此处 <https://pytorch.org/blog/introduction-to-quantization-"
"on-pytorch/#device-and-operator-support>`_了解更多信息。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "torchvision 0.6.0 or 0.7.0"
msgstr "torchvision 0.6.0 或 0.7.0"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Workflows"
msgstr "工作流程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Use one of the four workflows below to quantize a model."
msgstr "使用以下四种工作流程之一对模型进行量化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1. Use Pretrained Quantized MobileNet v2"
msgstr "1. 使用预训练的量化MobileNet v2模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To get the MobileNet v2 quantized model, simply do:"
msgstr "要获取MobileNet v2量化模型，只需执行："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To compare the size difference of a non-quantized MobileNet v2 model with "
"its quantized version:"
msgstr "比较未量化的MobileNet v2模型与其量化版本的大小差异："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The outputs will be:"
msgstr "输出将是："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Post Training Dynamic Quantization"
msgstr "2. 训练后动态量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To apply Dynamic Quantization, which converts all the weights in a model "
"from 32-bit floating numbers to 8-bit integers but doesn't convert the "
"activations to int8 till just before performing the computation on the "
"activations, simply call `torch.quantization.quantize_dynamic`:"
msgstr ""
"要应用动态量化，将模型中的所有权重从32位浮点数转换为8位整数，但只在计算激活值前将激活值转换为int8，只需调用`torch.quantization.quantize_dynamic`："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"where `qconfig_spec` specifies the list of submodule names in `model` to "
"apply quantization to."
msgstr "其中`qconfig_spec`指定`model`中要应用量化的子模块名称列表。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"An important limitation of Dynamic Quantization, while it is the easiest "
"workflow if you do not have a pre-trained quantized model ready for use, is "
"that it currently only supports `nn.Linear` and `nn.LSTM` in `qconfig_spec`,"
" meaning that you will have to use Static Quantization or Quantization Aware"
" Training, to be discussed later, to quantize other modules such as "
"`nn.Conv2d`."
msgstr ""
"动态量化的一个重要限制是，它目前仅支持`qconfig_spec`中的`nn.Linear`和`nn.LSTM`，这意味着要对其他模块（如`nn.Conv2d`）进行量化，需使用后面将讨论的静态量化或量化感知训练。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The full documentation of the `quantize_dynamic` API call is `here "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_."
" Three other examples of using the post training dynamic quantization are "
"`the Bert example "
"<https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html>`_,"
" `an LSTM model example "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-"
"dynamic-quantization>`_, and another `demo LSTM example "
"<https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#do-"
"the-quantization>`_."
msgstr ""
"`quantize_dynamic`API的完整文档在`此处 "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_。还可以参阅三个动态量化训练后示例：`Bert示例"
" "
"<https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html>`_，`LSTM模型示例"
" "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-"
"dynamic-quantization>`_，以及另一个`LSTM模型示例 "
"<https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#do-"
"the-quantization>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Post Training Static Quantization"
msgstr "3. 训练后静态量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This method converts both the weights and the activations to 8-bit integers "
"beforehand so there won’t be on-the-fly conversion on the activations during"
" the inference, as the dynamic quantization does. While post-training static"
" quantization can significantly enhance inference speed and reduce model "
"size, this method may degrade the original model's accuracy more compared to"
" post training dynamic quantization."
msgstr ""
"此方法事先将权重和激活值都转换为8位整数，这样推理过程中激活值不需要动态转换。相比动态量化，训练后静态量化可以显著提升推理速度和减少模型大小，但可能对模型的原始准确性有更多影响。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To apply static quantization on a model, run the following code:"
msgstr "要对模型进行静态量化，运行以下代码："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"After this, running `print_model_size(model_static_quantized)` shows the "
"static quantized model is `3.98MB`."
msgstr ""
"运行`print_model_size(model_static_quantized)`后，可以看到静态量化后的模型大小为`3.98MB`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A complete model definition and static quantization example is `here "
"<https://pytorch.org/docs/stable/quantization.html#quantization-api-"
"summary>`_. A dedicated static quantization tutorial is `here "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_."
msgstr ""
"完整的模型定义和静态量化示例在`此处 "
"<https://pytorch.org/docs/stable/quantization.html#quantization-api-"
"summary>`_。专门的静态量化教程在`此处 "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To make the model run on mobile devices which normally have arm "
"architecture, you need to use `qnnpack` for `backend`; to run the model on "
"computer with x86 architecture, use `x86`` (the old `fbgemm` is still "
"available but 'x86' is the recommended default)."
msgstr ""
"为了使模型能在通常具有ARM架构的移动设备上运行，需为`backend`使用`qnnpack`；在具有x86架构的计算机上运行时，需使用`x86``（旧的`fbgemm`仍可用，但推荐使用默认的`x86`）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Quantization Aware Training"
msgstr "4. 量化感知训练"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Quantization aware training inserts fake quantization to all the weights and"
" activations during the model training process and results in higher "
"inference accuracy than the post-training quantization methods. It is "
"typically used in CNN models."
msgstr "量化感知训练在模型训练过程中对所有权重和激活值插入伪量化，从而在推理时获得比训练后量化方法更高的精度。这通常用于CNN模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To enable a model for quantization aware traing, define in the `__init__` "
"method of the model definition a `QuantStub` and a `DeQuantStub` to convert "
"tensors from floating point to quantized type and vice versa:"
msgstr ""
"为启用模型的量化感知训练，在模型定义的`__init__`方法中定义一个`QuantStub`和一个`DeQuantStub`，以便在浮点数和量化类型之间转换张量："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Then in the beginning and the end of the `forward` method of the model "
"definition, call `x = self.quant(x)` and `x = self.dequant(x)`."
msgstr "然后在模型定义的`forward`方法开始和结束时调用`x = self.quant(x)`和`x = self.dequant(x)`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To do a quantization aware training, use the following code snippet:"
msgstr "对量化感知训练，使用以下代码段："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For more detailed examples of the quantization aware training, see `here "
"<https://pytorch.org/docs/master/quantization.html#quantization-aware-"
"training>`_ and `here "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-"
"aware-training>`_."
msgstr ""
"关于量化感知训练的详细示例，可以参见`此处 "
"<https://pytorch.org/docs/master/quantization.html#quantization-aware-"
"training>`_和`此处 "
"<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-"
"aware-training>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A pre-trained quantized model can also be used for quantized aware transfer "
"learning, using the same `quant` and `dequant` calls shown above. See `here "
"<https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-"
"a-custom-classifier-based-on-a-quantized-feature-extractor>`_ for a complete"
" example."
msgstr ""
"预训练的量化模型也可以用于量化感知迁移学习，使用上文显示的相同`quant`和`dequant`调用。参见`此处 "
"<https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-"
"a-custom-classifier-based-on-a-quantized-feature-extractor>`_获取完整示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"After a quantized model is generated using one of the steps above, before "
"the model can be used to run on mobile devices, it needs to be further "
"converted to the `TorchScript` format and then optimized for mobile apps. "
"See the `Script and Optimize for Mobile recipe <script_optimized.html>`_ for"
" details."
msgstr ""
"在使用上述步骤生成量化模型后，在模型能用于移动设备运行之前，需要进一步将其转换为`TorchScript`格式并优化以支持移动应用。详情参见`脚本和优化移动配方"
" <script_optimized.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For more info on the different workflows of quantization, see `here "
"<https://pytorch.org/docs/stable/quantization.html#quantization-workflows>`_"
" and `here <https://pytorch.org/blog/introduction-to-quantization-on-"
"pytorch/#post-training-static-quantization>`_."
msgstr ""
"关于量化不同工作流的更多信息，请参见`此处 "
"<https://pytorch.org/docs/stable/quantization.html#quantization-"
"workflows>`_和`此处 <https://pytorch.org/blog/introduction-to-quantization-on-"
"pytorch/#post-training-static-quantization>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_Captum_Recipe.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`此处 <sphx_glr_download_recipes_recipes_Captum_Recipe.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Interpretability using Captum"
msgstr "使用Captum解释模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Captum helps you understand how the data features impact your model "
"predictions or neuron activations, shedding light on how your model "
"operates."
msgstr "Captum帮助您了解数据特征如何影响您的模型预测或神经元激活，揭示模型运作方式。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Using Captum, you can apply a wide range of state-of-the-art feature "
"attribution algorithms such as \\ ``Guided GradCam``\\  and \\ ``Integrated "
"Gradients``\\  in a unified way."
msgstr ""
"通过Captum，您可以以统一的方式应用多种最先进的特征归因算法，例如 \\ ``Guided GradCam``\\ 和 \\ "
"``Integrated Gradients``\\。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In this recipe you will learn how to use Captum to:"
msgstr "在本配方中，您将学习如何使用Captum："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Attribute the predictions of an image classifier to their corresponding "
"image features."
msgstr "将图像分类器的预测归因到相应的图像特征。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Visualize the attribution results."
msgstr "可视化归因结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Before you begin"
msgstr "在您开始之前"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Make sure Captum is installed in your active Python environment. Captum is "
"available both on GitHub, as a ``pip`` package, or as a ``conda`` package. "
"For detailed instructions, consult the installation guide at "
"https://captum.ai/"
msgstr ""
"确保在当前的Python环境中已安装Captum。Captum可以在GitHub上获取，也可以通过``pip``或``conda``安装。详细安装指南请参考https://captum.ai/"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For a model, we use a built-in image classifier in PyTorch. Captum can "
"reveal which parts of a sample image support certain predictions made by the"
" model."
msgstr "对于模型，我们使用PyTorch中的内置图像分类器。Captum可以揭示支持模型做出某些预测的样本图像的哪些部分。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Computing Attribution"
msgstr "计算归因"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Among the top-3 predictions of the models are classes 208 and 283 which "
"correspond to dog and cat."
msgstr "模型的前三个预测类别中包括类别208和283，对应狗和猫。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let us attribute each of these predictions to the corresponding part of the "
"input, using Captum’s \\ ``Occlusion``\\  algorithm."
msgstr "让我们使用Captum的\\ ``Occlusion``\\ 算法将每个预测归因到输入的相关部分。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Besides ``Occlusion``, Captum features many algorithms such as \\ "
"``Integrated Gradients``\\ , \\ ``Deconvolution``\\ , \\ "
"``GuidedBackprop``\\ , \\ ``Guided GradCam``\\ , \\ ``DeepLift``\\ , and \\ "
"``GradientShap``\\ . All of these algorithms are subclasses of "
"``Attribution`` which expects your model as a callable ``forward_func`` upon"
" initialization and has an ``attribute(...)`` method which returns the "
"attribution result in a unified format."
msgstr ""
"除了``Occlusion``，Captum还具有许多算法，例如\\ ``Integrated Gradients``\\ 、\\ "
"``Deconvolution``\\ 、\\ ``GuidedBackprop``\\ 、\\ ``Guided GradCam``\\ 、\\ "
"``DeepLift``\\ 和\\ ``GradientShap``\\ "
"。所有这些算法都是``Attribution``的子类，它在初始化时期望您的模型作为可调用的``forward_func``，并具有``attribute(...)``方法，返回统一格式的归因结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Let us visualize the computed attribution results in case of images."
msgstr "让我们在图像情况下可视化计算出的归因结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Visualizing the Results"
msgstr "结果可视化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Captum’s \\ ``visualization``\\  utility provides out-of-the-box methods to "
"visualize attribution results both for pictorial and for textual inputs."
msgstr "Captum的\\ ``visualization``\\ 实用程序提供了开箱即用的方法，可视化图像和文本输入的归因结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If your data is textual, ``visualization.visualize_text()`` offers a "
"dedicated view to explore attribution on top of the input text. Find out "
"more at http://captum.ai/tutorials/IMDB_TorchText_Interpret"
msgstr ""
"如果您的数据是文本类型，``visualization.visualize_text()``提供了一个专门的视图，用于基于输入文本探索归因。详情请访问http://captum.ai/tutorials/IMDB_TorchText_Interpret"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Final Notes"
msgstr "最后说明"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Captum can handle most model types in PyTorch across modalities including "
"vision, text, and more. With Captum you can: \\* Attribute a specific output"
" to the model input as illustrated above. \\* Attribute a specific output to"
" a hidden-layer neuron (see Captum API reference). \\* Attribute a hidden-"
"layer neuron response to the model input (see Captum API reference)."
msgstr ""
"Captum可以处理PyTorch中大多数模型类型，包括视觉、文本等多种模式。使用Captum，您可以：\\* "
"如上所述，将特定输出归因到模型输入。\\* 将特定输出归因到隐藏层神经元（请参阅Captum API参考）。\\* "
"将隐藏层神经元的响应归因到模型输入（请参阅Captum API参考）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For complete API of the supported methods and a list of tutorials, consult "
"our website http://captum.ai"
msgstr "有关支持方法的完整API以及教程列表，请访问我们的网站http://captum.ai"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Another useful post by Gilbert Tanner: "
"https://gilberttanner.com/blog/interpreting-pytorch-models-with-captum"
msgstr ""
"Gilbert Tanner的另一个有用的帖子：https://gilberttanner.com/blog/interpreting-pytorch-"
"models-with-captum"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: Captum_Recipe.py <Captum_Recipe.py>`"
msgstr ":download:`下载Python源码: Captum_Recipe.py <Captum_Recipe.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: Captum_Recipe.ipynb "
"<Captum_Recipe.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: Captum_Recipe.ipynb <Captum_Recipe.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_amp_recipe.py>` to "
"download the full example code"
msgstr "点击:ref:`此处<sphx_glr_download_recipes_recipes_amp_recipe.py>`下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Automatic Mixed Precision"
msgstr "自动混合精度"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author**: `Michael Carilli <https://github.com/mcarilli>`_"
msgstr "**作者**: `Michael Carilli <https://github.com/mcarilli>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.cuda.amp <https://pytorch.org/docs/stable/amp.html>`_ provides "
"convenience methods for mixed precision, where some operations use the "
"``torch.float32`` (``float``) datatype and other operations use "
"``torch.float16`` (``half``). Some ops, like linear layers and convolutions,"
" are much faster in ``float16`` or ``bfloat16``. Other ops, like reductions,"
" often require the dynamic range of ``float32``.  Mixed precision tries to "
"match each op to its appropriate datatype, which can reduce your network's "
"runtime and memory footprint."
msgstr ""
"`torch.cuda.amp <https://pytorch.org/docs/stable/amp.html>`_ "
"提供了便捷方法用于混合精度，其中某些操作使用``torch.float32``（``float``）数据类型，而其他操作使用``torch.float16``（``half``）数据类型。一些操作，例如线性层和卷积层，在``float16``或``bfloat16``中运行速度更快。而其他操作，例如归约操作，通常需要``float32``的动态范围。混合精度尝试将每个操作匹配到其适当的数据类型，这可以减少网络的运行时间和内存占用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Ordinarily, \"automatic mixed precision training\" uses `torch.autocast "
"<https://pytorch.org/docs/stable/amp.html#torch.autocast>`_ and "
"`torch.cuda.amp.GradScaler "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler>`_ "
"together."
msgstr ""
"通常，“自动混合精度训练”一起使用`torch.autocast "
"<https://pytorch.org/docs/stable/amp.html#torch.autocast>`_和`torch.cuda.amp.GradScaler"
" <https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe measures the performance of a simple network in default "
"precision, then walks through adding ``autocast`` and ``GradScaler`` to run "
"the same network in mixed precision with improved performance."
msgstr ""
"此配方测量了简单网络在默认精度下的性能，然后逐步添加``autocast``和``GradScaler``以在改进性能的混合精度下运行相同的网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You may download and run this recipe as a standalone Python script. The only"
" requirements are PyTorch 1.6 or later and a CUDA-capable GPU."
msgstr "您可以下载并将此配方作为独立的Python脚本运行。唯一的要求是PyTorch 1.6或更高版本以及支持CUDA的GPU。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Mixed precision primarily benefits Tensor Core-enabled architectures (Volta,"
" Turing, Ampere). This recipe should show significant (2-3X) speedup on "
"those architectures. On earlier architectures (Kepler, Maxwell, Pascal), you"
" may observe a modest speedup. Run ``nvidia-smi`` to display your GPU's "
"architecture."
msgstr ""
"混合精度主要受益于启用了Tensor "
"Core的架构（Volta、Turing、Ampere）。此配方在这些架构上应显示显著（2-3倍）的速度提升。在较早的架构（Kepler、Maxwell、Pascal）上，您可能观察到温和的速度提升。运行``nvidia-"
"smi``以显示您的GPU架构。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "A simple network"
msgstr "一个简单的网络"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The following sequence of linear layers and ReLUs should show a speedup with"
" mixed precision."
msgstr "以下线性层和ReLU序列在混合精度下应显示速度提升。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``batch_size``, ``in_size``, ``out_size``, and ``num_layers`` are chosen to "
"be large enough to saturate the GPU with work. Typically, mixed precision "
"provides the greatest speedup when the GPU is saturated. Small networks may "
"be CPU bound, in which case mixed precision won't improve performance. Sizes"
" are also chosen such that linear layers' participating dimensions are "
"multiples of 8, to permit Tensor Core usage on Tensor Core-capable GPUs (see"
" :ref:`Troubleshooting<troubleshooting>` below)."
msgstr ""
"``batch_size``、``in_size``、``out_size``和``num_layers``的选择足够大以让GPU饱和工作。通常，混合精度在GPU饱和时提供最大的速度提升。小型网络可能受CPU约束，在这种情况下混合精度无法提高性能。尺寸还选择了线性层的参与维度是8的倍数，以允许Tensor"
" Core在支持Tensor Core的GPU上使用（请参阅:ref:`故障排除<troubleshooting>`）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Exercise: Vary participating sizes and see how the mixed precision speedup "
"changes."
msgstr "练习：改变参与尺寸，看看混合精度速度提升的变化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Default Precision"
msgstr "默认精度"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Without ``torch.cuda.amp``, the following simple network executes all ops in"
" default precision (``torch.float32``):"
msgstr "没有``torch.cuda.amp``时，以下简单网络在默认精度（``torch.float32``）下执行所有操作："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding ``torch.autocast``"
msgstr "添加``torch.autocast``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Instances of `torch.autocast "
"<https://pytorch.org/docs/stable/amp.html#autocasting>`_ serve as context "
"managers that allow regions of your script to run in mixed precision."
msgstr ""
"实例`torch.autocast "
"<https://pytorch.org/docs/stable/amp.html#autocasting>`_作为上下文管理器，允许脚本区域以混合精度运行。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In these regions, CUDA ops run in a ``dtype`` chosen by ``autocast`` to "
"improve performance while maintaining accuracy. See the `Autocast Op "
"Reference <https://pytorch.org/docs/stable/amp.html#autocast-op-reference>`_"
" for details on what precision ``autocast`` chooses for each op, and under "
"what circumstances."
msgstr ""
"在这些区域中，CUDA操作在由``autocast``选择的``dtype``中运行，以提高性能并保持准确性。有关每个操作选择的精度详情以及在何种情况下，请参阅`Autocast操作参考"
" <https://pytorch.org/docs/stable/amp.html#autocast-op-reference>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding ``GradScaler``"
msgstr "添加``GradScaler``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Gradient scaling <https://pytorch.org/docs/stable/amp.html#gradient-"
"scaling>`_ helps prevent gradients with small magnitudes from flushing to "
"zero (\"underflowing\") when training with mixed precision."
msgstr ""
"`梯度缩放 <https://pytorch.org/docs/stable/amp.html#gradient-"
"scaling>`_帮助防止梯度在混合精度训练中因幅度小而归零（“下溢”）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.cuda.amp.GradScaler "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler>`_ "
"performs the steps of gradient scaling conveniently."
msgstr ""
"`torch.cuda.amp.GradScaler "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler>`_方便地执行梯度缩放步骤。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "All together: \"Automatic Mixed Precision\""
msgstr "汇总：“自动混合精度”"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"(The following also demonstrates ``enabled``, an optional convenience "
"argument to ``autocast`` and ``GradScaler``. If False, ``autocast`` and "
"``GradScaler``\\ 's calls become no-ops. This allows switching between "
"default precision and mixed precision without if/else statements.)"
msgstr ""
"(以下还演示了``enabled``，这是``autocast``和``GradScaler``\\ "
"的一个可选便捷参数。如果为False，``autocast``和``GradScaler``的调用将变为空操作。这允许在默认精度和混合精度之间切换，而不需要if/else语句。)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inspecting/modifying gradients (e.g., clipping)"
msgstr "检查/修改梯度（例如裁剪）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"All gradients produced by ``scaler.scale(loss).backward()`` are scaled.  If "
"you wish to modify or inspect the parameters' ``.grad`` attributes between "
"``backward()`` and ``scaler.step(optimizer)``, you should unscale them first"
" using `scaler.unscale_(optimizer) "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.unscale_>`_."
msgstr ""
"所有由``scaler.scale(loss).backward()``产生的梯度都被缩放。如果您希望在``backward()``和``scaler.step(optimizer)``之间修改或检查参数的``.grad``属性，您应该先使用`scaler.unscale_(optimizer)"
" "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.unscale_>`_取消缩放它们。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Saving/Resuming"
msgstr "保存/恢复"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To save/resume Amp-enabled runs with bitwise accuracy, use "
"`scaler.state_dict "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.state_dict>`_"
" and `scaler.load_state_dict "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.load_state_dict>`_."
msgstr ""
"要以位级精度保存/恢复支持Amp的运行，请使用`scaler.state_dict "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.state_dict>`_和`scaler.load_state_dict"
" "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler.load_state_dict>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When saving, save the ``scaler`` state dict alongside the usual model and "
"optimizer state ``dicts``. Do this either at the beginning of an iteration "
"before any forward passes, or at the end of an iteration after "
"``scaler.update()``."
msgstr ""
"保存时，将``scaler``状态字典与通常的模型和优化器状态``dicts``一起保存。在迭代开始时的任意位置或在``scaler.update()``之后的迭代结束时执行此操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When resuming, load the ``scaler`` state dict alongside the model and "
"optimizer state ``dicts``. Read checkpoint as desired, for example:"
msgstr "恢复时，将``scaler``状态字典与模型和优化器状态``dicts``一起加载。按需读取检查点，例如："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If a checkpoint was created from a run *without* Amp, and you want to resume"
" training *with* Amp, load model and optimizer states from the checkpoint as"
" usual.  The checkpoint won't contain a saved ``scaler`` state, so use a "
"fresh instance of ``GradScaler``."
msgstr ""
"如果一个检查点是从不带Amp的运行中创建的，并且您希望在带Amp的情况下恢复训练，请像往常一样从检查点加载模型和优化器状态。检查点不会包含保存的``scaler``状态，因此请使用一个新的``GradScaler``实例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If a checkpoint was created from a run *with* Amp and you want to resume "
"training *without* ``Amp``, load model and optimizer states from the "
"checkpoint as usual, and ignore the saved ``scaler`` state."
msgstr ""
"如果一个检查点是从带Amp的运行中创建的，而您希望在不带``Amp``的情况下恢复训练，请像往常一样从检查点加载模型和优化器状态，并忽略保存的``scaler``状态。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inference/Evaluation"
msgstr "推理/评估"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``autocast`` may be used by itself to wrap inference or evaluation forward "
"passes. ``GradScaler`` is not necessary."
msgstr "``autocast``可以单独用于包装推理或评估的前向过程。不需要``GradScaler``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Advanced topics"
msgstr "高级主题"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"See the `Automatic Mixed Precision Examples "
"<https://pytorch.org/docs/stable/notes/amp_examples.html>`_ for advanced use"
" cases including:"
msgstr ""
"有关高级用例，请参阅`自动混合精度示例 "
"<https://pytorch.org/docs/stable/notes/amp_examples.html>`_，包括："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Gradient accumulation"
msgstr "梯度累积"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Gradient penalty/double backward"
msgstr "梯度惩罚/双向传播"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Networks with multiple models, optimizers, or losses"
msgstr "包含多个模型、优化器或损失的网络"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Multiple GPUs (``torch.nn.DataParallel`` or "
"``torch.nn.parallel.DistributedDataParallel``)"
msgstr ""
"多GPU（``torch.nn.DataParallel``或``torch.nn.parallel.DistributedDataParallel``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Custom autograd functions (subclasses of ``torch.autograd.Function``)"
msgstr "自定义自动求导函数（``torch.autograd.Function``的子类）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you perform multiple convergence runs in the same script, each run should"
" use a dedicated fresh ``GradScaler`` instance. ``GradScaler`` instances are"
" lightweight."
msgstr ""
"如果您在同一脚本中执行多个收敛运行，每次运行应使用一个专门的新的``GradScaler``实例。``GradScaler``实例是轻量级的。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you're registering a custom C++ op with the dispatcher, see the `autocast"
" section <https://pytorch.org/tutorials/advanced/dispatcher.html#autocast>`_"
" of the dispatcher tutorial."
msgstr ""
"如果您正在使用调度器注册自定义C++操作，请参阅调度器教程的`autocast部分 "
"<https://pytorch.org/tutorials/advanced/dispatcher.html#autocast>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Troubleshooting"
msgstr "故障排除"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Speedup with Amp is minor"
msgstr "使用Amp的速度提升较小"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Your network may fail to saturate the GPU(s) with work, and is therefore CPU"
" bound. Amp's effect on GPU performance won't matter."
msgstr "您的网络可能未能通过工作量饱和GPU，因此受到CPU约束。Amp对GPU性能的影响将不会显现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A rough rule of thumb to saturate the GPU is to increase batch and/or "
"network size(s) as much as you can without running OOM."
msgstr "一个粗略的经验法则是尽可能增加批量大小和/或网络尺寸，而不会发生OOM。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Try to avoid excessive CPU-GPU synchronization (``.item()`` calls, or "
"printing values from CUDA tensors)."
msgstr "尽量避免过多的CPU-GPU同步（例如``.item()``调用或从CUDA张量打印值）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Try to avoid sequences of many small CUDA ops (coalesce these into a few "
"large CUDA ops if you can)."
msgstr "尽量避免许多小型CUDA操作的序列（如果可以，将它们合并为少量大型CUDA操作）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Your network may be GPU compute bound (lots of ``matmuls``/convolutions) but"
" your GPU does not have Tensor Cores. In this case a reduced speedup is "
"expected."
msgstr ""
"您的网络可能是GPU计算约束型（具有大量``matmuls``/卷积操作），但您的GPU没有Tensor Cores。在这种情况下，预期速度提升会减少。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The ``matmul`` dimensions are not Tensor Core-friendly.  Make sure "
"``matmuls`` participating sizes are multiples of 8. (For NLP models with "
"encoders/decoders, this can be subtle.  Also, convolutions used to have "
"similar size constraints for Tensor Core use, but for CuDNN versions 7.3 and"
" later, no such constraints exist.  See `here "
"<https://github.com/NVIDIA/apex/issues/221#issuecomment-478084841>`_ for "
"guidance.)"
msgstr ""
"``matmul``维度不适合Tensor "
"Core。确保``matmuls``的参与尺寸是8的倍数。（对于带有编码器/解码器的NLP模型，这可能会很微妙。此外，卷积曾经对Tensor "
"Core使用有类似的尺寸约束，但对于CuDNN版本7.3及以上，没有这样的约束。参见`此处 "
"<https://github.com/NVIDIA/apex/issues/221#issuecomment-478084841>`_以获取指导。）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Loss is inf/NaN"
msgstr "损失是inf/NaN"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"First, check if your network fits an :ref:`advanced use case<advanced-"
"topics>`. See also `Prefer binary_cross_entropy_with_logits over "
"binary_cross_entropy <https://pytorch.org/docs/stable/amp.html#prefer-"
"binary-cross-entropy-with-logits-over-binary-cross-entropy>`_."
msgstr ""
"首先，检查您的网络是否符合:ref:`高级用例<advanced-"
"topics>`。另见`推荐使用binary_cross_entropy_with_logits而不是binary_cross_entropy "
"<https://pytorch.org/docs/stable/amp.html#prefer-binary-cross-entropy-with-"
"logits-over-binary-cross-entropy>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you're confident your Amp usage is correct, you may need to file an "
"issue, but before doing so, it's helpful to gather the following "
"information:"
msgstr "如果您确信您的Amp使用是正确的，可能需要提交一个问题，但在提交之前，收集以下信息会很有帮助："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Disable ``autocast`` or ``GradScaler`` individually (by passing "
"``enabled=False`` to their constructor) and see if ``infs``/``NaNs`` "
"persist."
msgstr ""
"分别禁用``autocast``或``GradScaler``（通过将``enabled=False``传递给它们的构造函数），看看``infs``/``NaNs``是否仍然存在。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you suspect part of your network (e.g., a complicated loss function) "
"overflows , run that forward region in ``float32`` and see if "
"``infs``/``NaN``s persist. `The autocast docstring "
"<https://pytorch.org/docs/stable/amp.html#torch.autocast>`_'s last code "
"snippet shows forcing a subregion to run in ``float32`` (by locally "
"disabling ``autocast`` and casting the subregion's inputs)."
msgstr ""
"如果您怀疑网络的一部分（例如，一个复杂的损失函数）溢出，请使用``float32``运行该区域的前向传播，看看是否仍然存在``inf``或``NaN``。`autocast文档字符串"
" "
"<https://pytorch.org/docs/stable/amp.html#torch.autocast>`_中的最后一个代码片段显示了如何强制一个子区域以``float32``运行（通过局部禁用``autocast``并对该子区域的输入进行类型转换）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Type mismatch error (may manifest as ``CUDNN_STATUS_BAD_PARAM``)"
msgstr "类型不匹配错误（可能表现为``CUDNN_STATUS_BAD_PARAM``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``Autocast`` tries to cover all ops that benefit from or require casting. "
"`Ops that receive explicit coverage "
"<https://pytorch.org/docs/stable/amp.html#autocast-op-reference>`_ are "
"chosen based on numerical properties, but also on experience. If you see a "
"type mismatch error in an ``autocast`` enabled forward region or a backward "
"pass following that region, it's possible ``autocast`` missed an op."
msgstr ""
"``Autocast``试图覆盖所有有益于或需要类型转换的操作。`获得显式覆盖的操作 "
"<https://pytorch.org/docs/stable/amp.html#autocast-op-reference>`_ "
"是根据数值特性以及经验选择的。如果您在启用了``autocast``的前向区域中或在随后区域的反向传播中看到类型不匹配错误，可能是``autocast``遗漏了某个操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Please file an issue with the error backtrace.  ``export "
"TORCH_SHOW_CPP_STACKTRACES=1`` before running your script to provide fine-"
"grained information on which backend op is failing."
msgstr ""
"请通过错误回溯报告问题。在运行脚本之前执行``export "
"TORCH_SHOW_CPP_STACKTRACES=1``以提供详细信息，说明哪个后端操作失败。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":download:`Download Python source code: amp_recipe.py <amp_recipe.py>`"
msgstr ":download:`下载Python源码: amp_recipe.py <amp_recipe.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: amp_recipe.ipynb <amp_recipe.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: amp_recipe.ipynb <amp_recipe.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_benchmark.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_recipes_recipes_benchmark.py>` 下载完整的示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch Benchmark"
msgstr "PyTorch基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe provides a quick-start guide to using PyTorch ``benchmark`` "
"module to measure and compare code performance."
msgstr "本教程提供了使用PyTorch``benchmark``模块测量和比较代码性能的快速入门指南。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Benchmarking is an important step in writing code. It helps us validate that"
" our code meets performance expectations, compare different approaches to "
"solving the same problem and prevent performance regressions."
msgstr "基准测试是编写代码的重要步骤。它可以帮助我们验证代码是否符合性能期望，比较解决同一问题的不同方法并防止性能下降。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are many options when it comes to benchmarking PyTorch code including "
"the Python builtin ``timeit`` module. However, benchmarking PyTorch code has"
" many caveats that can be easily overlooked such as managing the number of "
"threads and synchronizing CUDA devices. Moreover, generating Tensor inputs "
"for benchmarking can be quite tedious."
msgstr ""
"在进行PyTorch代码基准测试时有许多选择，包括Python内置的``timeit``模块。然而，PyTorch代码的基准测试有许多容易被忽略的注意事项，例如管理线程数和同步CUDA设备。此外，为基准测试生成Tensor输入可能相当繁琐。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe demonstrates how to use PyTorch ``benchmark`` module to avoid "
"common mistakes while making it easier to compare performance of different "
"code, generate input for benchmarking and more."
msgstr ""
"本教程演示了如何使用PyTorch``benchmark``模块避免常见错误，同时使比较不同代码性能、生成基准测试输入和其他操作变得更加容易。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Setup"
msgstr "环境设置"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Before we begin, install ``torch`` if it isn’t already available."
msgstr "在开始之前，请安装``torch``，如果还没有安装。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Defining functions to benchmark"
msgstr "定义基准测试的函数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Benchmarking with ``timeit.Timer``"
msgstr "使用``timeit.Timer``进行基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Benchmarking with ``torch.utils.benchmark.Timer``"
msgstr "使用``torch.utils.benchmark.Timer``进行基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Benchmarking with ``Blocked Autorange``"
msgstr "使用``Blocked Autorange``进行基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Comparing benchmark results"
msgstr "比较基准测试结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Saving/Loading benchmark results"
msgstr "保存/加载基准测试结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Generating inputs with ``Fuzzed Parameters``"
msgstr "使用``Fuzzed Parameters``生成输入"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Collecting instruction counts with ``Callgrind``"
msgstr "使用``Callgrind``收集指令计数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1. Defining functions to benchmark"
msgstr "1. 定义基准测试的函数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As of the time of this writing, `torch.dot "
"<https://pytorch.org/docs/stable/generated/torch.dot.html?highlight=dot#torch.dot>`__"
" does not support batched mode, so we will compare two approaches to "
"implementing it using existing ``torch`` operators: one approach uses a "
"combination of ``mul`` and ``sum`` while the other reduces the problem to "
"``bmm``."
msgstr ""
"截至本教程撰写时，`torch.dot "
"<https://pytorch.org/docs/stable/generated/torch.dot.html?highlight=dot#torch.dot>`__"
" "
"不支持批处理模式，所以我们将比较两种使用现有``torch``操作实现的解决方案：一种使用``mul``和``sum``的组合，而另一种将问题简化为``bmm``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Benchmarking with ``timeit.Timer``"
msgstr "2. 使用``timeit.Timer``进行基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"First, let's benchmark the code using Python's builtin ``timeit`` module. We"
" keep the benchmark code simple here so we can compare the defaults of "
"``timeit`` and ``torch.utils.benchmark``."
msgstr ""
"首先，让我们使用Python的内置``timeit``模块对代码进行基准测试。为了与``timeit``和``torch.utils.benchmark``的默认设置进行比较，我们尽量保持基准测试代码的简单性。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Output"
msgstr "输出结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Benchmarking with ``torch.utils.benchmark.Timer``"
msgstr "3. 使用``torch.utils.benchmark.Timer``进行基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch ``benchmark`` module was designed to be familiar to those who have "
"used the ``timeit`` module before. However, its defaults make it easier and "
"safer to use for benchmarking PyTorch code. Let's first compare the same "
"basic API as above."
msgstr ""
"PyTorch``benchmark``模块的设计使其对那些已经使用过``timeit``模块的用户来说很熟悉。然而，它的默认设置使其在针对PyTorch代码进行基准测试时更加安全和易于使用。首先让我们比较与上面相同的基本API。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Even though the APIs are the same for the basic functionality, there are "
"some important differences. ``benchmark.Timer.timeit()`` returns the time "
"per run as opposed to the total runtime like ``timeit.Timer.timeit()`` does."
" PyTorch ``benchmark`` module also provides formatted string representations"
" for printing the results."
msgstr ""
"尽管对于基本功能，API是相同的，但有一些重要差异。``benchmark.Timer.timeit()``返回单次运行的时间，而不像``timeit.Timer.timeit()``那样返回总运行时间。PyTorch``benchmark``模块还提供了格式化字符串表示，以便打印结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Another important difference, and the reason why the results diverge is that"
" PyTorch benchmark module runs in a single thread by default. We can change "
"the number of threads with the ``num_threads`` argument."
msgstr "另一个重要的不同之处是，PyTorch基准测试模块默认情况下运行在单线程中。我们可以通过``num_threads``参数更改线程数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.utils.benchmark.Timer`` takes several additional arguments "
"including: ``label``, ``sub_label``, ``description`` and ``env`` which "
"change the __repr__ of the measurement object returned and are used for "
"grouping the results (more on this later)."
msgstr ""
"``torch.utils.benchmark.Timer``接受几个额外的参数，包括：``label``、``sub_label``、``description``和``env``，这些参数将更改返回的测量对象的__repr__表示，并用于对结果进行分组（稍后会详细介绍）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Running ``benchmark`` with all threads available gives similar results as "
"the ``timeit`` module. More importantly, which version is faster depends on "
"how many threads we run the code with. This is why it's important to "
"benchmark the code with thread settings that are representative of real use "
"cases. Another important thing to remember is to synchronize CPU and CUDA "
"when benchmarking on the GPU. Let's run the above benchmarks again on a CUDA"
" tensor and see what happens."
msgstr ""
"使用所有可用线程运行``benchmark``提供了与``timeit``模块相似的结果。更重要的是，哪个版本更快取决于代码运行时使用的线程数。这就是为什么以代表实际使用场景的线程设置对代码进行基准测试很重要的原因。另一个重要的事情是，在GPU上进行基准测试时同步CPU和CUDA。让我们再次在CUDA张量上运行上面的基准测试，看看会发生什么。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The results reveal something interesting. The first run of the ``bmm`` "
"version using the ``timeit`` module takes much longer than the second run. "
"This is because ``bmm`` calls into `cuBLAS` which needs to be loaded the "
"first time it's called which takes some time. This is why it's important to "
"do a warm-up run before benchmarking, luckily for us, PyTorch's "
"``benchmark`` module takes care of that."
msgstr ""
"结果显示了一些有趣的事情。使用``timeit``模块的``bmm``版本的第一次运行时间大大超过了第二次运行。这是因为``bmm``调用了`cuBLAS`，而`cuBLAS`第一次被调用时需要加载，这需要一些时间。这就是为什么在进行基准测试之前进行预热运行是很重要的，幸运的是，PyTorch的``benchmark``模块为我们处理了这一点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The difference in the results between ``timeit`` and ``benchmark`` modules "
"is because the `timeit` module is not synchronizing CUDA and is thus only "
"timing the time to launch the kernel. PyTorch's ``benchmark`` module does "
"the synchronization for us."
msgstr ""
"``timeit``和``benchmark``模块的结果差异是因为``timeit``模块没有同步CUDA，因此只计算了启动内核所需的时间。PyTorch的``benchmark``模块为我们进行了同步操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Benchmarking with `Blocked Autorange`"
msgstr "4. 使用`Blocked Autorange`进行基准测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"While ``timeit.Timer.autorange`` takes a single continuous measurement of at"
" least 0.2 seconds, `torch.utils.benchmark.blocked_autorange` takes many "
"measurements whose times total at least 0.2 seconds (which can be changed by"
" the `min_run_time` parameter) subject to the constraint that timing "
"overhead is a small fraction of the overall measurement. This is "
"accomplished by first running with an increasing number of runs per loop "
"until the runtime is much larger than measurement overhead (which also "
"serves as a warm up), and then taking measurements until the target time is "
"reached. This has the useful properties that it wastes less data and allows "
"us to compute statistics to estimate the reliability of the measurements."
msgstr ""
"虽然``timeit.Timer.autorange``进行单个持续至少0.2秒的测量，`torch.utils.benchmark.blocked_autorange`进行多个测量，总时间至少为0.2秒（可以通过`min_run_time`参数更改），并且时间测量的开销是总体测量的一小部分。实现此功能的方法是先以递增的每次循环运行次数使运行时间远大于测量开销（这也起到了预热的作用），然后继续测量直到达到目标时间。这具有有用的属性：它减少了无意义的数据，同时使我们可以计算统计数据以估计测量的可靠性。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We can also inspect the individual statistics from the returned measurements"
" object."
msgstr "我们还可以从返回的测量对象中检查单个统计数据。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5. Comparing benchmark results"
msgstr "5. 比较基准测试结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"So far we've been comparing our two versions of batched dot against a single"
" input. In practice, we want to try a combination of inputs as well as "
"different number of threads. The ``Compare`` class helps display the results"
" of many measurements in a formatted table. It uses the annotations "
"described above (`label`, `sub_label`, `num_threads`, etc.) as well as "
"`description` to group and organize the table. Let's use ``Compare`` to see "
"how our functions perform for different input sizes and number of threads."
msgstr ""
"到目前为止，我们对两种批处理点积版本仅使用单个输入进行了比较。在实际中，我们希望尝试输入的组合以及不同的线程数。``Compare``类帮助以格式化的表格显示多个测量结果。它使用前面描述的注释（`label`、`sub_label`、`num_threads`等）以及`description`来分组和组织表格。让我们使用``Compare``来查看我们的函数在不同输入大小和线程数情况下的性能表现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The results above indicate that the version which reduces to ``bmm`` is "
"better for larger tensors running on multiple threads, while for smaller "
"and/or single thread code, the other version is better."
msgstr "上面的结果表明，对于运行在多线程的大张量，简化为``bmm``的版本更好，而对于小型和/或单线程代码，另一种版本更优。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``Compare`` also provides functions for changing the table format"
msgstr "``Compare``还提供了更改表格格式的功能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6. Saving/Loading benchmark results"
msgstr "6. 保存/加载基准测试结果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Measurements` (and ``CallgrindStats`` which are described in section 8) can"
" be serialized by the ``pickle`` module. This makes A/B testing easy, as you"
" can collect measurements from two separate environments, pickle them, and "
"then load both in a single environment. Timer even takes an `env` "
"constructor argument so that such A/B testing works seamlessly."
msgstr ""
"`Measurements`（以及第8部分中描述的``CallgrindStats``）可以通过``pickle``模块进行序列化。这使A/B测试变得简单，因为您可以从两个独立环境中收集测量值，对其进行序列化，然后在单个环境中加载两个序列化的结果。计时器甚至接受一个`env`构造函数参数，以便这种A/B测试可以无缝运行。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let's imagine that rather than two Python functions, the add/sum and ``bmm``"
" approaches were in two different builds of PyTorch. The example below "
"demonstrates how one might A/B test them. For simplicity, we only use a "
"subset of shapes, and simply round trip results through pickle rather than "
"actually using multiple environments and writing results to disk."
msgstr ""
"假设不是两个Python函数，add/sum方法和``bmm``方法在PyTorch的两个不同构建中实现。以下示例演示了如何对它们进行A/B测试。为了简单，我们仅使用少量形状，并通过pickle进行结果的简单往返，而不实际使用多个环境或将结果写入磁盘。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7. Generating inputs with `Fuzzed Parameters`"
msgstr "7. 使用`Fuzzed Parameters`生成输入"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As we've seen in the previous section, there can be some stark performance "
"differences depending on the input tensors. Hence, it is a good idea to run "
"benchmarks on a number of different inputs. However, creating all these "
"input tensors can be tedious which is where ``torch.utils.benchmark.Fuzzer``"
" and related classes come in. Let's take a look at how we can use the "
"``Fuzzer`` to create some test cases for the benchmark."
msgstr ""
"如上节所见，根据输入张量可能存在显著的性能差异。因此，在多个不同输入上运行基准测试是个好主意。但是，创建所有这些输入张量可能会很繁琐，这就是``torch.utils.benchmark.Fuzzer``和相关类派上用场的地方。让我们看看如何使用``Fuzzer``为基准测试创建一些测试用例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There is a lot of flexibility for defining your own ``fuzzers`` which is "
"great for creating a powerful set of inputs to benchmark. But to make things"
" even simpler, PyTorch benchmark module comes with some built-in ``fuzzers``"
" for common benchmarking needs. Let's take a look at how we can use one of "
"these built-in ``fuzzers``."
msgstr ""
"定义您自己的``fuzzers``具有很大的灵活性，这对于创建强大的基准测试输入集非常有用。然而，为了更加简单，PyTorch基准测试模块提供了一些内置的``fuzzers``以满足常见的基准测试需求。让我们看看如何使用其中一个内置的``fuzzers``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "8. Collecting instruction counts with ``Callgrind``"
msgstr "8. 使用``Callgrind``收集指令计数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One of the challenges of optimizing code is the variation and opacity of "
"wall time. There are many sources of non-determinism, from adaptive clock "
"speeds to resource contention with other processes. Furthermore, end-to-end "
"time gives no insight into where time is being spent, which is really what "
"we're interested in when optimizing code."
msgstr ""
"优化代码的一个挑战是墙时的变化和不透明性。存在许多非确定性来源，从自适应时钟速度到与其他进程的资源争用。此外，端到端时间不能提供关于时间花费位置的洞察，而这正是真正优化代码时关注的内容。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A complementary approach is to also collect instruction counts. These counts"
" are a proxy metric and do not capture all aspects of performance (e.g. "
"memory or I/O bound tasks), however they do have several useful properties. "
"Instruction counts are reproducible, insensitive to environmental variation,"
" and offer fine grained insight into where a program is spending cycles."
msgstr ""
"一种补充方法是收集指令计数。这些计数是一种代理指标，不能捕获性能的所有方面（例如内存或I/O绑定任务），但它们具有几个有用的特性。指令计数可重现，对环境变化不敏感，并提供关于程序消耗周期位置的细粒度洞察。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To see the utility of instruction counts, let us look at how we might reduce"
" the overhead of `batched_dot_mul_sum`. The obvious solution is to move it "
"to C++, so we avoid going between Python and C++ multiple times."
msgstr ""
"为了展示指令计数的实用性，让我们看看如何减少`batched_dot_mul_sum`的开销。显而易见的解决方案是将其移动到C++中，从而避免在Python和C++之间多次切换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Fortunately, the source is nearly identical. One question that we have to "
"ask in C++ is whether we should take arguments by value or reference."
msgstr "幸运的是，代码几乎相同。在C++中，我们需要考虑是否应该按值或按引用传递参数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Take a look at these other recipes to continue your learning:"
msgstr "查看这些其他教程以继续学习："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`PyTorch Profiler "
"<https://pytorch.org/tutorials/recipes/recipes/profiler.html>`_"
msgstr ""
"`PyTorch性能分析工具 "
"<https://pytorch.org/tutorials/recipes/recipes/profiler.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":download:`Download Python source code: benchmark.py <benchmark.py>`"
msgstr ":download:`下载Python源码: benchmark.py <benchmark.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: benchmark.ipynb <benchmark.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: benchmark.ipynb <benchmark.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_changing_default_device.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_recipes_recipes_changing_default_device.py>` "
"下载完整的示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Changing default device"
msgstr "更改默认设备"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"It is common practice to write PyTorch code in a device-agnostic way, and "
"then switch between CPU and CUDA depending on what hardware is available. "
"Typically, to do this you might have used if-statements and ``cuda()`` calls"
" to do this:"
msgstr ""
"以设备无关的方式编写PyTorch代码是常见做法，然后根据硬件的可用性在CPU和CUDA之间切换。通常，您可能会使用if语句和``cuda()``调用来实现这一点："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This recipe requires PyTorch 2.0.0 or later."
msgstr "本教程需要PyTorch 2.0.0或更高版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch now also has a context manager which can take care of the device "
"transfer automatically. Here is an example:"
msgstr "PyTorch现在也提供了一个上下文管理器，可以自动处理设备转移。这是一个示例："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You can also set it globally like this:"
msgstr "您还可以像这样全局设置："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This function imposes a slight performance cost on every Python call to the "
"torch API (not just factory functions). If this is causing problems for you,"
" please comment on `this issue "
"<https://github.com/pytorch/pytorch/issues/92701>`__"
msgstr ""
"此功能会对每次调用torch API（不仅仅是工厂函数）带来轻微的性能开销。如果这对您造成问题，请在`此问题 "
"<https://github.com/pytorch/pytorch/issues/92701>`__下发表评论。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: changing_default_device.py "
"<changing_default_device.py>`"
msgstr ""
":download:`下载 Python 源代码: changing_default_device.py "
"<changing_default_device.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: changing_default_device.ipynb "
"<changing_default_device.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: changing_default_device.ipynb "
"<changing_default_device.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_defining_a_neural_network.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_recipes_recipes_defining_a_neural_network.py>` 下载完整示例代码。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Defining a Neural Network in PyTorch"
msgstr "在 PyTorch 中定义神经网络"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Deep learning uses artificial neural networks (models), which are computing "
"systems that are composed of many layers of interconnected units. By passing"
" data through these interconnected units, a neural network is able to learn "
"how to approximate the computations required to transform inputs into "
"outputs. In PyTorch, neural networks can be constructed using the "
"``torch.nn`` package."
msgstr ""
"深度学习使用人工神经网络（模型），它是由许多层相互连接的单元组成的计算系统。通过将数据从这些互联的单元传递，神经网络会学习如何近似地完成将输入转换为输出所需的计算。在"
" PyTorch 中，可以使用``torch.nn``包构造神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch provides the elegantly designed modules and classes, including "
"``torch.nn``, to help you create and train neural networks. An ``nn.Module``"
" contains layers, and a method ``forward(input)`` that returns the "
"``output``."
msgstr ""
"PyTorch提供了优雅设计的模块和类，包括``torch.nn``，可帮助您创建和训练神经网络。``nn.Module``包含层，以及返回``output``的方法``forward(input)``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we will use ``torch.nn`` to define a neural network intended"
" for the `MNIST dataset "
"<hhttps://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST>`__."
msgstr ""
"在本教程中，我们将使用``torch.nn``定义一个针对`MNIST数据集 "
"<hhttps://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST>`__的神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Before we begin, we need to install ``torch`` if it isn’t already available."
msgstr "在开始之前，我们需要安装``torch``，如果尚未安装。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Import all necessary libraries for loading our data"
msgstr "导入加载数据所需的所有库。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Define and initialize the neural network"
msgstr "定义并初始化神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Specify how data will pass through your model"
msgstr "指定数据如何通过您的模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "[Optional] Pass data through your model to test"
msgstr "[可选] 将数据传递给您的模型进行测试。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1. Import necessary libraries for loading our data"
msgstr "1. 导入加载数据所需的库。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For this recipe, we will use ``torch`` and its subsidiaries ``torch.nn`` and"
" ``torch.nn.functional``."
msgstr "在本教程中，我们将使用``torch``及其子模块``torch.nn``和``torch.nn.functional``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Define and initialize the neural network"
msgstr "2. 定义并初始化神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Our network will recognize images. We will use a process built into PyTorch "
"called convolution. Convolution adds each element of an image to its local "
"neighbors, weighted by a kernel, or a small matrix, that helps us extract "
"certain features (like edge detection, sharpness, blurriness, etc.) from the"
" input image."
msgstr ""
"我们的网络将识别图像。我们将使用PyTorch中构建的一个叫做卷积的过程。卷积将图像中的每个元素与其局部邻域相加，由一个核或者小矩阵加权，用于从输入图像中提取某些特征（例如边缘检测、锐度、模糊度等）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are two requirements for defining the ``Net`` class of your model. The"
" first is writing an __init__ function that references ``nn.Module``. This "
"function is where you define the fully connected layers in your neural "
"network."
msgstr ""
"定义模型的``Net``类有两个要求。第一是编写一个引用``nn.Module``的__init__函数。这个函数是您在神经网络中定义完全连接层的地方。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Using convolution, we will define our model to take 1 input image channel, "
"and output match our target of 10 labels representing numbers 0 through 9. "
"This algorithm is yours to create, we will follow a standard MNIST "
"algorithm."
msgstr ""
"使用卷积，我们将定义模型来接受1个输入图像通道，并输出与目标匹配的10个标签，分别表示数字0到9。该算法由您创造，我们将遵循一个标准的MNIST算法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We have finished defining our neural network, now we have to define how our "
"data will pass through it."
msgstr "我们已经完成了神经网络的定义，现在需要定义数据如何通过它。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Specify how data will pass through your model"
msgstr "3. 指定数据如何通过您的模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When you use PyTorch to build a model, you just have to define the "
"``forward`` function, that will pass the data into the computation graph "
"(i.e. our neural network). This will represent our feed-forward algorithm."
msgstr "使用PyTorch构建模型时，您只需定义``forward``函数，该函数将数据传递到计算图（即神经网络）。这将表示我们的前馈算法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You can use any of the Tensor operations in the ``forward`` function."
msgstr "您可以在``forward``函数中使用任何张量操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. [Optional] Pass data through your model to test"
msgstr "4. [可选] 将数据传递给您的模型进行测试。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To ensure we receive our desired output, let’s test our model by passing "
"some random data through it."
msgstr "为了确保我们得到所需的输出，让我们通过模型传递一些随机数据进行测试。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Each number in this resulting tensor equates to the prediction of the label "
"the random tensor is associated to."
msgstr "结果张量中的每个数字对应于随机张量关联的标签预测。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Congratulations! You have successfully defined a neural network in PyTorch."
msgstr "恭喜！您已成功在 PyTorch 中定义了一个神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`What is a state_dict in PyTorch "
"<https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html>`__"
msgstr ""
"`PyTorch 中的 state_dict 是什么 "
"<https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Saving and loading models for inference in PyTorch "
"<https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html>`__"
msgstr ""
"`PyTorch 中用于推理的模型保存和加载 "
"<https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: defining_a_neural_network.py "
"<defining_a_neural_network.py>`"
msgstr ""
":download:`下载 Python 源代码: defining_a_neural_network.py "
"<defining_a_neural_network.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: defining_a_neural_network.ipynb "
"<defining_a_neural_network.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: defining_a_neural_network.ipynb "
"<defining_a_neural_network.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_dynamic_quantization.py>` to download the"
" full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_recipes_recipes_dynamic_quantization.py>` "
"下载完整示例代码。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Dynamic Quantization"
msgstr "动态量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe you will see how to take advantage of Dynamic Quantization to"
" accelerate inference on an LSTM-style recurrent neural network. This "
"reduces the size of the model weights and speeds up model execution."
msgstr "在本教程中，您将看到如何利用动态量化加速LSTM样式的循环神经网络推理。这减少了模型权重的大小并加快了模型执行速度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are a number of trade-offs that can be made when designing neural "
"networks. During model development and training you can alter the number of "
"layers and number of parameters in a recurrent neural network and trade-off "
"accuracy against model size and/or model latency or throughput. Such changes"
" can take lot of time and compute resources because you are iterating over "
"the model training. Quantization gives you a way to make a similar trade off"
" between performance and model accuracy with a known model after training is"
" completed."
msgstr ""
"在设计神经网络时可以进行许多权衡。在模型开发和训练期间，您可以改变循环神经网络中的层数和参数数量，并针对模型大小和/或模型延迟或吞吐量进行权衡。这种改变需要大量的时间和计算资源，因为您在模型训练上进行迭代。量化让您能够在训练完成后以已知的模型在性能和模型准确性之间进行类似的权衡。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You can give it a try in a single session and you will certainly reduce your"
" model size significantly and may get a significant latency reduction "
"without losing a lot of accuracy."
msgstr "您可以在一个会话中尝试它，并且肯定会显著减少模型大小，并可能在准确性损失不大的情况下获得显著的延迟减小。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is dynamic quantization?"
msgstr "什么是动态量化？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Quantizing a network means converting it to use a reduced precision integer "
"representation for the weights and/or activations. This saves on model size "
"and allows the use of higher throughput math operations on your CPU or GPU."
msgstr "量化网络意味着将其转换为使用低精度整数表示的权重和/或激活。这节省了模型大小，并允许在CPU或GPU上使用更高吞吐量的数学运算。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When converting from floating point to integer values you are essentially "
"multiplying the floating point value by some scale factor and rounding the "
"result to a whole number. The various quantization approaches differ in the "
"way they approach determining that scale factor."
msgstr "从浮点值转换为整数值时，本质上是将浮点值乘以某种比例因子，并将结果四舍五入为整数。各种量化方法在确定比例因子的方法上有所不同。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The key idea with dynamic quantization as described here is that we are "
"going to determine the scale factor for activations dynamically based on the"
" data range observed at runtime. This ensures that the scale factor is "
"\"tuned\" so that as much signal as possible about each observed dataset is "
"preserved."
msgstr ""
"如这里描述的动态量化的关键思想是，我们将在运行时根据观察到的数据范围动态确定激活的比例因子。这确保了比例因子被“调节”以尽可能保留每个观察到的数据集的信号。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The model parameters on the other hand are known during model conversion and"
" they are converted ahead of time and stored in INT8 form."
msgstr "另一方面，模型参数在模型转换时是已知的，它们会被预先转换并存储为INT8格式。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Arithmetic in the quantized model is done using vectorized INT8 "
"instructions. Accumulation is typically done with INT16 or INT32 to avoid "
"overflow. This higher precision value is scaled back to INT8 if the next "
"layer is quantized or converted to FP32 for output."
msgstr ""
"量化模型中的算术运算使用矢量化的INT8指令完成。累加通常使用INT16或INT32以避免溢出。如果下一层是量化的，则将高精度值缩放回INT8，或转换为FP32作为输出。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Dynamic quantization is relatively free of tuning parameters which makes it "
"well suited to be added into production pipelines as a standard part of "
"converting LSTM models to deployment."
msgstr "动态量化相对不需要调整参数，使其非常适合作为将LSTM模型转换为部署的一部分添加到生产流水线中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Limitations on the approach taken here"
msgstr "对这里使用的方法的限制"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe provides a quick introduction to the dynamic quantization "
"features in PyTorch and the workflow for using it. Our focus is on "
"explaining the specific functions used to convert the model. We will make a "
"number of significant simplifications in the interest of brevity and clarity"
msgstr ""
"此教程简要介绍了PyTorch中的动态量化功能及其使用工作流程。我们专注于解释用于转换模型的特定函数。在追求简洁和清晰的前提下，我们将做出一些重要简化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You will start with a minimal LSTM network"
msgstr "您将从一个最小化的LSTM网络开始。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You are simply going to initialize the network with a random hidden state"
msgstr "您只需使用一个随机隐藏状态初始化网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You are going to test the network with random inputs"
msgstr "您将使用随机输入来测试网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You are not going to train the network in this tutorial"
msgstr "您不会在本教程中训练网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You will see that the quantized form of this network is smaller and runs "
"faster than the floating point network we started with"
msgstr "您将看到此网络的量化形式比我们开始时的浮点网络更小、更快。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You will see that the output values are generally in the same ballpark as "
"the output of the FP32 network, but we are not demonstrating here the "
"expected accuracy loss on a real trained network"
msgstr "您将看到输出值通常与FP32网络的输出处于相同范围内，但我们这里没有演示实际训练网络的预期准确性损失。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You will see how dynamic quantization is done and be able to see suggestive "
"reductions in memory use and latency times. Providing a demonstration that "
"the technique can preserve high levels of model accuracy on a trained LSTM "
"is left to a more advanced tutorial. If you want to move right away to that "
"more rigorous treatment please proceed to the `advanced dynamic quantization"
" tutorial "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`__."
msgstr ""
"您将看到如何进行动态量化，并能够看到内存使用和延迟时间方面显著的减少。为训练的LSTM网络保留高水平模型准确性的技术将在更高级的教程中进行演示。如果您想立即进入更严格的处理，请转到`高级动态量化教程"
" "
"<https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This recipe has 5 steps."
msgstr "本教程分为五个步骤。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Set Up - Here you define a very simple LSTM, import modules, and establish "
"some random input tensors."
msgstr "设置阶段 - 在这里您定义一个非常简单的LSTM，导入模块并建立一些随机输入张量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Do the Quantization - Here you instantiate a floating point model and then "
"create quantized version of it."
msgstr "进行量化 - 在这里您实例化一个浮点模型，然后创建其量化版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Look at Model Size - Here you show that the model size gets smaller."
msgstr "查看模型大小 - 在这里您展示模型大小变得更小。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Look at Latency - Here you run the two models and compare model runtime "
"(latency)."
msgstr "查看延迟 - 在这里您运行两个模型并比较模型运行时间（延迟）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Look at Accuracy - Here you run the two models and compare outputs."
msgstr "查看准确性 - 在这里您运行两个模型并比较输出。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1: Set Up"
msgstr "步骤1：设置"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is a straightforward bit of code to set up for the rest of the recipe."
msgstr "这是为教程其余部分设置的简单代码片段。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The unique module we are importing here is torch.quantization which includes"
" PyTorch's quantized operators and conversion functions. We also define a "
"very simple LSTM model and set up some inputs."
msgstr ""
"我们在这里导入的唯一模块是torch.quantization，其中包括PyTorch的量化操作符和转换函数。我们还定义了一个非常简单的LSTM模型并设置了一些输入。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2: Do the Quantization"
msgstr "步骤2：进行量化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Now we get to the fun part. First we create an instance of the model called "
"``float_lstm`` then we are going to quantize it. We're going to use the "
"`torch.quantization.quantize_dynamic "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`__"
" function, which takes the model, then a list of the submodules which we "
"want to have quantized if they appear, then the datatype we are targeting. "
"This function returns a quantized version of the original model as a new "
"module."
msgstr ""
"现在进入有趣的部分。首先我们创建一个名为``float_lstm``的模型实例，然后对其进行量化。我们将使用`torch.quantization.quantize_dynamic"
" "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`__函数，该函数需要模型，然后是我们希望量化的可能出现子模块的列表，然后是我们目标的数据类型。此函数将原始模型量化为一个新的模块。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "That's all it takes."
msgstr "就是这么简单。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Look at Model Size"
msgstr "步骤3：查看模型大小"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We've quantized the model. What does that get us? Well the first benefit is "
"that we've replaced the FP32 model parameters with INT8 values (and some "
"recorded scale factors). This means about 75% less data to store and move "
"around. With the default values the reduction shown below will be less than "
"75% but if you increase the model size above (for example you can set model "
"dimension to something like 80) this will converge towards 4x smaller as the"
" stored model size dominated more and more by the parameter values."
msgstr ""
"我们已经对模型进行了量化。这给我们带来了什么好处？第一个好处是我们用INT8值（以及一些记录的比例因子）替换了FP32模型参数。这意味着保存和传输的数据减少了约75%。以下显示的减少幅度会小于75%，但如果您将模型大小增加（例如，可以将模型维度设置为80），随着存储的模型大小更多地由参数值主导，这将趋于比原始模型小4倍。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Look at Latency"
msgstr "步骤4：查看延迟"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The second benefit is that the quantized model will typically run faster. "
"This is due to a combinations of effects including at least:"
msgstr "第二个好处是量化模型通常会运行得更快。这是由于以下组合因素的影响："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Less time spent moving parameter data in"
msgstr "单元参数数据移动时间更少。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Faster INT8 operations"
msgstr "INT8操作更快。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As you will see the quantized version of this super-simple network runs "
"faster. This will generally be true of more complex networks but as they say"
" \"your mileage may vary\" depending on a number of factors including the "
"structure of the model and the hardware you are running on."
msgstr ""
"正如您所看到的，这个超级简单网络的量化版本运行得更快。这通常适用于更复杂的网络，但正如人们所说，这可能因诸如模型结构和运行硬件等诸多因素而有所不同。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5: Look at Accuracy"
msgstr "5: 查看准确性"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We are not going to do a careful look at accuracy here because we are "
"working with a randomly initialized network rather than a properly trained "
"one. However, I think it is worth quickly showing that the quantized network"
" does produce output tensors that are \"in the same ballpark\" as the "
"original one."
msgstr ""
"在这里我们不会仔细研究准确性，因为我们正在处理一个随机初始化的网络，而不是一个经过适当训练的网络。不过，我认为值得快速展示一下量化网络确实产生了与原始网络“在同一范围内”的输出张量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For a more detailed analysis please see the more advanced tutorials "
"referenced at the end of this recipe."
msgstr "对于更详细的分析，请参阅本教程结尾提到的更高级教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We've explained what dynamic quantization is, what benefits it brings, and "
"you have used the ``torch.quantization.quantize_dynamic()`` function to "
"quickly quantize a simple LSTM model."
msgstr ""
"我们已经解释了动态量化是什么，带来了什么好处，并使用了``torch.quantization.quantize_dynamic()``函数快速量化了一个简单的LSTM模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This was a fast and high level treatment of this material; for more detail "
"please continue learning with `(beta) Dynamic Quantization on an LSTM Word "
"Language Model Tutorial "
"<https://pytorch.org/tutorials/advanced/dynamic\\_quantization\\_tutorial.html>`_."
msgstr ""
"这是对该材料的快速和高层次的处理；更多细节请继续学习`(beta) LSTM词语语言模型动态量化教程 "
"<https://pytorch.org/tutorials/advanced/dynamic\\_quantization\\_tutorial.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Additional Resources"
msgstr "附加资源"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Quantization API Documentaion "
"<https://pytorch.org/docs/stable/quantization.html>`_"
msgstr "`量化API文档 <https://pytorch.org/docs/stable/quantization.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`(beta) Dynamic Quantization on BERT "
"<https://pytorch.org/tutorials/intermediate/dynamic\\_quantization\\_bert\\_tutorial.html>`_"
msgstr ""
"`(beta) BERT的动态量化 "
"<https://pytorch.org/tutorials/intermediate/dynamic\\_quantization\\_bert\\_tutorial.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`(beta) Dynamic Quantization on an LSTM Word Language Model "
"<https://pytorch.org/tutorials/advanced/dynamic\\_quantization\\_tutorial.html>`_"
msgstr ""
"`(beta) LSTM词语语言模型动态量化 "
"<https://pytorch.org/tutorials/advanced/dynamic\\_quantization\\_tutorial.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Introduction to Quantization on PyTorch "
"<https://pytorch.org/blog/introduction-to-quantization-on-pytorch/>`_"
msgstr ""
"`PyTorch中的量化简介 <https://pytorch.org/blog/introduction-to-quantization-on-"
"pytorch/>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: dynamic_quantization.py "
"<dynamic_quantization.py>`"
msgstr ""
":download:`下载Python源码: dynamic_quantization.py <dynamic_quantization.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: dynamic_quantization.ipynb "
"<dynamic_quantization.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: dynamic_quantization.ipynb "
"<dynamic_quantization.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch Recipes"
msgstr "PyTorch配方"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "defining_a_neural_network.py"
msgstr "defining_a_neural_network.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Defining a Neural Network in PyTorch "
"https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html"
msgstr ""
"在PyTorch中定义一个神经网络 "
"https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "what_is_state_dict.py"
msgstr "what_is_state_dict.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"What is a state_dict in PyTorch "
"https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html"
msgstr ""
"什么是PyTorch中的state_dict "
"https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "saving_and_loading_models_for_inference.py"
msgstr "saving_and_loading_models_for_inference.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Saving and loading models for inference in PyTorch "
"https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html"
msgstr ""
"在PyTorch中保存和加载用于推理的模型 "
"https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "custom_dataset_transforms_loader.py"
msgstr "custom_dataset_transforms_loader.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Developing Custom PyTorch Dataloaders "
"https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html"
msgstr ""
"开发自定义PyTorch数据加载器 "
"https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Captum_Recipe.py"
msgstr "Captum_Recipe.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Model Interpretability using Captum "
"https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html"
msgstr ""
"使用Captum进行模型可解释性 "
"https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "dynamic_quantization.py"
msgstr "dynamic_quantization.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Dynamic Quantization "
"https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html"
msgstr ""
"动态量化 https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "warmstarting_model_using_parameters_from_a_different_model.py"
msgstr "warmstarting_model_using_parameters_from_a_different_model.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Warmstarting models using parameters from different model "
"https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html"
msgstr ""
"使用不同模型的参数进行模型热启动 "
"https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "zeroing_out_gradients.py"
msgstr "zeroing_out_gradients.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Zeroing out gradients "
"https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html"
msgstr ""
"将梯度置零 "
"https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "mobile_perf.py"
msgstr "mobile_perf.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch Mobile Performance Recipes "
"https://pytorch.org/tutorials/recipes/mobile_perf.html"
msgstr "PyTorch移动端性能配方 https://pytorch.org/tutorials/recipes/mobile_perf.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "amp_recipe.py"
msgstr "amp_recipe.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Automatic Mixed Precision "
"https://pytorch.org/tutorials/recipes/amp_recipe.html"
msgstr "自动混合精度 https://pytorch.org/tutorials/recipes/amp_recipe.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "regional_compilation.py"
msgstr "regional_compilation.py"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Reducing torch.compile cold start compilation time with regional compilation"
msgstr "使用区域编译减少torch.compile冷启动编译时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "https://pytorch.org/tutorials/recipes/regional_compilation.html"
msgstr "https://pytorch.org/tutorials/recipes/regional_compilation.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_changing_default_device.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_changing_default_device.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use TensorBoard with PyTorch"
msgstr "如何将TensorBoard与PyTorch结合使用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_tensorboard_with_pytorch.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_tensorboard_with_pytorch.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is a state_dict in PyTorch"
msgstr "PyTorch中的state_dict是什么"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_what_is_state_dict.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_what_is_state_dict.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Warmstarting model using parameters from a different model in PyTorch"
msgstr "在PyTorch中使用不同模型参数对模型进行热启动"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":ref:`sphx_glr_recipes_recipes_warmstarting_model_using_parameters_from_a_different_model.py`"
msgstr ""
":ref:`sphx_glr_recipes_recipes_warmstarting_model_using_parameters_from_a_different_model.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Reasoning about Shapes in PyTorch"
msgstr "在PyTorch中关于形状的推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_reasoning_about_shapes.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_reasoning_about_shapes.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Performance Tuning Guide"
msgstr "性能调优指南"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_tuning_guide.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_tuning_guide.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Tips for Loading an ``nn.Module`` from a Checkpoint"
msgstr "从检查点加载``nn.Module``的一些提示"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_module_load_state_dict_tips.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_module_load_state_dict_tips.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_defining_a_neural_network.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_defining_a_neural_network.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Zeroing out gradients in PyTorch"
msgstr "在PyTorch中将梯度置零"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_zeroing_out_gradients.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_zeroing_out_gradients.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Timer quick start"
msgstr "定时器快速入门"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_timer_quick_start.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_timer_quick_start.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_dynamic_quantization.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_dynamic_quantization.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_Captum_Recipe.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_Captum_Recipe.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Extension points in ``nn.Module`` for ``load_state_dict`` and tensor "
"subclasses"
msgstr "``nn.Module``中的扩展点，用于``load_state_dict``和张量子类"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_swap_tensors.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_swap_tensors.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch Profiler"
msgstr "PyTorch分析器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_profiler_recipe.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_profiler_recipe.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_amp_recipe.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_amp_recipe.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":ref:`sphx_glr_recipes_recipes_benchmark.py`"
msgstr ":ref:`sphx_glr_recipes_recipes_benchmark.py`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_module_load_state_dict_tips.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_recipes_recipes_module_load_state_dict_tips.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author:** `Mikayla Gawarecki <https://github.com/mikaylagawarecki>`_"
msgstr "**作者:** `Mikayla Gawarecki <https://github.com/mikaylagawarecki>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you're loading a checkpoint and want to reduce compute and memory as much"
" as possible, this tutorial shares some recommended practices. In "
"particular, we will discuss"
msgstr "如果您正在加载检查点并希望尽可能减少计算和内存消耗，本教程分享了一些推荐的实践方法。特别是我们将讨论"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The ``mmap`` keyword argument on ``torch.load``"
msgstr "``torch.load``的``mmap``关键字参数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The ``torch.device()`` context manager"
msgstr "``torch.device()``上下文管理器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The ``assign`` keyword argument on ``nn.Module.load_state_dict()``"
msgstr "``nn.Module.load_state_dict()``的``assign``关键字参数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This recipe requires PyTorch 2.1.0 or later."
msgstr "本教程需要PyTorch 2.1.0或更高版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let us consider a simple ``nn.Module`` that contains a list of Linear "
"layers:"
msgstr "让我们考虑一个包含线性层列表的简单``nn.Module``:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The following snippet demonstrates the use of the the ``mmap`` keyword "
"argument to ``torch.load``, the ``torch.device()`` context manager and the "
"``assign`` keyword argument to ``nn.Module.load_state_dict()``."
msgstr ""
"以下代码片段演示了``torch.load``的``mmap``关键字参数、``torch.device()``上下文管理器和``nn.Module.load_state_dict()``的``assign``关键字参数的使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Compare the snippet below to the one above:"
msgstr "将以下代码片段与上一个代码片段进行比较:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The second example does not use any of the features listed above and will be"
" less compute and memory efficient for loading a checkpoint. In the "
"following sections, we will discuss each of the features in further detail."
msgstr "第二个示例没有使用上述功能，因此在加载检查点时计算和内存效率较低。在接下来的部分中，我们将详细讨论每个功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using ``torch.load(mmap=True)``"
msgstr "使用``torch.load(mmap=True)``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"First, let us consider what happens when we load the checkpoint with "
"``torch.load``. When we save a checkpoint with ``torch.save``, tensor "
"storages are tagged with the device they are saved on. With ``torch.load``, "
"tensor storages will be loaded to the device they were tagged with (unless "
"this behavior is overridden using the ``map_location`` flag). For ease of "
"explanation, let us assume that the tensors were saved on CPU. This means "
"that on the first line all tensor storages will be loaded into CPU RAM, "
"which can be undesirable when:"
msgstr ""
"首先，让我们看看使用``torch.load``加载检查点时会发生什么。当我们使用``torch.save``保存检查点时，张量存储会被标记为保存时所在的设备。使用``torch.load``时，张量存储会被加载到其标记的设备上（除非使用``map_location``标志覆盖此行为）。为了便于说明，我们假设张量存储在CPU上。这意味着在第一行中，所有张量存储都会被加载到CPU"
" RAM中，这在以下情况下可能不理想:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "CPU RAM is smaller than the size of the checkpoint."
msgstr "CPU RAM小于检查点大小。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Waiting for the entire checkpoint to be loaded into RAM before performing, "
"for example, some per-tensor processing."
msgstr "等待整个检查点加载到RAM中，然后再执行每个张量的处理。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The ``mmap`` keyword argument to ``torch.load`` attempts to solve the above "
"two problems. As its name implies, the ``mmap`` keyword argument to "
"``torch.load`` makes use of an `mmap call <https://man7.org/linux/man-"
"pages/man2/mmap.2.html>`_ which maps a file on disk into virtual memory and "
"lets the OS handle loading and unloading into physical memory automatically."
" When this flag is passed, tensor storages will be memory-mapped."
msgstr ""
"``torch.load``的``mmap``关键字参数尝试解决上述两个问题。顾名思义，``mmap``关键字参数通过使用`mmap调用 "
"<https://man7.org/linux/man-"
"pages/man2/mmap.2.html>`_，将磁盘文件映射到虚拟内存并让操作系统自动处理加载和卸载到物理内存。当传递此标志时，张量存储将被内存映射。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As mentioned above, one can use this argument to do per-tensor processing on"
" a checkpoint without loading all tensor storages into CPU memory upfront. "
"For example:"
msgstr "如上所述，可以使用此参数对检查点进行每个张量的处理，而无需提前将所有张量存储加载到CPU内存。例如:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using ``torch.device('meta')``"
msgstr "使用``torch.device('meta')``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Next, let's consider the creation of the module."
msgstr "接下来，让我们考虑模块的创建。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This allocates memory for all parameters/buffers and initializes them per "
"the default initialization schemes defined in ``SomeModule.__init__()``, "
"which is wasteful when we want to load a checkpoint for the following "
"reasons:"
msgstr ""
"这会为所有参数/缓冲区分配内存并按照``SomeModule.__init__()``中定义的默认初始化方案初始化它们，当我们要加载检查点时，这种做法会导致以下问题:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The result of the initialization kernels will be overwritten by "
"``load_state_dict()`` without ever being used, so initialization is "
"wasteful."
msgstr "初始化内核的结果将被``load_state_dict()``覆盖，而这些结果从未被使用，因此初始化是浪费的。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We are allocating memory for these parameters/buffers in RAM while "
"``torch.load`` of the saved state dictionary also allocates memory in RAM "
"for the parameters/buffers in the checkpoint."
msgstr ""
"我们为这些参数/缓冲区在RAM中分配了内存，而保存的状态字典的``torch.load``仍需要在检查点中为参数/缓冲区在RAM中分配额外的内存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In order to solve these two problems, we can use the ``torch.device()`` "
"context manager with ``device='meta'`` when we instantiate the "
"``nn.Module()``."
msgstr ""
"为了解决这两个问题，我们可以在实例化``nn.Module()``时使用``torch.device()``上下文管理器，并指定``device='meta'``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The `torch.device() "
"<https://pytorch.org/docs/main/tensor_attributes.html#torch-device>`_ "
"context manager makes sure that factory calls will be performed as if they "
"were passed the specified ``device`` as an argument. Tensors on "
"``torch.device('meta')`` do not carry data. However, they possess all other "
"metadata a tensor carries such as ``.size()``, ``.stride()``, "
"``.requires_grad``, and others."
msgstr ""
"`torch.device() <https://pytorch.org/docs/main/tensor_attributes.html#torch-"
"device>`_上下文管理器确保工厂调用将在指定的``device``作为参数的情况下执行。在``torch.device('meta')``上的张量不携带数据。然而，它们拥有张量携带的所有其他元数据，如``.size()``、``.stride()``、``.requires_grad``等。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using ``load_state_dict(assign=True)``"
msgstr "使用``load_state_dict(assign=True)``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Next, we consider the loading of the state dictionary."
msgstr "接下来，我们考虑状态字典的加载。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``nn.Module.load_state_dict()`` is usually implemented via an in-place "
"``param_in_model.copy_(param_in_state_dict)``. This means that the "
"parameter/buffer with the corresponding key in the state dictionary is "
"copied into the parameter/buffer in the ``nn.Module``."
msgstr ""
"``nn.Module.load_state_dict()``通常通过一个就地``param_in_model.copy_(param_in_state_dict)``实现。这意味着，状态字典中具有相应键的参数/缓冲区将被复制到``nn.Module``中的相应参数/缓冲区。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"However, an in-place copy into a tensor on the ``meta`` device is a no-op. "
"In order to avoid this, we can pass the ``assign=True`` keyword argument to "
"``load_state_dict()``."
msgstr ""
"然而，就地复制到``meta``设备上的张量是一个空操作。为避免这种情况，我们可以将``assign=True``关键字参数传递给``load_state_dict()``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A caveat here is that since optimizers hold a reference to "
"``nn.Module.parameters()``, the optimizer must be initialized after the "
"module is loaded from state dict if ``assign=True`` is passed."
msgstr ""
"需要注意的是，由于优化器持有``nn.Module.parameters()``的引用，如果传递了``assign=True``，则优化器必须在从状态字典加载模块后初始化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To recap, in this tutorial we learned about ``torch.load(mmap=True)``, the "
"``torch.device()`` context manager with ``device=meta``, and "
"``nn.Module.load_state_dict(assign=True)`` as well as how these tools could "
"be used to aid when loading a model from a checkpoint."
msgstr ""
"回顾一下，在本教程中，我们学习了``torch.load(mmap=True)``，具有``device=meta``的``torch.device()``上下文管理器，以及``nn.Module.load_state_dict(assign=True)``，以及如何使用这些工具帮助从检查点加载模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: module_load_state_dict_tips.py "
"<module_load_state_dict_tips.py>`"
msgstr ""
":download:`下载Python源码: module_load_state_dict_tips.py "
"<module_load_state_dict_tips.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: module_load_state_dict_tips.ipynb "
"<module_load_state_dict_tips.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: module_load_state_dict_tips.ipynb "
"<module_load_state_dict_tips.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_profiler_recipe.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_recipes_recipes_profiler_recipe.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe explains how to use PyTorch profiler and measure the time and "
"memory consumption of the model's operators."
msgstr "本配方解释了如何使用PyTorch分析器并测量模型操作的时间和内存消耗。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch includes a simple profiler API that is useful when user needs to "
"determine the most expensive operators in the model."
msgstr "PyTorch包含一个简单的分析器API，当用户需要确定模型中最耗费资源的操作时非常有用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we will use a simple Resnet model to demonstrate how to use "
"profiler to analyze model performance."
msgstr "在本配方中，我们将使用一个简单的Resnet模型来演示如何使用分析器来分析模型性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To install ``torch`` and ``torchvision`` use the following command:"
msgstr "使用以下命令安装``torch``和``torchvision``:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Import all necessary libraries"
msgstr "导入所有必要的库"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Instantiate a simple Resnet model"
msgstr "实例化一个简单的Resnet模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using profiler to analyze execution time"
msgstr "使用分析器分析执行时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using profiler to analyze memory consumption"
msgstr "使用分析器分析内存消耗"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using tracing functionality"
msgstr "使用跟踪功能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Examining stack traces"
msgstr "检查堆栈跟踪"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using profiler to analyze long-running jobs"
msgstr "使用分析器分析长期运行任务"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1. Import all necessary libraries"
msgstr "1.导入所有必要的库"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe we will use ``torch``, ``torchvision.models`` and "
"``profiler`` modules:"
msgstr "在本示例中，我们将使用``torch``、``torchvision.models``和``profiler``模块："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Instantiate a simple Resnet model"
msgstr "2. 实例化一个简单的 Resnet 模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let's create an instance of a Resnet model and prepare an input for it:"
msgstr "让我们创建一个 Resnet 模型的实例并为其准备输入："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Using profiler to analyze execution time"
msgstr "3. 使用 profiler 分析执行时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch profiler is enabled through the context manager and accepts a number"
" of parameters, some of the most useful are:"
msgstr "通过上下文管理器启用 PyTorch 的 profiler，并接收一系列参数，其中一些最有用的参数有："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``activities`` - a list of activities to profile:"
msgstr "``activities`` - 要分析的活动列表："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``ProfilerActivity.CPU`` - PyTorch operators, TorchScript functions and "
"user-defined code labels (see ``record_function`` below);"
msgstr ""
"``ProfilerActivity.CPU`` - PyTorch 操作符、TorchScript 函数和用户定义的代码标签（请参阅下面的 "
"``record_function``）；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``ProfilerActivity.CUDA`` - on-device CUDA kernels;"
msgstr "``ProfilerActivity.CUDA`` - CUDA 设备内核；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``ProfilerActivity.XPU`` - on-device XPU kernels;"
msgstr "``ProfilerActivity.XPU`` - XPU 设备内核；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``record_shapes`` - whether to record shapes of the operator inputs;"
msgstr "``record_shapes`` - 是否记录操作符输入的形状；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``profile_memory`` - whether to report amount of memory consumed by model's "
"Tensors;"
msgstr "``profile_memory`` - 是否报告模型的张量消耗的内存量；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note: when using CUDA, profiler also shows the runtime CUDA events occurring"
" on the host."
msgstr "注意：当使用 CUDA 时，profiler 还会显示主机上发生的运行时 CUDA 事件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Let's see how we can use profiler to analyze the execution time:"
msgstr "让我们看看如何使用 profiler 分析执行时间："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note that we can use ``record_function`` context manager to label arbitrary "
"code ranges with user provided names (``model_inference`` is used as a label"
" in the example above)."
msgstr ""
"请注意，我们可以使用 ``record_function`` "
"上下文管理器为任意代码范围标注用户提供的名称（在上例中，``model_inference`` 被用作标签）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Profiler allows one to check which operators were called during the "
"execution of a code range wrapped with a profiler context manager. If "
"multiple profiler ranges are active at the same time (e.g. in parallel "
"PyTorch threads), each profiling context manager tracks only the operators "
"of its corresponding range. Profiler also automatically profiles the "
"asynchronous tasks launched with ``torch.jit._fork`` and (in case of a "
"backward pass) the backward pass operators launched with ``backward()`` "
"call."
msgstr ""
"Profiler 允许用户检查在使用 profiler 上下文管理器包装的代码范围内调用了哪些操作符。如果同时激活多个 profiler "
"范围（例如在并行 PyTorch 线程中），每个 profiler 上下文管理器仅跟踪其对应范围的操作符。Profiler 还会自动分析通过 "
"``torch.jit._fork`` 启动的异步任务，以及（在执行后向传播时）通过 ``backward()`` 调用启动的后向传播操作符。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Let's print out the stats for the execution above:"
msgstr "让我们输出上述执行的统计信息："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The output will look like (omitting some columns):"
msgstr "输出可能如下所示（省略了部分列）："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Here we see that, as expected, most of the time is spent in convolution (and"
" specifically in ``mkldnn_convolution`` for PyTorch compiled with ``MKL-"
"DNN`` support). Note the difference between self cpu time and cpu time - "
"operators can call other operators, self cpu time excludes time spent in "
"children operator calls, while total cpu time includes it. You can choose to"
" sort by the self cpu time by passing ``sort_by=\"self_cpu_time_total\"`` "
"into the ``table`` call."
msgstr ""
"我们看到，正如预期，大部分时间花费在卷积操作上（具体来说是启用了 ``MKL-DNN`` 支持的 PyTorch 中的 "
"``mkldnn_convolution``）。请注意 self cpu time 和 cpu time 的区别 - 操作符可以调用其他操作符，self"
" cpu time 不包括子操作符调用的时间，而 total cpu time 包括这些时间。您可以通过在 ``table`` 调用中传递 "
"``sort_by=\"self_cpu_time_total\"`` 指定按 self cpu time 排序。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To get a finer granularity of results and include operator input shapes, "
"pass ``group_by_input_shape=True`` (note: this requires running the profiler"
" with ``record_shapes=True``):"
msgstr ""
"为了获得更细粒度的结果并包括操作符的输入形状，请传递 ``group_by_input_shape=True``（注意：这需要运行带有 "
"``record_shapes=True`` 的 profiler）："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The output might look like this (omitting some columns):"
msgstr "输出可能如下所示（省略了部分列）："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note the occurrence of ``aten::convolution`` twice with different input "
"shapes."
msgstr "注意 ``aten::convolution`` 的两次出现，其输入形状不同。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Profiler can also be used to analyze performance of models executed on GPUs "
"and XPUs: Users could switch between cpu, cuda and xpu"
msgstr "Profiler 也可以用于分析在 GPU 和 XPU 上执行的模型性能：用户可以在 cpu、cuda 和 xpu 之间切换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(Note: the first use of CUDA profiling may bring an extra overhead.)"
msgstr "（注意：第一次使用 CUDA 分析时可能会带来额外开销。）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The resulting table output (omitting some columns):"
msgstr "生成的表格输出（省略了部分列）："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(Note: the first use of XPU profiling may bring an extra overhead.)"
msgstr "（注意：第一次使用 XPU 分析时可能会带来额外开销。）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"-------------------------------------------------------  ------------  "
"------------  ------------  ------------  ------------"
msgstr ""
"-------------------------------------------------------  ------------  "
"------------  ------------  ------------  ------------"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Name    Self XPU    Self XPU %     XPU total  XPU time avg    # of Calls"
msgstr "名称    自身 XPU 时间    自身 XPU 百分比     XPU 总时间  XPU 平均时间    调用次数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"-------------------------------------------------------   ------------  "
"------------  ------------  ------------  ------------"
msgstr ""
"-------------------------------------------------------   ------------  "
"------------  ------------  ------------  ------------"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"model_inference      0.000us         0.00%       2.567ms       2.567ms"
"             1"
msgstr ""
"model_inference      0.000us         0.00%       2.567ms       2.567ms"
"             1"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::conv2d      0.000us         0.00%       1.871ms      93.560us"
"            20"
msgstr ""
"aten::conv2d      0.000us         0.00%       1.871ms      93.560us"
"            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::convolution      0.000us         0.00%       1.871ms      93.560us"
"            20"
msgstr ""
"aten::convolution      0.000us         0.00%       1.871ms      93.560us"
"            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::_convolution      0.000us         0.00%       1.871ms      93.560us"
"            20"
msgstr ""
"aten::_convolution      0.000us         0.00%       1.871ms      93.560us"
"            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::convolution_overrideable      1.871ms        72.89%       1.871ms"
"      93.560us            20"
msgstr ""
"aten::convolution_overrideable      1.871ms        72.89%       1.871ms"
"      93.560us            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"gen_conv      1.484ms        57.82%       1.484ms      74.216us            "
"20"
msgstr ""
"gen_conv      1.484ms        57.82%       1.484ms      74.216us            "
"20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::batch_norm      0.000us         0.00%     432.640us      21.632us"
"            20"
msgstr ""
"aten::batch_norm      0.000us         0.00%     432.640us      21.632us"
"            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::_batch_norm_impl_index      0.000us         0.00%     432.640us      "
"21.632us            20"
msgstr ""
"aten::_batch_norm_impl_index      0.000us         0.00%     432.640us      "
"21.632us            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"aten::native_batch_norm      432.640us      16.85%     432.640us      "
"21.632us            20"
msgstr ""
"aten::native_batch_norm      432.640us      16.85%     432.640us      "
"21.632us            20"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"conv_reorder      386.880us      15.07%     386.880us       6.448us"
"            60"
msgstr ""
"conv_reorder      386.880us      15.07%     386.880us       6.448us"
"            60"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"-------------------------------------------------------   ------------  "
"------------  ------------  ------------  ------------ Self CPU time total: "
"712.486ms Self XPU time total: 2.567ms"
msgstr ""
"-------------------------------------------------------   ------------  "
"------------  ------------  ------------  ------------ 自身 CPU 时间总计: "
"712.486ms 自身 XPU 时间总计: 2.567ms"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note the occurrence of on-device kernels in the output (e.g. "
"``sgemm_32x32x32_NN``)."
msgstr "注意输出中的设备内核（例如 ``sgemm_32x32x32_NN``）的出现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Using profiler to analyze memory consumption"
msgstr "4. 使用 profiler 分析内存消耗"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch profiler can also show the amount of memory (used by the model's "
"tensors) that was allocated (or released) during the execution of the "
"model's operators. In the output below, 'self' memory corresponds to the "
"memory allocated (released) by the operator, excluding the children calls to"
" the other operators. To enable memory profiling functionality pass "
"``profile_memory=True``."
msgstr ""
"PyTorch profiler 还可以显示模型操作符执行期间分配（或释放）的内存（用于模型的张量）。在以下输出中，'self' "
"内存对应于操作符本身分配（释放）的内存，不包括对其他操作符的子调用。要启用内存分析功能，请传递 ``profile_memory=True``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5. Using tracing functionality"
msgstr "5. 使用追踪功能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Profiling results can be outputted as a ``.json`` trace file: Tracing CUDA "
"or XPU kernels Users could switch between cpu, cuda and xpu"
msgstr "分析结果可以输出为一个``.json``追踪文件：用户可以在 cpu、cuda 和 xpu 之间切换来追踪 CUDA 和 XPU 内核"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You can examine the sequence of profiled operators and CUDA/XPU kernels in "
"Chrome trace viewer (``chrome://tracing``):"
msgstr "您可以在 Chrome 追踪查看器（``chrome://tracing``）中检查被分析的操作符和 CUDA/XPU 内核的序列："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6. Examining stack traces"
msgstr "6. 检查堆栈追踪"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Profiler can be used to analyze Python and TorchScript stack traces:"
msgstr "Profiler 可用于分析 Python 和 TorchScript 的堆栈追踪："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note the two convolutions and the two call sites in "
"``torchvision/models/resnet.py`` script."
msgstr "注意在 ``torchvision/models/resnet.py`` 脚本中出现的两个卷积和两个调用点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(Warning: stack tracing adds an extra profiling overhead.)"
msgstr "（警告：堆栈追踪会增加额外的分析开销。）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7. Using profiler to analyze long-running jobs"
msgstr "7. 使用 profiler 分析长时间运行的任务"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch profiler offers an additional API to handle long-running jobs (such "
"as training loops). Tracing all of the execution can be slow and result in "
"very large trace files. To avoid this, use optional arguments:"
msgstr ""
"PyTorch profiler 提供了一个额外的 API "
"来处理长时间运行的任务（例如训练循环）。对所有执行进行追踪可能会很慢，并生成非常大的追踪文件。为了避免这种情况，可以使用一些可选参数："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``schedule`` - specifies a function that takes an integer argument (step "
"number) as an input and returns an action for the profiler, the best way to "
"use this parameter is to use ``torch.profiler.schedule`` helper function "
"that can generate a schedule for you;"
msgstr ""
"``schedule`` - 指定一个函数，该函数接收一个整数参数（步骤编号）作为输入，并返回一个 profiler 的操作；使用 "
"``torch.profiler.schedule`` 辅助函数生成 schedule 是最好的方式；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``on_trace_ready`` - specifies a function that takes a reference to the "
"profiler as an input and is called by the profiler each time the new trace "
"is ready."
msgstr ""
"``on_trace_ready`` - 指定一个函数，该函数接收一个 profiler 的引用作为输入，并在每次新追踪准备好时由 profiler "
"调用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To illustrate how the API works, let's first consider the following example "
"with ``torch.profiler.schedule`` helper function:"
msgstr "为了说明 API 的工作原理，让我们先看以下使用 ``torch.profiler.schedule`` 辅助函数的示例："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Profiler assumes that the long-running job is composed of steps, numbered "
"starting from zero. The example above defines the following sequence of "
"actions for the profiler:"
msgstr "Profiler 假定长时间运行的任务由步骤组成，步骤从零开始编号。在上述示例中，为 profiler 定义了以下操作序列："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Parameter ``skip_first`` tells profiler that it should ignore the first 10 "
"steps (default value of ``skip_first`` is zero);"
msgstr "参数 ``skip_first`` 告诉 profiler 应忽略前 10 个步骤（``skip_first`` 的默认值为零）；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"After the first ``skip_first`` steps, profiler starts executing profiler "
"cycles;"
msgstr "在前 ``skip_first`` 步骤后，profiler 开始执行 profiler 循环；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Each cycle consists of three phases:"
msgstr "每个循环包含三个阶段："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "idling (``wait=5`` steps), during this phase profiler is not active;"
msgstr "空闲（``wait=5`` 步骤），此阶段 profiler 不活跃；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"warming up (``warmup=1`` steps), during this phase profiler starts tracing, "
"but the results are discarded; this phase is used to discard the samples "
"obtained by the profiler at the beginning of the trace since they are "
"usually skewed by an extra overhead;"
msgstr ""
"预热（``warmup=1`` 步骤），此阶段 profiler 开始追踪，但结果会被丢弃；此阶段用于丢弃 profiler "
"在追踪开始时获得的样本，因为这些通常受到额外开销的影响；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"active tracing (``active=3`` steps), during this phase profiler traces and "
"records data;"
msgstr "活动追踪（``active=3`` 步骤），此阶段 profiler 记录数据；"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"An optional ``repeat`` parameter specifies an upper bound on the number of "
"cycles. By default (zero value), profiler will execute cycles as long as the"
" job runs."
msgstr "可选的 ``repeat`` 参数指定循环的最大次数。默认值（零值）表示 profiler 将随着任务的运行而无限循环。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Thus, in the example above, profiler will skip the first 15 steps, spend the"
" next step on the warm up, actively record the next 3 steps, skip another 5 "
"steps, spend the next step on the warm up, actively record another 3 steps. "
"Since the ``repeat=2`` parameter value is specified, the profiler will stop "
"the recording after the first two cycles."
msgstr ""
"因此，在上述示例中，profiler 将跳过前 15 个步骤，用下一步进行预热，活跃记录接下来的 3 个步骤，再跳过 5 "
"个步骤，用下一步进行预热，活跃记录接下来的 3 个步骤。由于指定了 ``repeat=2`` 参数值，profiler 将在头两个循环后停止记录。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"At the end of each cycle profiler calls the specified ``on_trace_ready`` "
"function and passes itself as an argument. This function is used to process "
"the new trace - either by obtaining the table output or by saving the output"
" on disk as a trace file."
msgstr ""
"在每个循环结束时，profiler 调用指定的 ``on_trace_ready`` 函数，并将自身作为参数传递。此函数用于处理新的追踪 - "
"通过获取表格输出或将输出保存到磁盘作为追踪文件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To send the signal to the profiler that the next step has started, call "
"``prof.step()`` function. The current profiler step is stored in "
"``prof.step_num``."
msgstr ""
"要向 profiler 发送信号表明下一个步骤已开始，调用 ``prof.step()`` 函数。当前的 profiler 步骤保存在 "
"``prof.step_num`` 中。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The following example shows how to use all of the concepts above for CUDA "
"and XPU Kernels:"
msgstr "以下示例展示了如何将上述所有概念用于 CUDA 和 XPU 内核："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Take a look at the following recipes/tutorials to continue your learning:"
msgstr "查看以下示例/教程以继续学习："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`PyTorch Benchmark "
"<https://pytorch.org/tutorials/recipes/recipes/benchmark.html>`_"
msgstr ""
"`PyTorch 基准测试 "
"<https://pytorch.org/tutorials/recipes/recipes/benchmark.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Visualizing models, data, and training with TensorBoard "
"<https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html>`_ "
"tutorial"
msgstr ""
"`使用 TensorBoard 进行模型、数据和训练的可视化 "
"<https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html>`_ 教程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: profiler_recipe.py "
"<profiler_recipe.py>`"
msgstr ":download:`下载 Python 源代码: profiler_recipe.py <profiler_recipe.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: profiler_recipe.ipynb "
"<profiler_recipe.ipynb>`"
msgstr ""
":download:`下载 Jupyter Notebook: profiler_recipe.ipynb "
"<profiler_recipe.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_reasoning_about_shapes.py>` to download "
"the full example code"
msgstr ""
"单击 :ref:`此处 <sphx_glr_download_recipes_recipes_reasoning_about_shapes.py>` "
"下载完整的示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When writing models with PyTorch, it is commonly the case that the "
"parameters to a given layer depend on the shape of the output of the "
"previous layer. For example, the ``in_features`` of an ``nn.Linear`` layer "
"must match the ``size(-1)`` of the input. For some layers, the shape "
"computation involves complex equations, for example convolution operations."
msgstr ""
"使用 PyTorch 编写模型时，通常某一层的参数依赖于上一层输出的形状。例如，``nn.Linear`` 层的 ``in_features`` "
"必须与输入数据的 ``size(-1)`` 匹配。对于某些层，形状计算涉及复杂公式，例如卷积操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One way around this is to run the forward pass with random inputs, but this "
"is wasteful in terms of memory and compute."
msgstr "一种解决方法是使用随机输入执行前向传播，但这在内存和计算方面是低效的。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Instead, we can make use of the ``meta`` device to determine the output "
"shapes of a layer without materializing any data."
msgstr "相反，我们可以利用 ``meta`` 设备来确定层的输出形状，而无需生成任何数据。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Observe that since data is not materialized, passing arbitrarily large "
"inputs will not significantly alter the time taken for shape computation."
msgstr "由于数据未生成，传递任意大的输入数据不会显著影响形状计算所需的时间。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Consider an arbitrary network such as the following:"
msgstr "考虑以下任意定义的网络："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We can view the intermediate shapes within an entire network by registering "
"a forward hook to each layer that prints the shape of the output."
msgstr "通过为每一层注册一个前向钩子，我们可以查看整个网络中每层的中间形状，该钩子会打印输出形状。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: reasoning_about_shapes.py "
"<reasoning_about_shapes.py>`"
msgstr ""
":download:`下载 Python 源代码: reasoning_about_shapes.py "
"<reasoning_about_shapes.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: reasoning_about_shapes.ipynb "
"<reasoning_about_shapes.ipynb>`"
msgstr ""
":download:`下载 Jupyter Notebook: reasoning_about_shapes.ipynb "
"<reasoning_about_shapes.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Save Load Across Devices"
msgstr "跨设备保存和加载"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This tutorial was deprecated. There is a newer tutorial that covers the same"
" topic:  https://pytorch.org/tutorials/beginner/saving_loading_models.html"
msgstr ""
"本教程已被废弃。请参考涵盖相同主题的更新教程：https://pytorch.org/tutorials/beginner/saving_loading_models.html"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Saving And Loading A General Checkpoint"
msgstr "保存和加载通用检查点"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Saving And Loading Models For Inference"
msgstr "用于推理的模型保存和加载"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Saving Multiple Models In One File"
msgstr "在同一个文件中保存多个模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_swap_tensors.py>` to "
"download the full example code"
msgstr ""
"单击 :ref:`此处 <sphx_glr_download_recipes_recipes_swap_tensors.py>` 下载完整的示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe introduces a new utility function ``torch.utils.swap_tensors`` "
"as well as two new extension points where it has been integrated in "
"``nn.Module``:"
msgstr ""
"本食谱介绍了一个新的实用函数``torch.utils.swap_tensors``以及两个已经集成到``nn.Module``中的新扩展点："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``nn.Module.to()`` and related methods"
msgstr "``nn.Module.to()``及其相关方法"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``nn.Module.load_state_dict()``"
msgstr "``nn.Module.load_state_dict()``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This recipe requires PyTorch 2.3.0 or later."
msgstr "本食谱需要PyTorch 2.3.0或更高版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.utils.swap_tensors``"
msgstr "``torch.utils.swap_tensors``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.utils.swap_tensors`` (hereafter referred to as ``swap_tensors``) is "
"a utility function that takes in two Python tensors and swaps them."
msgstr ""
"``torch.utils.swap_tensors``（以下简称为``swap_tensors``）是一个实用函数，用于交换两个Python张量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"More specifically, ``swap_tensors`` swaps the Python ``__class__``, "
"``__dict__`` and ``__slots__`` of the two tensors, as well as their "
"associated ``at::Tensor``."
msgstr ""
"更具体地说，``swap_tensors``交换了两个张量的Python``__class__``、``__dict__``和``__slots__``，以及它们关联的``at::Tensor``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Application to ``nn.Module``"
msgstr "应用到``nn.Module``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This utility is pertinent to ``nn.Module`` when a Python object outside of "
"the module holds a reference to parameters of the module. If an "
"``nn.Module`` modifies any of its parameters out of place, the object "
"holding references to the parameters will not see the change. A classic "
"example of this is the optimizer, which holds a reference to the parameters "
"of the ``nn.Module``. This leads to a silent correctness issue where the "
"``optimizer.step()`` will run without error but the weights of the "
"``nn.Module`` will not be updated."
msgstr ""
"当模块外部的Python对象持有模块参数的引用时，此工具适用于``nn.Module``。如果``nn.Module``中的任何参数被非原地修改，持有参数引用的对象将无法看到更改。一个经典的例子是优化器，它持有``nn.Module``参数的引用。这会导致一个隐性正确性问题，即``optimizer.step()``将运行而不报错，但``nn.Module``的权重不会更新。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This includes methods that change the device of the module (such as "
"``nn.Module.cpu()``), methods that change the ``dtype`` of the module (such "
"as ``nn.Module.float()``) as well as methods that allow the module to be "
"materialized (such as ``nn.Module.to_empty()``)."
msgstr ""
"这包括更改模块设备的方法（例如``nn.Module.cpu()``），更改模块``dtype``的方法（例如``nn.Module.float()``），以及允许模块被物化的方法（例如``nn.Module.to_empty()``）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"At first glance, it might be non-intuitive that these methods are able to "
"modify the parameters of the module in-place. The existing approach has been"
" to use a nasty hack dating back from the first days of PyTorch."
msgstr "乍一看，这些方法能够原地方便地修改模块参数可能显得不直观。现有方法利用了一个可以追溯到PyTorch初期的糟糕黑客手段。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Notably, the existing approach does not work in these cases:"
msgstr "值得注意的是，现有方法在以下情况下不起作用："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "when using ``__torch_dispatch__`` subclasses"
msgstr "使用``__torch_dispatch__``子类时"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"when ``param`` and ``new_param`` do not have the same Python ``type()``"
msgstr "当``param``和``new_param``的Python``type()``不同"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For tensors with special C++ representations (such as sparse tensors and "
"``XLA`` tensors)"
msgstr "对于具有特殊C++表示形式的张量（例如稀疏张量和``XLA``张量）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the following part of this recipe, we will define a toy "
"``__torch_dispatch__`` subclass ``MyQuantizedLinearWeight`` that represents "
"quantized linear weights. This subclass will be used for illustration "
"purposes throughout the rest of the tutorial. For brevity, we omit most of "
"the ``__torch_dispatch__`` implementation."
msgstr ""
"在食谱的以下部分，我们将定义一个玩具型``__torch_dispatch__``子类``MyQuantizedLinearWeight``，该子类表示量化的线性权重。在整个教程中，该子类将用于说明目的。为了简洁起见，我们省略了大部分``__torch_dispatch__``的实现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let us create an ``nn.Linear`` layer of ``dtype`` ``torch.float32`` where "
"the weight is a ``MyQuantizedLinearWeight`` and try to convert it to "
"``torch.bfloat16``. Observe that the weight's ``dtype`` changes as expected."
" However, the ``dtype`` of the subclass' payload (``elem``) does not change."
msgstr ""
"创建一个``dtype``为``torch.float32``且权重为``MyQuantizedLinearWeight``的``nn.Linear``层，并尝试将其转换为``torch.bfloat16``。观察权重的``dtype``如预期变化。但子类的有效载荷（``elem``）的``dtype``并未改变。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To this end, we introduce a global config "
"``torch.__future__.set_swap_module_params_on_conversion`` that will use "
"``swap_tensors`` to swap the parameters of the module while preserving "
"references in place of ``.data`` setting. When this config is set, "
"``swap_tensors`` will be used during the conversion, which ensures that the "
"``dtype`` of the payload is properly converted."
msgstr ""
"为此，我们引入了一个全局配置``torch.__future__.set_swap_module_params_on_conversion``，该配置将使用``swap_tensors``交换模块参数，同时保留原位引用，取代``.data``设置。当启用此配置时，将在转换过程中使用``swap_tensors``，这确保了有效载荷的``dtype``得到正确转换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Depending on the value of the ``assign`` keyword argument passed to "
"``load_state_dict()``, there are two ways to load the ``state_dict``:"
msgstr "根据传递给``load_state_dict()``的``assign``关键字参数的值，有两种方法加载``state_dict``："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``assign=False``: preserves the properties of ``module.param`` and only "
"takes the values from ``state_dict['param_name']``"
msgstr ""
"``assign=False``：保留``module.param``的属性，仅从``state_dict['param_name']``获取值"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``assign=True``: preserves the properties and values of "
"``state_dict['param_name']``."
msgstr "``assign=True``：保留``state_dict['param_name']``的属性和值。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Previously, these were implemented with in-place ``copy_`` and "
"``__setattr__`` respectively. With the existing implementation, each "
"approach had its own limitations -- ``assign=False`` imposes the constraint "
"that the type of the parameter in the ``state_dict`` must be the same as the"
" type of the parameter in the module while ``assign=True`` imposes the "
"constraint that anything that holds references to the module's parameters "
"must be initialized after ``nn.Module.load_state_dict()``."
msgstr ""
"以前，这些分别通过就地``copy_``和``__setattr__``实现。采用现有实现的每种方法都有其局限性——``assign=False``要求``state_dict``中参数的类型必须与模块中参数的类型相同，而``assign=True``要求所有持有模块参数引用的内容必须在``nn.Module.load_state_dict()``之后初始化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Now, we address both constraints by adding a ``swap_tensors`` path to "
"``load_state_dict()`` and introducing a new extension point "
"``torch.Tensor.module_load(self, other, assign=False)``. When the "
"``swap_tensors`` path is enabled via the ``__future__`` mentioned above, we "
"can use a ``__torch_function__`` handler for ``module_load`` to apply a "
"custom transformation to the value in the ``state_dict``. The result of this"
" transformation will be swapped with the parameter in the module."
msgstr ""
"现在，我们通过在``load_state_dict()``中添加一个``swap_tensors``路径并引入一个新的扩展点``torch.Tensor.module_load(self,"
" other, "
"assign=False)``来解决这两个限制。当上述``__future__``启用``swap_tensors``路径时，我们可以使用``__torch_function__``处理器为``module_load``应用自定义的转换到``state_dict``中的值。该转换结果将与模块中的参数交换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the following example, we will use the ``MyQuantizedLinearWeight`` "
"subclass defined above to illustrate how we can use these features to apply "
"a custom quantization scheme to the weights of a linear layer when loading "
"the ``state_dict``."
msgstr ""
"在以下示例中，我们将使用前面定义的``MyQuantizedLinearWeight``子类说明如何使用这些功能在加载``state_dict``时对线性层权重应用自定义量化方案。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Recall that the ``__torch_function__`` handler for ``module_load`` will be "
"invoked if either ``self`` or ``other`` (in this case ``param`` or "
"``state_dict[param_key]``) are ``MyQuantizedLinearWeight`` subclasses."
msgstr ""
"回想一下，如果``self``或``other``（在这种情况下为``param``或``state_dict[param_key]``）是``MyQuantizedLinearWeight``子类，则会调用``module_load``的``__torch_function__``处理器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Assume that we expect the ``state_dict`` to contain plain tensors and the "
"module to contain ``MyQuantizedLinearWeight`` parameters where we want the "
"tensors in the ``state_dict`` to be transformed into the subclass. Then we "
"can define a ``__torch_function__`` handler for ``torch.Tensor.module_load``"
" as such:"
msgstr ""
"假如我们期望``state_dict``包含普通张量，而模块包含``MyQuantizedLinearWeight``参数，那么我们想把``state_dict``中的张量转换为子类。我们可以为``torch.Tensor.module_load``定义如下的``__torch_function__``处理器："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"First, let us create a skeleton of a model on the meta device to avoid "
"materializing storages. We convert all weights in the modules to "
"``MyQuantizedLinearWeight`` subclasses while leaving biases intact."
msgstr ""
"首先，让我们在元设备上创建一个模型骨架以避免物化存储。在模块中将所有权重转换为``MyQuantizedLinearWeight``子类，同时保留偏置不变。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We can then load the ``state_dict``. Observe that we use ``assign=True`` "
"because for biases, we want to preserve the properties of the tensor in the "
"``state_dict`` (for example, we do not want the bias to be on the ``meta`` "
"device after loading)."
msgstr ""
"然后我们可以加载``state_dict``。注意我们使用``assign=True``，因为对于偏置，我们希望保留``state_dict``中张量的属性（例如，我们不希望偏置在加载后位于``meta``设备上）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The above is a toy example of how we can use the new extension point in "
"``nn.Module.load_state_dict()``. One can also imagine alternate scenarios "
"such as when we have tensor subclasses in the ``state_dict`` and plain "
"``nn.Parameters``/ tensors in the module or when both are tensor subclasses."
" Based on the use case, we can define the ``__torch_function__`` handler for"
" ``module_load`` to apply the transforms as needed."
msgstr ""
"上述是一个如何在``nn.Module.load_state_dict()``中利用新扩展点的小例子。可以想象其他场景，例如当``state_dict``中有张量子类而模块中是普通``nn.Parameters``或张量，或两者都为张量子类。根据使用场景，我们可以为``module_load``定义``__torch_function__``处理器以按需应用转换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we learned about ``swap_tensors``, the importance of "
"preserving references for parameters in ``nn.Module`` as well as how to use "
"the two new extension points that are gated by "
"``torch.__future__.set_swap_module_params_on_conversion``."
msgstr ""
"在本食谱中，我们了解了``swap_tensors``、在``nn.Module``中保持参数引用的重要性以及如何使用由``torch.__future__.set_swap_module_params_on_conversion``启用的两个新扩展点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: swap_tensors.py <swap_tensors.py>`"
msgstr ":download:`下载Python源代码：swap_tensors.py <swap_tensors.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: swap_tensors.ipynb "
"<swap_tensors.ipynb>`"
msgstr ":download:`下载Jupyter笔记本：swap_tensors.ipynb <swap_tensors.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_tensorboard_with_pytorch.py>` to download"
" the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_recipes_recipes_tensorboard_with_pytorch.py>`"
" 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"TensorBoard is a visualization toolkit for machine learning experimentation."
" TensorBoard allows tracking and visualizing metrics such as loss and "
"accuracy, visualizing the model graph, viewing histograms, displaying images"
" and much more. In this tutorial we are going to cover TensorBoard "
"installation, basic usage with PyTorch, and how to visualize data you logged"
" in TensorBoard UI."
msgstr ""
"TensorBoard是一个机器学习实验的可视化工具。TensorBoard允许跟踪和可视化度量（如损失和准确率）、查看模型图表、直方图、显示图像等。在本教程中，我们将介绍TensorBoard的安装、与PyTorch的基础用法以及如何在TensorBoard"
" UI中可视化记录的数据。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch should be installed to log models and metrics into TensorBoard log "
"directory. The following command will install PyTorch 1.4+ via Anaconda "
"(recommended):"
msgstr ""
"需要安装PyTorch才能将模型和指标记录到TensorBoard日志目录下。以下命令将通过Anaconda（推荐）安装PyTorch 1.4+："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "or pip"
msgstr "或通过pip"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using TensorBoard in PyTorch"
msgstr "在PyTorch中使用TensorBoard"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let’s now try using TensorBoard with PyTorch! Before logging anything, we "
"need to create a ``SummaryWriter`` instance."
msgstr "现在让我们尝试在PyTorch中使用TensorBoard！在记录任何内容之前，我们需要创建一个``SummaryWriter``实例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Writer will output to ``./runs/`` directory by default."
msgstr "默认情况下，Writer会输出到``./runs/``目录。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Log scalars"
msgstr "记录标量"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In machine learning, it’s important to understand key metrics such as loss "
"and how they change during training. Scalar helps to save the loss value of "
"each training step, or the accuracy after each epoch."
msgstr "在机器学习中，理解诸如损失等关键指标及其在训练过程中的变化很重要。标量帮助保存每步训练的损失值，或每个epoch后的准确率。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To log a scalar value, use ``add_scalar(tag, scalar_value, global_step=None,"
" walltime=None)``. For example, lets create a simple linear regression "
"training, and log loss value using ``add_scalar``"
msgstr ""
"要记录标量值，可使用``add_scalar(tag, scalar_value, global_step=None, "
"walltime=None)``。例如，创建一个简单的线性回归训练，并使用``add_scalar``记录损失值。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Call ``flush()`` method to make sure that all pending events have been "
"written to disk."
msgstr "调用``flush()``方法以确保所有待处理事件已被写入磁盘。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"See `torch.utils.tensorboard tutorials "
"<https://pytorch.org/docs/stable/tensorboard.html>`_ to find more "
"TensorBoard visualization types you can log."
msgstr ""
"查看 `torch.utils.tensorboard 教程 "
"<https://pytorch.org/docs/stable/tensorboard.html>`_ "
"以了解更多可以记录的TensorBoard可视化类型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you do not need the summary writer anymore, call ``close()`` method."
msgstr "如果不再需要SummaryWriter，请调用``close()``方法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Run TensorBoard"
msgstr "运行TensorBoard"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Install TensorBoard through the command line to visualize data you logged"
msgstr "通过命令行安装TensorBoard来可视化您记录的数据"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Now, start TensorBoard, specifying the root log directory you used above. "
"Argument ``logdir`` points to directory where TensorBoard will look to find "
"event files that it can display. TensorBoard will recursively walk the "
"directory structure rooted at ``logdir``, looking for ``.*tfevents.*`` "
"files."
msgstr ""
"现在，启动TensorBoard，指定您之前使用的根日志目录。参数``logdir``指向TensorBoard查找可显示事件文件的目录。TensorBoard将递归遍历以``logdir``为根的目录结构，寻找``.*tfevents.*``文件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Go to the URL it provides OR to `http://localhost:6006/ "
"<http://localhost:6006/>`_"
msgstr "转到其提供的URL或`http://localhost:6006/ <http://localhost:6006/>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This dashboard shows how the loss and accuracy change with every epoch. You "
"can use it to also track training speed, learning rate, and other scalar "
"values. It’s helpful to compare these metrics across different training runs"
" to improve your model."
msgstr ""
"该仪表板展示了每个epoch的损失和准确率的变化情况。您还可以使用它跟踪训练速度、学习率以及其他标量值。对比不同训练运行的这些指标有助于改进模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.utils.tensorboard "
"<https://pytorch.org/docs/stable/tensorboard.html>`_ docs"
msgstr ""
"`torch.utils.tensorboard "
"<https://pytorch.org/docs/stable/tensorboard.html>`_ 文档"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: tensorboard_with_pytorch.py "
"<tensorboard_with_pytorch.py>`"
msgstr ""
":download:`下载Python源代码：tensorboard_with_pytorch.py "
"<tensorboard_with_pytorch.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: tensorboard_with_pytorch.ipynb "
"<tensorboard_with_pytorch.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：tensorboard_with_pytorch.ipynb "
"<tensorboard_with_pytorch.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_timer_quick_start.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_recipes_recipes_timer_quick_start.py>` "
"下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we're going to cover the primary APIs of "
"`torch.utils.benchmark.Timer`. The PyTorch Timer is based on the "
"`timeit.Timer "
"<https://docs.python.org/3/library/timeit.html#timeit.Timer>`__ API, with "
"several PyTorch specific modifications. Familiarity with the builtin `Timer`"
" class is not required for this tutorial, however we assume that the reader "
"is familiar with the fundamentals of performance work."
msgstr ""
"在本教程中，我们将介绍`torch.utils.benchmark.Timer`的主要API。PyTorch计时器基于`timeit.Timer "
"<https://docs.python.org/3/library/timeit.html#timeit.Timer>`__ "
"API，并进行了多处PyTorch特定的修改。尽管对内置计时器类的熟悉不是本教程的前提，但我们假设读者了解性能工作的基础知识。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For a more comprehensive performance tuning tutorial, see `PyTorch Benchmark"
" <https://pytorch.org/tutorials/recipes/recipes/benchmark.html>`__."
msgstr ""
"有关更全面的性能调优教程，请参见`PyTorch Benchmark "
"<https://pytorch.org/tutorials/recipes/recipes/benchmark.html>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Contents:**"
msgstr "**内容:**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "`Defining a Timer <#defining-a-timer>`__"
msgstr "`定义计时器 <#defining-a-timer>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Wall time: Timer.blocked_autorange(...) <#wall-time-timer-blocked-"
"autorange>`__"
msgstr ""
"`墙上时间: Timer.blocked_autorange(...) <#wall-time-timer-blocked-autorange>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "`C++ snippets <#c-snippets>`__"
msgstr "`C++代码段 <#c-snippets>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Instruction counts: Timer.collect_callgrind(...) <#instruction-counts-"
"timer-collect-callgrind>`__"
msgstr ""
"`指令计数: Timer.collect_callgrind(...) <#instruction-counts-timer-collect-"
"callgrind>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Instruction counts: Delving deeper <#instruction-counts-delving-deeper>`__"
msgstr "`指令计数: 深入研究 <#instruction-counts-delving-deeper>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "`A/B testing with Callgrind <#a-b-testing-with-callgrind>`__"
msgstr "`使用Callgrind进行A/B测试 <#a-b-testing-with-callgrind>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "`Wrapping up <#wrapping-up>`__"
msgstr "`总结 <#wrapping-up>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "`Footnotes <#footnotes>`__"
msgstr "`脚注 <#footnotes>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "1. Defining a Timer"
msgstr "1. 定义计时器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "A `Timer` serves as a task definition."
msgstr "`Timer`是一个任务定义工具。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Wall time: ``Timer.blocked_autorange(...)``"
msgstr "2. 墙上时间: ``Timer.blocked_autorange(...)``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This method will handle details such as picking a suitable number if "
"repeats, fixing the number of threads, and providing a convenient "
"representation of the results."
msgstr "该方法将处理细节，如选择适当的重复次数、固定线程数，并提供结果的方便表示。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Snippet wall time.**"
msgstr "**代码片段壁钟时间**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. C++ snippets"
msgstr "3. C++代码片段"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**C++ snippet wall time.**"
msgstr "**C++代码片段壁钟时间**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Unsurprisingly, the C++ snippet is both faster and has lower variation."
msgstr "毫不意外的是，C++代码片段既更快又具有更低的变化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Instruction counts: ``Timer.collect_callgrind(...)``"
msgstr "4. 指令计数: ``Timer.collect_callgrind(...)``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For deep dive investigations, ``Timer.collect_callgrind`` wraps `Callgrind "
"<https://valgrind.org/docs/manual/cl-manual.html>`__ in order to collect "
"instruction counts. These are useful as they offer fine grained and "
"deterministic (or very low noise in the case of Python) insights into how a "
"snippet is run."
msgstr ""
"为了深入研究，``Timer.collect_callgrind`` 包装了`Callgrind "
"<https://valgrind.org/docs/manual/cl-"
"manual.html>`__，以收集指令计数。这些计数非常有用，因为它们提供了对代码片段运行方式的细粒度和确定性（在Python中噪声非常低）见解。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**C++ Callgrind stats (summary)**"
msgstr "**C++ Callgrind统计数据（摘要）**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5. Instruction counts: Delving deeper"
msgstr "5. 指令计数: 更深入探索"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The string representation of ``CallgrindStats`` is similar to that of "
"Measurement. `Noisy symbols` are a Python concept (removing calls in the "
"CPython interpreter which are known to be noisy)."
msgstr ""
"``CallgrindStats``的字符串表示类似于Measurement。`噪声符号`是一个Python的概念（移除CPython解释器中被认为是噪声的调用）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For more detailed analysis, however, we will want to look at specific calls."
" ``CallgrindStats.stats()`` returns a ``FunctionCounts`` object to make this"
" easier. Conceptually, ``FunctionCounts`` can be thought of as a tuple of "
"pairs with some utility methods, where each pair is `(number of "
"instructions, file path and function name)`."
msgstr ""
"然而，对于更详细的分析，我们需要查看具体的调用。``CallgrindStats.stats()`` "
"会返回``FunctionCounts``对象以方便分析。从概念上讲，``FunctionCounts``可以看作是一个带一些实用方法的键值对元组，其中每对是`（指令数，文件路径和函数名称）`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "A note on paths:"
msgstr "关于路径的说明:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One generally doesn't care about absolute path. For instance, the full path "
"and function name for a multiply call is something like:"
msgstr "通常情况下，我们并不关心绝对路径。例如，乘法调用的完整路径和函数名称类似以下内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**C++ Callgrind stats (detailed)**"
msgstr "**C++ Callgrind统计数据（详细）**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"That's still quite a lot to digest. Let's use the `FunctionCounts.transform`"
" method to trim some of the function path, and discard the function called. "
"When we do, the counts of any collisions (e.g. `foo.h:a()` and `foo.h:b()` "
"will both map to `foo.h`) will be added together."
msgstr ""
"这仍然有很多内容需要消化。我们可以使用`FunctionCounts.transform`方法修剪一些函数路径，并丢弃被调用的函数。当我们这样做时，任何碰撞的计数（例如，`foo.h:a()`"
" 和 `foo.h:b()` 都将映射到 `foo.h`）会被加起来。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Callgrind stats (condensed)**"
msgstr "**Callgrind统计数据（简化版）**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "6. A/B testing with ``Callgrind``"
msgstr "6. 使用``Callgrind``进行A/B测试"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One of the most useful features of instruction counts is they allow fine "
"grained comparison of computation, which is critical when analyzing "
"performance."
msgstr "指令计数最有用的特性之一是它们允许对计算进行细粒度的比较，这对于分析性能至关重要。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To see this in action, lets compare our multiplication of two size 128 "
"Tensors with a {128} x {1} multiplication, which will broadcast the second "
"Tensor:"
msgstr "为了实际观察这一点，让我们比较两个大小为128的张量分别与 {128} x {1} 的乘法，该操作将广播第二个张量："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "result = {a0 * b0, a1 * b0, ..., a127 * b0}"
msgstr "result = {a0 * b0, a1 * b0, ..., a127 * b0}"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Often we want to A/B test two different environments. (e.g. testing a PR, or"
" experimenting with compile flags.) This is quite simple, as "
"``CallgrindStats``, ``FunctionCounts``, and Measurement are all pickleable. "
"Simply save measurements from each environment, and load them in a single "
"process for analysis."
msgstr ""
"通常我们希望在两个不同环境中进行A/B测试。（例如测试PR，或试验编译标志。）这非常简单，因为 ``CallgrindStats``, "
"``FunctionCounts`` 和Measurement都是可序列化的。只需保存来自每个环境的测量数据，并在单个进程中加载它们以进行分析。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Instruction count delta**"
msgstr "**指令计数差异**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"So the broadcasting version takes an extra 580 instructions per call (recall"
" that we're collecting 100 runs per sample), or about 10%. There are quite a"
" few ``TensorIterator`` calls, so lets drill down to those. "
"``FunctionCounts.filter`` makes this easy."
msgstr ""
"因此广播版本每次调用多消耗额外的580条指令（请记住，我们是每个样本收集100次运行），大约多了10%。有相当多的``TensorIterator``调用，所以我们深入分析这些。``FunctionCounts.filter``让这很简单。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Instruction count delta (filter)**"
msgstr "**指令计数差异（过滤后）**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This makes plain what is going on: there is a fast path in "
"``TensorIterator`` setup, but in the {128} x {1} case we miss it and have to"
" do a more general analysis which is more expensive. The most prominent call"
" omitted by the filter is `c10::SmallVectorImpl<long>::operator=(...)`, "
"which is also part of the more general setup."
msgstr ""
"这清楚地表明发生了什么：在``TensorIterator``设置中存在快速路径，但在 {128} x {1} "
"的情况下，我们错过了这条路径，因此必须执行更为一般的分析，这更为昂贵。过滤器省略的最突出的调用是 "
"`c10::SmallVectorImpl<long>::operator=(...)`，这也是更一般设置的一部分。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "7. Wrapping up"
msgstr "7. 总结"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In summary, use `Timer.blocked_autorange` to collect wall times. If timing "
"variation is too high, increase `min_run_time`, or move to C++ snippets if "
"convenient."
msgstr ""
"总之，使用`Timer.blocked_autorange`收集壁钟时间。如果计时变化太大，请增加`min_run_time`，或者如果方便可以使用C++代码片段。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For fine grained analysis, use `Timer.collect_callgrind` to measure "
"instruction counts and `FunctionCounts.(__add__ / __sub__ / transform / "
"filter)` to slice-and-dice them."
msgstr ""
"对于细粒度分析，使用`Timer.collect_callgrind`测量指令计数，并使用`FunctionCounts.(__add__ / "
"__sub__ / transform / filter)`对它们进行切片和处理。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "8. Footnotes"
msgstr "8. 脚注"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Implied ``import torch``"
msgstr "隐含``import torch``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If `globals` does not contain \"torch\", Timer will automatically populate "
"it. This means that ``Timer(\"torch.empty(())\")`` will work. (Though other "
"imports should be placed in `setup`, e.g. ``Timer(\"np.zeros(())\", \"import"
" numpy as np\")``)"
msgstr ""
"如果`globals`中不包含“torch”，Timer会自动填充它。这意味着``Timer(\"torch.empty(())\")`` "
"可以工作。（不过其他导入应放在`setup`中，例如 ``Timer(\"np.zeros(())\", \"import numpy as "
"np\")``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``REL_WITH_DEB_INFO``"
msgstr "``REL_WITH_DEB_INFO``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In order to provide full information about the PyTorch internals which are "
"executed, ``Callgrind`` needs access to C++ debug symbols. This is "
"accomplished by setting ``REL_WITH_DEB_INFO=1`` when building PyTorch. "
"Otherwise function calls will be opaque. (The resultant ``CallgrindStats`` "
"will warn if debug symbols are missing.)"
msgstr ""
"为了提供有关执行的PyTorch内部细节完整信息，``Callgrind``需要访问C++调试符号。这通过在构建PyTorch时设置``REL_WITH_DEB_INFO=1``实现。否则函数调用将是模糊的。（生成的``CallgrindStats``会在缺少调试符号的情况下发出警告。）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: timer_quick_start.py "
"<timer_quick_start.py>`"
msgstr ":download:`下载Python源代码: timer_quick_start.py <timer_quick_start.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: timer_quick_start.ipynb "
"<timer_quick_start.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: timer_quick_start.ipynb "
"<timer_quick_start.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_tuning_guide.py>` to "
"download the full example code"
msgstr ""
"点击:ref:`这里 <sphx_glr_download_recipes_recipes_tuning_guide.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author**: `Szymon Migacz <https://github.com/szmigacz>`_"
msgstr "**作者**: `Szymon Migacz <https://github.com/szmigacz>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Performance Tuning Guide is a set of optimizations and best practices which "
"can accelerate training and inference of deep learning models in PyTorch. "
"Presented techniques often can be implemented by changing only a few lines "
"of code and can be applied to a wide range of deep learning models across "
"all domains."
msgstr ""
"性能调优指南是一套优化和最佳实践，可以加速PyTorch中深度学习模型的训练和推理。这些技术通常只需修改几行代码即可实现，并可应用于跨领域的各种深度学习模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "General optimizations"
msgstr "通用优化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Enable asynchronous data loading and augmentation"
msgstr "启用异步数据加载和数据增强"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.utils.data.DataLoader "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>`_ "
"supports asynchronous data loading and data augmentation in separate worker "
"subprocesses. The default setting for ``DataLoader`` is ``num_workers=0``, "
"which means that the data loading is synchronous and done in the main "
"process. As a result the main training process has to wait for the data to "
"be available to continue the execution."
msgstr ""
"`torch.utils.data.DataLoader "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>`_ "
"支持在单独的工作子进程中执行异步数据加载和数据增强。``DataLoader``的默认设置为``num_workers=0``，这意味着数据加载是同步的，并在主进程中完成。结果是主训练进程必须等待数据可用才能继续执行。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Setting ``num_workers > 0`` enables asynchronous data loading and overlap "
"between the training and data loading. ``num_workers`` should be tuned "
"depending on the workload, CPU, GPU, and location of training data."
msgstr ""
"设置``num_workers > "
"0``可以启用异步数据加载并允许训练与数据加载重叠。``num_workers``应根据工作负载、CPU、GPU及训练数据的位置进行调优。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``DataLoader`` accepts ``pin_memory`` argument, which defaults to ``False``."
" When using a GPU it's better to set ``pin_memory=True``, this instructs "
"``DataLoader`` to use pinned memory and enables faster and asynchronous "
"memory copy from the host to the GPU."
msgstr ""
"``DataLoader``接受``pin_memory``参数，默认为``False``。使用GPU时，最好设置``pin_memory=True``，这会指示``DataLoader``使用固定内存，并允许从主机到GPU更快且异步的内存拷贝。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Disable gradient calculation for validation or inference"
msgstr "对验证或推理禁用梯度计算"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch saves intermediate buffers from all operations which involve tensors"
" that require gradients. Typically gradients aren't needed for validation or"
" inference. `torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad>`_"
" context manager can be applied to disable gradient calculation within a "
"specified block of code, this accelerates execution and reduces the amount "
"of required memory. `torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad>`_"
" can also be used as a function decorator."
msgstr ""
"PyTorch保存了所有涉及需要梯度的张量操作的中间缓冲区。通常情况下，验证或推理不需要梯度。可以应用`torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad>`_上下文管理器禁止特定代码块内的梯度计算，这加快了执行速度并减少了所需内存量。`torch.no_grad()"
" "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad>`_也可以用作函数装饰器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Disable bias for convolutions directly followed by a batch norm"
msgstr "禁用紧随批量归一化之后卷积的偏置项"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.nn.Conv2d() "
"<https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d>`_"
" has ``bias`` parameter which defaults to ``True`` (the same is true for "
"`Conv1d "
"<https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d>`_"
" and `Conv3d "
"<https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d>`_"
" )."
msgstr ""
"`torch.nn.Conv2d() "
"<https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d>`_具有``bias``参数，默认值为``True``（`Conv1d"
" "
"<https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d>`_"
" 和 `Conv3d "
"<https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d>`_也是如此）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If a ``nn.Conv2d`` layer is directly followed by a ``nn.BatchNorm2d`` layer,"
" then the bias in the convolution is not needed, instead use "
"``nn.Conv2d(..., bias=False, ....)``. Bias is not needed because in the "
"first step ``BatchNorm`` subtracts the mean, which effectively cancels out "
"the effect of bias."
msgstr ""
"如果``nn.Conv2d``层紧随``nn.BatchNorm2d``层之后，则卷积中的偏置项是不必要的，应当使用``nn.Conv2d(..., "
"bias=False, ....)``。偏置项不是必要的，因为在第一步中``BatchNorm``减去了均值，这实际上抵消了偏置的影响。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is also applicable to 1d and 3d convolutions as long as ``BatchNorm`` "
"(or other normalization layer) normalizes on the same dimension as "
"convolution's bias."
msgstr "当``BatchNorm``（或其他归一化层）在卷积的同一维度上进行归一化时，这同样适用于一维和三维卷积。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Models available from `torchvision <https://github.com/pytorch/vision>`_ "
"already implement this optimization."
msgstr "来自`torchvision <https://github.com/pytorch/vision>`_的模型已经实施了此优化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Use parameter.grad = None instead of model.zero_grad() or "
"optimizer.zero_grad()"
msgstr "使用parameter.grad = None代替model.zero_grad()或optimizer.zero_grad()"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Instead of calling:"
msgstr "代替以下调用："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "to zero out gradients, use the following method instead:"
msgstr "为了清除梯度，请使用以下方法："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The second code snippet does not zero the memory of each individual "
"parameter, also the subsequent backward pass uses assignment instead of "
"addition to store gradients, this reduces the number of memory operations."
msgstr "第二种代码片段不会清零每个单独参数的内存，此外后续的反向传播使用赋值而不是加法来存储梯度，这减少了内存操作次数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Setting gradient to ``None`` has a slightly different numerical behavior "
"than setting it to zero, for more details refer to the `documentation "
"<https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad>`_."
msgstr ""
"将梯度设置为``None``与将梯度设置为零的数值行为略有不同，详细信息请参阅`文档 "
"<https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Alternatively, starting from PyTorch 1.7, call ``model`` or "
"``optimizer.zero_grad(set_to_none=True)``."
msgstr ""
"或者，从PyTorch 1.7开始，可以调用``model``或``optimizer.zero_grad(set_to_none=True)``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Fuse operations"
msgstr "融合操作"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Pointwise operations such as elementwise addition, multiplication, and math "
"functions like `sin()`, `cos()`, `sigmoid()`, etc., can be combined into a "
"single kernel. This fusion helps reduce memory access and kernel launch "
"times. Typically, pointwise operations are memory-bound; PyTorch eager-mode "
"initiates a separate kernel for each operation, which involves loading data "
"from memory, executing the operation (often not the most time-consuming "
"step), and writing the results back to memory."
msgstr ""
"逐点操作（例如逐元素加法、乘法以及数学函数如`sin()`、`cos()`、`sigmoid()`等）可以合并到单个内核中。这种融合有助于减少内存访问和内核启动时间。通常情况下，逐点操作是内存受限的；PyTorch即时模式为每个操作启动单独的内核，这涉及从内存加载数据、执行操作（通常不是最耗时的步骤）以及将结果写回内存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By using a fused operator, only one kernel is launched for multiple "
"pointwise operations, and data is loaded and stored just once. This "
"efficiency is particularly beneficial for activation functions, optimizers, "
"and custom RNN cells etc."
msgstr "通过使用融合操作符，多个逐点操作仅需启动一个内核，数据仅加载和存储一次。这种效率对于激活函数、优化器和自定义RNN单元等特别有益。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch 2 introduces a compile-mode facilitated by TorchInductor, an "
"underlying compiler that automatically fuses kernels. TorchInductor extends "
"its capabilities beyond simple element-wise operations, enabling advanced "
"fusion of eligible pointwise and reduction operations for optimized "
"performance."
msgstr ""
"PyTorch "
"2引入了由TorchInductor支持的编译模式，一个底层编译器自动融合内核。TorchInductor不仅扩展了简单逐元素操作的能力，还实现了对合格逐元素和化简操作的高级融合以优化性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the simplest case fusion can be enabled by applying `torch.compile "
"<https://pytorch.org/docs/stable/generated/torch.compile.html>`_ decorator "
"to the function definition, for example:"
msgstr ""
"在最简单的情况下，可以通过应用`torch.compile "
"<https://pytorch.org/docs/stable/generated/torch.compile.html>`_装饰器到函数定义来启用融合，例如："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Refer to `Introduction to torch.compile "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_ "
"for more advanced use cases."
msgstr ""
"有关更高级的使用案例，请参阅`Introduction to torch.compile "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Enable channels_last memory format for computer vision models"
msgstr "为计算机视觉模型启用channels_last内存格式"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch 1.5 introduced support for ``channels_last`` memory format for "
"convolutional networks. This format is meant to be used in conjunction with "
"`AMP <https://pytorch.org/docs/stable/amp.html>`_ to further accelerate "
"convolutional neural networks with `Tensor Cores <https://www.nvidia.com/en-"
"us/data-center/tensor-cores/>`_."
msgstr ""
"PyTorch 1.5为卷积网络引入了对``channels_last``内存格式的支持。此格式需配合`AMP "
"<https://pytorch.org/docs/stable/amp.html>`_一起使用以进一步加速具有`Tensor Cores "
"<https://www.nvidia.com/en-us/data-center/tensor-cores/>`_的卷积神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Support for ``channels_last`` is experimental, but it's expected to work for"
" standard computer vision models (e.g. ResNet-50, SSD). To convert models to"
" ``channels_last`` format follow `Channels Last Memory Format Tutorial "
"<https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html>`_. "
"The tutorial includes a section on `converting existing models "
"<https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#converting-"
"existing-models>`_."
msgstr ""
"对``channels_last``的支持是实验性的，但预计可以在标准计算机视觉模型（例如 "
"ResNet-50、SSD）中正常工作。要将模型转换为``channels_last``格式，请参阅`Channels Last Memory "
"Format Tutorial "
"<https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html>`_。教程中包括一节关于`转换现有模型"
" "
"<https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#converting-"
"existing-models>`_的内容。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Checkpoint intermediate buffers"
msgstr "检查点中间缓冲区"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Buffer checkpointing is a technique to mitigate the memory capacity burden "
"of model training. Instead of storing inputs of all layers to compute "
"upstream gradients in backward propagation, it stores the inputs of a few "
"layers and the others are recomputed during backward pass. The reduced "
"memory requirements enables increasing the batch size that can improve "
"utilization."
msgstr ""
"缓冲区检查点是一种减轻模型训练内存容量负担的技术。与其存储所有层的输入以便在反向传播中计算上游梯度，它只存储少数几层的输入，其余部分在反向传播时重新计算。减小的内存需求允许增加可以提高利用率的批量大小。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Checkpointing targets should be selected carefully. The best is not to store"
" large layer outputs that have small re-computation cost. The example target"
" layers are activation functions (e.g. ``ReLU``, ``Sigmoid``, ``Tanh``), "
"up/down sampling and matrix-vector operations with small accumulation depth."
msgstr ""
"检查点目标应谨慎选择。最佳做法是不存储具有小重新计算成本的大型层输出。例如目标层包括激活函数（例如``ReLU``、``Sigmoid``、``Tanh``），上下采样和积累深度较小的矩阵向量运算。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch supports a native `torch.utils.checkpoint "
"<https://pytorch.org/docs/stable/checkpoint.html>`_ API to automatically "
"perform checkpointing and recomputation."
msgstr ""
"PyTorch 提供支持自动执行检查点和重新计算的原生`torch.utils.checkpoint "
"<https://pytorch.org/docs/stable/checkpoint.html>`_ API。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Disable debugging APIs"
msgstr "禁用调试相关 API"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Many PyTorch APIs are intended for debugging and should be disabled for "
"regular training runs:"
msgstr "许多 PyTorch API 是用于调试的，应禁用这些 API以进行常规训练:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"anomaly detection: `torch.autograd.detect_anomaly "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly>`_"
" or `torch.autograd.set_detect_anomaly(True) "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly>`_"
msgstr ""
"异常检测: `torch.autograd.detect_anomaly "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly>`_或`torch.autograd.set_detect_anomaly(True)"
" "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"profiler related: `torch.autograd.profiler.emit_nvtx "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx>`_,"
" `torch.autograd.profiler.profile "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile>`_"
msgstr ""
"性能分析相关: `torch.autograd.profiler.emit_nvtx "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx>`_，`torch.autograd.profiler.profile"
" "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"autograd ``gradcheck``: `torch.autograd.gradcheck "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck>`_ "
"or `torch.autograd.gradgradcheck "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck>`_"
msgstr ""
"自动梯度``gradcheck``: `torch.autograd.gradcheck "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck>`_ 或"
" `torch.autograd.gradgradcheck "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "CPU specific optimizations"
msgstr "CPU 特定优化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Utilize Non-Uniform Memory Access (NUMA) Controls"
msgstr "利用非统一内存访问（NUMA）控制"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"NUMA or non-uniform memory access is a memory layout design used in data "
"center machines meant to take advantage of locality of memory in multi-"
"socket machines with multiple memory controllers and blocks. Generally "
"speaking, all deep learning workloads, training or inference, get better "
"performance without accessing hardware resources across NUMA nodes. Thus, "
"inference can be run with multiple instances, each instance runs on one "
"socket, to raise throughput. For training tasks on single node, distributed "
"training is recommended to make each training process run on one socket."
msgstr ""
"NUMA "
"或非统一内存访问是一种内存布局设计，用于数据中心机器以利用多插槽机器中的本地内存，在这些机器中有多个内存控制器和块。一般来说，所有深度学习工作负载，无论是训练还是推断，都能在不跨"
" NUMA "
"节点访问硬件资源时获得更佳性能。因此，可运行多个实例，每个实例在一个插座上运行，以提升吞吐量。对于单节点上的训练任务，推荐进行分布式训练使每个训练进程运行在一个插座上。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In general cases the following command executes a PyTorch script on cores on"
" the Nth node only, and avoids cross-socket memory access to reduce memory "
"access overhead."
msgstr "在一般情况下，以下命令在仅 N 号节点上的核心上执行 PyTorch 脚本，并避免跨插座内存访问以减少内存访问开销。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"More detailed descriptions can be found `here "
"<https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html>`_."
msgstr ""
"更详细的描述可见`此处 <https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Utilize OpenMP"
msgstr "利用 OpenMP"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"OpenMP is utilized to bring better performance for parallel computation "
"tasks. ``OMP_NUM_THREADS`` is the easiest switch that can be used to "
"accelerate computations. It determines number of threads used for OpenMP "
"computations. CPU affinity setting controls how workloads are distributed "
"over multiple cores. It affects communication overhead, cache line "
"invalidation overhead, or page thrashing, thus proper setting of CPU "
"affinity brings performance benefits. ``GOMP_CPU_AFFINITY`` or "
"``KMP_AFFINITY`` determines how to bind OpenMP* threads to physical "
"processing units. Detailed information can be found `here "
"<https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html>`_."
msgstr ""
"OpenMP 用于提升并行计算任务的性能。``OMP_NUM_THREADS`` 是用于加速计算最简单的开关。它决定了用于 OpenMP "
"计算的线程数。CPU 亲和性设置控制任务如何分配到多个核心上。它影响通信开销、缓存行失效开销或页面抖动，因此正确设置 CPU "
"亲和性带来性能提升。``GOMP_CPU_AFFINITY`` 或 ``KMP_AFFINITY`` 决定如何将 OpenMP "
"*线程绑定到物理处理单元。详细信息可见`此处 <https://intel.github.io/intel-extension-for-"
"pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "With the following command, PyTorch run the task on N OpenMP threads."
msgstr "使用以下命令，PyTorch 在 N 个 OpenMP 线程上运行任务。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Typically, the following environment variables are used to set for CPU "
"affinity with GNU OpenMP implementation. ``OMP_PROC_BIND`` specifies whether"
" threads may be moved between processors. Setting it to CLOSE keeps OpenMP "
"threads close to the primary thread in contiguous place partitions. "
"``OMP_SCHEDULE`` determines how OpenMP threads are scheduled. "
"``GOMP_CPU_AFFINITY`` binds threads to specific CPUs. An important tuning "
"parameter is core pinning which prevent the threads of migrating between "
"multiple CPUs, enhancing data location and minimizing inter core "
"communication."
msgstr ""
"通常情况下，以下环境变量用于通过 GNU OpenMP 实现设置 CPU 亲和性。``OMP_PROC_BIND`` "
"指定线程是否可以在处理器之间移动。设置 CLOSE 会使 OpenMP 线程接近主线程并在连续的分区内运行。``OMP_SCHEDULE`` 决定 "
"OpenMP 线程的调度方式。``GOMP_CPU_AFFINITY`` 将线程绑定到特定 CPU。一个重要的优化参数是核心固定，它可以防止线程在多个 "
"CPU 之间迁移，从而优化数据位置并最小化内核间的通信。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Intel OpenMP Runtime Library (``libiomp``)"
msgstr "Intel OpenMP Runtime Library（``libiomp``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default, PyTorch uses GNU OpenMP (GNU ``libgomp``) for parallel "
"computation. On Intel platforms, Intel OpenMP Runtime Library (``libiomp``) "
"provides OpenMP API specification support. It sometimes brings more "
"performance benefits compared to ``libgomp``. Utilizing environment variable"
" ``LD_PRELOAD`` can switch OpenMP library to ``libiomp``:"
msgstr ""
"默认情况下，PyTorch 使用 GNU OpenMP（GNU ``libgomp``）进行并行计算。在 Intel 平台上，Intel OpenMP "
"Runtime Library（``libiomp``）提供 OpenMP API 规范支持。有时，它比 ``libgomp`` "
"带来更大的性能优势。利用环境变量``LD_PRELOAD``可以将 OpenMP 库切换到 ``libiomp``:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Similar to CPU affinity settings in GNU OpenMP, environment variables are "
"provided in ``libiomp`` to control CPU affinity settings. ``KMP_AFFINITY`` "
"binds OpenMP threads to physical processing units. ``KMP_BLOCKTIME`` sets "
"the time, in milliseconds, that a thread should wait, after completing the "
"execution of a parallel region, before sleeping. In most cases, setting "
"``KMP_BLOCKTIME`` to 1 or 0 yields good performances. The following commands"
" show a common settings with Intel OpenMP Runtime Library."
msgstr ""
"类似于 GNU OpenMP 中的 CPU 亲和性设置，在 ``libiomp`` 中提供环境变量以控制 CPU "
"亲和性设置。``KMP_AFFINITY`` 将 OpenMP 线程绑定到物理处理单元。``KMP_BLOCKTIME`` "
"设置线程在完成并行区执行后等待进入休眠的时间（以毫秒为单位）。在大多数情况下，将 ``KMP_BLOCKTIME`` 设置为 1 或 0 "
"会产生良好的性能。以下命令显示了使用 Intel OpenMP Runtime Library 的常见设置。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Switch Memory allocator"
msgstr "切换内存分配器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For deep learning workloads, ``Jemalloc`` or ``TCMalloc`` can get better "
"performance by reusing memory as much as possible than default ``malloc`` "
"function. `Jemalloc <https://github.com/jemalloc/jemalloc>`_ is a general "
"purpose ``malloc`` implementation that emphasizes fragmentation avoidance "
"and scalable concurrency support. `TCMalloc "
"<https://google.github.io/tcmalloc/overview.html>`_ also features a couple "
"of optimizations to speed up program executions. One of them is holding "
"memory in caches to speed up access of commonly-used objects. Holding such "
"caches even after deallocation also helps avoid costly system calls if such "
"memory is later re-allocated. Use environment variable ``LD_PRELOAD`` to "
"take advantage of one of them."
msgstr ""
"对于深度学习工作负载，``Jemalloc`` 或 ``TCMalloc`` 比默认的 ``malloc`` "
"函数通过尽可能多地重用内存能获得更好的性能。`Jemalloc <https://github.com/jemalloc/jemalloc>`_ "
"是一种通用的``malloc``实现，强调避免内存碎片和支持可扩展的并发性。`TCMalloc "
"<https://google.github.io/tcmalloc/overview.html>`_ "
"也包含一些优化以加速程序执行，其中之一是将内存保存在缓存中以加快访问常用对象。即使在释放后保留这些缓存也有助于避免昂贵的系统调用，如果此类内存随后被重新分配。使用环境变量"
" ``LD_PRELOAD`` 可以利用其中之一。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Use oneDNN Graph with TorchScript for inference"
msgstr "使用 TorchScript 一起搭配 oneDNN Graph 进行推断"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"oneDNN Graph can significantly boost inference performance. It fuses some "
"compute-intensive operations such as convolution, matmul with their neighbor"
" operations. In PyTorch 2.0, it is supported as a beta feature for "
"``Float32`` & ``BFloat16`` data-types. oneDNN Graph receives the model’s "
"graph and identifies candidates for operator-fusion with respect to the "
"shape of the example input. A model should be JIT-traced using an example "
"input. Speed-up would then be observed after a couple of warm-up iterations "
"for inputs with the same shape as the example input. The example code-"
"snippets below are for resnet50, but they can very well be extended to use "
"oneDNN Graph with custom models as well."
msgstr ""
"oneDNN Graph 可以显著提高推断性能。它将一些计算密集型操作（例如卷积、矩阵乘法）与其周围的操作进行融合。在 PyTorch 2.0 "
"中，它作为一种Beta功能支持 ``Float32`` 和 ``BFloat16`` 数据类型。oneDNN Graph "
"接收模型的图表，并根据示例输入的形状确认可进行操作融合的候选项。模型应使用示例输入进行 JIT "
"跟踪。在为输入进行几次预热迭代后，输入的形状与示例输入相同时，速度提升将是可见的。以下代码片段是针对 resnet50 的示例，但也可以扩展为使用 "
"oneDNN Graph 和自定义模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Using the oneDNN Graph API requires just one extra line of code for "
"inference with Float32. If you are using oneDNN Graph, please avoid calling "
"``torch.jit.optimize_for_inference``."
msgstr ""
"使用 oneDNN Graph API 进行 Float32 推断仅需额外添加一行代码。如果使用 oneDNN Graph，请避免调用 "
"``torch.jit.optimize_for_inference``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Once a model is JIT-traced with a sample input, it can then be used for "
"inference after a couple of warm-up runs."
msgstr "一旦模型使用示例输入进行 JIT 跟踪，它就可以在几次预热运行之后用于推断。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"While the JIT fuser for oneDNN Graph also supports inference with "
"``BFloat16`` datatype, performance benefit with oneDNN Graph is only "
"exhibited by machines with AVX512_BF16 instruction set architecture (ISA). "
"The following code snippets serves as an example of using ``BFloat16`` "
"datatype for inference with oneDNN Graph:"
msgstr ""
"虽然 oneDNN Graph 的 JIT 融合器也支持使用 ``BFloat16`` 数据类型进行推断，但只有具有 AVX512_BF16 "
"指令集架构（ISA）的机器才能体现 oneDNN Graph 的性能优势。以下代码片段是一个使用``BFloat16``数据类型进行推断的一例:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Train a model on CPU with PyTorch ``DistributedDataParallel``(DDP) "
"functionality"
msgstr "使用 PyTorch ``DistributedDataParallel``(DDP) 功能在 CPU 上训练模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For small scale models or memory-bound models, such as DLRM, training on CPU"
" is also a good choice. On a machine with multiple sockets, distributed "
"training brings a high-efficient hardware resource usage to accelerate the "
"training process. `Torch-ccl <https://github.com/intel/torch-ccl>`_, "
"optimized with Intel(R) ``oneCCL`` (collective communications library) for "
"efficient distributed deep learning training implementing such collectives "
"like ``allreduce``, ``allgather``, ``alltoall``, implements PyTorch C10D "
"``ProcessGroup`` API and can be dynamically loaded as external "
"``ProcessGroup``. Upon optimizations implemented in PyTorch DDP module, "
"``torch-ccl`` accelerates communication operations. Beside the optimizations"
" made to communication kernels, ``torch-ccl`` also features simultaneous "
"computation-communication functionality."
msgstr ""
"对于小规模模型或受内存限制的模型，例如 DLRM，在 CPU "
"上训练也是一个很好的选择。在具有多个插座的机器上，分布式训练通过高效使用硬件资源加速训练过程。`Torch-ccl "
"<https://github.com/intel/torch-ccl>`_，使用 Intel(R) "
"``oneCCL``（集体通信库）优化，用于高效的分布式深度学习训练，实施了诸如 "
"``allreduce``、``allgather``、``alltoall`` 等集合操作，实施了 PyTorch C10D "
"``ProcessGroup`` API，并且可以动态加载为外部``ProcessGroup``。通过 PyTorch DDP "
"模块实现的优化，``torch-ccl``加速了通信操作。除了为通信内核做出的优化，``torch-ccl``还具有同时计算与通信的功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "GPU specific optimizations"
msgstr "GPU 特定优化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Enable Tensor cores"
msgstr "启用 Tensor cores"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Tensor cores are specialized hardware designed to compute matrix-matrix "
"multiplication operations, primarily utilized in deep learning and AI "
"workloads. Tensor cores have specific precision requirements which can be "
"adjusted manually or via the Automatic Mixed Precision API."
msgstr ""
"Tensor cores 是设计用于计算矩阵乘法操作的专用硬件，主要用于深度学习和 AI 工作负载。Tensor cores "
"有特定的精度要求，这些精度可以手动调整或通过自动混合精度 API 控制。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In particular, tensor operations take advantage of lower precision "
"workloads. Which can be controlled via "
"``torch.set_float32_matmul_precision``. The default format is set to "
"'highest,' which utilizes the tensor data type. However, PyTorch offers "
"alternative precision settings: 'high' and 'medium.' These options "
"prioritize computational speed over numerical precision.\""
msgstr ""
"尤其是，Tensor "
"操作利用了较低精度的工作负载。这可以通过``torch.set_float32_matmul_precision``控制。默认设置为'highest'，优先利用"
" Tensor 数据类型。然而，PyTorch 提供了备用的精度设置：'high' 和 'medium'。这些选项优先考虑计算速度而非数值精度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Use CUDA Graphs"
msgstr "使用 CUDA Graphs"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"At the time of using a GPU, work first must be launched from the CPU and in "
"some cases the context switch between CPU and GPU can lead to bad resource "
"utilization. CUDA graphs are a way to keep computation within the GPU "
"without paying the extra cost of kernel launches and host synchronization."
msgstr ""
"在使用 GPU 时，任务必须首先由 CPU 启动，有时 CPU 和 GPU 之间的上下文切换可能导致资源利用率较差。CUDA graphs "
"是一种将计算保持在 GPU 内而无需支付额外的内核启动和主机同步成本的方法。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Support for CUDA graph is in development, and its usage can incur in "
"increased device memory consumption and some models might not compile."
msgstr "对 CUDA graphs 的支持正在开发中，它的使用可能会导致设备的内存使用量增加，并非所有模型都能成功编译。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Enable cuDNN auto-tuner"
msgstr "启用 cuDNN 自动调优器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`NVIDIA cuDNN <https://developer.nvidia.com/cudnn>`_ supports many "
"algorithms to compute a convolution. Autotuner runs a short benchmark and "
"selects the kernel with the best performance on a given hardware for a given"
" input size."
msgstr ""
"`NVIDIA cuDNN <https://developer.nvidia.com/cudnn>`_ "
"支持许多算法以计算卷积。自动调优器运行短暂的基准测试，并为指定输入大小的指定硬件选择性能最佳的内核。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For convolutional networks (other types currently not supported), enable "
"cuDNN autotuner before launching the training loop by setting:"
msgstr "对于卷积网络（目前不支持其他类型），在启动训练循环前通过设置启用 cuDNN 自动调优器："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"the auto-tuner decisions may be non-deterministic; different algorithm may "
"be selected for different runs.  For more details see `PyTorch: "
"Reproducibility "
"<https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism>`_"
msgstr ""
"自动调优器的决策可能是非确定性的：不同的运行可能会选择不同的算法。有关更多详细信息，请参阅`PyTorch: Reproducibility "
"<https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"in some rare cases, such as with highly variable input sizes,  it's better "
"to run convolutional networks with autotuner disabled to avoid the overhead "
"associated with algorithm selection for each input size."
msgstr "在一些罕见的情况下，例如输入尺寸高度可变的情况下，关闭自动调优器运行卷积网络可能更好，因为可以避免为每个输入尺寸选择算法的相关开销。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Avoid unnecessary CPU-GPU synchronization"
msgstr "避免不必要的CPU-GPU同步"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Avoid unnecessary synchronizations, to let the CPU run ahead of the "
"accelerator as much as possible to make sure that the accelerator work queue"
" contains many operations."
msgstr "避免不必要的同步操作，以尽可能让CPU在加速器之前运行，确保加速器的工作队列包含多个操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When possible, avoid operations which require synchronizations, for example:"
msgstr "尽可能避免需要同步的操作，例如："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``print(cuda_tensor)``"
msgstr "``print(cuda_tensor)``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``cuda_tensor.item()``"
msgstr "``cuda_tensor.item()``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"memory copies: ``tensor.cuda()``,  ``cuda_tensor.cpu()`` and equivalent "
"``tensor.to(device)`` calls"
msgstr ""
"内存拷贝：``tensor.cuda()``、``cuda_tensor.cpu()``及等效的``tensor.to(device)``调用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``cuda_tensor.nonzero()``"
msgstr "``cuda_tensor.nonzero()``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"python control flow which depends on results of operations performed on CUDA"
" tensors e.g. ``if (cuda_tensor != 0).all()``"
msgstr "取决于CUDA张量操作结果的Python控制流程，例如``if (cuda_tensor != 0).all()``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Create tensors directly on the target device"
msgstr "直接在目标设备上创建张量"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Instead of calling ``torch.rand(size).cuda()`` to generate a random tensor, "
"produce the output directly on the target device: ``torch.rand(size, "
"device='cuda')``."
msgstr ""
"不要使用``torch.rand(size).cuda()``生成随机张量，而是在目标设备上直接生成输出：``torch.rand(size, "
"device='cuda')``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is applicable to all functions which create new tensors and accept "
"``device`` argument: `torch.rand() "
"<https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand>`_, "
"`torch.zeros() "
"<https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros>`_, "
"`torch.full() "
"<https://pytorch.org/docs/stable/generated/torch.full.html#torch.full>`_ and"
" similar."
msgstr ""
"这适用于所有创建新张量并接受``device``参数的函数：`torch.rand() "
"<https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand>`_、`torch.zeros()"
" "
"<https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros>`_、`torch.full()"
" "
"<https://pytorch.org/docs/stable/generated/torch.full.html#torch.full>`_等类似函数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Use mixed precision and AMP"
msgstr "使用混合精度和AMP"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Mixed precision leverages `Tensor Cores <https://www.nvidia.com/en-us/data-"
"center/tensor-cores/>`_ and offers up to 3x overall speedup on Volta and "
"newer GPU architectures. To use Tensor Cores AMP should be enabled and "
"matrix/tensor dimensions should satisfy requirements for calling kernels "
"that use Tensor Cores."
msgstr ""
"混合精度利用了`张量核心 <https://www.nvidia.com/en-us/data-center/tensor-"
"cores/>`_，在Volta及更新的GPU架构上提供最多3倍的整体加速。要使用张量核心，需启用AMP，并满足使用张量核心内核调用的矩阵/张量维度要求。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To use Tensor Cores:"
msgstr "使用张量核心："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)"
msgstr "将尺寸设置为8的倍数（以适应张量核心的维度）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"see `Deep Learning Performance Documentation "
"<https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-"
"performance>`_ for more details and guidelines specific to layer type"
msgstr ""
"查看 `深度学习性能文档 "
"<https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-"
"performance>`_ 的更多详情及特定层类型的指南"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"if layer size is derived from other parameters rather than fixed, it can "
"still be explicitly padded e.g. vocabulary size in NLP models"
msgstr "如果层尺寸是由其他参数导出而非固定值，它仍然可以明确填充，例如NLP模型中的词汇表大小"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "enable AMP"
msgstr "启用AMP"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Introduction to Mixed Precision Training and AMP: `video "
"<https://www.youtube.com/watch?v=jF4-_ZK_tyc&feature=youtu.be>`_, `slides "
"<https://nvlabs.github.io/eccv2020-mixed-precision-"
"tutorial/files/dusan_stosic-training-neural-networks-with-tensor-"
"cores.pdf>`_"
msgstr ""
"混合精度训练和AMP简介：`视频 "
"<https://www.youtube.com/watch?v=jF4-_ZK_tyc&feature=youtu.be>`_、`幻灯片 "
"<https://nvlabs.github.io/eccv2020-mixed-precision-"
"tutorial/files/dusan_stosic-training-neural-networks-with-tensor-"
"cores.pdf>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"native PyTorch AMP is available starting from PyTorch 1.6: `documentation "
"<https://pytorch.org/docs/stable/amp.html>`_, `examples "
"<https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples>`_, "
"`tutorial <https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html>`_"
msgstr ""
"原生PyTorch的AMP从1.6版开始可用：`文档 <https://pytorch.org/docs/stable/amp.html>`_、`示例 "
"<https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples>`_、`教程"
" <https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Preallocate memory in case of variable input length"
msgstr "在输入长度可变的情况下预分配内存"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Models for speech recognition or for NLP are often trained on input tensors "
"with variable sequence length. Variable length can be problematic for "
"PyTorch caching allocator and can lead to reduced performance or to "
"unexpected out-of-memory errors. If a batch with a short sequence length is "
"followed by an another batch with longer sequence length, then PyTorch is "
"forced to release intermediate buffers from previous iteration and to re-"
"allocate new buffers. This process is time consuming and causes "
"fragmentation in the caching allocator which may result in out-of-memory "
"errors."
msgstr ""
"用于语音识别或NLP的模型通常在输入张量上具有可变的序列长度。可变长度可能会对PyTorch缓存分配器造成问题，导致性能下降或出现意外的内存不足错误。如果一个包含短序列长度的批次后紧跟一个包含长序列长度的批次，PyTorch会被迫释放之前迭代的中间缓冲区并重新分配新缓冲区。这一过程耗时，并会造成缓存分配器的碎片化，可能导致内存不足错误。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A typical solution is to implement preallocation. It consists of the "
"following steps:"
msgstr "一个典型的解决方案是实现预分配。它包括以下步骤："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"generate a (usually random) batch of inputs with maximum sequence length "
"(either corresponding to max length in the training dataset or to some "
"predefined threshold)"
msgstr "生成一个（通常为随机的）最大序列长度的输入批次（对应于训练数据集的最大长度或某个预定义阈值）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"execute a forward and a backward pass with the generated batch, do not "
"execute an optimizer or a learning rate scheduler, this step preallocates "
"buffers of maximum size, which can be reused in subsequent training "
"iterations"
msgstr "使用生成的批次执行正向和反向传播，不执行优化器或学习率调度器，此步骤会预分配最大尺寸的缓冲区，可以在后续训练迭代中重复使用"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "zero out gradients"
msgstr "清零梯度"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "proceed to regular training"
msgstr "继续正常的训练"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Distributed optimizations"
msgstr "分布式优化"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Use efficient data-parallel backend"
msgstr "使用高效的数据并行后端"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch has two ways to implement data-parallel training:"
msgstr "PyTorch有两种实现数据并行训练的方式："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.nn.DataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel>`_"
msgstr ""
"`torch.nn.DataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.nn.parallel.DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_"
msgstr ""
"`torch.nn.parallel.DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``DistributedDataParallel`` offers much better performance and scaling to "
"multiple-GPUs. For more information refer to the `relevant section of CUDA "
"Best Practices <https://pytorch.org/docs/stable/notes/cuda.html#use-nn-"
"parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-"
"dataparallel>`_ from PyTorch documentation."
msgstr ""
"``DistributedDataParallel``提供了更好的性能和多GPU扩展性。要了解更多信息，请参考PyTorch文档的`CUDA最佳实践相关部分"
" <https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-"
"distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Skip unnecessary all-reduce if training with ``DistributedDataParallel`` and"
" gradient accumulation"
msgstr "如果使用``DistributedDataParallel``训练并进行梯度累积，跳过不必要的全归约操作"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default `torch.nn.parallel.DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_"
" executes gradient all-reduce after every backward pass to compute the "
"average gradient over all workers participating in the training. If training"
" uses gradient accumulation over N steps, then all-reduce is not necessary "
"after every training step, it's only required to perform all-reduce after "
"the last call to backward, just before the execution of the optimizer."
msgstr ""
"默认情况下，`torch.nn.parallel.DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_"
" "
"在每次反向传播后执行梯度全归约，以计算参与训练的所有工作者的平均梯度。如果训练使用了N步的梯度累积，那么不需要每个训练步后都执行全归约，只需在最后一次调用反向传播后，优化器执行之前进行全归约即可。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``DistributedDataParallel`` provides `no_sync() "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync>`_"
" context manager which disables gradient all-reduce for particular "
"iteration. ``no_sync()`` should be applied to first ``N-1`` iterations of "
"gradient accumulation, the last iteration should follow the default "
"execution and perform the required gradient all-reduce."
msgstr ""
"``DistributedDataParallel``提供了`no_sync() "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync>`_上下文管理器，可禁用特定迭代中的梯度全归约。``no_sync()``应应用于梯度累积的前``N-1``次迭代，最后一次迭代应按照默认执行并完成所需的梯度全归约。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Match the order of layers in constructors and during the execution if using "
"``DistributedDataParallel(find_unused_parameters=True)``"
msgstr ""
"如果使用``DistributedDataParallel(find_unused_parameters=True)``，确保构造器中层的顺序与执行期间的顺序一致"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.nn.parallel.DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_"
" with ``find_unused_parameters=True`` uses the order of layers and "
"parameters from model constructors to build buckets for "
"``DistributedDataParallel`` gradient all-reduce. ``DistributedDataParallel``"
" overlaps all-reduce with the backward pass. All-reduce for a particular "
"bucket is asynchronously triggered only when all gradients for parameters in"
" a given bucket are available."
msgstr ""
"`torch.nn.parallel.DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_结合``find_unused_parameters=True``使用模型构造器中的层和参数顺序来构建用于``DistributedDataParallel``梯度全归约的桶。``DistributedDataParallel``将全归约与反向传播重叠。某个桶的全归约只能在该桶内的所有参数梯度均可用时异步触发。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To maximize the amount of overlap, the order in model constructors should "
"roughly match the order during the execution. If the order doesn't match, "
"then all-reduce for the entire bucket waits for the gradient which is the "
"last to arrive, this may reduce the overlap between backward pass and all-"
"reduce, all-reduce may end up being exposed, which slows down the training."
msgstr ""
"为了最大化重叠量，模型构造器中的顺序应大致匹配执行期间的顺序。如果顺序不匹配，则整个桶的全归约会等待最后到达的梯度，这可能会减少反向传播和全归约之间的重叠，全归约可能会暴露出来，从而降低训练速度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``DistributedDataParallel`` with ``find_unused_parameters=False`` (which is "
"the default setting) relies on automatic bucket formation based on order of "
"operations encountered during the backward pass. With "
"``find_unused_parameters=False`` it's not necessary to reorder layers or "
"parameters to achieve optimal performance."
msgstr ""
"``DistributedDataParallel``结合``find_unused_parameters=False``（默认设置）依赖于基于反向传播过程中遇到的操作顺序的自动桶构建。对于``find_unused_parameters=False``，无需重新排序层或参数即可实现最佳性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Load-balance workload in a distributed setting"
msgstr "在分布式环境中均衡负载"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Load imbalance typically may happen for models processing sequential data "
"(speech recognition, translation, language models etc.). If one device "
"receives a batch of data with sequence length longer than sequence lengths "
"for the remaining devices, then all devices wait for the worker which "
"finishes last. Backward pass functions as an implicit synchronization point "
"in a distributed setting with `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_"
" backend."
msgstr ""
"负载不均衡通常会发生在处理序列数据（语音识别、翻译、语言模型等）的模型中。如果一个设备接收到的数据批次的序列长度超过其他设备的序列长度，则所有设备需要等待最后完成工作的工作者。在分布式环境中以`DistributedDataParallel"
" "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel>`_作为后端时，反向传播函数作为隐式同步点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are multiple ways to solve the load balancing problem. The core idea "
"is to distribute workload over all workers as uniformly as possible within "
"each global batch. For example Transformer solves imbalance by forming "
"batches with approximately constant number of tokens (and variable number of"
" sequences in a batch), other models solve imbalance by bucketing samples "
"with similar sequence length or even by sorting dataset by sequence length."
msgstr ""
"有多种方法可以解决负载均衡问题。核心思想是在每个全局批次内尽可能均匀地分配工作负载。例如Transformer通过形成包含大约恒定数量的标记的批次（且批次中的序列数量可变）来解决不均衡问题，其他模型通过对具有相似序列长度的样本进行分桶，甚至通过按序列长度对数据集排序来解决不均衡问题。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: tuning_guide.py <tuning_guide.py>`"
msgstr ":download:`下载Python源代码：tuning_guide.py <tuning_guide.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: tuning_guide.ipynb "
"<tuning_guide.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook：tuning_guide.ipynb <tuning_guide.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_warmstarting_model_using_parameters_from_a_different_model.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_recipes_recipes_warmstarting_model_using_parameters_from_a_different_model.py>`"
" 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Partially loading a model or loading a partial model are common scenarios "
"when transfer learning or training a new complex model. Leveraging trained "
"parameters, even if only a few are usable, will help to warmstart the "
"training process and hopefully help your model converge much faster than "
"training from scratch."
msgstr ""
"部分加载模型或加载部分模型是迁移学习或训练新复杂模型时的常见场景。利用已训练的参数，即使只有少量可用，也将有助于为训练过程提供热启动，并有望帮助您的模型比从头开始训练更快地收敛。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Whether you are loading from a partial ``state_dict``, which is missing some"
" keys, or loading a ``state_dict`` with more keys than the model that you "
"are loading into, you can set the strict argument to ``False`` in the "
"``load_state_dict()`` function to ignore non-matching keys. In this recipe, "
"we will experiment with warmstarting a model using parameters of a different"
" model."
msgstr ""
"无论您是从一个缺少某些键的部分``state_dict``加载，还是从包含更多键的``state_dict``加载到另一个模型，您可以在``load_state_dict()``函数中将strict参数设置为``False``以忽略不匹配的键。在本教程中，我们将尝试使用不同模型的参数对模型进行热启动。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Define and initialize the neural network A and B"
msgstr "定义并初始化神经网络A和B"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Save model A"
msgstr "保存模型A"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Load into model B"
msgstr "加载到模型B"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For this recipe, we will use ``torch`` and its subsidiaries ``torch.nn`` and"
" ``torch.optim``."
msgstr "在本教程中，我们将使用``torch``及其组件``torch.nn``和``torch.optim``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Define and initialize the neural network A and B"
msgstr "2. 定义并初始化神经网络A和B"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For sake of example, we will create a neural network for training images. To"
" learn more see the Defining a Neural Network recipe. We will create two "
"neural networks for sake of loading one parameter of type A into type B."
msgstr "为了示例，我们会为训练图像创建一个神经网络。要了解更多信息，请参阅定义神经网络教程。我们将创建两个神经网络，以便加载类型A的参数到类型B。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Save model A"
msgstr "3. 保存模型A"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Load into model B"
msgstr "4. 加载到模型B"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you want to load parameters from one layer to another, but some keys do "
"not match, simply change the name of the parameter keys in the state_dict "
"that you are loading to match the keys in the model that you are loading "
"into."
msgstr "如果您想从一个层加载参数到另一个层，但某些键不匹配，可以简单地更改您加载的state_dict中的参数键名称，以匹配您要加载的模型中的键。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You can see that all keys matched successfully!"
msgstr "您可以看到所有的键匹配成功！"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Congratulations! You have successfully warmstarted a model using parameters "
"from a different model in PyTorch."
msgstr "恭喜！您已经成功通过使用PyTorch中不同模型的参数对模型进行了热启动。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Saving and loading multiple models in one file using PyTorch "
"<https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html>`__"
msgstr ""
"`使用PyTorch在一个文件中保存和加载多个模型 "
"<https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Saving and loading models across devices in PyTorch "
"<https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html>`__"
msgstr ""
"`在 PyTorch 中跨设备保存和加载模型 "
"<https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: "
"warmstarting_model_using_parameters_from_a_different_model.py "
"<warmstarting_model_using_parameters_from_a_different_model.py>`"
msgstr ""
":download:`下载 Python "
"源代码：warmstarting_model_using_parameters_from_a_different_model.py "
"<warmstarting_model_using_parameters_from_a_different_model.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: "
"warmstarting_model_using_parameters_from_a_different_model.ipynb "
"<warmstarting_model_using_parameters_from_a_different_model.ipynb>`"
msgstr ""
":download:`下载 Jupyter "
"notebook：warmstarting_model_using_parameters_from_a_different_model.ipynb "
"<warmstarting_model_using_parameters_from_a_different_model.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_recipes_what_is_state_dict.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_recipes_recipes_what_is_state_dict.py>` "
"下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In PyTorch, the learnable parameters (i.e. weights and biases) of a "
"``torch.nn.Module`` model are contained in the model’s parameters (accessed "
"with ``model.parameters()``). A ``state_dict`` is simply a Python dictionary"
" object that maps each layer to its parameter tensor."
msgstr ""
"在 PyTorch 中，``torch.nn.Module`` 模型的可学习参数（即权重和偏置）包含在模型的参数中（通过 "
"``model.parameters()`` 访问）。``state_dict`` 只是一个 Python 字典对象，它将每层映射到其参数张量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A ``state_dict`` is an integral entity if you are interested in saving or "
"loading models from PyTorch. Because ``state_dict`` objects are Python "
"dictionaries, they can be easily saved, updated, altered, and restored, "
"adding a great deal of modularity to PyTorch models and optimizers. Note "
"that only layers with learnable parameters (convolutional layers, linear "
"layers, etc.) and registered buffers (batchnorm’s running_mean) have entries"
" in the model’s ``state_dict``. Optimizer objects (``torch.optim``) also "
"have a ``state_dict``, which contains information about the optimizer’s "
"state, as well as the hyperparameters used. In this recipe, we will see how "
"``state_dict`` is used with a simple model."
msgstr ""
"如果您对从 PyTorch 保存或加载模型感兴趣，``state_dict`` 是一个不可或缺的实体。由于 ``state_dict`` 对象是 "
"Python 字典，它们可以轻松保存、更新、更改和恢复，从而为 PyTorch "
"模型和优化器添加了很大的模块化。请注意，只有具有可学习参数的层（卷积层、线性层等）和注册的缓冲区（如 batchnorm 的运行平均值）在模型的 "
"``state_dict`` 中有条目。优化器对象（``torch.optim``）也有一个 "
"``state_dict``，其中包含有关优化器状态的信息以及使用的超参数。在本教程中，我们将了解如何使用简单的模型来使用 "
"``state_dict``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Initialize the optimizer"
msgstr "初始化优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Access the model and optimizer ``state_dict``"
msgstr "访问模型和优化器的 ``state_dict``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For sake of example, we will create a neural network for training images. To"
" learn more see the Defining a Neural Network recipe."
msgstr "为了举例说明，我们将创建一个用于训练图像的神经网络。欲了解更多信息，请参阅定义神经网络教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Initialize the optimizer"
msgstr "3. 初始化优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "We will use SGD with momentum."
msgstr "我们将使用带动量的 SGD。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Access the model and optimizer ``state_dict``"
msgstr "4. 访问模型和优化器的 ``state_dict``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Now that we have constructed our model and optimizer, we can understand what"
" is preserved in their respective ``state_dict`` properties."
msgstr "现在我们已经构建了我们的模型和优化器，我们可以了解它们各自的 ``state_dict`` 属性中保存了什么内容。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This information is relevant for saving and loading the model and optimizers"
" for future use."
msgstr "此信息与保存和加载模型及优化器以供将来使用有关。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Congratulations! You have successfully used ``state_dict`` in PyTorch."
msgstr "恭喜您！您已成功在 PyTorch 中使用了 ``state_dict``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Saving and loading a general checkpoint in PyTorch "
"<https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html>`__"
msgstr ""
"`在 PyTorch 中保存和加载通用检查点 "
"<https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: what_is_state_dict.py "
"<what_is_state_dict.py>`"
msgstr ""
":download:`下载 Python 源代码：what_is_state_dict.py <what_is_state_dict.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: what_is_state_dict.ipynb "
"<what_is_state_dict.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook：what_is_state_dict.ipynb "
"<what_is_state_dict.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_recipes_zeroing_out_gradients.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_recipes_recipes_zeroing_out_gradients.py>` "
"下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"It is beneficial to zero out gradients when building a neural network. This "
"is because by default, gradients are accumulated in buffers (i.e, not "
"overwritten) whenever ``.backward()`` is called."
msgstr "在构建神经网络时清零梯度是有益的。这是因为默认情况下，每次调用 ``.backward()`` 时，梯度会累积在缓冲区中（即不会被覆盖）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When training your neural network, models are able to increase their "
"accuracy through gradient descent. In short, gradient descent is the process"
" of minimizing our loss (or error) by tweaking the weights and biases in our"
" model."
msgstr "在训练神经网络时，模型能够通过梯度下降过程提高其准确性。简而言之，梯度下降是通过调整模型中的权重和偏置来最小化损失（或误差）的过程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.Tensor`` is the central class of PyTorch. When you create a tensor, "
"if you set its attribute ``.requires_grad`` as ``True``, the package tracks "
"all operations on it. This happens on subsequent backward passes. The "
"gradient for this tensor will be accumulated into ``.grad`` attribute. The "
"accumulation (or sum) of all the gradients is calculated when .backward() is"
" called on the loss tensor."
msgstr ""
"``torch.Tensor`` 是 PyTorch 的核心类。当您创建一个张量时，如果将其属性 ``.requires_grad`` 设置为 "
"``True``，则包会跟踪它的所有操作。这发生在随后进行的反向传播过程中。该张量的梯度将累积到 ``.grad`` "
"属性中。所有梯度的累积（或总和）是在调用损失张量的 ``.backward()`` 时计算的。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are cases where it may be necessary to zero-out the gradients of a "
"tensor. For example: when you start your training loop, you should zero out "
"the gradients so that you can perform this tracking correctly. In this "
"recipe, we will learn how to zero out gradients using the PyTorch library. "
"We will demonstrate how to do this by training a neural network on the "
"``CIFAR10`` dataset built into PyTorch."
msgstr ""
"在某些情况下，可能需要清零张量的梯度。例如：当您开始训练循环时，您应该清零梯度以便正确地执行跟踪。在本教程中，我们将学习如何使用 PyTorch "
"库清零梯度。我们将通过对 PyTorch 内置的 ``CIFAR10`` 数据集训练一个神经网络来演示这一操作。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Since we will be training data in this recipe, if you are in a runnable "
"notebook, it is best to switch the runtime to GPU or TPU. Before we begin, "
"we need to install ``torch`` and ``torchvision`` if they aren’t already "
"available."
msgstr ""
"由于我们将在本教程中进行数据训练，如果您处于可运行的 notebook 中，最好将运行时切换到 GPU 或 TPU。在开始之前，如果尚未安装 "
"``torch`` 和 ``torchvision``，需要安装它们。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Steps 1 through 4 set up our data and neural network for training. The "
"process of zeroing out the gradients happens in step 5. If you already have "
"your data and neural network built, skip to 5."
msgstr "步骤 1 到 4 设置了我们的数据和神经网络以进行训练。清零梯度的过程发生在步骤 5。如果您已经构建了数据和神经网络，请直接跳到步骤 5。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Load and normalize the dataset"
msgstr "加载并归一化数据集"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Build the neural network"
msgstr "构建神经网络"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Define the loss function"
msgstr "定义损失函数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Zero the gradients while training the network"
msgstr "训练网络时清零梯度"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For this recipe, we will just be using ``torch`` and ``torchvision`` to "
"access the dataset."
msgstr "在本教程中，我们将仅使用 ``torch`` 和 ``torchvision`` 来访问数据集。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "2. Load and normalize the dataset"
msgstr "2. 加载并归一化数据集"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch features various built-in datasets (see the Loading Data recipe for "
"more information)."
msgstr "PyTorch 提供了各种内置数据集（有关详细信息，请参阅加载数据教程）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "3. Build the neural network"
msgstr "3. 构建神经网络"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We will use a convolutional neural network. To learn more see the Defining a"
" Neural Network recipe."
msgstr "我们将使用卷积神经网络。欲了解更多信息，请参阅定义神经网络教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "4. Define a Loss function and optimizer"
msgstr "4. 定义损失函数和优化器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Let’s use a Classification Cross-Entropy loss and SGD with momentum."
msgstr "我们将使用分类交叉熵损失和带动量的 SGD。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "5. Zero the gradients while training the network"
msgstr "5. 训练网络时清零梯度"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This is when things start to get interesting. We simply have to loop over "
"our data iterator, and feed the inputs to the network and optimize."
msgstr "这时事情开始变得有趣。我们只需循环遍历数据迭代器，并将输入提供给网络并优化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Notice that for each entity of data, we zero out the gradients. This is to "
"ensure that we aren’t tracking any unnecessary information when we train our"
" neural network."
msgstr "注意，对于每个数据实体，我们都会清零梯度。这是为了确保在训练神经网络时我们不会跟踪任何不必要的信息。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You can also use ``model.zero_grad()``. This is the same as using "
"``optimizer.zero_grad()`` as long as all your model parameters are in that "
"optimizer. Use your best judgment to decide which one to use."
msgstr ""
"您还可以使用 ``model.zero_grad()``。这与使用 ``optimizer.zero_grad()`` "
"是一样的，只要您的所有模型参数都在该优化器中。根据需要选择最适合的使用方式。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Congratulations! You have successfully zeroed out gradients in PyTorch."
msgstr "恭喜您！您已成功在 PyTorch 中清零梯度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Loading data in PyTorch "
"<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>`__"
msgstr ""
"`在 PyTorch 中加载数据 "
"<https://pytorch.org/tutorials/beginner/basics/data_tutorial.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: zeroing_out_gradients.py "
"<zeroing_out_gradients.py>`"
msgstr ""
":download:`下载 Python 源代码：zeroing_out_gradients.py "
"<zeroing_out_gradients.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: zeroing_out_gradients.ipynb "
"<zeroing_out_gradients.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook：zeroing_out_gradients.ipynb "
"<zeroing_out_gradients.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Recipes are bite-sized, actionable examples of how to use specific PyTorch "
"features, different from our full-length tutorials."
msgstr "教程是一些实用的简明示例，展示如何使用特定的 PyTorch 功能，与我们完整的教程有所不同。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_regional_compilation.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_recipes_regional_compilation.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author:** `Animesh Jain <https://github.com/anijain2305>`_"
msgstr "**作者：** `Animesh Jain <https://github.com/anijain2305>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As deep learning models get larger, the compilation time of these models "
"also increases. This extended compilation time can result in a large startup"
" time in inference services or wasted resources in large-scale training. "
"This recipe shows an example of how to reduce the cold start compilation "
"time by choosing to compile a repeated region of the model instead of the "
"entire model."
msgstr ""
"随着深度学习模型变得越来越庞大，这些模型的编译时间也随之增加。这种延长的编译时间可能会导致推理服务的启动时间变长或在大规模训练中浪费资源。本教程展示了如何通过选择编译模型的重复区域而不是整个模型来减少冷启动编译时间。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Pytorch 2.5 or later"
msgstr "PyTorch 2.5 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Before we begin, we need to install ``torch`` if it is not already "
"available."
msgstr "在开始之前，如果尚未安装 ``torch``，需要安装它。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This feature is available starting with the 2.5 release. If you are using "
"version 2.4, you can enable the configuration flag "
"``torch._dynamo.config.inline_inbuilt_nn_modules=True`` to prevent "
"recompilations during regional compilation. In version 2.5, this flag is "
"enabled by default."
msgstr ""
"此功能从 2.5 版本开始可用。如果您使用的是 2.4 版本，可以启用配置标志 "
"``torch._dynamo.config.inline_inbuilt_nn_modules=True`` 来防止区域编译期间的重新编译。在 2.5"
" 版本中，此标志默认启用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In this recipe, we will follow these steps:"
msgstr "在本教程中，我们将遵循以下步骤："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Import all necessary libraries."
msgstr "导入所有必要的库。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Define and initialize a neural network with repeated regions."
msgstr "定义并初始化具有重复区域的神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Understand the difference between the full model and the regional "
"compilation."
msgstr "理解完整模型和区域编译之间的区别。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Measure the compilation time of the full model and the regional compilation."
msgstr "测量完整模型和区域编译的编译时间。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "First, let's import the necessary libraries for loading our data:"
msgstr "首先，让我们导入加载数据所需的库："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Next, let's define and initialize a neural network with repeated regions."
msgstr "接下来，我们定义并初始化一个具有重复区域的神经网络。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Typically, neural networks are composed of repeated layers. For example, a "
"large language model is composed of many Transformer blocks. In this recipe,"
" we will create a ``Layer`` using the ``nn.Module`` class as a proxy for a "
"repeated region. We will then create a ``Model`` which is composed of 64 "
"instances of this ``Layer`` class."
msgstr ""
"通常，神经网络由重复的层组成。例如，一个大型语言模型由多个 Transformer 块组成。在本教程中，我们将使用 ``nn.Module`` "
"类创建一个 ``Layer`` 作为重复区域的代理。然后我们将创建一个由 64 个 ``Layer`` 类实例组成的 ``Model``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Next, let's review the difference between the full model and the regional "
"compilation."
msgstr "接下来，我们回顾完整模型编译和区域编译之间的区别。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In full model compilation, the entire model is compiled as a whole. This is "
"the common approach most users take with ``torch.compile``. In this example,"
" we apply ``torch.compile`` to the ``Model`` object. This will effectively "
"inline the 64 layers, producing a large graph to compile. You can look at "
"the full graph by running this recipe with ``TORCH_LOGS=graph_code``."
msgstr ""
"在完整模型编译中，整个模型作为一个整体被编译。这是大多数用户使用 ``torch.compile`` 的常见方法。在本示例中，我们将 "
"``torch.compile`` 应用于 ``Model`` 对象。这将有效地内联 64 个层，生成一个需要编译的大型图。运行此教程时，可通过设置 "
"``TORCH_LOGS=graph_code`` 查看完整图。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The regional compilation, on the other hand, compiles a region of the model."
" By strategically choosing to compile a repeated region of the model, we can"
" compile a much smaller graph and then reuse the compiled graph for all the "
"regions. In the example, ``torch.compile`` is applied only to the ``layers``"
" and not the full model."
msgstr ""
"另一方面，区域编译只编译模型的一部分。通过策略性地选择编译模型的重复区域，我们可以编译一个小得多的图并重复使用该已编译的图。在本示例中，``torch.compile``"
" 仅应用于 ``layers`` 而不是整个模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Applying compilation to a repeated region, instead of full model, leads to "
"large savings in compile time. Here, we will just compile a layer instance "
"and then reuse it 64 times in the ``Model`` object."
msgstr ""
"将编译应用于重复区域而不是整个模型，可以显著节省编译时间。在这里，我们仅编译一个层实例，然后在 ``Model`` 对象中重用该实例 64 次。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note that with repeated regions, some part of the model might not be "
"compiled. For example, the ``self.linear`` in the ``Model`` is outside of "
"the scope of regional compilation."
msgstr ""
"请注意，对于重复区域，模型的某些部分可能未被编译。例如，在 ``Model`` 中的 ``self.linear`` 超出了区域编译的范围。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Also, note that there is a tradeoff between performance speedup and compile "
"time. Full model compilation involves a larger graph and, theoretically, "
"offers more scope for optimizations. However, for practical purposes and "
"depending on the model, we have observed many cases with minimal speedup "
"differences between the full model and regional compilation."
msgstr ""
"还需注意的是，性能加速和编译时间之间存在权衡。完整模型编译涉及更大的图，并且理论上提供了更多优化的空间。然而，对于实际用途而言，根据模型的具体情况，我们已观察到完整模型编译和区域编译之间的加速差异通常很小。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Next, let's measure the compilation time of the full model and the regional "
"compilation."
msgstr "接下来，让我们测量完整模型和区域编译的编译时间。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compile`` is a JIT compiler, which means that it compiles on the "
"first invocation. In the code below, we measure the total time spent in the "
"first invocation. While this method is not precise, it provides a good "
"estimate since the majority of the time is spent in compilation."
msgstr ""
"``torch.compile`` 是一个 JIT "
"编译器，这意味着它会在第一次调用时进行编译。在以下代码中，我们测量第一次调用所花费的总时间。虽然此方法并不精确，但由于主要时间花费在编译过程中，提供了一个不错的估计。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe shows how to control the cold start compilation time if your "
"model has repeated regions. This approach requires user modifications to "
"apply `torch.compile` to the repeated regions instead of more commonly used "
"full model compilation. We are continually working on reducing cold start "
"compilation time."
msgstr ""
"此教程展示了如何控制模型具有重复区域时的冷启动编译时间。此方法需要用户修改以在重复区域而非更常用的完整模型编译中应用 "
"`torch.compile`。我们将继续致力于减少冷启动编译时间。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: regional_compilation.py "
"<regional_compilation.py>`"
msgstr ""
":download:`下载 Python 源代码：regional_compilation.py <regional_compilation.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: regional_compilation.ipynb "
"<regional_compilation.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook：regional_compilation.ipynb "
"<regional_compilation.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Script and Optimize for Mobile Recipe"
msgstr "脚本并优化移动端教程"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This tutorial has been deprecated. There is a new tutorial on this topic."
msgstr "本教程已被废弃。针对该主题有一个新的教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Intel® Extension for PyTorch* Backend on Intel® CPUs"
msgstr "Intel® 扩展适用于 Intel® CPU 的 PyTorch* 后端"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To work better with `torch.compile` on Intel® CPUs, Intel® Extension for "
"PyTorch* implements a backend ``ipex``. It targets to improve hardware "
"resource usage efficiency on Intel platforms for better performance. The "
"`ipex` backend is implemented with further customizations designed in Intel®"
" Extension for PyTorch* for the model compilation."
msgstr ""
"为了在 Intel® CPU 上更好地使用 `torch.compile`，Intel® Extension for PyTorch* 实现了一个名为 "
"`ipex` 的后端。它旨在提高 Intel 平台上的硬件资源利用效率，从而实现更好的性能。`ipex` 后端通过 Intel® Extension "
"for PyTorch* 中设计的进一步自定义功能来实现模型编译。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Usage Example"
msgstr "使用示例"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Train FP32"
msgstr "训练 FP32"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Check the example below to learn how to utilize the `ipex` backend with "
"`torch.compile` for model training with FP32 data type."
msgstr "查看下面的示例，了解如何使用 `torch.compile` 和 `ipex` 后端进行 FP32 数据类型的模型训练。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Train BF16"
msgstr "训练 BF16"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Check the example below to learn how to utilize the `ipex` backend with "
"`torch.compile` for model training with BFloat16 data type."
msgstr "查看下面的示例，了解如何使用 `torch.compile` 和 `ipex` 后端进行 BFloat16 数据类型的模型训练。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inference FP32"
msgstr "推理 FP32"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Check the example below to learn how to utilize the `ipex` backend with "
"`torch.compile` for model inference with FP32 data type."
msgstr "查看下面的示例，了解如何使用 `torch.compile` 和 `ipex` 后端进行 FP32 数据类型的模型推理。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inference BF16"
msgstr "推理 BF16"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Check the example below to learn how to utilize the `ipex` backend with "
"`torch.compile` for model inference with BFloat16 data type."
msgstr "查看下面的示例，了解如何使用 `torch.compile` 和 `ipex` 后端进行 BFloat16 数据类型的模型推理。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Compile Time Caching Configuration"
msgstr "编译时缓存配置"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Authors:** `Oguz Ulgen <https://github.com/oulgen>`_ and `Sam Larsen "
"<https://github.com/masnesral>`_"
msgstr ""
"**作者：** `Oguz Ulgen <https://github.com/oulgen>`_ 和 `Sam Larsen "
"<https://github.com/masnesral>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch Compiler implements several caches to reduce compilation latency. "
"This recipe demonstrates how you can configure various parts of the caching "
"in ``torch.compile``."
msgstr "PyTorch 编译器实现了多个缓存，用于减少编译延迟。此教程演示了如何配置 ``torch.compile`` 中的各种缓存部分。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Before starting this recipe, make sure that you have the following:"
msgstr "开始此教程之前，请确保您具有以下内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Basic understanding of ``torch.compile``. See:"
msgstr "对 ``torch.compile`` 的基本理解。参见："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.compiler API documentation "
"<https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler>`__"
msgstr ""
"`torch.compiler API 文档 "
"<https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Introduction to torch.compile "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__"
msgstr ""
"`torch.compile 入门指南 "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Compile Time Caching in torch.compile "
"<https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html>`__"
msgstr ""
"`torch.compile 中的编译时缓存 "
"<https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 2.4 or later"
msgstr "PyTorch 2.4 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Inductor Cache Settings"
msgstr "Inductor 缓存设置"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Most of these caches are in-memory, only used within the same process, and "
"are transparent to the user. An exception is caches that store compiled FX "
"graphs (``FXGraphCache``, ``AOTAutogradCache``). These caches allow Inductor"
" to avoid recompilation across process boundaries when it encounters the "
"same graph with the same Tensor input shapes (and the same configuration). "
"The default implementation stores compiled artifacts in the system temp "
"directory. An optional feature also supports sharing those artifacts within "
"a cluster by storing them in a Redis database."
msgstr ""
"大多数缓存是内存中的，仅在同一进程内使用，并对用户透明。一个例外是存储编译后 FX 图的缓存（``FXGraphCache`` 和 "
"``AOTAutogradCache``）。这些缓存允许 Inductor 在遇到相同的图（具有相同的 Tensor "
"输入形状和配置）时，避免跨进程重新编译。默认实现将编译工件存储在系统临时目录中。一项可选功能支持将这些工件存储在 Redis "
"数据库中，从而在集群中共享。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are a few settings relevant to caching and to FX graph caching in "
"particular. The settings are accessible via environment variables listed "
"below or can be hard-coded in the Inductor’s config file."
msgstr "以下设置与缓存尤其是 FX 图缓存相关。这些设置可以通过下面列出的环境变量访问，或直接在 Inductor 的配置文件中硬编码。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_FX_GRAPH_CACHE"
msgstr "TORCHINDUCTOR_FX_GRAPH_CACHE"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This setting enables the local FX graph cache feature, which stores "
"artifacts in the host’s temp directory. Setting it to ``1`` enables the "
"feature while any other value disables it. By default, the disk location is "
"per username, but users can enable sharing across usernames by specifying "
"``TORCHINDUCTOR_CACHE_DIR`` (below)."
msgstr ""
"此设置启用本地 FX 图缓存功能，将工件存储在主机的临时目录中。将其设置为 ``1`` "
"即可启用功能，设置为其他任何值则禁用功能。默认情况下，磁盘位置是按用户名分隔的，但用户可以通过指定 "
"``TORCHINDUCTOR_CACHE_DIR``（如下）启用跨用户名共享。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_AUTOGRAD_CACHE"
msgstr "TORCHINDUCTOR_AUTOGRAD_CACHE"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This setting extends ``FXGraphCache`` to store cached results at the "
"``AOTAutograd`` level, rather than at the Inductor level. Setting it to "
"``1`` enables this feature, while any other value disables it. By default, "
"the disk location is per username, but users can enable sharing across "
"usernames by specifying ``TORCHINDUCTOR_CACHE_DIR`` (below). "
"``TORCHINDUCTOR_AUTOGRAD_CACHE`` requires ``TORCHINDUCTOR_FX_GRAPH_CACHE`` "
"to work. The same cache dir stores cache entries for ``AOTAutogradCache`` "
"(under ``{TORCHINDUCTOR_CACHE_DIR}/aotautograd``) and ``FXGraphCache`` "
"(under ``{TORCHINDUCTOR_CACHE_DIR}/fxgraph``)."
msgstr ""
"此设置将 ``FXGraphCache`` 功能扩展到在 ``AOTAutograd`` 层级存储缓存结果，而不是在 Inductor 层级。将其设置为"
" ``1`` 可启用此功能，设置为其他任何值则禁用功能。默认情况下，磁盘位置是按用户名分隔的，但用户可以通过指定 "
"``TORCHINDUCTOR_CACHE_DIR``（如下）启用跨用户名共享。``TORCHINDUCTOR_AUTOGRAD_CACHE`` 需要 "
"``TORCHINDUCTOR_FX_GRAPH_CACHE`` 支持。相同的缓存目录用于存储 ``AOTAutogradCache``（位于 "
"``{TORCHINDUCTOR_CACHE_DIR}/aotautograd``）和 ``FXGraphCache``（位于 "
"``{TORCHINDUCTOR_CACHE_DIR}/fxgraph``）的缓存条目。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_CACHE_DIR"
msgstr "TORCHINDUCTOR_CACHE_DIR"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This setting specifies the location of all on-disk caches. By default, the "
"location is in the system temp directory under ``torchinductor_<username>``,"
" for example, ``/tmp/torchinductor_myusername``."
msgstr ""
"此设置指定所有磁盘缓存的位置。默认情况下，位置为系统临时目录下的 ``torchinductor_<username>``，例如 "
"``/tmp/torchinductor_myusername``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note that if ``TRITON_CACHE_DIR`` is not set in the environment, Inductor "
"sets the ``Triton`` cache directory to this same temp location, under the "
"Triton sub-directory."
msgstr ""
"请注意，如果环境中未设置 ``TRITON_CACHE_DIR``，Inductor 会将 ``Triton`` 缓存目录设置为相同的临时位置，位于 "
"Triton 子目录下。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE"
msgstr "TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This setting enables the remote FX graph cache feature. The current "
"implementation uses ``Redis``. ``1`` enables caching, and any other value "
"disables it. The following environment variables configure the host and port"
" of the Redis server:"
msgstr ""
"此设置启用远程 FX 图缓存功能。目前的实现使用 ``Redis``。``1`` 表示启用缓存，其他任何值则禁用缓存。以下环境变量配置 Redis "
"服务器的主机和端口："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``TORCHINDUCTOR_REDIS_HOST`` (defaults to ``localhost``) "
"``TORCHINDUCTOR_REDIS_PORT`` (defaults to ``6379``)"
msgstr ""
"``TORCHINDUCTOR_REDIS_HOST``（默认为 ``localhost``） "
"``TORCHINDUCTOR_REDIS_PORT``（默认为 ``6379``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note that if Inductor locates a remote cache entry, it stores the compiled "
"artifact in the local on-disk cache; that local artifact would be served on "
"subsequent runs on the same machine."
msgstr ""
"请注意，如果 Inductor 找到一个远程缓存条目，它会将编译后的工件存储在本地磁盘缓存中；之后在同一台机器上的后续运行中，本地工件将被用于提供服务。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE"
msgstr "TORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Similar to ``TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE``, this setting enables the"
" remote ``AOTAutogradCache`` feature. The current implementation uses Redis."
" Setting it to ``1`` enables caching, while any other value disables it. The"
" following environment variables are used to configure the host and port of "
"the ``Redis`` server: * ``TORCHINDUCTOR_REDIS_HOST`` (defaults to "
"``localhost``) * ``TORCHINDUCTOR_REDIS_PORT`` (defaults to ``6379``)"
msgstr ""
"类似于 ``TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE``，此设置启用远程 ``AOTAutogradCache`` "
"功能。目前的实现使用 Redis。将其设置为 ``1`` 表示启用缓存，其他任何值则禁用缓存。以下环境变量用于配置 ``Redis`` "
"服务器的主机和端口：* ``TORCHINDUCTOR_REDIS_HOST``（默认为 ``localhost``）* "
"``TORCHINDUCTOR_REDIS_PORT``（默认为 ``6379``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`TORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE`` requires "
"``TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE`` to be enabled in order to function. "
"The same Redis server can be used to store both AOTAutograd and FXGraph "
"cache results."
msgstr ""
"``TORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE`` 需要启用 "
"``TORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE`` 才能正常工作。相同的 Redis 服务器可以用于存储 "
"AOTAutograd 和 FXGraph 缓存结果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE"
msgstr "TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This setting enables a remote cache for ``TorchInductor``’s autotuner. "
"Similar to remote FX graph cache, the current implementation uses Redis. "
"Setting it to ``1`` enables caching, while any other value disables it. The "
"same host / port environment variables mentioned above apply to this cache."
msgstr ""
"此设置启用 ``TorchInductor`` 的自动调优器的远程缓存。与远程 FX 图缓存类似，目前的实现使用 Redis。将其设置为 ``1`` "
"表示启用缓存，其他值将禁用该功能。上述主机/端口环境变量同样适用于此缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TORCHINDUCTOR_FORCE_DISABLE_CACHES"
msgstr "TORCHINDUCTOR_FORCE_DISABLE_CACHES"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Set this value to ``1`` to disable all Inductor caching. This setting is "
"useful for tasks like experimenting with cold-start compile times or forcing"
" recompilation for debugging purposes."
msgstr "将此值设置为 ``1`` 可禁用所有 Inductor 缓存。此设置适用于诸如实验冷启动编译时间或出于调试目的强制重新编译等任务。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we have learned how to configure PyTorch Compiler's caching "
"mechanisms. Additionally, we explored the various settings and environment "
"variables that allow users to configure and optimize these caching features "
"according to their specific needs."
msgstr ""
"在此教程中，我们学习了如何配置 PyTorch "
"编译器的缓存机制。此外，我们探讨了各种设置和环境变量，这些设置和变量使用户能够根据其特定需求配置和优化这些缓存功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Compile Time Caching in ``torch.compile``"
msgstr "``torch.compile`` 中的编译时缓存"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author:** `Oguz Ulgen <https://github.com/oulgen>`_"
msgstr "**作者：** `Oguz Ulgen <https://github.com/oulgen>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch Compiler provides several caching offerings to reduce compilation "
"latency. This recipe will explain these offerings in detail to help users "
"pick the best option for their use case."
msgstr "PyTorch 编译器提供了多种缓存功能，用于减少编译延迟。本教程将详细解释这些功能，以帮助用户选择其用例的最佳选项。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Check out `Compile Time Caching Configurations "
"<https://pytorch.org/tutorials/recipes/torch_compile_caching_configuration_tutorial.html>`__"
" for how to configure these caches."
msgstr ""
"查看 `Compile Time Caching Configurations "
"<https://pytorch.org/tutorials/recipes/torch_compile_caching_configuration_tutorial.html>`__"
" 了解如何配置这些缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Also check out our caching benchmark at `PT CacheBench Benchmarks "
"<https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fpytorch&benchmarkName=TorchCache+Benchmark>`__."
msgstr ""
"还可以查看我们的缓存基准测试 `PT CacheBench Benchmarks "
"<https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fpytorch&benchmarkName=TorchCache+Benchmark>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Triton language documentation <https://triton-lang.org/main/index.html>`__"
msgstr "`Triton 语言文档 <https://triton-lang.org/main/index.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Caching Offerings"
msgstr "缓存功能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.compile`` provides the following caching offerings:"
msgstr "``torch.compile`` 提供了以下缓存功能："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "End to end caching (also known as ``Mega-Cache``)"
msgstr "端到端缓存（也称为 ``Mega-Cache``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Modular caching of ``TorchDynamo``, ``TorchInductor``, and ``Triton``"
msgstr "``TorchDynamo``、``TorchInductor`` 和 ``Triton`` 的模块化缓存"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"It is important to note that caching validates that the cache artifacts are "
"used with the same PyTorch and Triton version, as well as, same GPU when "
"device is set to be cuda."
msgstr ""
"需要注意的是，缓存会验证缓存工件是否与相同的 PyTorch 和 Triton 版本，以及在设备设置为 CUDA 时与相同的 GPU 一起使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.compile`` end-to-end caching (``Mega-Cache``)"
msgstr "``torch.compile`` 端到端缓存（``Mega-Cache``）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"End to end caching, from here onwards referred to ``Mega-Cache``, is the "
"ideal solution for users looking for a portable caching solution that can be"
" stored in a database and can later be fetched possibly on a separate "
"machine."
msgstr ""
"端到端缓存，从此之后称为 ``Mega-"
"Cache``，是为寻求可移植缓存解决方案的用户设计的理想选择，该解决方案可以存储在数据库中，并可能在另一台机器上获取。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``Mega-Cache`` provides two compiler APIs:"
msgstr "``Mega-Cache`` 提供了两个编译器 API："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.compiler.save_cache_artifacts()``"
msgstr "``torch.compiler.save_cache_artifacts()``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.compiler.load_cache_artifacts()``"
msgstr "``torch.compiler.load_cache_artifacts()``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The intended use case is after compiling and executing a model, the user "
"calls ``torch.compiler.save_cache_artifacts()`` which will return the "
"compiler artifacts in a portable form. Later, potentially on a different "
"machine, the user may call ``torch.compiler.load_cache_artifacts()`` with "
"these artifacts to pre-populate the ``torch.compile`` caches in order to "
"jump-start their cache."
msgstr ""
"预期用例是在编译并执行模型后，用户调用 "
"``torch.compiler.save_cache_artifacts()``，它将以可移植的形式返回编译器工件。稍后，可能在另一台机器上，用户可以使用这些工件调用"
" ``torch.compiler.load_cache_artifacts()``，以预填充 ``torch.compile`` "
"缓存，从而快速启动其缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Consider the following example. First, compile and save the cache artifacts."
msgstr "请考虑以下示例。首先，编译并保存缓存工件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Later, you can jump-start the cache by the following:"
msgstr "稍后，您可以通过以下方式快速启动缓存："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This operation populates all the modular caches that will be discussed in "
"the next section, including ``PGO``, ``AOTAutograd``, ``Inductor``, "
"``Triton``, and ``Autotuning``."
msgstr ""
"此操作会填充将要讨论的下一节中的所有模块缓存，包括 ``PGO``、``AOTAutograd``、``Inductor``、``Triton`` 和 "
"``Autotuning``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The aforementioned ``Mega-Cache`` is composed of individual components that "
"can be used without any user intervention. By default, PyTorch Compiler "
"comes with local on-disk caches for ``TorchDynamo``, ``TorchInductor``, and "
"``Triton``. These caches include:"
msgstr ""
"上述 ``Mega-Cache`` 由可以无需用户干预而单独使用的组件组成。默认情况下，PyTorch 编译器自带适用于 "
"``TorchDynamo``、``TorchInductor`` 和 ``Triton`` 的本地磁盘缓存。这些缓存包括："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``FXGraphCache``: A cache of graph-based IR components used in compilation."
msgstr "``FXGraphCache``：用于编译的基于图的 IR 组件的缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``TritonCache``: A cache of Triton-compilation results, including ``cubin`` "
"files generated by ``Triton`` and other caching artifacts."
msgstr ""
"``TritonCache``：包括由 ``Triton`` 生成的 ``cubin`` 文件及其他缓存工件的 Triton 编译结果缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``InductorCache``: A bundle of ``FXGraphCache`` and ``Triton`` cache."
msgstr "``InductorCache``：``FXGraphCache`` 和 ``Triton`` 缓存的集合。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``AOTAutogradCache``: A cache of joint graph artifacts."
msgstr "``AOTAutogradCache``：联合图工件的缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``PGO-cache``: A cache of dynamic shape decisions to reduce number of "
"recompilations."
msgstr "``PGO-cache``：动态形状决策的缓存，用于减少重新编译的次数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"All these cache artifacts are written to ``TORCHINDUCTOR_CACHE_DIR`` which "
"by default will look like ``/tmp/torchinductor_myusername``."
msgstr ""
"所有这些缓存工件都会写入 ``TORCHINDUCTOR_CACHE_DIR``，默认情况下该目录为 "
"``/tmp/torchinductor_myusername``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Remote Caching"
msgstr "远程缓存"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We also provide a remote caching option for users who would like to take "
"advantage of a Redis based cache. Check out `Compile Time Caching "
"Configurations "
"<https://pytorch.org/tutorials/recipes/torch_compile_caching_configuration_tutorial.html>`__"
" to learn more about how to enable the Redis-based caching."
msgstr ""
"对于希望利用基于 Redis 的缓存的用户，我们还提供了一种远程缓存选项。查看 `Compile Time Caching Configurations"
" "
"<https://pytorch.org/tutorials/recipes/torch_compile_caching_configuration_tutorial.html>`__"
" 了解如何启用基于 Redis 的缓存。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we have learned that PyTorch Inductor's caching mechanisms "
"significantly reduce compilation latency by utilizing both local and remote "
"caches, which operate seamlessly in the background without requiring user "
"intervention."
msgstr ""
"在此教程中，我们了解到，PyTorch Inductor 的缓存机制通过利用本地和远程缓存，大大减少了编译延迟。这些缓存无缝运行，无需用户干预。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_torch_compile_torch_function_modes.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_recipes_torch_compile_torch_function_modes.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(beta) Utilizing Torch Function modes with torch.compile"
msgstr "（测试版）结合 torch.compile 使用 Torch Function modes"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This recipe covers how to use a key torch extensibility point,"
msgstr ""
"本教程介绍如何在 ``torch.compile`` 中使用重要的 PyTorch 扩展点，即 torch Function "
"modes，以便在不增加运行时开销的情况下，在跟踪时覆写 torch 操作符（也称为 **ops**）的行为。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"torch function modes, in tandem with ``torch.compile`` to override the "
"behavior of torch operators, also know as **ops**, at trace time, with no "
"runtime overhead."
msgstr "此教程需要 PyTorch 2.7.0 或更高版本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This recipe requires PyTorch 2.7.0 or later."
msgstr "重写 torch 操作符（torch.add -> torch.mul）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Rewriting a torch op (torch.add -> torch.mul)"
msgstr ""
"在本示例中，我们将使用 torch Function modes "
"将加法的出现改写为乘法。这种类型的覆写很常见，如果某个特定的后端对某个操作符有自定义实现，就应该为其分派此实现。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe we demonstrated how to override the behavior of ``torch.*`` "
"operators using torch function modes from within ``torch.compile``. This "
"enables users to utilize the extensibility benefits of torch function modes "
"without the runtime overhead of calling torch function on every op "
"invocation."
msgstr ""
"在这个教程中，我们演示了如何在 ``torch.compile`` 中使用 torch 函数模式覆盖 ``torch.*`` "
"操作的行为。这使用户能够利用 torch 函数模式的扩展性优势，而无需在每次操作调用时承受调用 torch 函数的运行时开销。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"See `Extending Torch API with Modes "
"<https://pytorch.org/docs/stable/notes/extending.html#extending-all-torch-"
"api-with-modes>`__  for other examples and background on Torch Function "
"modes."
msgstr ""
"参见 `使用模式扩展 Torch API "
"<https://pytorch.org/docs/stable/notes/extending.html#extending-all-torch-"
"api-with-modes>`__ ，了解其他示例和关于 Torch 函数模式的背景知识。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: "
"torch_compile_torch_function_modes.py "
"<torch_compile_torch_function_modes.py>`"
msgstr ""
":download:`下载 Python 源代码: torch_compile_torch_function_modes.py "
"<torch_compile_torch_function_modes.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: "
"torch_compile_torch_function_modes.ipynb "
"<torch_compile_torch_function_modes.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: torch_compile_torch_function_modes.ipynb "
"<torch_compile_torch_function_modes.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_torch_compile_user_defined_triton_kernel_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_recipes_torch_compile_user_defined_triton_kernel_tutorial.py>`"
" 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using User-Defined Triton Kernels with ``torch.compile``"
msgstr "在 ``torch.compile`` 中使用用户定义的 Triton 内核"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"User-defined Triton kernels can be used to optimize specific parts of your "
"model's computation. These kernels are written in Triton's language, which "
"is designed to make it easier to achieve peak hardware performance. By using"
" user-defined Triton kernels with ``torch.compile``, you can integrate these"
" optimized computations into your PyTorch model, potentially achieving "
"significant performance improvements."
msgstr ""
"用户定义的 Triton 内核可用于优化模型计算的特定部分。这些内核用 Triton 语言编写，旨在更容易达到硬件峰值性能。通过将用户定义的 "
"Triton 内核与 ``torch.compile`` 结合使用，您可以将这些优化计算集成到 PyTorch 模型中，从而可能获得显著的性能提升。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipes demonstrates how you can use user-defined Triton kernels with "
"``torch.compile``."
msgstr "本教程演示了如何将用户定义的 Triton 内核与 ``torch.compile`` 一起使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Basic understanding of ``torch.compile`` and Triton. See:"
msgstr "需要 ``torch.compile`` 和 Triton 的基本了解。详见："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 2.3 or later"
msgstr "PyTorch 2.3 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "A GPU that supports Triton"
msgstr "支持 Triton 的 GPU"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this example, we will use a simple vector addition kernel from the Triton"
" documentation with ``torch.compile``. For reference, see `Triton "
"documentation <https://triton-lang.org/main/getting-"
"started/tutorials/01-vector-add.html>`__."
msgstr ""
"在本示例中，我们将从 Triton 文档中使用一个简单的向量加法内核结合 ``torch.compile``。参考文档见 `Triton 文档 "
"<https://triton-lang.org/main/getting-started/tutorials/01-vector-"
"add.html>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Advanced Usage"
msgstr "高级用法"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Triton's autotune feature is a powerful tool that automatically optimizes "
"the configuration parameters of your Triton kernels. It explores a range of "
"possible configurations and selects the one that delivers the best "
"performance for your specific use case."
msgstr ""
"Triton 的 autotune 特性是一个强大的工具，可以自动优化 Triton "
"内核的配置参数。它会探索一系列可能的配置，并选择在您的特定用例中性能最佳的那个。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"When used with ``torch.compile``, ``triton.autotune`` can help ensure that "
"your PyTorch model is running as efficiently as possible. Here is an example"
" of using ``torch.compile`` and ``triton.autotune``."
msgstr ""
"与 ``torch.compile`` 配合使用时，``triton.autotune`` 可以确保您的 PyTorch 模型运行尽可能高效。以下是使用"
" ``torch.compile`` 和 ``triton.autotune`` 的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compile`` only supports configs and key arguments to "
"``triton.autotune``."
msgstr "``torch.compile`` 仅支持传递给 ``triton.autotune`` 的 config 和关键参数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Composability"
msgstr "可组合性"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"User-defined Triton kernels do not automatically support all PyTorch "
"subsystems. This can be seen in the following use cases:"
msgstr "用户定义的 Triton 内核并不自动支持所有 PyTorch 子系统。以下用例中可以体现："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding a CPU fallback"
msgstr "添加 CPU 回退"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding a ``FlopCounter`` formula"
msgstr "添加 ``FlopCounter`` 公式"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Composing with Tensor Subclasses"
msgstr "与 Tensor 子类组合"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To compose with additional PyTorch subsystems, use "
"``torch.library.triton_op``."
msgstr "要与其他 PyTorch 子系统组合，请使用 ``torch.library.triton_op``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``triton_op is`` a structured way of defining a custom operator that is "
"backed by one or more Triton kernels: like regular custom operators "
"(``torch.library.custom_op``), you are able to specify the interactions with"
" PyTorch subsystems via ``torch.library``. However, unlike "
"``torch.library.custom_op``, which creates opaque callables with respect to "
"``torch.compile``, ``torch.compile`` traces into ``triton_op`` to apply "
"optimizations."
msgstr ""
"``triton_op`` 是一种定义由一个或多个 Triton "
"内核支持的自定义操作符的结构化方式：与常规自定义操作符（``torch.library.custom_op``）一样，您可以通过 "
"``torch.library`` 指定与 PyTorch 子系统的交互方式。然而，与 ``torch.library.custom_op`` 创建的对"
" ``torch.compile`` 不透明可调用对象不同，``torch.compile`` 会追踪 ``triton_op`` 以应用优化。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Here’s a chart of which API to use when integrating Triton kernels with "
"PyTorch."
msgstr "以下是将 Triton 内核与 PyTorch 集成时使用的 API 表。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Triton kernel (no explicit ``torch.library`` wrapper)"
msgstr "Triton 内核（无显式 ``torch.library`` 包装）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.library.triton_op``"
msgstr "``torch.library.triton_op``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.library.custom_op``"
msgstr "``torch.library.custom_op``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Supports inference"
msgstr "支持推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Yes"
msgstr "是"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Supports training"
msgstr "支持训练"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In the majority of cases"
msgstr "大多数情况下"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Supports ``torch.compile``"
msgstr "支持 ``torch.compile``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Supports ``torch.compile(fullgraph=True)``"
msgstr "支持 ``torch.compile(fullgraph=True)``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In all cases"
msgstr "所有情况都支持"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Does torch.compile trace into the implementation?"
msgstr "``torch.compile`` 是否追踪到实现中？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "No"
msgstr "否"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Supports AOTInductor"
msgstr "支持 AOTInductor"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Supports PyTorch Subsystems like FlopCounterMode, CPU Fallback, Tensor "
"Subclasses"
msgstr "支持 PyTorch 子系统，如 FlopCounterMode、CPU 回退、Tensor 子类"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Wrapping Triton kernels with ``triton_op``"
msgstr "使用 ``triton_op`` 包装 Triton 内核"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Use ``torch.library.triton_op`` to wrap a function that may invoke one or "
"more Triton kernels. Use ``torch.library.wrap_triton`` to wrap the calls to "
"the Triton kernel."
msgstr ""
"使用 ``torch.library.triton_op`` 包装一个可能调用一个或多个 Triton 内核的函数。使用 "
"``torch.library.wrap_triton`` 包装对 Triton 内核的调用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You can invoke the ``triton_op`` in one of the following two ways."
msgstr "可以通过以下两种方式之一调用 ``triton_op``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The resulting ``triton_op`` works with ``torch.compile`` and "
"``AOTInductor``."
msgstr "生成的 ``triton_op`` 可与 ``torch.compile`` 和 ``AOTInductor`` 配合使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding training support"
msgstr "添加训练支持"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Use ``register_autograd`` to add an autograd formula for the ``triton_op``. "
"Prefer this to using ``torch.autograd.Function`` (which has various "
"composability footguns with ``torch.compile``)."
msgstr ""
"使用 ``register_autograd`` 为 ``triton_op`` 添加自动梯度公式。优先使用这种方式，而不是 "
"``torch.autograd.Function``（后者在与 ``torch.compile`` 组合使用时存在某些隐藏问题）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Note that the backward must be a composition of PyTorch-understood "
"operators. If you want the backward to call Triton kernels, then those must "
"be wrapped in ``triton_op`` as well:"
msgstr ""
"请注意，反向传播必须由 PyTorch 能够理解的操作符组成。如果您希望反向传播调用 Triton 内核，则这些内核也必须用 ``triton_op``"
" 包装："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding a CPU Fallback"
msgstr "添加 CPU 回退"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Triton kernels don’t run on CPU. Use  ``register_kernel`` to add a CPU (or "
"any other device) fallback for the ``triton_op``:"
msgstr ""
"Triton 内核无法在 CPU 上运行。使用 ``register_kernel`` 为 ``triton_op`` 添加 CPU（或其他设备）回退："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The fallback must be composed of PyTorch operators."
msgstr "回退必须由 PyTorch 操作符组成。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Adding a FlopCounter Formula"
msgstr "添加 FlopCounter 公式"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To specify how many flops the triton kernel reports under PyTorch's flop "
"counter, use ``register_flop_formula``."
msgstr ""
"要在 PyTorch 的 flop 计数器中指定 Triton 内核报告多少浮点操作数（flops），请使用 "
"``register_flop_formula``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``FlopCounterMode`` requires `tabulate "
"<https://pypi.org/project/tabulate/>`__. Before running the code below, make"
" sure you have ``tabulate`` installed or install by running ``pip install "
"tabulate``."
msgstr ""
"``FlopCounterMode`` 需要 `tabulate <https://pypi.org/project/tabulate/>`__ "
"。在运行以下代码之前，请确保已安装 ``tabulate``，或者通过运行 ``pip install tabulate`` 进行安装。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Limitations"
msgstr "限制"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As of PyTorch 2.3, the support for user-defined Triton kernels in "
"``torch.compile`` includes dynamic shapes, ``torch.autograd.Function``, JIT "
"inductor, and AOT inductor. You can use these features together to build "
"complex, high-performance models."
msgstr ""
"从 PyTorch 2.3 开始，``torch.compile`` 对用户定义的 Triton "
"内核的支持包括动态形状、``torch.autograd.Function``、JIT inductor 和 AOT "
"inductor。您可以一起使用这些功能来构建复杂的高性能模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch 2.6 added ``torch.library.triton_op``, which adds support for user-"
"defined Triton kernels in tensor subclasses and other advanced features."
msgstr ""
"PyTorch 2.6 添加了 ``torch.library.triton_op``，为张量子类和其他高级功能中的用户定义的 Triton "
"内核提供支持。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "However, there are certain limitations to be aware of:"
msgstr "然而，需要注意某些限制："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Triton Features:** While ``triton.heuristics`` can be used either "
"standalone or before ``triton.autotune``, it cannot be used after "
"``triton.autotune``. This implies that if ``triton.heuristics`` and "
"``triton.autotune`` are to be used together, ``triton.heuristics`` must be "
"used first."
msgstr ""
"**Triton 功能：** 虽然 ``triton.heuristics`` 可以单独使用，也可以在 ``triton.autotune`` "
"之前使用，但不能在 ``triton.autotune`` 之后使用。这意味着如果 ``triton.heuristics`` 和 "
"``triton.autotune`` 需要一起使用，必须先使用 ``triton.heuristics``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we explored how to utilize user-defined Triton kernels with "
"``torch.compile``. We delved into the basic usage of a simple vector "
"addition kernel and advanced usage involving Triton's autotune feature. We "
"also discussed the composability of user-defined Triton kernels with other "
"PyTorch features and highlighted some current limitations."
msgstr ""
"在本教程中，我们探讨了如何在 ``torch.compile`` 中使用用户定义的 Triton "
"内核。我们深入学习了一个简单的向量加法内核的基本使用，以及涉及 Triton 的 autotune 功能的高级用法。我们还讨论了用户定义的 Triton"
" 内核与其他 PyTorch 功能的可组合性，并指出了一些当前的限制。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Compiling the Optimizers "
"<https://pytorch.org/tutorials/recipes/compiling_optimizer.html>`__"
msgstr ""
"`编译优化器 <https://pytorch.org/tutorials/recipes/compiling_optimizer.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Implementing High-Performance Transformers with Scaled Dot Product "
"Attention "
"<https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html>`__"
msgstr ""
"`实现高性能带缩放点积注意力的 Transformer "
"<https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: "
"torch_compile_user_defined_triton_kernel_tutorial.py "
"<torch_compile_user_defined_triton_kernel_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: "
"torch_compile_user_defined_triton_kernel_tutorial.py "
"<torch_compile_user_defined_triton_kernel_tutorial.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: "
"torch_compile_user_defined_triton_kernel_tutorial.ipynb "
"<torch_compile_user_defined_triton_kernel_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: "
"torch_compile_user_defined_triton_kernel_tutorial.ipynb "
"<torch_compile_user_defined_triton_kernel_tutorial.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here "
"<sphx_glr_download_recipes_torch_compiler_set_stance_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_recipes_torch_compiler_set_stance_tutorial.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Dynamic Compilation Control with ``torch.compiler.set_stance``"
msgstr "通过 ``torch.compiler.set_stance`` 动态控制编译"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author:** `William Wen <https://github.com/williamwen42>`_"
msgstr "**作者：** `William Wen <https://github.com/williamwen42>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compiler.set_stance`` is a ``torch.compiler`` API that enables you "
"to change the behavior of ``torch.compile`` across different calls to your "
"model without having to reapply ``torch.compile`` to your model."
msgstr ""
"``torch.compiler.set_stance`` 是 ``torch.compiler`` 的一种 API，允许您在多次调用模型时更改 "
"``torch.compile`` 的行为，而无需对模型重新应用 ``torch.compile``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This recipe provides some examples on how to use "
"``torch.compiler.set_stance``."
msgstr "本教程提供了一些关于如何使用 ``torch.compiler.set_stance`` 的示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch >= 2.6``"
msgstr "``torch >= 2.6``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Description"
msgstr "描述"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compile.set_stance`` can be used as a decorator, context manager, or"
" raw function to change the behavior of ``torch.compile`` across different "
"calls to your model."
msgstr ""
"``torch.compile.set_stance`` 可以作为装饰器、上下文管理器或普通函数使用，以更改 ``torch.compile`` "
"在不同模型调用中的行为。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the example below, the ``\"force_eager\"`` stance ignores all "
"``torch.compile`` directives."
msgstr "在以下示例中，``\"force_eager\"`` 状态会忽略所有 ``torch.compile`` 指令。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Sample decorator usage"
msgstr "装饰器用法示例"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Sample context manager usage"
msgstr "上下文管理器用法示例"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Sample raw function usage"
msgstr "普通函数用法示例"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compile`` stance can only be changed **outside** of any "
"``torch.compile`` region. Attempts to do otherwise will result in an error."
msgstr ""
"``torch.compile`` 状态只能在任何 ``torch.compile`` 区域**之外**进行更改。尝试违反此规则会导致错误。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Other stances include:"
msgstr "其他状态包括："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``\"default\"``: The default stance, used for normal compilation."
msgstr "``\"default\"``：默认状态，用于正常编译。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``\"eager_on_recompile\"``: Run code eagerly when a recompile is necessary. "
"If there is cached compiled code valid for the input, it will still be used."
msgstr "``\"eager_on_recompile\"``：在需要重新编译时以急切模式运行代码。如果存在适合输入的缓存编译代码，仍然会使用该代码。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``\"fail_on_recompile\"``: Raise an error when recompiling a function."
msgstr "``\"fail_on_recompile\"``：在重新编译函数时抛出错误。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"See the ``torch.compiler.set_stance`` `doc page "
"<https://pytorch.org/docs/main/generated/torch.compiler.set_stance.html#torch.compiler.set_stance>`__"
" for more stances and options. More stances/options may also be added in the"
" future."
msgstr ""
"查看 ``torch.compiler.set_stance`` 的 `文档页面 "
"<https://pytorch.org/docs/main/generated/torch.compiler.set_stance.html#torch.compiler.set_stance>`__"
" ，了解更多状态和选项。将来也可能会添加更多状态/选项。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Examples"
msgstr "示例"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Preventing recompilation"
msgstr "防止重新编译"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Some models do not expect any recompilations - for example, you may always "
"have inputs with the same shape. Since recompilations may be expensive, we "
"may wish to error out when we attempt to recompile so we can detect and fix "
"recompilation cases. The ``\"fail_on_recompilation\"`` stance can be used "
"for this."
msgstr ""
"某些模型不希望有任何重新编译。例如，您可能总是有相同形状的输入。由于重新编译可能代价昂贵，我们可能希望在尝试重新编译时抛出错误，以便检测并修复相关问题。可以使用"
" ``\"fail_on_recompilation\"`` 状态。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If erroring out is too disruptive, we can use ``\"eager_on_recompile\"`` "
"instead, which will cause ``torch.compile`` to fall back to eager instead of"
" erroring out. This may be useful if we don't expect recompilations to "
"happen frequently, but when one is required, we'd rather pay the cost of "
"running eagerly over the cost of recompilation."
msgstr ""
"如果抛出错误过于中断流程，我们可以改用 ``\"eager_on_recompile\"``，这会使 ``torch.compile`` "
"回退到急切模式，而不是抛出错误。如果我们不期望重新编译频繁发生，但当确实需要时，更愿意接受急切运行的成本而非重新编译的成本，这种方式可能更有用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Measuring performance gains"
msgstr "测量性能提升"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.compiler.set_stance`` can be used to compare eager vs. compiled "
"performance without having to define a separate eager model."
msgstr "``torch.compiler.set_stance`` 可用于在不定义单独急切模型的情况下比较急切模式和编译模式的性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Crashing sooner"
msgstr "尽早发现错误"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Running an eager iteration first before a compiled iteration using the "
"``\"force_eager\"`` stance can help us to catch errors unrelated to "
"``torch.compile`` before attempting a very long compile."
msgstr ""
"通过在编译迭代之前先运行一个急切模式迭代（使用 ``\"force_eager\"`` 状态），可以帮助我们在尝试长时间编译之前捕捉与 "
"``torch.compile`` 无关的错误。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we have learned how to use the ``torch.compiler.set_stance``"
" API to modify the behavior of ``torch.compile`` across different calls to a"
" model without needing to reapply it. The recipe demonstrates using "
"``torch.compiler.set_stance`` as a decorator, context manager, or raw "
"function to control compilation stances like ``force_eager``, ``default``, "
"``eager_on_recompile``, and \"fail_on_recompile.\""
msgstr ""
"在本教程中，我们学习了如何使用 ``torch.compiler.set_stance`` API 来在不需要重新应用 "
"``torch.compile`` 的情况下修改对模型的多次调用中的编译行为。教程展示了将 ``torch.compiler.set_stance`` "
"作为装饰器、上下文管理器或普通函数使用，以控制 ``force_eager``、``default``、``eager_on_recompile`` 和"
" ``fail_on_recompile`` 等编译状态。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For more information, see: `torch.compiler.set_stance API documentation "
"<https://pytorch.org/docs/main/generated/torch.compiler.set_stance.html#torch.compiler.set_stance>`__."
msgstr ""
"更多信息，参见：`torch.compiler.set_stance API 文档 "
"<https://pytorch.org/docs/main/generated/torch.compiler.set_stance.html#torch.compiler.set_stance>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: "
"torch_compiler_set_stance_tutorial.py "
"<torch_compiler_set_stance_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: torch_compiler_set_stance_tutorial.py "
"<torch_compiler_set_stance_tutorial.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: "
"torch_compiler_set_stance_tutorial.ipynb "
"<torch_compiler_set_stance_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: torch_compiler_set_stance_tutorial.ipynb "
"<torch_compiler_set_stance_tutorial.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_torch_export_aoti_python.py>` to"
" download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_recipes_torch_export_aoti_python.py>` "
"下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "An end-to-end example of how to use AOTInductor for Python runtime."
msgstr "一个关于如何在 Python 运行时使用 AOTInductor 的端到端示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"torch.export, AOTInductor, torch._inductor.aoti_compile_and_package, "
"aot_compile, torch._export.aoti_load_package"
msgstr ""
"torch.export, AOTInductor, torch._inductor.aoti_compile_and_package, "
"aot_compile, torch._export.aoti_load_package"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``torch.export`` AOTInductor Tutorial for Python runtime (Beta)"
msgstr "为 Python 运行时提供的 ``torch.export`` AOTInductor 教程（Beta 版）"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Author:** Ankith Gunapal, Bin Bao, Angela Yi"
msgstr "**作者:** Ankith Gunapal, Bin Bao, Angela Yi"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch._inductor.aoti_compile_and_package`` and "
"``torch._inductor.aoti_load_package`` are in Beta status and are subject to "
"backwards compatibility breaking changes. This tutorial provides an example "
"of how to use these APIs for model deployment using Python runtime."
msgstr ""
"``torch._inductor.aoti_compile_and_package`` 和 "
"``torch._inductor.aoti_load_package`` 处于 Beta 状态，可能会有向后兼容性破坏的变更。本教程提供了一个使用这些"
" API 进行模型部署的示例，使用 Python 运行时。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"It has been shown `previously "
"<https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html#>`__ how "
"AOTInductor can be used to do Ahead-of-Time compilation of PyTorch exported "
"models by creating an artifact that can be run in a non-Python environment. "
"In this tutorial, you will learn an end-to-end example of how to use "
"AOTInductor for Python runtime."
msgstr ""
"之前已展示过 `如何使用 AOTInductor 对 PyTorch 导出的模型进行预编译 "
"<https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html#>`__，通过创建一个可以在非"
" Python 环境下运行的工件。在本教程中，您将学习如何在 Python 运行时中使用 AOTInductor 的端到端示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "**Contents**"
msgstr "**内容**"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "PyTorch 2.6 or later"
msgstr "PyTorch 2.6 或更高版本"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Basic understanding of ``torch.export`` and AOTInductor"
msgstr "对 ``torch.export`` 和 AOTInductor 的基础理解"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Complete the `AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed "
"Models <https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html#>`_"
" tutorial"
msgstr ""
"完成 `AOTInductor: Torch.Export 模型的预编译教程 "
"<https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html#>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What you will learn"
msgstr "您将学到什么"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use AOTInductor for Python runtime."
msgstr "如何在 Python 运行时中使用 AOTInductor。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to use :func:`torch._inductor.aoti_compile_and_package` along with "
":func:`torch.export.export` to generate a compiled artifact"
msgstr ""
"如何使用 :func:`torch._inductor.aoti_compile_and_package` 和 "
":func:`torch.export.export` 来生成编译后的工件"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to load and run the artifact in a Python runtime using "
":func:`torch._export.aot_load`."
msgstr "如何使用 :func:`torch._export.aot_load` 在 Python 运行时中加载并运行工件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "When to you use AOTInductor with a Python runtime"
msgstr "何时在 Python 运行时中使用 AOTInductor"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Compilation"
msgstr "模型编译"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We will use the TorchVision pretrained ``ResNet18`` model as an example."
msgstr "我们将使用 TorchVision 预训练的 ``ResNet18`` 模型作为示例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The first step is to export the model to a graph representation using "
":func:`torch.export.export`. To learn more about using this function, you "
"can check out the `docs <https://pytorch.org/docs/main/export.html>`_ or the"
" `tutorial "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html>`_."
msgstr ""
"第一步是通过 :func:`torch.export.export` 将模型导出为图表示形式。要了解有关此函数的更多信息，可以查看 `文档 "
"<https://pytorch.org/docs/main/export.html>`_ 或 `教程 "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html>`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Once we have exported the PyTorch model and obtained an ``ExportedProgram``,"
" we can apply :func:`torch._inductor.aoti_compile_and_package` to "
"AOTInductor to compile the program to a specified device, and save the "
"generated contents into a \".pt2\" artifact."
msgstr ""
"一旦我们导出了 PyTorch 模型并获得了一个 ``ExportedProgram``，我们可以使用 "
":func:`torch._inductor.aoti_compile_and_package` 将其传递给 "
"AOTInductor，编译程序到指定的设备，并将生成的内容保存为一个 ``.pt2`` 工件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This API supports the same available options that :func:`torch.compile` has,"
" such as ``mode`` and ``max_autotune`` (for those who want to enable CUDA "
"graphs and leverage Triton based matrix multiplications and convolutions)"
msgstr ""
"此 API 支持与 :func:`torch.compile` 相同的选项，例如 ``mode`` 和 ``max_autotune``（对于那些想启用"
" CUDA 图并利用基于 Triton 的矩阵乘法和卷积的用户）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The result of :func:`aoti_compile_and_package` is an artifact "
"\"resnet18.pt2\" which can be loaded and executed in Python and C++."
msgstr ""
"使用 :func:`aoti_compile_and_package` 的结果是一个名为 \"resnet18.pt2\" 的工件，可以在 Python"
" 和 C++ 中加载和执行。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The artifact itself contains a bunch of AOTInductor generated code, such as "
"a generated C++ runner file, a shared library compiled from the C++ file, "
"and CUDA binary files, aka cubin files, if optimizing for CUDA."
msgstr ""
"该工件本身包含大量由 AOTInductor 生成的代码，例如生成的 C++ 运行文件、从 C++ 文件编译的共享库以及 CUDA 二进制文件（即 "
"cubin 文件，如果针对 CUDA 优化）。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Structure-wise, the artifact is a structured ``.zip`` file, with the "
"following specification:"
msgstr "从结构上看，该工件是一个结构化的 ``.zip`` 文件，具有以下规范："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "We can use the following command to inspect the artifact contents:"
msgstr "我们可以使用以下命令检查工件内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Model Inference in Python"
msgstr "在 Python 中进行模型推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To load and run the artifact in Python, we can use "
":func:`torch._inductor.aoti_load_package`."
msgstr "要在 Python 中加载并运行工件，我们可以使用 :func:`torch._inductor.aoti_load_package`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "When to use AOTInductor with a Python Runtime"
msgstr "何时使用带有 Python 运行时的 AOTInductor"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are mainly two reasons why one would use AOTInductor with a Python "
"Runtime:"
msgstr "使用带有 Python 运行时的 AOTInductor 的主要原因有两个："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch._inductor.aoti_compile_and_package`` generates a singular serialized"
" artifact. This is useful for model versioning for deployments and tracking "
"model performance over time."
msgstr ""
"``torch._inductor.aoti_compile_and_package`` "
"会生成单一序列化工件。这对于模型部署的版本管理以及跟踪模型性能变化很有用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"With :func:`torch.compile` being a JIT compiler, there is a warmup cost "
"associated with the first compilation. Your deployment needs to account for "
"the compilation time taken for the first inference. With AOTInductor, the "
"compilation is done ahead of time using ``torch.export.export`` and "
"``torch._inductor.aoti_compile_and_package``. At deployment time, after "
"loading the model, running inference does not have any additional cost."
msgstr ""
"由于 :func:`torch.compile` 是一个 JIT 编译器，首次编译时会有一个启动成本。部署时需要考虑首次推理编译所需的时间。通过使用 "
"AOTInductor，编译在部署之前通过 ``torch.export.export`` 和 "
"``torch._inductor.aoti_compile_and_package`` 完成。在部署期间，加载模型后运行推理不会产生额外的成本。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The section below shows the speedup achieved with AOTInductor for first "
"inference"
msgstr "以下部分展示了使用 AOTInductor 在首次推理中获得的加速效果"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We define a utility function ``timed`` to measure the time taken for "
"inference"
msgstr "我们定义了一个实用函数 ``timed`` 来测量推理所需时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Lets measure the time for first inference using AOTInductor"
msgstr "让我们测量使用 AOTInductor 进行首次推理的时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Lets measure the time for first inference using ``torch.compile``"
msgstr "让我们测量使用 ``torch.compile`` 进行首次推理的时间"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We see that there is a drastic speedup in first inference time using "
"AOTInductor compared to ``torch.compile``"
msgstr "我们发现，与使用 ``torch.compile`` 相比，使用 AOTInductor 的首次推理时间显著减少"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we have learned how to effectively use the AOTInductor for "
"Python runtime by compiling and loading a pretrained ``ResNet18`` model. "
"This process demonstrates the practical application of generating a compiled"
" artifact and running it within a Python environment. We also looked at the "
"advantage of using AOTInductor in model deployments, with regards to speed "
"up in first inference time."
msgstr ""
"在本示例中，我们学习了如何通过编译和加载预训练的 ``ResNet18`` 模型来有效地使用 AOTInductor 在 Python "
"运行时中运行。这一过程展示了生成编译工件并在 Python 环境中运行它的实际应用。我们还研究了在模型部署中使用 AOTInductor "
"的优势，比如首次推理时间的加速效果。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Python source code: torch_export_aoti_python.py "
"<torch_export_aoti_python.py>`"
msgstr ""
":download:`下载 Python 源代码: torch_export_aoti_python.py "
"<torch_export_aoti_python.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: torch_export_aoti_python.ipynb "
"<torch_export_aoti_python.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: torch_export_aoti_python.ipynb "
"<torch_export_aoti_python.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Demonstration of torch.export flow, common challenges and the solutions to "
"address them"
msgstr "展示 torch.export 流程的演示，以及常见挑战和解决方案"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Authors:** `Ankith Gunapal <https://github.com/agunapal>`__, `Jordi Ramon "
"<https://github.com/JordiFB>`__, `Marcos Carranza "
"<https://github.com/macarran>`__"
msgstr ""
"**作者:** `Ankith Gunapal <https://github.com/agunapal>`__, `Jordi Ramon "
"<https://github.com/JordiFB>`__, `Marcos Carranza "
"<https://github.com/macarran>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In the `Introduction to torch.export Tutorial "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html>`__ ,"
" we learned how to use `torch.export "
"<https://pytorch.org/docs/stable/export.html>`__. This tutorial expands on "
"the previous one and explores the process of exporting popular models with "
"code, as well as addresses common challenges that may arise with "
"``torch.export``."
msgstr ""
"在 `torch.export 教程简介 "
"<https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html>`__ "
"中，我们学习了如何使用 `torch.export <https://pytorch.org/docs/stable/export.html>`__ "
"。这个教程是先前的扩展教程，探索了如何使用代码导出流行的模型，并解决了 ``torch.export`` 中可能会遇到的常见问题。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, you will learn how to export models for these use cases:"
msgstr "在本教程中，您将学习如何针对以下应用场景导出模型："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Video classifier (`MViT "
"<https://pytorch.org/vision/main/models/video_mvit.html>`__)"
msgstr ""
"视频分类 (`MViT <https://pytorch.org/vision/main/models/video_mvit.html>`__)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Automatic Speech Recognition (`OpenAI Whisper-Tiny "
"<https://huggingface.co/openai/whisper-tiny>`__)"
msgstr ""
"自动语音识别 (`OpenAI Whisper-Tiny <https://huggingface.co/openai/whisper-"
"tiny>`__)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Image Captioning (`BLIP <https://github.com/salesforce/BLIP>`__)"
msgstr "图像描述 (`BLIP <https://github.com/salesforce/BLIP>`__)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Promptable Image Segmentation (`SAM2 <https://ai.meta.com/sam2/>`__)"
msgstr "可提示图像分割 (`SAM2 <https://ai.meta.com/sam2/>`__)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Each of the four models were chosen to demonstrate unique features of "
"`torch.export`, as well as some practical considerations and issues faced in"
" the implementation."
msgstr "选择这四个模型是为了展示 `torch.export` 的独特功能，以及在实际实施中遇到的某些注意事项和问题。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Basic understanding of ``torch.export`` and PyTorch Eager inference."
msgstr "对 ``torch.export`` 和 PyTorch Eager 推理的基础理解。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Key requirement for ``torch.export``: No graph break"
msgstr "``torch.export`` 的关键要求: 无图断裂"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`torch.compile "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__ "
"speeds up PyTorch code by using JIT to compile PyTorch code into optimized "
"kernels. It optimizes the given model using ``TorchDynamo`` and creates an "
"optimized graph , which is then lowered into the hardware using the backend "
"specified in the API. When TorchDynamo encounters unsupported Python "
"features, it breaks the computation graph, lets the default Python "
"interpreter handle the unsupported code, and then resumes capturing the "
"graph. This break in the computation graph is called a `graph break "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#torchdynamo-"
"and-fx-graphs>`__."
msgstr ""
"`torch.compile "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__ "
"通过使用 JIT 编译 PyTorch 代码为优化的内核加速 PyTorch 代码。它使用 ``TorchDynamo`` "
"优化给定的模型并创建一个优化的图，然后通过 API 指定的后端将其加载到硬件中。当 TorchDynamo 遇到不支持的 Python "
"功能时，它会中断计算图，让默认 Python 解释器处理不支持的代码，然后恢复图的捕获。此图的中断被称为 `图断裂 "
"<https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#torchdynamo-"
"and-fx-graphs>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"One of the key differences between ``torch.export`` and ``torch.compile`` is"
" that ``torch.export`` doesn’t support graph breaks which means that the "
"entire model or part of the model that you are exporting needs to be a "
"single graph. This is because handling graph breaks involves interpreting "
"the unsupported operation with default Python evaluation, which is "
"incompatible with what ``torch.export`` is designed for. You can read "
"details about the differences between the various PyTorch frameworks in this"
" `link <https://pytorch.org/docs/main/export.html#existing-frameworks>`__"
msgstr ""
"``torch.export`` 和 ``torch.compile`` 的一个关键区别是 ``torch.export`` "
"不支持图断裂，这意味着您导出的整个模型或模型的一部分需要是单个图。这是因为处理图断裂涉及使用默认 Python 评价解释不支持的操作，这与 "
"``torch.export`` 的设计不兼容。您可以在此 `链接 "
"<https://pytorch.org/docs/main/export.html#existing-frameworks>`__ 中阅读有关不同 "
"PyTorch 框架之间差异的详细信息。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You can identify graph breaks in your program by using the following "
"command:"
msgstr "您可以使用以下命令标识程序中的图断裂："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You will need to modify your program to get rid of graph breaks. Once "
"resolved, you are ready to export the model. PyTorch runs `nightly "
"benchmarks <https://hud.pytorch.org/benchmark/compilers>`__ for "
"`torch.compile` on popular HuggingFace and TIMM models. Most of these models"
" have no graph breaks."
msgstr ""
"您需要修改程序以消除图断裂。一旦解决，您就可以准备导出模型。PyTorch 在流行的 HuggingFace 和 TIMM 模型上运行 `每夜基准测试 "
"<https://hud.pytorch.org/benchmark/compilers>`__ 以分析 "
"``torch.compile``。这其中的大多数模型都没有图断裂。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The models in this recipe have no graph breaks, but fail with "
"`torch.export`."
msgstr "本教程中的模型没有图断裂，但使用 ``torch.export`` 时会失败。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Video Classification"
msgstr "视频分类"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"MViT is a class of models based on `MultiScale Vision Transformers "
"<https://arxiv.org/abs/2104.11227>`__. This model has been trained for video"
" classification using the `Kinetics-400 Dataset "
"<https://arxiv.org/abs/1705.06950>`__. This model with a relevant dataset "
"can be used for action recognition in the context of gaming."
msgstr ""
"MViT 是基于 `多尺度视觉Transformer <https://arxiv.org/abs/2104.11227>`__ "
"的模型类别。此模型已使用 `Kinetics-400 数据集 <https://arxiv.org/abs/1705.06950>`__ "
"进行视频分类训练。搭载相关数据集，该模型可用于游戏中的动作识别。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code below exports MViT by tracing with ``batch_size=2`` and then checks"
" if the ExportedProgram can run with ``batch_size=4``."
msgstr ""
"以下代码通过设置 ``batch_size=2`` 对 MViT 进行追踪导出，并检查 ``ExportedProgram`` 是否可以在 "
"``batch_size=4`` 下运行。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Error: Static batch size"
msgstr "错误: 静态批大小"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default, the exporting flow will trace the program assuming that all "
"input shapes are static, so if you run the program with input shapes that "
"are different than the ones you used while tracing, you will run into an "
"error."
msgstr "默认情况下，导出流程将假定所有输入形状是静态的，因此如果您使用不同于跟踪时输入形状的输入形状运行程序，将会遇到错误。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Solution"
msgstr "解决方案"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To address the error, we specify the first dimension of the input "
"(``batch_size``) to be dynamic , specifying the expected range of "
"``batch_size``. In the corrected example shown below, we specify that the "
"expected ``batch_size`` can range from 1 to 16. One detail to notice that "
"``min=2``  is not a bug and is explained in `The 0/1 Specialization Problem "
"<https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-"
"yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk>`__."
" A detailed description of dynamic shapes for ``torch.export`` can be found "
"in the export tutorial. The code shown below demonstrates how to export mViT"
" with dynamic batch sizes:"
msgstr ""
"为解决错误，我们指定输入的第一个维度（``batch_size``）为动态，指定 ``batch_size`` "
"的预期范围。在下面显示的修正示例中，我们指定预期的 ``batch_size`` 范围为 1 至 16。请注意， ``min=2`` "
"不是一个错误，其原因可以在 `0/1 专业化问题 "
"<https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-"
"yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk>`__"
" 中找到解释。有关 ``torch.export`` 动态形状的详细描述可在导出教程中找到。以下代码演示了如何在批大小动态化的情况下导出 mViT："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Automatic Speech Recognition"
msgstr "自动语音识别"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Automatic Speech Recognition** (ASR) is the use of machine learning to "
"transcribe spoken language into text. `Whisper "
"<https://arxiv.org/abs/2212.04356>`__ is a Transformer based encoder-decoder"
" model from OpenAI, which was trained on 680k hours of labelled data for ASR"
" and speech translation. The code below tries to export ``whisper-tiny`` "
"model for ASR."
msgstr ""
"**自动语音识别** (ASR) 是机器学习的一种应用，旨在将口语语言转录成文本。`Whisper "
"<https://arxiv.org/abs/2212.04356>`__ 是 OpenAI 的基于 Transformer 的编码器-"
"解码器模型，训练了 68 万小时的标记数据，用于 ASR 和语音翻译。以下代码尝试导出 ASR 的 ``whisper-tiny`` 模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Error: strict tracing with TorchDynamo"
msgstr "错误: 使用 TorchDynamo 的严格追踪"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default ``torch.export`` traces your code using `TorchDynamo "
"<https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html>`__, a "
"byte-code analysis engine,  which symbolically analyzes your code and builds"
" a graph. This analysis provides a stronger guarantee about safety but not "
"all Python code is supported. When we export the ``whisper-tiny`` model  "
"using the default strict mode, it typically returns an error in Dynamo due "
"to an unsupported feature. To understand why this errors in Dynamo, you can "
"refer to this `GitHub issue "
"<https://github.com/pytorch/pytorch/issues/144906>`__."
msgstr ""
"默认情况下，``torch.export`` 使用 `TorchDynamo "
"<https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html>`__ "
"对您的代码进行追踪，这是一种字节代码分析引擎，它以符号方式分析您的代码并构建图。这种分析提供了更强的安全性保证，但并不支持所有 Python "
"代码。当我们使用默认严格模式导出 ``whisper-tiny`` 模型时，它通常会由于不支持的功能而在 Dynamo 中返回错误。要了解为什么 "
"Dynamo 出错，您可以参考此 `GitHub 问题 "
"<https://github.com/pytorch/pytorch/issues/144906>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To address the above error , ``torch.export`` supports  the ``non_strict`` "
"mode where the program is traced using the Python interpreter, which works "
"similar to PyTorch eager execution. The only difference is that all "
"``Tensor`` objects will be replaced by ``ProxyTensors``, which will record "
"all their operations into a graph. By using ``strict=False``, we are able to"
" export the program."
msgstr ""
"为了解决上述错误，“torch.export”支持“非严格(non_strict)”模式，在这种模式下程序通过Python解释器进行追踪，相当于PyTorch的即时执行(eager"
" "
"execution)。唯一不同的是，所有“Tensor”对象将被替换为“ProxyTensors”，后者将在图中记录它们的所有操作。通过设置“strict=False”，我们可以导出程序。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Image Captioning"
msgstr "图像描述生成"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Image Captioning** is the task of defining the contents of an image in "
"words. In the context of gaming, Image Captioning can be used to enhance the"
" gameplay experience by dynamically generating text description of the "
"various game objects in the scene, thereby providing the gamer with "
"additional details. `BLIP <https://arxiv.org/pdf/2201.12086>`__ is a popular"
" model for Image Captioning `released by SalesForce Research "
"<https://github.com/salesforce/BLIP>`__. The code below tries to export BLIP"
" with ``batch_size=1``."
msgstr ""
"**图像描述生成**是一种用语言定义图像内容的任务。在游戏场景中，图像描述生成可以通过动态生成场景中各种游戏对象的文本描述来增强游戏体验，从而为玩家提供额外的细节。`BLIP"
" <https://arxiv.org/pdf/2201.12086>`__ "
"是一个由SalesForce研究团队发布的流行图像描述生成模型。以下代码尝试使用“batch_size=1”导出BLIP。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Error: Cannot mutate tensors with frozen storage"
msgstr "错误：无法修改冻结存储的张量"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"While exporting a model, it might fail because the model implementation "
"might contain certain Python operations which are not yet supported by "
"``torch.export``. Some of these failures may have a workaround. BLIP is an "
"example where the original model errors, which can be resolved by making a "
"small change in the code. ``torch.export`` lists the common cases of "
"supported and unsupported operations in `ExportDB "
"<https://pytorch.org/docs/main/generated/exportdb/index.html>`__ and shows "
"how you can modify your code to make it export compatible."
msgstr ""
"在导出模型时可能会失败，因为模型实现可能包含“torch.export”尚未支持的某些Python操作。这些错误中的一些可能有解决方法。BLIP是一个案例，其中原始模型出现错误，这可以通过对代码进行一些小修改来解决。“torch.export”列出了在"
" `ExportDB <https://pytorch.org/docs/main/generated/exportdb/index.html>`__ "
"中支持和不支持的常见操作，并展示了如何修改代码使其符合导出要求。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Clone the `tensor "
"<https://github.com/salesforce/BLIP/blob/main/models/blip.py#L112>`__ where "
"export fails."
msgstr ""
"克隆 `模型张量 "
"<https://github.com/salesforce/BLIP/blob/main/models/blip.py#L112>`__，以解决导出时的错误。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This constraint has been relaxed in PyTorch 2.7 nightlies. This should work "
"out-of-the-box in PyTorch 2.7"
msgstr "此限制已在PyTorch 2.7的夜间版本中放松。在PyTorch 2.7中应该可以直接使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Promptable Image Segmentation"
msgstr "可提示的图像分割"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**Image segmentation** is a computer vision technique that divides a digital"
" image into distinct groups of pixels, or segments, based on their "
"characteristics. `Segment Anything Model (SAM) "
"<https://ai.meta.com/blog/segment-anything-foundation-model-image-"
"segmentation/>`__) introduced promptable image segmentation, which predicts "
"object masks given prompts that indicate the desired object. `SAM 2 "
"<https://ai.meta.com/sam2/>`__ is the first unified model for segmenting "
"objects across images and videos. The `SAM2ImagePredictor "
"<https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py#L20>`__"
" class provides an easy interface to the model for prompting the model. The "
"model can take as input both point and box prompts, as well as masks from "
"the previous iteration of prediction. Since SAM2 provides strong zero-shot "
"performance for object tracking, it can be used for tracking game objects in"
" a scene."
msgstr ""
"**图像分割**是一种计算机视觉技术，根据数字图像的特性将其划分为不同的像素组或段。`万物分割模型(SAM) "
"<https://ai.meta.com/blog/segment-anything-foundation-model-image-"
"segmentation/>`__ 提出了可提示的图像分割，它能够根据提示预测对象的掩码。而 `SAM 2 "
"<https://ai.meta.com/sam2/>`__ 是第一个统一的模型，可用于图像和视频中的对象分割。`SAM2ImagePredictor "
"<https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py#L20>`__"
" "
"类为模型提供了简单的接口用于提示。模型可接受点和框提示，以及预测的上一轮迭代生成的掩码。鉴于SAM2在对象跟踪方面提供了强大的零样本性能，它可以用来跟踪场景中的游戏对象。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The tensor operations in the predict method of `SAM2ImagePredictor "
"<https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py#L20>`__"
"  are happening in the `_predict "
"<https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py#L291>`__"
" method. So, we try to export like this."
msgstr ""
"在 `SAM2ImagePredictor "
"<https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py#L20>`__"
" 的预测方法中，张量操作发生在 `_predict "
"<https://github.com/facebookresearch/sam2/blob/main/sam2/sam2_image_predictor.py#L291>`__"
" 方法中。因此，我们尝试以下方式导出。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Error: Model is not of type ``torch.nn.Module``"
msgstr "错误：模型不是类型“torch.nn.Module”"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"``torch.export`` expects the module to be of type ``torch.nn.Module``. "
"However, the module we are trying to export is a class method. Hence it "
"errors."
msgstr "“torch.export”要求模块类型是“torch.nn.Module”。然而，我们尝试导出的模块是一个类方法，因此出错。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"We write a helper class, which inherits from ``torch.nn.Module`` and call "
"the ``_predict method`` in the ``forward`` method of the class. The complete"
" code can be found `here "
"<https://github.com/anijain2305/sam2/blob/ued/sam2/sam2_image_predictor.py#L293-L311>`__."
msgstr ""
"我们编写一个辅助类，该类继承自“torch.nn.Module”并在类的“forward”方法中调用“_predict”方法。完整代码可以在 `这里 "
"<https://github.com/anijain2305/sam2/blob/ued/sam2/sam2_image_predictor.py#L293-L311>`__"
" 找到。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we have learned how to use ``torch.export`` to export "
"models for popular use cases by addressing challenges through correct "
"configuration and simple code modifications. Once you are able to export a "
"model, you can lower the ``ExportedProgram`` into your hardware using "
"`AOTInductor "
"<https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html>`__ in "
"case of servers and `ExecuTorch "
"<https://pytorch.org/executorch/stable/index.html>`__ in case of edge "
"device. To learn more about ``AOTInductor`` (AOTI), please refer to the "
"`AOTI tutorial "
"<https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html>`__. To"
" learn more about ``ExecuTorch`` , please refer to the `ExecuTorch tutorial "
"<https://pytorch.org/executorch/stable/tutorials/export-to-executorch-"
"tutorial.html>`__."
msgstr ""
"在本教程中，我们学习了如何通过正确的配置和简单的代码修改使用“torch.export”导出流行使用场景下的模型。一旦可以导出模型，就可以在硬件上将“ExportedProgram”降级使用，例如在服务器上使用"
" `AOTInductor "
"<https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html>`__，或在边缘设备上使用"
" `ExecuTorch "
"<https://pytorch.org/executorch/stable/index.html>`__。要了解更多关于“AOTInductor”（AOTI）的内容，请参考"
" `AOTI教程 "
"<https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html>`__。要了解更多关于“ExecuTorch”的内容，请参考"
" `ExecuTorch教程 <https://pytorch.org/executorch/stable/tutorials/export-to-"
"executorch-tutorial.html>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Click :ref:`here <sphx_glr_download_recipes_torch_logs.py>` to download the "
"full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_recipes_torch_logs.py>` 下载完整示例代码"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "(beta) Using TORCH_LOGS python API with torch.compile"
msgstr "(测试版) 使用TORCH_LOGS Python API 和 torch.compile"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This tutorial introduces the ``TORCH_LOGS`` environment variable, as well as"
" the Python API, and demonstrates how to apply it to observe the phases  of "
"``torch.compile``."
msgstr "本教程介绍了“TORCH_LOGS”环境变量和Python API，并演示了如何应用它来观察“torch.compile”阶段。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this example, we'll set up a simple Python function which performs an "
"elementwise add and observe the compilation process with ``TORCH_LOGS`` "
"Python API."
msgstr "在这个例子中，我们将设置一个执行元素级加法的简单Python函数，并利用“TORCH_LOGS”Python API观察编译过程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There is also an environment variable ``TORCH_LOGS``, which can be used to "
"change logging settings at the command line. The equivalent environment "
"variable setting is shown for each example."
msgstr "还有一个环境变量“TORCH_LOGS”，可以用来在命令行中更改日志记录设置。每个示例都显示了等效的环境变量设置。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial we introduced the TORCH_LOGS environment variable and "
"python API by experimenting with a small number of the available logging "
"options. To view descriptions of all available options, run any python "
"script which imports torch and set TORCH_LOGS to \"help\"."
msgstr ""
"在本教程中，我们通过实验少量可用的日志选项介绍了TORCH_LOGS环境变量和Python "
"API。要查看所有可用选项的描述，运行任何导入torch的Python脚本，并将TORCH_LOGS设置为“help”。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Alternatively, you can view the `torch._logging documentation`_ to see "
"descriptions of all available logging options."
msgstr "或者，您可以查看 `torch._logging 文档`_ 来了解所有可用日志选项的描述。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"For more information on torch.compile, see the `torch.compile tutorial`_."
msgstr "有关 torch.compile 的更多信息，请参见 `torch.compile教程`_。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ":download:`Download Python source code: torch_logs.py <torch_logs.py>`"
msgstr ":download:`下载Python源代码: torch_logs.py <torch_logs.py>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
":download:`Download Jupyter notebook: torch_logs.ipynb <torch_logs.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: torch_logs.ipynb <torch_logs.ipynb>`"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "TorchScript for Deployment"
msgstr "用于部署的TorchScript"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What TorchScript is"
msgstr "什么是TorchScript"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to export your trained model in TorchScript format"
msgstr "如何以TorchScript格式导出您的训练模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to load your TorchScript model in C++ and do inference"
msgstr "如何在C++中加载TorchScript模型并进行推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "libtorch 1.5"
msgstr "libtorch 1.5"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "C++ compiler"
msgstr "C++编译器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The instructions for installing the three PyTorch components are available "
"at `pytorch.org`_. The C++ compiler will depend on your platform."
msgstr "安装三个PyTorch组件的说明可在 `pytorch.org`_ 上找到。C++编译器将取决于您的平台。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is TorchScript?"
msgstr "什么是TorchScript?"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"**TorchScript** is an intermediate representation of a PyTorch model "
"(subclass of ``nn.Module``) that can then be run in a high-performance "
"environment like C++. It’s a high-performance subset of Python that is meant"
" to be consumed by the **PyTorch JIT Compiler,** which performs run-time "
"optimization on your model’s computation. TorchScript is the recommended "
"model format for doing scaled inference with PyTorch models. For more "
"information, see the PyTorch `Introduction to TorchScript tutorial`_, the "
"`Loading A TorchScript Model in C++ tutorial`_, and the `full TorchScript "
"documentation`_, all of which are available on `pytorch.org`_."
msgstr ""
"**TorchScript** "
"是PyTorch模型（“nn.Module”的子类）的中间表示，可以在像C++这样的高性能环境中运行。它是一种高性能的Python子集，旨在被 "
"**PyTorch JIT编译器** "
"使用，该编译器对模型的计算进行运行时优化。TorchScript是使用PyTorch模型进行扩展推理的推荐模型格式。有关更多信息，请参见PyTorch "
"`TorchScript介绍教程`_、`在C++中加载TorchScript模型教程`_ 和完整的 `TorchScript文档`_，它们均可在 "
"`pytorch.org`_ 上找到。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to Export Your Model"
msgstr "如何导出您的模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"As an example, let’s take a pretrained vision model. All of the pretrained "
"models in TorchVision are compatible with TorchScript."
msgstr "作为示例，我们采用一个预训练视觉模型。TorchVision中的所有预训练模型都兼容TorchScript。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Run the following Python 3 code, either in a script or from the REPL:"
msgstr "运行以下Python 3代码，无论是在脚本中还是在REPL中:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Let’s do a sanity check on the equivalence of the two models:"
msgstr "让我们对两个模型进行同等性检查:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "You should see that both versions of the model give the same results:"
msgstr "您应该会看到两种版本的模型都给出了相同的结果:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "With that check confirmed, go ahead and save the model:"
msgstr "确认检查后，继续保存模型:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Loading TorchScript Models in C++"
msgstr "在C++中加载TorchScript模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Create the following C++ file and name it ``ts-infer.cpp``:"
msgstr "创建以下C++文件，并命名为“ts-infer.cpp”:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "This program:"
msgstr "该程序:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Loads the model you specify on the command line"
msgstr "加载您在命令行中指定的模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Creates a dummy “image” input tensor"
msgstr "创建一个dummy“图像”输入张量"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Performs inference on the input"
msgstr "对输入进行推理"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Also, notice that there is no dependency on TorchVision in this code. The "
"saved version of your TorchScript model has your learning weights *and* your"
" computation graph - nothing else is needed."
msgstr ""
"此外，请注意，此代码中没有对TorchVision的依赖。您的TorchScript模型的保存版包含您的学习权重*以及*您的计算图 - "
"不需要其他任何东西。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Building and Running Your C++ Inference Engine"
msgstr "构建和运行您的C++推理引擎"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Create the following ``CMakeLists.txt`` file:"
msgstr "创建以下“CMakeLists.txt”文件:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Make the program:"
msgstr "生成程序:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Now, we can run inference in C++, and verify that we get a result:"
msgstr "现在，我们可以在C++中进行推理，并验证我们是否得到了一个结果:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`pytorch.org`_ for installation instructions, and more documentation and "
"tutorials."
msgstr "`pytorch.org`_ 提供安装说明及更多文档和教程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Introduction to TorchScript tutorial`_ for a deeper initial exposition of "
"TorchScript"
msgstr "`TorchScript介绍教程`_ 作为TorchScript的深入初步介绍"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Full TorchScript documentation`_ for complete TorchScript language and API "
"reference"
msgstr "`完整TorchScript文档`_ 提供完整的TorchScript语言和API参考"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Deploying a PyTorch Stable Diffusion model as a Vertex AI Endpoint"
msgstr "将PyTorch稳定扩散模型部署为Vertex AI端点"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Deploying large models, like Stable Diffusion, can be challenging and time-"
"consuming."
msgstr "部署像稳定扩散这样的大型模型可能具有挑战性且耗时。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this recipe, we will show how you can streamline the deployment of a "
"PyTorch Stable Diffusion model by leveraging Vertex AI."
msgstr "在本教程中，我们将展示如何通过利用Vertex AI简化PyTorch稳定扩散模型的部署。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"PyTorch is the framework used by Stability AI on Stable Diffusion v1.5.  "
"Vertex AI is a fully-managed machine learning platform with tools and "
"infrastructure designed to help ML practitioners accelerate and scale ML in "
"production with the benefit of open-source frameworks like PyTorch."
msgstr ""
"PyTorch是Stability AI在稳定扩散v1.5中使用的框架。Vertex "
"AI是一个完全托管的机器学习平台，提供工具和基础设施，旨在帮助ML从业者加速和扩展生产中的机器学习，同时受益于像PyTorch这样的开源框架。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In four steps you can deploy a PyTorch Stable Diffusion model (v1.5)."
msgstr "通过四个步骤，您可以部署PyTorch稳定扩散模型(v1.5)。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Deploying your Stable Diffusion model on a Vertex AI Endpoint can be done in"
" four steps:"
msgstr "在Vertex AI端点上部署稳定扩散模型可以通过四个步骤完成:"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Create a custom TorchServe handler."
msgstr "创建一个自定义TorchServe处理器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Upload model artifacts to Google Cloud Storage (GCS)."
msgstr "将模型工件上传到Google Cloud Storage (GCS)。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Create a Vertex AI model with the model artifacts and a prebuilt PyTorch "
"container image."
msgstr "使用模型工件和预构建的PyTorch容器镜像创建一个Vertex AI模型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Deploy the Vertex AI model onto an endpoint."
msgstr "将Vertex AI模型部署到端点上。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Let’s have a look at each step in more detail. You can follow and implement "
"the steps using the `Notebook example "
"<https://github.com/GoogleCloudPlatform/vertex-ai-"
"samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb>`__."
msgstr ""
"让我们详细了解每个步骤。您可以通过 `Notebook示例 "
"<https://github.com/GoogleCloudPlatform/vertex-ai-"
"samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb>`__"
" 进行跟随和实施。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"NOTE: Please keep in mind that this recipe requires a billable Vertex AI as "
"explained in more details in the notebook example."
msgstr "注意：请记住，此教程需要可收费的Vertex AI，如Notebook示例中更详细地阐述。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Create a custom TorchServe handler"
msgstr "创建一个自定义TorchServe处理器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"TorchServe is an easy and flexible tool for serving PyTorch models. The "
"model deployed to Vertex AI uses TorchServe to handle requests and return "
"responses from the model. You must create a custom TorchServe handler to "
"include in the model artifacts uploaded to Vertex AI. Include the handler "
"file in the directory with the other model artifacts, like this: "
"`model_artifacts/handler.py`."
msgstr ""
"TorchServe是一个服务PyTorch模型的简单灵活工具。部署到Vertex "
"AI的模型使用TorchServe来处理请求并从模型返回响应。您必须创建一个自定义TorchServe处理器并将其包含在上传到Vertex "
"AI的模型工件中。将处理器文件与其他模型工件放在同一目录中，例如 `model_artifacts/handler.py`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"After creating the handler file, you must package the handler as a model "
"archiver (MAR) file. The output file must be named `model.mar`."
msgstr "创建处理器文件后，您必须将处理器打包为一个模型归档文件(MAR文件)。输出文件必须命名为`model.mar`。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Upload model artifacts to Google Cloud Storage (GCS)"
msgstr "将模型工件上传到Google Cloud Storage (GCS)"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this step we are uploading `model artifacts "
"<https://github.com/pytorch/serve/tree/master/model-archiver#artifact-"
"details>`__ to GCS, like the model file or handler. The advantage of storing"
" your artifacts on GCS is that you can track the artifacts in a central "
"bucket."
msgstr ""
"在此步骤中，我们将 `模型工件 <https://github.com/pytorch/serve/tree/master/model-"
"archiver#artifact-details>`__ "
"上传到GCS，例如模型文件或处理器文件。将工件存储在GCS上的优势在于，您可以在中央存储桶中跟踪这些工件。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Create a Vertex AI model with the model artifacts and a prebuilt PyTorch "
"container image"
msgstr "使用模型工件和预构建的PyTorch容器镜像创建一个Vertex AI模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Once you've uploaded the model artifacts into a GCS bucket, you can upload "
"your PyTorch model to `Vertex AI Model Registry "
"<https://cloud.google.com/vertex-ai/docs/model-registry/introduction>`__. "
"From the Vertex AI Model Registry, you have an overview of your models so "
"you can better organize, track, and train new versions. For this you can use"
" the `Vertex AI SDK <https://cloud.google.com/vertex-ai/docs/python-sdk/use-"
"vertex-ai-python-sdk>`__ and this `pre-built PyTorch container "
"<https://cloud.google.com/blog/products/ai-machine-learning/prebuilt-"
"containers-with-pytorch-and-vertex-ai>`__."
msgstr ""
"一旦您将模型文件上传到GCS存储桶中，您可以将您的PyTorch模型上传到`Vertex AI 模型注册表 "
"<https://cloud.google.com/vertex-ai/docs/model-"
"registry/introduction>`__。通过Vertex "
"AI模型注册表，您可以一览您的模型，以便更好地组织、追踪和训练新版本。为此，您可以使用`Vertex AI SDK "
"<https://cloud.google.com/vertex-ai/docs/python-sdk/use-vertex-ai-python-"
"sdk>`__以及这个`预构建PyTorch容器 <https://cloud.google.com/blog/products/ai-machine-"
"learning/prebuilt-containers-with-pytorch-and-vertex-ai>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Deploy the Vertex AI model onto an endpoint"
msgstr "在端点上部署Vertex AI模型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Once the model has been uploaded to Vertex AI Model Registry you can then "
"take it and deploy it to an Vertex AI Endpoint. For this you can use the "
"Console or the Vertex AI SDK. In this example you will deploy the model on a"
" NVIDIA Tesla P100 GPU and n1-standard-8 machine. You can specify your "
"machine type."
msgstr ""
"将模型上传到Vertex AI模型注册表之后，您可以将其部署到Vertex AI的端点上。您可以使用控制台或者Vertex AI "
"SDK。在本示例中，您将在NVIDIA Tesla P100 GPU和n1-standard-8机器上部署模型。您可以指定您的机器类型。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If you follow this `notebook <https://github.com/GoogleCloudPlatform/vertex-"
"ai-"
"samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb>`__"
" you can also get online predictions using the Vertex AI SDK as shown in the"
" following snippet."
msgstr ""
"如果您遵循这篇`笔记本 <https://github.com/GoogleCloudPlatform/vertex-ai-"
"samples/blob/main/notebooks/community/vertex_endpoints/torchserve/dreambooth_stablediffusion.ipynb>`__，还可以像以下代码片段展示的那样，使用Vertex"
" AI SDK进行在线预测。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "More resources"
msgstr "更多资源"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"This tutorial was created using the vendor documentation. To refer to the "
"original documentation on the vendor site, please see `torchserve example "
"<https://cloud.google.com/blog/products/ai-machine-learning/get-your-genai-"
"model-going-in-four-easy-steps>`__."
msgstr ""
"本教程是基于供应商文档创建的。如需参考供应商网站上的原始文档，请参见`torchserve 示例 "
"<https://cloud.google.com/blog/products/ai-machine-learning/get-your-genai-"
"model-going-in-four-easy-steps>`__。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Optimizing CPU Performance on Intel® Xeon® with run_cpu Script"
msgstr "通过run_cpu脚本优化Intel® Xeon®上的CPU性能"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"There are several configuration options that can impact the performance of "
"PyTorch inference when executed on Intel® Xeon® Scalable Processors. To get "
"peak performance, the ``torch.backends.xeon.run_cpu`` script is provided "
"that optimizes the configuration of thread and memory management. For thread"
" management, the script configures thread affinity and the preload of Intel®"
" OMP library. For memory management, it configures NUMA binding and preloads"
" optimized memory allocation libraries, such as TCMalloc and JeMalloc. In "
"addition, the script provides tunable parameters for compute resource "
"allocation in both single instance and multiple instance scenarios, helping "
"the users try out an optimal coordination of resource utilization for the "
"specific workloads."
msgstr ""
"有多种配置选项会影响在Intel® "
"Xeon®可扩展处理器上执行PyTorch推理时的性能。为获得最佳性能，提供了名为``torch.backends.xeon.run_cpu``的脚本，用于优化线程和内存管理的配置。对于线程管理，该脚本配置线程亲和性并预加载Intel®"
" "
"OMP库；对于内存管理，它配置NUMA绑定并预加载优化的内存分配库，比如TCMalloc和JeMalloc。此外，脚本为计算资源分配提供了可调参变量，适用于单实例和多实例场景，帮助用户尝试针对特定工作负载的最佳资源利用协调。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What You Will Learn"
msgstr "您将学习到的内容"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to utilize tools like ``numactl``, ``taskset``, Intel® OpenMP Runtime "
"Library and optimized memory allocators such as ``TCMalloc`` and "
"``JeMalloc`` for enhanced performance."
msgstr ""
"如何利用诸如``numactl``、``taskset``、Intel® "
"OpenMP运行时库以及优化的内存分配器（如``TCMalloc``和``JeMalloc``）等工具来提升性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to configure CPU resources and memory management to maximize PyTorch "
"inference performance on Intel® Xeon® processors."
msgstr "如何配置CPU资源和内存管理，以最大化Intel® Xeon®处理器上的PyTorch推理性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Introduction of the Optimizations"
msgstr "优化介绍"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Applying NUMA Access Control"
msgstr "应用NUMA访问控制"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"It is beneficial that an increasing number of CPU cores are being provided "
"to users within a single socket, as this offers greater computational "
"resources. However, this also leads to competition for memory access, which "
"can cause programs to stall due to busy memory. To address this problem, "
"Non-Uniform Memory Access (NUMA) was introduced. Unlike Uniform Memory "
"Access (UMA), where all memories are equally accessible to all cores, NUMA "
"organizes memory into multiple groups. Certain number of memories are "
"directly attached to one socket's integrated memory controller to become "
"local memory of this socket. Local memory access is much faster than remote "
"memory access."
msgstr ""
"在单个插槽内可用的CPU核心数量增加对用户有利，因为提供了更多计算资源。但这也会导致对内存访问的竞争，从而因内存繁忙导致程序停滞。为解决这个问题，引入了非一致性内存访问（NUMA）。与统一内存访问（UMA）不同，NUMA将内存组织为多个组。某些内存直接连接到某个插槽的集成内存控制器中，成为该插槽的局部内存。局部内存访问比远程内存访问快得多。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Users can get CPU information with ``lscpu`` command on Linux to learn how "
"many cores and sockets are there on the machine. Additionally, this command "
"provides NUMA information, such as the distribution of CPU cores. Below is "
"an example of executing  ``lscpu`` on a machine equipped with an Intel® "
"Xeon® CPU Max 9480:"
msgstr ""
"用户可以在Linux上使用``lscpu``命令获取CPU信息，以了解机器中有多少核心和插槽。此外，此命令还提供了NUMA信息，例如CPU核心的分布。以下是使用Intel®"
" Xeon® CPU Max 9480机器执行``lscpu``的示例："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Two sockets were detected, each containing 56 physical cores. With Hyper-"
"Threading enabled, each core can handle 2 threads, resulting in 56 logical "
"cores per socket. Therefore, the machine has a total of 224 CPU cores in "
"service."
msgstr ""
"检测到两个插槽，每个插槽包含56个物理核心。启用超线程后，每个核心可以处理2个线程，因此每个插槽有56个逻辑核心。机器总共有224个CPU核心可用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Typically, physical cores are indexed before logical cores. In this "
"scenario, cores 0-55 are the physical cores on the first NUMA node, and "
"cores 56-111 are the physical cores on the second NUMA node."
msgstr ""
"通常，物理核心的索引在逻辑核心之前。在此情况下，核心0-55是第一个NUMA节点上的物理核心，核心56-111是第二个NUMA节点上的物理核心。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Logical cores are indexed subsequently: cores 112-167 correspond to the "
"logical cores on the first NUMA node, and cores 168-223 to those on the "
"second NUMA node."
msgstr "逻辑核心随后被索引：核心112-167对应第一个NUMA节点上的逻辑核心，核心168-223对应第二个NUMA节点上的逻辑核心。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Typically, running PyTorch programs with compute intense workloads should "
"avoid using logical cores to get good performance."
msgstr "通常，运行具有计算密集型工作负载的PyTorch程序时应避免使用逻辑核心以获得良好的性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Linux provides a tool called ``numactl`` that allows user control of NUMA "
"policy for processes or shared memory. It runs processes with a specific "
"NUMA scheduling or memory placement policy. As described above, cores share "
"high-speed cache in one socket, thus it is a good idea to avoid cross socket"
" computations. From a memory access perspective, bounding memory access "
"locally is much faster than accessing remote memories. ``numactl`` command "
"should have been installed in recent Linux distributions. In case it is "
"missing, you can install it manually with the installation command, like on "
"Ubuntu:"
msgstr ""
"Linux提供了一个名为``numactl``的工具，允许用户为进程或共享内存控制NUMA策略。它可以使用特定的NUMA调度或内存放置策略运行进程。如上所述，核心在一个插槽内共享高速缓存，因此避免跨插槽计算是一个好主意。从内存访问的角度来看，限制在本地内存访问比访问远程内存快得多。最新的Linux分发版中应该已经安装``numactl``命令。如果缺失，您可以通过安装命令手动安装，例如在Ubuntu上："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "on CentOS you can run the following command:"
msgstr "在CentOS上，您可以运行以下命令："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The ``taskset`` command in Linux is another powerful utility that allows you"
" to set or retrieve the CPU affinity of a running process. ``taskset`` are "
"pre-installed in most Linux distributions and in case it's not, on Ubuntu "
"you can install it with the command:"
msgstr ""
"Linux中的``taskset``命令是另一个强大的实用程序，它允许您设置或检索正在运行的进程的CPU亲和性。大多数Linux分发版中预安装了``taskset``，如果没有，在Ubuntu上您可以通过以下命令安装："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using Intel® OpenMP Runtime Library"
msgstr "使用Intel® OpenMP运行时库"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"OpenMP is an implementation of multithreading, a method of parallelizing "
"where a primary thread (a series of instructions executed consecutively) "
"forks a specified number of sub-threads and the system divides a task among "
"them. The threads then run concurrently, with the runtime environment "
"allocating threads to different processors. Users can control OpenMP "
"behaviors with some environment variable settings to fit for their "
"workloads, the settings are read and executed by OMP libraries. By default, "
"PyTorch uses GNU OpenMP Library (GNU libgomp) for parallel computation. On "
"Intel® platforms, Intel® OpenMP Runtime Library (libiomp) provides OpenMP "
"API specification support. It usually brings more performance benefits "
"compared to libgomp."
msgstr ""
"OpenMP是一种多线程实现，通过它一个主线程（一系列连续执行的指令）派生出指定数量的子线程，系统将任务分配给它们并行执行。子线程同时运行，运行时环境将线程分配给不同的处理器。用户可以通过一些环境变量设置控制OpenMP行为来适合其工作负载，这些设置由OMP库读取并执行。默认情况下，PyTorch使用GNU"
" OpenMP库（GNU libgomp）进行并行计算。在Intel®平台上，Intel® OpenMP Runtime "
"Library（libiomp）提供OpenMP API规范支持。与libgomp相比，它通常带来更多性能提升。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The Intel® OpenMP Runtime Library can be installed using one of these "
"commands:"
msgstr "您可以通过以下命令安装Intel® OpenMP运行时库："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "or"
msgstr "或"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Choosing an Optimized Memory Allocator"
msgstr "选择优化的内存分配器"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Memory allocator plays an important role from performance perspective as "
"well. A more efficient memory usage reduces overhead on unnecessary memory "
"allocations or destructions, and thus results in a faster execution. From "
"practical experiences, for deep learning workloads, ``TCMalloc`` or "
"``JeMalloc`` can get better performance by reusing memory as much as "
"possible than default malloc operations."
msgstr ""
"内存分配器从性能的角度来看也起着重要作用。更高效的内存使用可减少不必要的内存分配或销毁的开销，从而带来更快的执行。根据实践经验，在深度学习工作负载中，``TCMalloc``或``JeMalloc``通过最大限度地复用内存，可以比默认的malloc操作获得更好的性能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"You can install ``TCMalloc`` by running the following command on Ubuntu:"
msgstr "您可以在Ubuntu上通过以下命令安装``TCMalloc``："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "On CentOS, you can install it by running:"
msgstr "在CentOS上，您可以运行以下命令安装："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "In a conda environment, it can also be installed by running:"
msgstr "在conda环境中，也可以通过运行以下命令安装："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "On Ubuntu ``JeMalloc`` can be installed by this command:"
msgstr "在Ubuntu上，``JeMalloc``可以通过以下命令安装："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "On CentOS it can be installed by running:"
msgstr "在CentOS上，可以通过以下命令安装："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Quick Start Example Commands"
msgstr "快速开始示例命令"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To run single-instance inference with 1 thread on 1 CPU core (only Core #0 "
"would be used):"
msgstr "使用1个线程在1个CPU核心（仅使用核心#0）上运行单实例推理："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To run single-instance inference on a single CPU node (NUMA socket):"
msgstr "在单个CPU节点（NUMA插槽）上运行单实例推理："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To run multi-instance inference, 8 instances with 14 cores per instance on a"
" 112-core CPU:"
msgstr "运行多实例推理，112核CPU上每个实例使用14个核心，共8个实例："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To run inference in throughput mode, in which all the cores in each CPU node"
" set up an instance:"
msgstr "以吞吐量模式运行推理，每个CPU节点中的所有核心设置一个实例："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Term \"instance\" here doesn't refer to a cloud instance. This script is "
"executed as a single process which invokes multiple \"instances\" which are "
"formed from multiple threads. \"Instance\" is kind of group of threads in "
"this context."
msgstr "术语“实例”在此处并不指代云实例。这个脚本作为单个进程执行，它调用多个由多个线程组成的“实例”。在此上下文中，“实例”是一类线程组。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Using ``torch.backends.xeon.run_cpu``"
msgstr "使用``torch.backends.xeon.run_cpu``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The argument list and usage guidance can be shown with the following "
"command:"
msgstr "可以通过以下命令显示参数列表和使用指南："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The command above has the following positional arguments:"
msgstr "上述命令包含以下位置参数："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "knob"
msgstr "knob"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "help"
msgstr "帮助"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``program``"
msgstr "``program``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The full path of the program/script to be launched."
msgstr "要启动的程序/脚本的完整路径。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``program_args``"
msgstr "``program_args``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The input arguments for the program/script to be launched."
msgstr "要启动的程序/脚本的输入参数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Explanation of the options"
msgstr "选项解释"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "The generic option settings (knobs) include the following:"
msgstr "通用选项设置（选项）包括以下内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "type"
msgstr "类型"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "default value"
msgstr "默认值"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``-h``, ``--help``"
msgstr "``-h``，``--help``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To show the help message and exit."
msgstr "显示帮助信息并退出。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``-m``, ``--module``"
msgstr "``-m``，``--module``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To change each process to interpret the launch script as a python module, "
"executing with the same behavior as \"python -m\"."
msgstr "更改每个进程以将启动脚本解释为Python模块，行为类似于“python -m”。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--no-python``"
msgstr "``--no-python``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "bool"
msgstr "布尔值"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "False"
msgstr "False"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To avoid prepending the program with \"python\" - just execute it directly. "
"Useful when the script is not a Python script."
msgstr "避免在程序前加上“python”，直接执行它。适用于脚本不是Python脚本的情况。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--log-path``"
msgstr "``--log-path``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "str"
msgstr "字符串"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``''``"
msgstr "``&apos;&apos;``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To specify the log file directory. Default path is ``''``, which means "
"disable logging to files."
msgstr "指定日志文件目录。默认路径为``&apos;&apos;``，表示禁用文件日志记录。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--log-file-prefix``"
msgstr "``--log-file-prefix``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "\"run\""
msgstr "``run``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Prefix of the log file name."
msgstr "日志文件名的前缀。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Knobs for applying or disabling optimizations are:"
msgstr "用于应用或禁用优化的选项有："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--enable-tcmalloc``"
msgstr "``--enable-tcmalloc``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To enable ``TCMalloc`` memory allocator."
msgstr "启用``TCMalloc``内存分配器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--enable-jemalloc``"
msgstr "``--enable-jemalloc``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To enable ``JeMalloc`` memory allocator."
msgstr "启用``JeMalloc``内存分配器。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--use-default-allocator``"
msgstr "``--use-default-allocator``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To use default memory allocator. Neither ``TCMalloc`` nor ``JeMalloc`` would"
" be used."
msgstr "使用默认内存分配器。既不使用``TCMalloc``也不使用``JeMalloc``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--disable-iomp``"
msgstr "``--disable-iomp``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default, Intel® OpenMP lib will be used if installed. Setting this flag "
"would disable the usage of Intel® OpenMP."
msgstr "默认情况下，如果安装了Intel® OpenMP库将被使用。设置此标志将禁用Intel® OpenMP的使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Memory allocators influence performance. If the user does not specify a "
"desired memory allocator, the ``run_cpu`` script will search if any of them "
"is installed in the order of TCMalloc > JeMalloc > PyTorch default memory "
"allocator, and takes the first matched one."
msgstr ""
"内存分配器会影响性能。如果用户未指定所需的内存分配器，``run_cpu``脚本将按TCMalloc > JeMalloc > "
"PyTorch默认内存分配器的顺序搜索是否有安装的，选择第一个匹配项。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Knobs for controlling instance number and compute resource allocation are:"
msgstr "控制实例数量和计算资源分配的选项有："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--ninstances``"
msgstr "``--ninstances``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "int"
msgstr "整数"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "0"
msgstr "0"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Number of instances."
msgstr "实例数量。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--ncores-per-instance``"
msgstr "``--ncores-per-instance``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Number of cores used by each instance."
msgstr "每个实例使用的核心数。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--node-id``"
msgstr "``--node-id``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "-1"
msgstr "-1"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The node ID to be used for multi-instance, by default all nodes will be "
"used."
msgstr "用于多实例的节点ID，默认使用所有节点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--core-list``"
msgstr "``--core-list``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To specify the core list as ``'core_id, core_id, ....'`` or core range as "
"``'core_id-core_id'``. By dafault all the cores will be used."
msgstr ""
"指定核心列表为``&apos;core_id, core_id, ....&apos;``或核心范围为``&apos;core_id-"
"core_id&apos;``。默认使用所有核心。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--use-logical-core``"
msgstr "``--use-logical-core``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default only physical cores are used. Specifying this flag enables "
"logical cores usage."
msgstr "默认情况下仅使用物理核心。指定此标志将启用逻辑核心使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--skip-cross-node-cores``"
msgstr "``--skip-cross-node-cores``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To prevent the workload to be executed on cores across NUMA nodes."
msgstr "防止工作负载在跨NUMA节点的核心上执行。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--rank``"
msgstr "``--rank``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"To specify instance index to assign ncores_per_instance for rank; otherwise "
"ncores_per_instance will be assigned sequentially to the instances."
msgstr "为排名指定实例索引，以分配ncores_per_instance；否则ncores_per_instance将顺序分配给实例。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--multi-instance``"
msgstr "``--multi-instance``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A quick set to invoke multiple instances of the workload on multi-socket CPU"
" servers."
msgstr "快速设置以在多插槽CPU服务器上调动多实例工作负载。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--latency-mode``"
msgstr "``--latency-mode``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A quick set to invoke benchmarking with latency mode, in which all physical "
"cores are used and 4 cores per instance."
msgstr "快速设置以延迟模式启动基准测试，所有物理核心被使用，每个实例使用4个核心。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--throughput-mode``"
msgstr "``--throughput-mode``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"A quick set to invoke benchmarking with throughput mode, in which all "
"physical cores are used and 1 numa node per instance."
msgstr "快速设置以吞吐量模式启动基准测试，所有物理核心被使用，每个实例使用1个NUMA节点。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--disable-numactl``"
msgstr "``--disable-numactl``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"By default ``numactl`` command is used to control NUMA access. Setting this "
"flag will disable it."
msgstr "默认情况下使用``numactl``命令来控制NUMA访问。设置此标志将禁用该功能。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "``--disable-taskset``"
msgstr "``--disable-taskset``"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "To disable the usage of ``taskset`` command."
msgstr "禁用``taskset``命令的使用。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Environment variables that will be set by this script include the following:"
msgstr "此脚本将设置的环境变量包括以下内容："

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Environment Variable"
msgstr "环境变量"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Value"
msgstr "值"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "LD_PRELOAD"
msgstr "LD_PRELOAD"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Depending on knobs you set, <lib>/libiomp5.so, <lib>/libjemalloc.so, "
"<lib>/libtcmalloc.so might be appended to LD_PRELOAD."
msgstr ""
"根据您设置的选项，<lib>/libiomp5.so、<lib>/libjemalloc.so、<lib>/libtcmalloc.so 可能会被添加到"
" LD_PRELOAD。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "KMP_AFFINITY"
msgstr "KMP_AFFINITY"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If libiomp5.so is preloaded, KMP_AFFINITY could be set to "
"``\"granularity=fine,compact,1,0\"``."
msgstr ""
"如果预加载了 libiomp5.so，KMP_AFFINITY 将被设置为 ``\"granularity=fine,compact,1,0\"``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "KMP_BLOCKTIME"
msgstr "KMP_BLOCKTIME"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "If libiomp5.so is preloaded, KMP_BLOCKTIME is set to \"1\"."
msgstr "如果预加载了 libiomp5.so，KMP_BLOCKTIME 将被设置为 \"1\"。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "OMP_NUM_THREADS"
msgstr "OMP_NUM_THREADS"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Value of ``ncores_per_instance``"
msgstr "``ncores_per_instance`` 的值"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "MALLOC_CONF"
msgstr "MALLOC_CONF"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"If libjemalloc.so is preloaded, MALLOC_CONF will be set to "
"``\"oversize_threshold:1,background_thread:true,metadata_thp:auto\"``."
msgstr ""
"如果预加载了 libjemalloc.so，MALLOC_CONF 将被设置为 "
"``\"oversize_threshold:1,background_thread:true,metadata_thp:auto\"``。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Please note that the script respects environment variables set "
"preliminarily. For example, if you have set the environment variables "
"mentioned above before running the script, the values of the variables will "
"not be overwritten by the script."
msgstr "请注意，该脚本会尊重预先设置的环境变量。例如，如果您在运行脚本之前设置了上述环境变量，这些变量的值将不会被脚本覆盖。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"In this tutorial, we explored a variety of advanced configurations and tools"
" designed to optimize PyTorch inference performance on Intel® Xeon® Scalable"
" Processors. By leveraging the ``torch.backends.xeon.run_cpu`` script, we "
"demonstrated how to fine-tune thread and memory management to achieve peak "
"performance. We covered essential concepts such as NUMA access control, "
"optimized memory allocators like ``TCMalloc`` and ``JeMalloc``, and the use "
"of Intel® OpenMP for efficient multithreading."
msgstr ""
"在本教程中，我们探讨了一系列用于优化 PyTorch 在 Intel® Xeon® 可扩展处理器上的推理性能的高级配置和工具。通过使用 "
"``torch.backends.xeon.run_cpu`` 脚本，我们演示了如何通过微调线程和内存管理来实现最佳性能。我们涵盖了关键概念，例如 "
"NUMA 访问控制、优化内存分配器（如 ``TCMalloc`` 和 ``JeMalloc``），以及使用 Intel® OpenMP "
"来实现高效的多线程。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Additionally, we provided practical command-line examples to guide you "
"through setting up single and multiple instance scenarios, ensuring optimal "
"resource utilization tailored to specific workloads. By understanding and "
"applying these techniques, users can significantly enhance the efficiency "
"and speed of their PyTorch applications on Intel® Xeon® platforms."
msgstr ""
"此外，我们还提供了实用的命令行示例，以指导您设置单实例和多实例场景，从而确保针对特定工作负载的最佳资源利用率。通过理解和应用这些技术，用户可以显著提高 "
"PyTorch 应用程序在 Intel® Xeon® 平台上的效率和速度。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`PyTorch Performance Tuning Guide "
"<https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-"
"specific-optimizations>`__"
msgstr ""
"`PyTorch 性能调优指南 "
"<https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-"
"specific-optimizations>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`PyTorch Multiprocessing Best Practices "
"<https://pytorch.org/docs/stable/notes/multiprocessing.html#cpu-in-"
"multiprocessing>`__"
msgstr ""
"`PyTorch 多进程最佳实践 "
"<https://pytorch.org/docs/stable/notes/multiprocessing.html#cpu-in-"
"multiprocessing>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"Grokking PyTorch Intel CPU performance: `Part 1 "
"<https://pytorch.org/tutorials/intermediate/torchserve_with_ipex>`__  `Part "
"2 <https://pytorch.org/tutorials/intermediate/torchserve_with_ipex_2>`__"
msgstr ""
"深入了解 PyTorch 在 Intel CPU 上的性能：`第一部分 "
"<https://pytorch.org/tutorials/intermediate/torchserve_with_ipex>`__  `第二部分 "
"<https://pytorch.org/tutorials/intermediate/torchserve_with_ipex_2>`__"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "Shard Optimizer States with ZeroRedundancyOptimizer"
msgstr "使用 ZeroRedundancyOptimizer 分片优化器状态"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The high-level idea of `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__."
msgstr ""
"`ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ 的高级理念。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"How to use `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ in distributed "
"training and its impact."
msgstr ""
"如何在分布式训练中使用 `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ 以及其影响。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"`Getting Started With Distributed Data Parallel "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_"
msgstr ""
"`分布式数据并行入门 <https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "What is ``ZeroRedundancyOptimizer``?"
msgstr "什么是 ``ZeroRedundancyOptimizer``？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The idea of `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ comes from "
"`DeepSpeed/ZeRO project <https://github.com/microsoft/DeepSpeed>`_ and "
"`Marian <https://github.com/marian-nmt/marian-dev>`_ that shard optimizer "
"states across distributed data-parallel processes to reduce per-process "
"memory footprint. In the `Getting Started With Distributed Data Parallel "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_ tutorial, "
"we have shown how to use `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`_"
" (DDP) to train models. In that tutorial, each process keeps a dedicated "
"replica of the optimizer. Since DDP has already synchronized gradients in "
"the backward pass, all optimizer replicas will operate on the same parameter"
" and gradient values in every iteration, and this is how DDP keeps model "
"replicas in the same state. Oftentimes, optimizers also maintain local "
"states. For example, the ``Adam`` optimizer uses per-parameter ``exp_avg`` "
"and ``exp_avg_sq`` states. As a result, the ``Adam`` optimizer's memory "
"consumption is at least twice the model size. Given this observation, we can"
" reduce the optimizer memory footprint by sharding optimizer states across "
"DDP processes. More specifically, instead of creating per-param states for "
"all parameters, each optimizer instance in different DDP processes only "
"keeps optimizer states for a shard of all model parameters. The optimizer "
"``step()`` function only updates the parameters in its shard and then "
"broadcasts its updated parameters to all other peer DDP processes, so that "
"all model replicas still land in the same state."
msgstr ""
"`ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ 的理念来自 "
"`DeepSpeed/ZeRO 项目 <https://github.com/microsoft/DeepSpeed>`_ 和 `Marian "
"<https://github.com/marian-nmt/marian-"
"dev>`_，它通过在分布式数据并行进程间分片优化器状态来减少每个进程的内存占用。在 `分布式数据并行入门 "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_ "
"教程中，我们展示了如何使用 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`_"
" (DDP) 来训练模型。在该教程中，每个进程保留一个独立的优化器副本。由于 DDP "
"已在反向传播过程中同步了梯度，因此每次迭代中所有优化器副本都将对相同的参数和梯度值进行操作，这就是 DDP "
"确保模型副本状态一致的方式。通常情况下，优化器还会维护本地状态。例如，``Adam`` 优化器使用逐参数的 ``exp_avg`` 和 "
"``exp_avg_sq`` 状态。因此，``Adam`` 优化器的内存消耗至少是模型大小的两倍。鉴于此，我们可以通过在 DDP "
"进程间分片优化器状态来减少优化器的内存占用。更具体地说，不再为所有参数创建逐参数状态，而是每个 DDP "
"进程中的优化器实例仅保留一部分模型参数的优化器状态。优化器的 ``step()`` 函数只更新其分片中的参数，然后将更新后的参数广播到所有其他 DDP "
"对等进程，以确保所有模型副本仍保持一致状态。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid "How to use ``ZeroRedundancyOptimizer``?"
msgstr "如何使用 ``ZeroRedundancyOptimizer``？"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The code below demonstrates how to use `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__. The majority of"
" the code is similar to the simple DDP example presented in `Distributed "
"Data Parallel notes <https://pytorch.org/docs/stable/notes/ddp.html>`_. The "
"main difference is the ``if-else`` clause in the ``example`` function which "
"wraps optimizer constructions, toggling between `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ and ``Adam`` "
"optimizer."
msgstr ""
"下面的代码展示了如何使用 `ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__。大部分代码与 "
"`分布式数据并行注释 <https://pytorch.org/docs/stable/notes/ddp.html>`_ 中提供的简单 DDP "
"示例类似。主要区别在于 ``example`` 函数中的 ``if-else`` 子句，它封装了优化器的构造，在 "
"`ZeroRedundancyOptimizer "
"<https://pytorch.org/docs/master/distributed.optim.html>`__ 和 ``Adam`` "
"优化器之间切换。"

#: ../../recipes/zero_redundancy_optimizer.rst:131
msgid ""
"The output is shown below. When enabling ``ZeroRedundancyOptimizer`` with "
"``Adam``, the optimizer ``step()`` peak memory consumption is half of "
"vanilla ``Adam``'s memory consumption. This agrees with our expectation, as "
"we are sharding ``Adam`` optimizer states across two processes. The output "
"also shows that, with ``ZeroRedundancyOptimizer``, the model parameters "
"still end up with the same values after one iterations (the parameters sum "
"is the same with and without ``ZeroRedundancyOptimizer``)."
msgstr ""
"下面是输出结果。当启用 ``ZeroRedundancyOptimizer`` 和 ``Adam`` 时，优化器 ``step()`` "
"的峰值内存消耗是普通 ``Adam`` 的一半。这符合我们的预期，因为我们在两个进程之间分片了 ``Adam`` 优化器状态。输出还显示，即使使用 "
"``ZeroRedundancyOptimizer``，模型参数在一次迭代后仍保持相同的值（使用和不使用 "
"``ZeroRedundancyOptimizer`` 的参数总和相同）。"
