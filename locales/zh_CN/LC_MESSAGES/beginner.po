#
msgid ""
msgstr ""

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_Intro_to_TorchScript_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_Intro_to_TorchScript_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to TorchScript"
msgstr "TorchScript 入门"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Authors:** James Reed (jamesreed@fb.com), Michael Suo (suo@fb.com), rev2"
msgstr "**作者:** James Reed (jamesreed@fb.com), Michael Suo (suo@fb.com), 修订版2"

#: ../../beginner/vt_tutorial.rst:783
msgid "TorchScript is no longer in active development."
msgstr "TorchScript 不再处于活跃开发中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is an introduction to TorchScript, an intermediate "
"representation of a PyTorch model (subclass of ``nn.Module``) that can then "
"be run in a high-performance environment such as C++."
msgstr ""
"本教程是 TorchScript 的入门教程，TorchScript 是 PyTorch 模型（nn.Module "
"的子类）的中间表示，可以在高性能环境如 C++ 中运行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "In this tutorial we will cover:"
msgstr "在本教程中我们将覆盖以下内容："

#: ../../beginner/vt_tutorial.rst:783
msgid "The basics of model authoring in PyTorch, including:"
msgstr "PyTorch 模型编写的基础知识，包括："

#: ../../beginner/vt_tutorial.rst:783
msgid "Modules"
msgstr "模块"

#: ../../beginner/vt_tutorial.rst:783
msgid "Defining ``forward`` functions"
msgstr "定义 ``forward`` 函数"

#: ../../beginner/vt_tutorial.rst:783
msgid "Composing modules into a hierarchy of modules"
msgstr "将模块组成模块的层级结构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Specific methods for converting PyTorch modules to TorchScript, our high-"
"performance deployment runtime"
msgstr "将 PyTorch 模块转换为 TorchScript 的特定方法，即我们的高性能部署运行时"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tracing an existing module"
msgstr "追踪现有模块"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using scripting to directly compile a module"
msgstr "利用脚本语法直接编译模块"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to compose both approaches"
msgstr "如何合并这两种方法"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and loading TorchScript modules"
msgstr "保存和加载 TorchScript 模块"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We hope that after you complete this tutorial, you will proceed to go "
"through `the follow-on tutorial "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_ which will walk "
"you through an example of actually calling a TorchScript model from C++."
msgstr ""
"我们希望在完成本教程之后，您可以通过`后续教程 "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_进一步了解如何从 C++ 调用 "
"TorchScript 模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Basics of PyTorch Model Authoring"
msgstr "PyTorch 模型编写基础"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s start out by defining a simple ``Module``. A ``Module`` is the basic "
"unit of composition in PyTorch. It contains:"
msgstr "让我们从定义一个简单的 ``Module`` 开始。 ``Module`` 是 PyTorch 中的基本组合单元。 它包含："

#: ../../beginner/vt_tutorial.rst:783
msgid "A constructor, which prepares the module for invocation"
msgstr "一个构造函数，用于准备模块的调用"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A set of ``Parameters`` and sub-\\ ``Modules``. These are initialized by the"
" constructor and can be used by the module during invocation."
msgstr "一组 ``Parameters`` 和子 ``Modules``。它们由构造函数初始化，并可在模块调用期间使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A ``forward`` function. This is the code that is run when the module is "
"invoked."
msgstr "一个 ``forward`` 函数。这是模块调用时运行的代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s examine a small example:"
msgstr "让我们看一下一个简单的例子："

#: ../../beginner/vt_tutorial.rst:783
msgid "So we’ve:"
msgstr "因此，我们已经完成了以下操作："

#: ../../beginner/vt_tutorial.rst:783
msgid "Created a class that subclasses ``torch.nn.Module``."
msgstr "创建了一个继承自 ``torch.nn.Module`` 的类。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Defined a constructor. The constructor doesn’t do much, just calls the "
"constructor for ``super``."
msgstr "定义了一个构造函数。构造函数只调用了 ``super`` 的构造函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Defined a ``forward`` function, which takes two inputs and returns two "
"outputs. The actual contents of the ``forward`` function are not really "
"important, but it’s sort of a fake `RNN cell "
"<https://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__–that is–it’s"
" a function that is applied on a loop."
msgstr ""
"定义了一个 ``forward`` 函数，该函数接受两个输入并返回两个输出。 ``forward`` 函数的实际内容并不重要，但它类似于一个假的 "
"`RNN 单元 <https://colah.github.io/posts/2015-08-Understanding-"
"LSTMs/>`__。它是一个循环中应用的函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We instantiated the module, and made ``x`` and ``h``, which are just 3x4 "
"matrices of random values. Then we invoked the cell with ``my_cell(x, h)``. "
"This in turn calls our ``forward`` function."
msgstr ""
"我们实例化了模块，并创建了 ``x`` 和 ``h``，即 3x4 矩阵的随机值。然后我们使用 ``my_cell(x, h)`` "
"调用了该单元。这会调用我们的 ``forward`` 函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s do something a little more interesting:"
msgstr "让我们做一些更有趣的事情："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ve redefined our module ``MyCell``, but this time we’ve added a "
"``self.linear`` attribute, and we invoke ``self.linear`` in the forward "
"function."
msgstr ""
"我们重新定义了模块 ``MyCell``，但这次我们添加了一个 ``self.linear`` 属性，并在 forward 函数中调用了 "
"``self.linear``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What exactly is happening here? ``torch.nn.Linear`` is a ``Module`` from the"
" PyTorch standard library. Just like ``MyCell``, it can be invoked using the"
" call syntax. We are building a hierarchy of ``Module``\\ s."
msgstr ""
"这意味着什么？``torch.nn.Linear`` 是 PyTorch 标准库中的一个 ``Module``。与 ``MyCell`` "
"一样，可以通过调用语法调用它。我们正在构建一个 ``Module`` 的层次结构。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``print`` on a ``Module`` will give a visual representation of the "
"``Module``\\ ’s subclass hierarchy. In our example, we can see our "
"``Linear`` subclass and its parameters."
msgstr ""
"对一个 ``Module`` 使用 ``print`` 会显示该 ``Module`` 的子类层次结构的可视化表示。在我们的示例中，我们可以看到 "
"``Linear`` 子类及其参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By composing ``Module``\\ s in this way, we can succinctly and readably "
"author models with reusable components."
msgstr "通过以这种方式组合 ``Module``，我们可以简洁且可读地编写具有可重用组件的模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You may have noticed ``grad_fn`` on the outputs. This is a detail of "
"PyTorch’s method of automatic differentiation, called `autograd "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__. In"
" short, this system allows us to compute derivatives through potentially "
"complex programs. The design allows for a massive amount of flexibility in "
"model authoring."
msgstr ""
"您可能已经注意到输出中有 ``grad_fn``。这是 PyTorch 自动微分方法的一部分，称为 `autograd "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__。简单来说，这个系统允许我们通过潜在复杂的程序计算导数。该设计在模型编写方面提供了极大的灵活性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now let’s examine said flexibility:"
msgstr "现在让我们探讨这种灵活性："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ve once again redefined our ``MyCell`` class, but here we’ve defined "
"``MyDecisionGate``. This module utilizes **control flow**. Control flow "
"consists of things like loops and ``if``-statements."
msgstr ""
"我们再次重新定义了 ``MyCell`` 类，但这次我们定义了 ``MyDecisionGate``。 该模块利用了 **控制流**。 控制流包括循环和"
" ``if`` 语句。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Many frameworks take the approach of computing symbolic derivatives given a "
"full program representation. However, in PyTorch, we use a gradient tape. We"
" record operations as they occur, and replay them backwards in computing "
"derivatives. In this way, the framework does not have to explicitly define "
"derivatives for all constructs in the language."
msgstr ""
"许多框架使用的是根据完整程序表示计算符号导数的方法。然而，在 PyTorch "
"中，我们使用的是一种梯度带的方法。我们记录发生的操作，并在计算导数时反向重放它们。通过这种方式，框架无需为语言中的所有构造显式定义导数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How autograd works"
msgstr "autograd 的工作原理"

#: ../../beginner/vt_tutorial.rst:783
msgid "Basics of TorchScript"
msgstr "TorchScript 基础"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now let’s take our running example and see how we can apply TorchScript."
msgstr "现在让我们以正在运行的示例为基础，看看如何应用 TorchScript。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In short, TorchScript provides tools to capture the definition of your "
"model, even in light of the flexible and dynamic nature of PyTorch. Let’s "
"begin by examining what we call **tracing**."
msgstr ""
"简而言之，TorchScript 提供了工具来捕获模型的定义，即使在 PyTorch 的灵活和动态特性下也是如此。让我们从审视所谓的 "
"**追踪（tracing）** 开始。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tracing ``Modules``"
msgstr "追踪 ``Modules``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ve rewinded a bit and taken the second version of our ``MyCell`` class. "
"As before, we’ve instantiated it, but this time, we’ve called "
"``torch.jit.trace``, passed in the ``Module``, and passed in *example "
"inputs* the network might see."
msgstr ""
"我们回到 ``MyCell`` 类的第二版。与之前一样，我们实例化了它，但这次我们调用了 ``torch.jit.trace``，传入了 "
"``Module`` 和网络可能看到的 *示例输入*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What exactly has this done? It has invoked the ``Module``, recorded the "
"operations that occurred when the ``Module`` was run, and created an "
"instance of ``torch.jit.ScriptModule`` (of which ``TracedModule`` is an "
"instance)"
msgstr ""
"这具体完成了什么？它调用了 ``Module``，记录了运行 ``Module`` 时发生的操作，并创建了一个 "
"``torch.jit.ScriptModule`` 的实例（其中 ``TracedModule`` 是一个实例）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"TorchScript records its definitions in an Intermediate Representation (or "
"IR), commonly referred to in Deep learning as a *graph*. We can examine the "
"graph with the ``.graph`` property:"
msgstr ""
"TorchScript 将其定义记录在一个中间表示（IR）中，在深度学习中通常称为 *图（graph）*。我们可以通过 ``.graph`` "
"属性查看这个图："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"However, this is a very low-level representation and most of the information"
" contained in the graph is not useful for end users. Instead, we can use the"
" ``.code`` property to give a Python-syntax interpretation of the code:"
msgstr ""
"然而，这是一个非常底层的表示，图中包含的大多数信息对最终用户毫无用处。相反，我们可以使用 ``.code`` 属性来提供 Python 语法的代码解释："

#: ../../beginner/vt_tutorial.rst:783
msgid "So **why** did we do all this? There are several reasons:"
msgstr "那么我们为什么要做这些呢？有以下几个原因："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"TorchScript code can be invoked in its own interpreter, which is basically a"
" restricted Python interpreter. This interpreter does not acquire the Global"
" Interpreter Lock, and so many requests can be processed on the same "
"instance simultaneously."
msgstr ""
"TorchScript 代码可以在其自己的解释器中调用，这是一个基本的受限 Python "
"解释器。该解释器不会获取全局解释器锁，因此可以在同一实例上同时处理多个请求。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This format allows us to save the whole model to disk and load it into "
"another environment, such as in a server written in a language other than "
"Python"
msgstr "这种格式允许我们将整个模型保存到磁盘，并加载到其他环境中，例如用非 Python 编写的服务器中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"TorchScript gives us a representation in which we can do compiler "
"optimizations on the code to provide more efficient execution"
msgstr "TorchScript 为我们提供了一种表示，可以对代码进行编译器优化，以实现更高效的执行。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"TorchScript allows us to interface with many backend/device runtimes that "
"require a broader view of the program than individual operators."
msgstr "TorchScript 允许我们与需要比单个操作更广泛程序视图的许多后端/设备运行时进行接口。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can see that invoking ``traced_cell`` produces the same results as the "
"Python module:"
msgstr "我们可以看到调用 ``traced_cell`` 产生与 Python 模块相同的结果："

#: ../../beginner/vt_tutorial.rst:783
msgid "Using Scripting to Convert Modules"
msgstr "使用脚本将模块转换"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There’s a reason we used version two of our module, and not the one with the"
" control-flow-laden submodule. Let’s examine that now:"
msgstr "我们之所以使用模块的第二个版本，而不是包含大量控制流的子模块，是有原因的。让我们现在研究这一点："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Looking at the ``.code`` output, we can see that the ``if-else`` branch is "
"nowhere to be found! Why? Tracing does exactly what we said it would: run "
"the code, record the operations *that happen* and construct a "
"``ScriptModule`` that does exactly that. Unfortunately, things like control "
"flow are erased."
msgstr ""
"从 ``.code`` 输出中可以看出，``if-else`` 分支完全未出现！为什么？正如所说，追踪会运行代码，记录 *实际发生的* "
"操作，并构建一个精确执行这些操作的 ``ScriptModule``。然而，像控制流这样的内容会被抹去。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How can we faithfully represent this module in TorchScript? We provide a "
"**script compiler**, which does direct analysis of your Python source code "
"to transform it into TorchScript. Let’s convert ``MyDecisionGate`` using the"
" script compiler:"
msgstr ""
"我们如何在 TorchScript 中忠实地表示此模块？我们提供了一个 **脚本编译器**，它直接分析您的 Python 源代码，将其转换为 "
"TorchScript。让我们使用脚本编译器转换 ``MyDecisionGate``："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hooray! We’ve now faithfully captured the behavior of our program in "
"TorchScript. Let’s now try running the program:"
msgstr "成功了！ 我们现已将程序的行为忠实地捕捉在 TorchScript 中。现在让我们尝试运行程序："

#: ../../beginner/vt_tutorial.rst:783
msgid "Mixing Scripting and Tracing"
msgstr "混合使用脚本与追踪"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Some situations call for using tracing rather than scripting (e.g. a module "
"has many architectural decisions that are made based on constant Python "
"values that we would like to not appear in TorchScript). In this case, "
"scripting can be composed with tracing: ``torch.jit.script`` will inline the"
" code for a traced module, and tracing will inline the code for a scripted "
"module."
msgstr ""
"在某些情况下，应使用追踪而非脚本（例如，当模块有许多基于常量 Python 值的架构决策，而我们不希望这些值出现在 TorchScript "
"中时）。在这种情况下，可以将脚本与追踪结合使用：``torch.jit.script`` 会内联记录追踪模块的代码，而追踪会内联记录脚本模块的代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid "An example of the first case:"
msgstr "以下是第一个情况的例子："

#: ../../beginner/vt_tutorial.rst:783
msgid "And an example of the second case:"
msgstr "接下来是第二种情况的例子："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This way, scripting and tracing can be used when the situation calls for "
"each of them and used together."
msgstr "通过这种方式，当需要时可以单独或结合使用脚本和追踪。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and Loading models"
msgstr "保存和加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We provide APIs to save and load TorchScript modules to/from disk in an "
"archive format. This format includes code, parameters, attributes, and debug"
" information, meaning that the archive is a freestanding representation of "
"the model that can be loaded in an entirely separate process. Let’s save and"
" load our wrapped RNN module:"
msgstr ""
"我们提供了用于以存档格式保存和加载 TorchScript 模块的 "
"API。此格式包括代码、参数、属性和调试信息，这意味着存档是模型的独立表示，可以在完全独立的进程中加载。让我们保存和加载我们封装的 RNN 模块："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As you can see, serialization preserves the module hierarchy and the code "
"we’ve been examining throughout. The model can also be loaded, for example, "
"`into C++ <https://pytorch.org/tutorials/advanced/cpp_export.html>`__ for "
"python-free execution."
msgstr ""
"如您所见，序列化保留了模块层次结构以及我们一直在审视的代码。该模型还可以加载，例如，`到 C++ 中 "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`__ 用于无需 Python 的执行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Further Reading"
msgstr "进一步阅读"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ve completed our tutorial! For a more involved demonstration, check out "
"the NeurIPS demo for converting machine translation models using "
"TorchScript: "
"https://colab.research.google.com/drive/1HiICg6jRkBnr5hvK2-VnMi88Vi9pUzEJ"
msgstr ""
"我们完成了本教程！如需更深入的展示，请查看 NeurIPS 演示，了解使用 TorchScript "
"转换机器翻译模型：https://colab.research.google.com/drive/1HiICg6jRkBnr5hvK2-VnMi88Vi9pUzEJ"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.159 seconds)"
msgstr "**脚本总运行时间:** （0 分钟 0.159 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: Intro_to_TorchScript_tutorial.py "
"<Intro_to_TorchScript_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: Intro_to_TorchScript_tutorial.py "
"<Intro_to_TorchScript_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: Intro_to_TorchScript_tutorial.ipynb "
"<Intro_to_TorchScript_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本: Intro_to_TorchScript_tutorial.ipynb "
"<Intro_to_TorchScript_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`画廊由 Sphinx-Gallery 生成 <https://sphinx-gallery.github.io>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "edit"
msgstr "编辑"

#: ../../beginner/vt_tutorial.rst:783
msgid "Audio Data Augmentation"
msgstr "音频数据增强"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html"
msgstr ""
"本教程已迁移至 "
"https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "It will redirect in 3 seconds."
msgstr "将在 3 秒内跳转。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Audio Datasets"
msgstr "音频数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/audio_datasets_tutorial.html"
msgstr ""
"本教程已迁移至 "
"https://pytorch.org/audio/stable/tutorials/audio_datasets_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "Audio Feature Augmentation"
msgstr "音频特性增强"

#: ../../beginner/vt_tutorial.rst:783
msgid "Audio Feature Extractions"
msgstr "音频特性提取"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html"
msgstr ""
"本教程已迁移至 "
"https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "Audio I/O"
msgstr "音频输入/输出"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html"
msgstr ""
"本教程已迁移至 https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "Audio Resampling"
msgstr "音频重采样"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial has been moved to `a new location "
"<https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html>`_"
" You will be redirected in 3 seconds."
msgstr ""
"本教程已迁移到 `新地址 "
"<https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html>`_"
" 将在 3 秒内跳转。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_autogradqs_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击:ref:`这里 <sphx_glr_download_beginner_basics_autogradqs_tutorial.py>` "
"下载完整代码示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || `Tensors <tensorqs_tutorial.html>`_ || `Datasets & DataLoaders "
"<data_tutorial.html>`_ || `Transforms <transforms_tutorial.html>`_ || `Build"
" Model <buildmodel_tutorial.html>`_ || **Autograd** || `Optimization "
"<optimization_tutorial.html>`_ || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础 <intro.html>`_ || `快速入门 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || `数据集与数据加载器 <data_tutorial.html>`_ || `变换 "
"<transforms_tutorial.html>`_ || `构建模型 <buildmodel_tutorial.html>`_ || "
"**自动梯度** || `优化 <optimization_tutorial.html>`_ || `保存和加载模型 "
"<saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Automatic Differentiation with ``torch.autograd``"
msgstr "使用 ``torch.autograd`` 实现自动微分"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When training neural networks, the most frequently used algorithm is **back "
"propagation**. In this algorithm, parameters (model weights) are adjusted "
"according to the **gradient** of the loss function with respect to the given"
" parameter."
msgstr "在训练神经网络时，最常用的算法是 **反向传播**。在该算法中，参数（模型权重）会根据损失函数相对于给定参数的 **梯度** 进行调整。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To compute those gradients, PyTorch has a built-in differentiation engine "
"called ``torch.autograd``. It supports automatic computation of gradient for"
" any computational graph."
msgstr "为了计算那些梯度，PyTorch 有一个内置的微分引擎 ``torch.autograd``。它支持对任何计算图进行梯度的自动计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Consider the simplest one-layer neural network, with input ``x``, parameters"
" ``w`` and ``b``, and some loss function. It can be defined in PyTorch in "
"the following manner:"
msgstr ""
"考虑最简单的单层神经网络，其输入为 ``x``，参数为 ``w`` 和 ``b``，以及某个损失函数。它可以按照以下方式在 PyTorch 中定义："

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensors, Functions and Computational graph"
msgstr "张量、函数和计算图"

#: ../../beginner/vt_tutorial.rst:783
msgid "This code defines the following **computational graph**:"
msgstr "这段代码定义了以下 **计算图**："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this network, ``w`` and ``b`` are **parameters**, which we need to "
"optimize. Thus, we need to be able to compute the gradients of loss function"
" with respect to those variables. In order to do that, we set the "
"``requires_grad`` property of those tensors."
msgstr ""
"在这个网络中，``w`` 和 ``b`` 是需要优化的 **参数**。因此，我们需要能够计算损失函数对这些变量的梯度。为此，我们设置这些张量的 "
"``requires_grad`` 属性。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can set the value of ``requires_grad`` when creating a tensor, or later "
"by using ``x.requires_grad_(True)`` method."
msgstr ""
"你可以在创建张量时设置 ``requires_grad`` 的值，或者稍后通过使用 ``x.requires_grad_(True)`` 方法设置。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A function that we apply to tensors to construct computational graph is in "
"fact an object of class ``Function``. This object knows how to compute the "
"function in the *forward* direction, and also how to compute its derivative "
"during the *backward propagation* step. A reference to the backward "
"propagation function is stored in ``grad_fn`` property of a tensor. You can "
"find more information of ``Function`` `in the documentation "
"<https://pytorch.org/docs/stable/autograd.html#function>`__."
msgstr ""
"构造计算图时，我们应用于张量的函数实际上是一个类 ``Function`` "
"的对象。该对象不仅知道如何在*前向传播*方向上计算函数，还知道如何在*反向传播*步骤中计算其导数。反向传播函数的引用保存在张量的 ``grad_fn``"
" 属性中。您可以在 `文档中 <https://pytorch.org/docs/stable/autograd.html#function>`__ "
"找到有关 ``Function`` 的更多信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Computing Gradients"
msgstr "计算梯度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To optimize weights of parameters in the neural network, we need to compute "
"the derivatives of our loss function with respect to parameters, namely, we "
"need :math:`\\frac{\\partial loss}{\\partial w}` and :math:`\\frac{\\partial"
" loss}{\\partial b}` under some fixed values of ``x`` and ``y``. To compute "
"those derivatives, we call ``loss.backward()``, and then retrieve the values"
" from ``w.grad`` and ``b.grad``:"
msgstr ""
"为了优化神经网络中参数的权重，我们需要计算损失函数对参数的导数，即我们需要 :math:`\\frac{\\partial "
"loss}{\\partial w}` 和 :math:`\\frac{\\partial loss}{\\partial b}`，在某些特定值的 "
"``x`` 和 ``y`` 下。为了计算这些导数，我们调用 ``loss.backward()``，然后从 ``w.grad`` 和 "
"``b.grad`` 检索值："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can only obtain the ``grad`` properties for the leaf nodes of the "
"computational graph, which have ``requires_grad`` property set to ``True``. "
"For all other nodes in our graph, gradients will not be available."
msgstr ""
"我们只能获取计算图中叶子节点的 ``grad`` 属性，这些节点的 ``requires_grad`` 属性设置为 "
"``True``。对于计算图中所有其他节点，梯度将不可用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can only perform gradient calculations using ``backward`` once on a given"
" graph, for performance reasons. If we need to do several ``backward`` calls"
" on the same graph, we need to pass ``retain_graph=True`` to the "
"``backward`` call."
msgstr ""
"出于性能原因，我们只能在给定的图上使用一次 ``backward`` 进行梯度计算。如果我们需要在同一个图上做多次 ``backward`` "
"调用，我们需要在 ``backward`` 调用时传递 ``retain_graph=True``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Disabling Gradient Tracking"
msgstr "禁用梯度跟踪"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By default, all tensors with ``requires_grad=True`` are tracking their "
"computational history and support gradient computation. However, there are "
"some cases when we do not need to do that, for example, when we have trained"
" the model and just want to apply it to some input data, i.e. we only want "
"to do *forward* computations through the network. We can stop tracking "
"computations by surrounding our computation code with ``torch.no_grad()`` "
"block:"
msgstr ""
"默认情况下，所有设置了 ``requires_grad=True`` "
"的张量都会跟踪它们的计算历史并支持梯度计算。然而，有些情况我们不需要这样做，例如，当我们训练了模型后，只是想在某些输入数据上应用它，也就是说，我们只希望通过网络进行"
" *前向* 计算。我们可以通过在计算代码周围使用 ``torch.no_grad()`` 块来停止跟踪计算："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Another way to achieve the same result is to use the ``detach()`` method on "
"the tensor:"
msgstr "实现同样结果的另一种方式是对张量使用 ``detach()`` 方法："

#: ../../beginner/vt_tutorial.rst:783
msgid "There are reasons you might want to disable gradient tracking:"
msgstr "以下是可能希望禁用梯度跟踪的原因："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To mark some parameters in your neural network as **frozen parameters**."
msgstr "在神经网络中标记一些参数为 **冻结参数**。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To **speed up computations** when you are only doing forward pass, because "
"computations on tensors that do not track gradients would be more efficient."
msgstr "当您只做前向传递时，可以 **加速计算**，因为不跟踪梯度的张量计算效率更高。"

#: ../../beginner/vt_tutorial.rst:783
msgid "More on Computational Graphs"
msgstr "更多关于计算图的信息"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Conceptually, autograd keeps a record of data (tensors) and all executed "
"operations (along with the resulting new tensors) in a directed acyclic "
"graph (DAG) consisting of `Function "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`__ "
"objects. In this DAG, leaves are the input tensors, roots are the output "
"tensors. By tracing this graph from roots to leaves, you can automatically "
"compute the gradients using the chain rule."
msgstr ""
"从概念上讲，autograd 会记录数据（张量）及所有已执行的操作（以及由此生成的新张量），这些记录组成了由 `Function "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`__ "
"对象构成的有向无环图 (DAG)。在这个 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶子追踪这个图，您可以使用链式法则自动计算梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "In a forward pass, autograd does two things simultaneously:"
msgstr "在前向传播时，autograd 同时执行两个操作："

#: ../../beginner/vt_tutorial.rst:783
msgid "run the requested operation to compute a resulting tensor"
msgstr "运行请求的操作以计算结果张量"

#: ../../beginner/vt_tutorial.rst:783
msgid "maintain the operation’s *gradient function* in the DAG."
msgstr "在 DAG 中维护操作的 *梯度函数*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The backward pass kicks off when ``.backward()`` is called on the DAG root. "
"``autograd`` then:"
msgstr "当在 DAG 根节点调用 ``.backward()`` 时，反向传播开始。然后 ``autograd``："

#: ../../beginner/vt_tutorial.rst:783
msgid "computes the gradients from each ``.grad_fn``,"
msgstr "从每个 ``.grad_fn`` 中计算梯度，"

#: ../../beginner/vt_tutorial.rst:783
msgid "accumulates them in the respective tensor’s ``.grad`` attribute"
msgstr "将梯度累积到相应张量的 ``.grad`` 属性中"

#: ../../beginner/vt_tutorial.rst:783
msgid "using the chain rule, propagates all the way to the leaf tensors."
msgstr "使用链式法则，一路传播到叶子张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**DAGs are dynamic in PyTorch** An important thing to note is that the graph"
" is recreated from scratch; after each ``.backward()`` call, autograd starts"
" populating a new graph. This is exactly what allows you to use control flow"
" statements in your model; you can change the shape, size and operations at "
"every iteration if needed."
msgstr ""
"**PyTorch 中的 DAG 是动态的**。需要注意的一点是，在每次调用 ``.backward()`` "
"后，图都会从头开始重新创建；autograd "
"开始填充一个新的图。这正是使您可以在模型中使用控制流语句的原因；如果需要，您可以在每次迭代时更改形状、大小和操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optional Reading: Tensor Gradients and Jacobian Products"
msgstr "选读：张量梯度和雅可比积"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In many cases, we have a scalar loss function, and we need to compute the "
"gradient with respect to some parameters. However, there are cases when the "
"output function is an arbitrary tensor. In this case, PyTorch allows you to "
"compute so-called **Jacobian product**, and not the actual gradient."
msgstr ""
"在很多情况下，我们有一个标量损失函数，需要计算损失函数对某些参数的梯度。然而，也有某些情况下输出函数是任意张量。在这种情况下，PyTorch "
"允许您计算所谓的 **雅可比积**，而不是实际梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For a vector function :math:`\\vec{y}=f(\\vec{x})`, where "
":math:`\\vec{x}=\\langle x_1,\\dots,x_n\\rangle` and "
":math:`\\vec{y}=\\langle y_1,\\dots,y_m\\rangle`, a gradient of "
":math:`\\vec{y}` with respect to :math:`\\vec{x}` is given by **Jacobian "
"matrix**:"
msgstr ""
"对于一个矢量函数 :math:`\\vec{y}=f(\\vec{x})`，其中 :math:`\\vec{x}=\\langle "
"x_1,\\dots,x_n\\rangle` 且 :math:`\\vec{y}=\\langle y_1,\\dots,y_m\\rangle`， "
":math:`\\vec{y}` 对 :math:`\\vec{x}` 的梯度由 **雅可比矩阵** 给出："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"J=\\left(\\begin{array}{ccc}\n"
"   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n"
"   \\vdots & \\ddots & \\vdots\\\\\n"
"   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
"   \\end{array}\\right)"
msgstr ""
"J=\\left(\\begin{array}{ccc}\n"
"   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n"
"   \\vdots & \\ddots & \\vdots\\\\\n"
"   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
"   \\end{array}\\right)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Instead of computing the Jacobian matrix itself, PyTorch allows you to "
"compute **Jacobian Product** :math:`v^T\\cdot J` for a given input vector "
":math:`v=(v_1 \\dots v_m)`. This is achieved by calling ``backward`` with "
":math:`v` as an argument. The size of :math:`v` should be the same as the "
"size of the original tensor, with respect to which we want to compute the "
"product:"
msgstr ""
"PyTorch 允许您为给定的输入矢量 :math:`v=(v_1 \\dots v_m)` 计算 **雅可比积** :math:`v^T\\cdot "
"J`，而不是直接计算雅可比矩阵本身。这可以通过将 :math:`v` 作为参数传递给 ``backward`` 方法来完成。 :math:`v` "
"的大小应该与我们想要计算积的原始张量的大小相同："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Notice that when we call ``backward`` for the second time with the same "
"argument, the value of the gradient is different. This happens because when "
"doing ``backward`` propagation, PyTorch **accumulates the gradients**, i.e. "
"the value of computed gradients is added to the ``grad`` property of all "
"leaf nodes of computational graph. If you want to compute the proper "
"gradients, you need to zero out the ``grad`` property before. In real-life "
"training an *optimizer* helps us to do this."
msgstr ""
"请注意，当我们使用相同参数第二次调用 ``backward`` 时，梯度的值是不同的。这是因为在执行 ``backward`` 传播时，PyTorch "
"**累积梯度**，即计算出的梯度值会添加到计算图的所有叶子节点的 ``grad`` 属性中。如果您想要计算正确的梯度，需要先将 ``grad`` "
"属性置为零。在实际的训练中，*优化器* 会帮助我们完成这一步。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Previously we were calling ``backward()`` function without parameters. This "
"is essentially equivalent to calling ``backward(torch.tensor(1.0))``, which "
"is a useful way to compute the gradients in case of a scalar-valued "
"function, such as loss during neural network training."
msgstr ""
"之前我们调用 ``backward()`` 函数时没有传参。这实际上相当于调用 "
"``backward(torch.tensor(1.0))``，这对于计算标量值的函数（例如神经网络训练中的损失）来说是一种实用的方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Autograd Mechanics <https://pytorch.org/docs/stable/notes/autograd.html>`_"
msgstr "`Autograd 机制 <https://pytorch.org/docs/stable/notes/autograd.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.340 seconds)"
msgstr "**脚本总运行时间：**（0 分钟 0.340 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: autogradqs_tutorial.py "
"<autogradqs_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: autogradqs_tutorial.py <autogradqs_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: autogradqs_tutorial.ipynb "
"<autogradqs_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: autogradqs_tutorial.ipynb "
"<autogradqs_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_buildmodel_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_buildmodel_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || `Tensors <tensorqs_tutorial.html>`_ || `Datasets & DataLoaders "
"<data_tutorial.html>`_ || `Transforms <transforms_tutorial.html>`_ || "
"**Build Model** || `Autograd <autogradqs_tutorial.html>`_ || `Optimization "
"<optimization_tutorial.html>`_ || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础知识 <intro.html>`_ || `快速入门 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || `数据集与DataLoaders <data_tutorial.html>`_ || "
"`数据变换 <transforms_tutorial.html>`_ || **构建模型** || `自动微分 "
"<autogradqs_tutorial.html>`_ || `优化 <optimization_tutorial.html>`_ || "
"`保存与加载模型 <saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Build the Neural Network"
msgstr "构建神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Neural networks comprise of layers/modules that perform operations on data. "
"The `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ namespace provides"
" all the building blocks you need to build your own neural network. Every "
"module in PyTorch subclasses the `nn.Module "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_. A neural"
" network is a module itself that consists of other modules (layers). This "
"nested structure allows for building and managing complex architectures "
"easily."
msgstr ""
"神经网络由执行数据操作的层/模块组成。`torch.nn <https://pytorch.org/docs/stable/nn.html>`_ "
"命名空间提供了构建自定义神经网络所需要的所有构件。PyTorch 中的每个模块都是 `nn.Module "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_ "
"的子类。神经网络本身就是一个包含其他模块（层）的模块。这种嵌套结构使得构建和管理复杂的架构变得容易。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the following sections, we'll build a neural network to classify images "
"in the FashionMNIST dataset."
msgstr "在接下来的部分中，我们将构建一个神经网络来对 FashionMNIST 数据集中的图像进行分类。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Get Device for Training"
msgstr "获取训练设备"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We want to be able to train our model on an `accelerator "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__ such as CUDA, "
"MPS, MTIA, or XPU. If the current accelerator is available, we will use it. "
"Otherwise, we use the CPU."
msgstr ""
"我们希望能够在 CUDA、MPS、MTIA 或 XPU 等 `加速器 "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__上训练模型。如果当前加速器可用，我们将使用它。否则，我们使用"
" CPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define the Class"
msgstr "定义类"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We define our neural network by subclassing ``nn.Module``, and initialize "
"the neural network layers in ``__init__``. Every ``nn.Module`` subclass "
"implements the operations on input data in the ``forward`` method."
msgstr ""
"我们通过继承 ``nn.Module`` 来定义神经网络，并在 ``__init__`` 中初始化神经网络层。每个 ``nn.Module`` "
"子类都在其 ``forward`` 方法中对输入数据执行操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We create an instance of ``NeuralNetwork``, and move it to the ``device``, "
"and print its structure."
msgstr "我们创建一个 ``NeuralNetwork`` 的实例，并将其移动到 ``device`` 上，并打印其结构。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To use the model, we pass it the input data. This executes the model's "
"``forward``, along with some `background operations "
"<https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866>`_."
" Do not call ``model.forward()`` directly!"
msgstr ""
"为了使用模型，我们将输入数据传递给它。这会执行模型的 ``forward`` 方法以及一些 `后台操作 "
"<https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866>`_。不要直接调用"
" ``model.forward()``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Calling the model on the input returns a 2-dimensional tensor with dim=0 "
"corresponding to each output of 10 raw predicted values for each class, and "
"dim=1 corresponding to the individual values of each output. We get the "
"prediction probabilities by passing it through an instance of the "
"``nn.Softmax`` module."
msgstr ""
"调用模型时，输入会返回一个二维张量，dim=0 对应每个类别的 10 个原始预测值，dim=1 对应每个类别的各个单独值。通过将结果传递给 "
"``nn.Softmax`` 模块的实例，我们可以获得预测概率。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Model Layers"
msgstr "模型的层"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's break down the layers in the FashionMNIST model. To illustrate it, we "
"will take a sample minibatch of 3 images of size 28x28 and see what happens "
"to it as we pass it through the network."
msgstr ""
"让我们分解 FashionMNIST 模型中的层。为了说明这一点，我们将使用一个包含 3 张图片的样本小批量，这些图片尺寸为 "
"28x28，并查看当我们将它们通过网络时会发生什么。"

#: ../../beginner/vt_tutorial.rst:783
msgid "nn.Flatten"
msgstr "nn.Flatten"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We initialize the `nn.Flatten  "
"<https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html>`_ layer to"
" convert each 2D 28x28 image into a contiguous array of 784 pixel values ( "
"the minibatch dimension (at dim=0) is maintained)."
msgstr ""
"我们初始化 `nn.Flatten  "
"<https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html>`_ 层，将每个二维 "
"28x28 图像转换为连续的 784 像素值数组（保持小批量维度（在 dim=0））。"

#: ../../beginner/vt_tutorial.rst:783
msgid "nn.Linear"
msgstr "nn.Linear"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `linear layer "
"<https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>`_ is a "
"module that applies a linear transformation on the input using its stored "
"weights and biases."
msgstr ""
"`线性层 <https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>`_ "
"是一个模块，使用其存储的权重和偏置对输入进行线性转换。"

#: ../../beginner/vt_tutorial.rst:783
msgid "nn.ReLU"
msgstr "nn.ReLU"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Non-linear activations are what create the complex mappings between the "
"model's inputs and outputs. They are applied after linear transformations to"
" introduce *nonlinearity*, helping neural networks learn a wide variety of "
"phenomena."
msgstr "非线性激活是神经网络输入和输出之间创建复杂映射的关键。它们应用于线性转换之后，引入 *非线性*，帮助神经网络学习广泛的现象。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this model, we use `nn.ReLU "
"<https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>`_ between our"
" linear layers, but there's other activations to introduce non-linearity in "
"your model."
msgstr ""
"在这个模型中，我们在线性层之间使用 `nn.ReLU "
"<https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>`_，但是您可以使用其他激活方法在模型中引入非线性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "nn.Sequential"
msgstr "nn.Sequential"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`nn.Sequential "
"<https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>`_ is an"
" ordered container of modules. The data is passed through all the modules in"
" the same order as defined. You can use sequential containers to put "
"together a quick network like ``seq_modules``."
msgstr ""
"`nn.Sequential "
"<https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>`_ "
"是一个按顺序排列的模块容器。数据会按照定义的顺序依次通过所有模块。您可以使用顺序容器快速搭建一个网络，例如 ``seq_modules``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "nn.Softmax"
msgstr "nn.Softmax"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The last linear layer of the neural network returns `logits` - raw values in"
" [-\\infty, \\infty] - which are passed to the `nn.Softmax "
"<https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>`_ module. "
"The logits are scaled to values [0, 1] representing the model's predicted "
"probabilities for each class. ``dim`` parameter indicates the dimension "
"along which the values must sum to 1."
msgstr ""
"神经网络的最后一个线性层返回的 `logits` 是 [-\\infty, \\infty] 范围内的原始值，这些值会被传递给 `nn.Softmax "
"<https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>`_ "
"模块。logits 被缩放为 [0, 1] 的值，表示模型预测的每个类别的概率。``dim`` 参数指示这些值需要相加为1的维度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Model Parameters"
msgstr "模型参数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Many layers inside a neural network are *parameterized*, i.e. have "
"associated weights and biases that are optimized during training. "
"Subclassing ``nn.Module`` automatically tracks all fields defined inside "
"your model object, and makes all parameters accessible using your model's "
"``parameters()`` or ``named_parameters()`` methods."
msgstr ""
"神经网络中的许多层是 *参数化的*，也就是说它们有相关联的权重和偏置，这些权重和偏置会在训练过程中被优化。通过继承 "
"``nn.Module``，可以自动追踪模型对象内定义的所有字段，并通过调用模型的 ``parameters()`` 或 "
"``named_parameters()`` 方法访问所有参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example, we iterate over each parameter, and print its size and a "
"preview of its values."
msgstr "在这个示例中，我们迭代每个参数，并打印它们的大小及其值的预览。"

#: ../../beginner/vt_tutorial.rst:783
msgid "`torch.nn API <https://pytorch.org/docs/stable/nn.html>`_"
msgstr "`torch.nn API <https://pytorch.org/docs/stable/nn.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  1.015 seconds)"
msgstr "**脚本的总运行时间:** (0 分钟 1.015 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: buildmodel_tutorial.py "
"<buildmodel_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: buildmodel_tutorial.py <buildmodel_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: buildmodel_tutorial.ipynb "
"<buildmodel_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: buildmodel_tutorial.ipynb "
"<buildmodel_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_data_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_data_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || `Tensors <tensorqs_tutorial.html>`_ || **Datasets & DataLoaders** || "
"`Transforms <transforms_tutorial.html>`_ || `Build Model "
"<buildmodel_tutorial.html>`_ || `Autograd <autogradqs_tutorial.html>`_ || "
"`Optimization <optimization_tutorial.html>`_ || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础知识 <intro.html>`_ || `快速开始 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || **数据集和数据加载器** || `数据变换 "
"<transforms_tutorial.html>`_ || `构建模型 <buildmodel_tutorial.html>`_ || `自动求导 "
"<autogradqs_tutorial.html>`_ || `优化 <optimization_tutorial.html>`_ || "
"`保存和加载模型 <saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Datasets & DataLoaders"
msgstr "数据集和数据加载器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Code for processing data samples can get messy and hard to maintain; we "
"ideally want our dataset code to be decoupled from our model training code "
"for better readability and modularity. PyTorch provides two data primitives:"
" ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset`` that allow"
" you to use pre-loaded datasets as well as your own data. ``Dataset`` stores"
" the samples and their corresponding labels, and ``DataLoader`` wraps an "
"iterable around the ``Dataset`` to enable easy access to the samples."
msgstr ""
"处理数据的代码往往会变得混乱且难以维护；我们理想上的数据集代码应该与模型训练代码解耦，从而提高可读性和模块化。PyTorch "
"提供了两个数据原语：``torch.utils.data.DataLoader`` 和 "
"``torch.utils.data.Dataset``，它们允许您使用预加载的数据集以及自定义数据。``Dataset`` 存储样本及其对应的标签，而"
" ``DataLoader`` 为 ``Dataset`` 包装一个可迭代对象，以便轻松访问样本。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch domain libraries provide a number of pre-loaded datasets (such as "
"FashionMNIST) that subclass ``torch.utils.data.Dataset`` and implement "
"functions specific to the particular data. They can be used to prototype and"
" benchmark your model. You can find them here: `Image Datasets "
"<https://pytorch.org/vision/stable/datasets.html>`_, `Text Datasets  "
"<https://pytorch.org/text/stable/datasets.html>`_, and `Audio Datasets "
"<https://pytorch.org/audio/stable/datasets.html>`_"
msgstr ""
"PyTorch 的领域库提供了许多预加载的数据集（如 FashionMNIST），这些数据集继承自 "
"``torch.utils.data.Dataset`` "
"并实现了特定于特定数据的功能。这些数据集可用于原型设计和基准测试您的模型。您可以在此找到它们：`图像数据集 "
"<https://pytorch.org/vision/stable/datasets.html>`_，`文本数据集 "
"<https://pytorch.org/text/stable/datasets.html>`_，和 `音频数据集 "
"<https://pytorch.org/audio/stable/datasets.html>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loading a Dataset"
msgstr "加载一个数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here is an example of how to load the `Fashion-MNIST "
"<https://research.zalando.com/project/fashion_mnist/fashion_mnist/>`_ "
"dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article "
"images consisting of 60,000 training examples and 10,000 test examples. Each"
" example comprises a 28×28 grayscale image and an associated label from one "
"of 10 classes."
msgstr ""
"以下是加载 `Fashion-MNIST "
"<https://research.zalando.com/project/fashion_mnist/fashion_mnist/>`_ "
"数据集的示例代码。Fashion-MNIST 是 Zalando 文章图片的数据集，由 60,000 个训练样本和 10,000 "
"个测试样本组成。每个样本包含一张 28×28 的灰度图像及其 10 个类别中的一个关联标签。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We load the `FashionMNIST Dataset "
"<https://pytorch.org/vision/stable/datasets.html#fashion-mnist>`_ with the "
"following parameters:"
msgstr ""
"我们使用以下参数加载 `FashionMNIST 数据集 "
"<https://pytorch.org/vision/stable/datasets.html#fashion-mnist>`_："

#: ../../beginner/vt_tutorial.rst:783
msgid "``root`` is the path where the train/test data is stored,"
msgstr "``root`` 是存储训练/测试数据的路径，"

#: ../../beginner/vt_tutorial.rst:783
msgid "``train`` specifies training or test dataset,"
msgstr "``train`` 指定是训练集还是测试集，"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``download=True`` downloads the data from the internet if it's not available"
" at ``root``."
msgstr "``download=True`` 表示如果数据未在 ``root`` 路径下，则从互联网下载数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``transform`` and ``target_transform`` specify the feature and label "
"transformations"
msgstr "``transform`` 和 ``target_transform`` 分别指定特征和标签的变换。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Iterating and Visualizing the Dataset"
msgstr "迭代和可视化数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can index ``Datasets`` manually like a list: ``training_data[index]``. We"
" use ``matplotlib`` to visualize some samples in our training data."
msgstr ""
"我们可以像操作列表一样手动索引 ``Datasets`` ：``training_data[index]``。我们使用 ``matplotlib`` "
"来可视化训练数据的一些样本。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Creating a Custom Dataset for your files"
msgstr "为您的文件创建一个自定义数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A custom Dataset class must implement three functions: `__init__`, "
"`__len__`, and `__getitem__`. Take a look at this implementation; the "
"FashionMNIST images are stored in a directory ``img_dir``, and their labels "
"are stored separately in a CSV file ``annotations_file``."
msgstr ""
"自定义数据集类必须实现三个函数：`__init__`、`__len__` 和 `__getitem__`。以下是一个实现案例：FashionMNIST "
"的图像存储在一个名为 ``img_dir`` 的目录中，而它们的标签则另存于一个 CSV 文件 ``annotations_file`` 中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the next sections, we'll break down what's happening in each of these "
"functions."
msgstr "在接下来的部分中，我们将逐步解析这些函数中发生了什么。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``__init__``"
msgstr "``__init__``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The __init__ function is run once when instantiating the Dataset object. We "
"initialize the directory containing the images, the annotations file, and "
"both transforms (covered in more detail in the next section)."
msgstr "__init__ 函数在实例化数据集对象时运行一次。我们初始化包含图像的目录、注释文件，以及两种变换（详见后续部分）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The labels.csv file looks like: ::"
msgstr "labels.csv 文件内容如下： ::"

#: ../../beginner/vt_tutorial.rst:783
msgid "``__len__``"
msgstr "``__len__``"

#: ../../beginner/vt_tutorial.rst:783
msgid "The __len__ function returns the number of samples in our dataset."
msgstr "__len__ 函数返回数据集中样本的数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Example:"
msgstr "示例："

#: ../../beginner/vt_tutorial.rst:783
msgid "``__getitem__``"
msgstr "``__getitem__``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The __getitem__ function loads and returns a sample from the dataset at the "
"given index ``idx``. Based on the index, it identifies the image's location "
"on disk, converts that to a tensor using ``decode_image``, retrieves the "
"corresponding label from the csv data in ``self.img_labels``, calls the "
"transform functions on them (if applicable), and returns the tensor image "
"and corresponding label in a tuple."
msgstr ""
"__getitem__ 函数加载并返回给定索引 ``idx`` 的数据集样本。根据索引定位磁盘上的图像位置，用 ``decode_image`` "
"将其转换为张量，从 ``self.img_labels`` 中的 CSV "
"数据中检索到相应的标签，对它们应用变换函数（如果适用），并以元组形式返回张量图像及其对应的标签。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Preparing your data for training with DataLoaders"
msgstr "使用 DataLoaders 准备训练数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``Dataset`` retrieves our dataset's features and labels one sample at a "
"time. While training a model, we typically want to pass samples in "
"\"minibatches\", reshuffle the data at every epoch to reduce model "
"overfitting, and use Python's ``multiprocessing`` to speed up data "
"retrieval."
msgstr ""
"``Dataset`` "
"一次检索我们数据集的一个特征和标签。在训练模型时，我们通常希望以\"小批量\"的形式传递样本，在每个训练周期中对数据进行重新洗牌以减少模型过拟合，并使用"
" Python 的 ``multiprocessing`` 加速数据检索。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``DataLoader`` is an iterable that abstracts this complexity for us in an "
"easy API."
msgstr "``DataLoader`` 以简单的 API 抽象了这些复杂性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Iterate through the DataLoader"
msgstr "遍历 DataLoader"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have loaded that dataset into the ``DataLoader`` and can iterate through "
"the dataset as needed. Each iteration below returns a batch of "
"``train_features`` and ``train_labels`` (containing ``batch_size=64`` "
"features and labels respectively). Because we specified ``shuffle=True``, "
"after we iterate over all batches the data is shuffled (for finer-grained "
"control over the data loading order, take a look at `Samplers "
"<https://pytorch.org/docs/stable/data.html#data-loading-order-and-"
"sampler>`_)."
msgstr ""
"我们已将该数据集加载到了 ``DataLoader`` 中，并可根据需要遍历该数据集。以下迭代中的每次返回包括 ``batch_size=64`` 的 "
"``train_features`` 和 ``train_labels``（分别包含特征和标签）。由于我们指定了 "
"``shuffle=True``，在对所有批进行迭代后，数据会被重新洗牌（如需更精细地控制数据加载顺序，可以查看 `采样器 "
"<https://pytorch.org/docs/stable/data.html#data-loading-order-and-"
"sampler>`_）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "`torch.utils.data API <https://pytorch.org/docs/stable/data.html>`_"
msgstr "`torch.utils.data API <https://pytorch.org/docs/stable/data.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.700 seconds)"
msgstr "**脚本的总运行时间:** (0 分钟 0.700 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: data_tutorial.py <data_tutorial.py>`"
msgstr ":download:`下载 Python 源代码: data_tutorial.py <data_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: data_tutorial.ipynb "
"<data_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: data_tutorial.ipynb <data_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Learn the Basics"
msgstr "学习基础"

#: ../../beginner/vt_tutorial.rst:783
msgid "intro.py"
msgstr "intro.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Learn the Basics https://pytorch.org/tutorials/beginner/basics/intro.html"
msgstr "学习基础 https://pytorch.org/tutorials/beginner/basics/intro.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "quickstart_tutorial.py"
msgstr "quickstart_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Quickstart "
"https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
msgstr ""
"快速开始 https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "tensorqs_tutorial.py"
msgstr "tensorqs_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors https://pytorch.org/tutorials/beginner/basics/tensor_tutorial.html"
msgstr "张量 https://pytorch.org/tutorials/beginner/basics/tensor_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "data_tutorial.py"
msgstr "data_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Datasets & DataLoaders "
"https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
msgstr ""
"数据集和数据加载器 https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "transforms_tutorial.py"
msgstr "transforms_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Transforms "
"https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html"
msgstr ""
"数据变换 https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "buildmodel_tutorial.py"
msgstr "buildmodel_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Building the Neural Network "
"https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
msgstr ""
"构建神经网络 "
"https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "autogradqs_tutorial.py"
msgstr "autogradqs_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Automatic Differentiation with torch.autograd_tutorial "
"https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html"
msgstr ""
"自动微分 https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "optimization_tutorial.py"
msgstr "optimization_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Optimizing Model Parameters "
"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
msgstr ""
"优化模型参数 "
"https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "saveloadrun_tutorial.py"
msgstr "saveloadrun_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Save and Load the Model "
"https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html"
msgstr ""
"保存和加载模型 "
"https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Learn the Basics** ||"
msgstr "**学习基础** ||"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_intro.py`"
msgstr ":ref:`sphx_glr_beginner_basics_intro.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Learn the Basics <intro.html>`_ ||"
msgstr "`学习基础 <intro.html>`_ ||"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_saveloadrun_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_saveloadrun_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_transforms_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_transforms_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_autogradqs_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_autogradqs_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_buildmodel_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_buildmodel_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_tensorqs_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_tensorqs_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_data_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_data_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_optimization_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_optimization_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_quickstart_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_basics_quickstart_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_intro.py>` to download "
"the full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_beginner_basics_intro.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Learn the Basics** || `Quickstart <quickstart_tutorial.html>`_ || `Tensors"
" <tensorqs_tutorial.html>`_ || `Datasets & DataLoaders "
"<data_tutorial.html>`_ || `Transforms <transforms_tutorial.html>`_ || `Build"
" Model <buildmodel_tutorial.html>`_ || `Autograd "
"<autogradqs_tutorial.html>`_ || `Optimization <optimization_tutorial.html>`_"
" || `Save & Load Model <saveloadrun_tutorial.html>`_"
msgstr ""
"**学习基础** || `快速开始 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || `数据集和数据加载器 <data_tutorial.html>`_ || `数据变换 "
"<transforms_tutorial.html>`_ || `构建模型 <buildmodel_tutorial.html>`_ || `自动求导 "
"<autogradqs_tutorial.html>`_ || `优化 <optimization_tutorial.html>`_ || "
"`保存和加载模型 <saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Authors: `Suraj Subramanian <https://github.com/subramen>`_, `Seth Juarez "
"<https://github.com/sethjuarez/>`_, `Cassie Breviu "
"<https://github.com/cassiebreviu/>`_, `Dmitry Soshnikov "
"<https://soshnikov.com/>`_, `Ari Bornstein "
"<https://github.com/aribornstein/>`_"
msgstr ""
"作者: `Suraj Subramanian <https://github.com/subramen>`_, `Seth Juarez "
"<https://github.com/sethjuarez/>`_, `Cassie Breviu "
"<https://github.com/cassiebreviu/>`_, `Dmitry Soshnikov "
"<https://soshnikov.com/>`_, `Ari Bornstein "
"<https://github.com/aribornstein/>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Most machine learning workflows involve working with data, creating models, "
"optimizing model parameters, and saving the trained models. This tutorial "
"introduces you to a complete ML workflow implemented in PyTorch, with links "
"to learn more about each of these concepts."
msgstr ""
"大多数机器学习工作流程包括处理数据、创建模型、优化模型参数和保存训练后的模型。本教程向您介绍一个完整的 ML 工作流（用 PyTorch "
"实现），并附有学习这些概念的更多链接。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We'll use the FashionMNIST dataset to train a neural network that predicts "
"if an input image belongs to one of the following classes: T-shirt/top, "
"Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot."
msgstr ""
"我们将用 FashionMNIST "
"数据集来训练一个神经网络，该网络预测输入的图像是否属于下列类别中的某一类：T-shirt/top，Trouser，Pullover，Dress，Coat，Sandal，Shirt，Sneaker，Bag"
" 或 Ankle boot。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`This tutorial assumes a basic familiarity with Python and Deep Learning "
"concepts.`"
msgstr "`本教程假定您已对 Python 和深度学习概念有基本的熟悉。`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Running the Tutorial Code"
msgstr "运行教程代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "You can run this tutorial in a couple of ways:"
msgstr "您可以通过以下几种方式运行此教程："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**In the cloud**: This is the easiest way to get started! Each section has a"
" \"Run in Microsoft Learn\" and \"Run in Google Colab\" link at the top, "
"which opens an integrated notebook in Microsoft Learn or Google Colab, "
"respectively, with the code in a fully-hosted environment."
msgstr ""
"**在云端运行**：这是最快速的开始方式！每节开头都有一个\"在 Microsoft Learn 中运行\"和\"在 Google Colab "
"中运行\"的链接，点击即可在 Microsoft Learn 或 Google Colab 的集成环境中运行代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Locally**: This option requires you to setup PyTorch and TorchVision first"
" on your local machine (`installation instructions <https://pytorch.org/get-"
"started/locally/>`_). Download the notebook or copy the code into your "
"favorite IDE."
msgstr ""
"**本地运行**：此选项需要您先在本地计算机上安装 PyTorch 和 TorchVision（`安装说明 "
"<https://pytorch.org/get-started/locally/>`_）。下载 notebook 或将代码复制到您喜欢的 IDE。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to Use this Guide"
msgstr "如何使用本指南"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you're familiar with other deep learning frameworks, check out the `0. "
"Quickstart <quickstart_tutorial.html>`_ first to quickly familiarize "
"yourself with PyTorch's API."
msgstr ""
"如果您熟悉其他深度学习框架，请先查看 `0. 快速开始 <quickstart_tutorial.html>`_ 以快速熟悉 PyTorch 的 "
"API。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you're new to deep learning frameworks, head right into the first section"
" of our step-by-step guide: `1. Tensors <tensor_tutorial.html>`_."
msgstr "如果您是深度学习框架的新手，请直接进入我们分步指南的第一部分：`1. 张量 <tensor_tutorial.html>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "0. `Quickstart <quickstart_tutorial.html>`_"
msgstr "0. `快速开始 <quickstart_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "1. `Tensors <tensorqs_tutorial.html>`_"
msgstr "1. `张量 <tensorqs_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "2. `Datasets and DataLoaders <data_tutorial.html>`_"
msgstr "2. `数据集和数据加载器 <data_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "3. `Transforms <transforms_tutorial.html>`_"
msgstr "3. `数据变换 <transforms_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "4. `Build Model <buildmodel_tutorial.html>`_"
msgstr "4. `构建模型 <buildmodel_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "5. `Automatic Differentiation <autogradqs_tutorial.html>`_"
msgstr "5. `自动微分 <autogradqs_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "6. `Optimization Loop <optimization_tutorial.html>`_"
msgstr "6. `优化循环 <optimization_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "7. `Save, Load and Use Model <saveloadrun_tutorial.html>`_"
msgstr "7. `保存、加载和使用模型 <saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.000 seconds)"
msgstr "**脚本总运行时间：** (0 分钟 0.000 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Python source code: intro.py <intro.py>`"
msgstr ":download:`下载 Python 源代码: intro.py <intro.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Jupyter notebook: intro.ipynb <intro.ipynb>`"
msgstr ":download:`下载 Jupyter notebook: intro.ipynb <intro.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_basics_optimization_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_optimization_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || `Tensors <tensorqs_tutorial.html>`_ || `Datasets & DataLoaders "
"<data_tutorial.html>`_ || `Transforms <transforms_tutorial.html>`_ || `Build"
" Model <buildmodel_tutorial.html>`_ || `Autograd "
"<autogradqs_tutorial.html>`_ || **Optimization** || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础知识 <intro.html>`_ || `快速入门 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || `数据集和数据加载器 <data_tutorial.html>`_ || `数据转换 "
"<transforms_tutorial.html>`_ || `构建模型 <buildmodel_tutorial.html>`_ || `自动微分 "
"<autogradqs_tutorial.html>`_ || **优化** || `保存和加载模型 "
"<saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimizing Model Parameters"
msgstr "优化模型参数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have a model and data it's time to train, validate and test our "
"model by optimizing its parameters on our data. Training a model is an "
"iterative process; in each iteration the model makes a guess about the "
"output, calculates the error in its guess (*loss*), collects the derivatives"
" of the error with respect to its parameters (as we saw in the `previous "
"section  <autograd_tutorial.html>`_), and **optimizes** these parameters "
"using gradient descent. For a more detailed walkthrough of this process, "
"check out this video on `backpropagation from 3Blue1Brown "
"<https://www.youtube.com/watch?v=tIeHLnjs5U8>`__."
msgstr ""
"现在我们有了一个模型和数据，是时候通过优化数据上的参数来训练、验证和测试我们的模型了。训练模型是一个迭代过程；在每次迭代中，模型对输出进行预测，计算预测中的误差（*loss*），收集误差对其参数的导数（如我们在`上一节"
" <autograd_tutorial.html>`_ 中看到的），并使用梯度下降**优化**这些参数。有关此过程的详细演示，请参阅 "
"`3Blue1Brown 的反向传播视频 <https://www.youtube.com/watch?v=tIeHLnjs5U8>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Prerequisite Code"
msgstr "前置代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We load the code from the previous sections on `Datasets & DataLoaders "
"<data_tutorial.html>`_ and `Build Model  <buildmodel_tutorial.html>`_."
msgstr ""
"我们加载上一节关于`数据集和数据加载器 <data_tutorial.html>`_ 和 `构建模型 "
"<buildmodel_tutorial.html>`_ 中的代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Hyperparameters"
msgstr "超参数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hyperparameters are adjustable parameters that let you control the model "
"optimization process. Different hyperparameter values can impact model "
"training and convergence rates (`read more "
"<https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html>`__"
" about hyperparameter tuning)"
msgstr ""
"超参数是可调参数，允许您控制模型优化过程。不同的超参数值可能会影响模型的训练和收敛速度（`了解更多 "
"<https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html>`__"
" 超参数调优信息）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We define the following hyperparameters for training:"
msgstr "我们为模型训练定义以下超参数："

#: ../../beginner/vt_tutorial.rst:783
msgid "**Number of Epochs** - the number of times to iterate over the dataset"
msgstr "**迭代次数（Epochs）** - 数据集迭代的次数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Batch Size** - the number of data samples propagated through the network "
"before the parameters are updated"
msgstr "**批次大小（Batch Size）** - 在更新参数之前通过网络传播的数据样本数量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Learning Rate** - how much to update models parameters at each "
"batch/epoch. Smaller values yield slow learning speed, while large values "
"may result in unpredictable behavior during training."
msgstr ""
"**学习率（Learning Rate）** - "
"每个批次或每次迭代更新模型参数的程度。较小的值会导致学习速度较慢，而较大的值可能导致训练期间行为不可预测。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimization Loop"
msgstr "优化循环"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once we set our hyperparameters, we can then train and optimize our model "
"with an optimization loop. Each iteration of the optimization loop is called"
" an **epoch**."
msgstr "一旦设置好超参数，我们就可以通过优化循环来训练和优化模型。优化循环的每次迭代称为**迭代周期（epoch）**。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Each epoch consists of two main parts:"
msgstr "每个迭代周期主要包括两个部分："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**The Train Loop** - iterate over the training dataset and try to converge "
"to optimal parameters."
msgstr "**训练循环** - 遍历训练数据集并尝试收敛到最佳参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**The Validation/Test Loop** - iterate over the test dataset to check if "
"model performance is improving."
msgstr "**验证/测试循环** - 遍历测试数据集以检查模型性能是否在改善。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's briefly familiarize ourselves with some of the concepts used in the "
"training loop. Jump ahead to see the :ref:`full-impl-label` of the "
"optimization loop."
msgstr "让我们简要了解训练循环中使用的一些概念。跳到 :ref:`完整实现代码 <full-impl-label>` 以查看优化循环的全貌。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loss Function"
msgstr "损失函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When presented with some training data, our untrained network is likely not "
"to give the correct answer. **Loss function** measures the degree of "
"dissimilarity of obtained result to the target value, and it is the loss "
"function that we want to minimize during training. To calculate the loss we "
"make a prediction using the inputs of our given data sample and compare it "
"against the true data label value."
msgstr ""
"对一些训练数据进行操作时，我们未训练的网络可能无法给出正确答案。**损失函数**度量获得的结果与目标值的差异程度，我们希望在训练期间最小化损失函数。计算损失时，我们使用给定数据样本的输入进行预测，并将其与真实数据标签的值进行比较。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Common loss functions include `nn.MSELoss "
"<https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss>`_"
" (Mean Square Error) for regression tasks, and `nn.NLLLoss "
"<https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>`_"
" (Negative Log Likelihood) for classification. `nn.CrossEntropyLoss "
"<https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss>`_"
" combines ``nn.LogSoftmax`` and ``nn.NLLLoss``."
msgstr ""
"常见的损失函数包括用于回归任务的`nn.MSELoss "
"<https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss>`_（均方误差），以及用于分类任务的`nn.NLLLoss"
" "
"<https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss>`_（负对数似然）。`nn.CrossEntropyLoss"
" "
"<https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss>`_结合了``nn.LogSoftmax``和``nn.NLLLoss``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We pass our model's output logits to ``nn.CrossEntropyLoss``, which will "
"normalize the logits and compute the prediction error."
msgstr "我们将模型输出 logits 传递给``nn.CrossEntropyLoss``，它将对 logits 进行归一化并计算预测误差。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimizer"
msgstr "优化器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Optimization is the process of adjusting model parameters to reduce model "
"error in each training step. **Optimization algorithms** define how this "
"process is performed (in this example we use Stochastic Gradient Descent). "
"All optimization logic is encapsulated in  the ``optimizer`` object. Here, "
"we use the SGD optimizer; additionally, there are many `different optimizers"
" <https://pytorch.org/docs/stable/optim.html>`_ available in PyTorch such as"
" ADAM and RMSProp, that work better for different kinds of models and data."
msgstr ""
"优化是调整模型参数以减少每个训练步骤中模型误差的过程。**优化算法**定义此过程如何执行（在此示例中我们使用随机梯度下降）。所有优化逻辑都封装在``优化器``对象中。在这里，我们使用"
" SGD 优化器；此外，PyTorch 中还有许多适合不同模型和数据的 `不同优化器 "
"<https://pytorch.org/docs/stable/optim.html>`_，例如 ADAM 和 RMSProp。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We initialize the optimizer by registering the model's parameters that need "
"to be trained, and passing in the learning rate hyperparameter."
msgstr "我们通过注册模型需要训练的参数并传入学习率超参数来初始化优化器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Inside the training loop, optimization happens in three steps:"
msgstr "在训练循环中，优化过程分为三个步骤："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Call ``optimizer.zero_grad()`` to reset the gradients of model parameters. "
"Gradients by default add up; to prevent double-counting, we explicitly zero "
"them at each iteration."
msgstr "调用``optimizer.zero_grad()``来重置模型参数的梯度。梯度默认相加；为了防止重复计算，我们在每次迭代时显式将其归零。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Backpropagate the prediction loss with a call to ``loss.backward()``. "
"PyTorch deposits the gradients of the loss w.r.t. each parameter."
msgstr "通过调用``loss.backward()``对预测损失进行回传。PyTorch 会记录损失对每个参数的梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once we have our gradients, we call ``optimizer.step()`` to adjust the "
"parameters by the gradients collected in the backward pass."
msgstr "获得梯度后，我们调用``optimizer.step()``以根据回传中收集的梯度调整参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Full Implementation"
msgstr "完整实现"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We define ``train_loop`` that loops over our optimization code, and "
"``test_loop`` that evaluates the model's performance against our test data."
msgstr "我们定义了``train_loop``，它循环运行我们的优化代码，并定义了``test_loop``评估模型对测试数据的性能。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We initialize the loss function and optimizer, and pass it to ``train_loop``"
" and ``test_loop``. Feel free to increase the number of epochs to track the "
"model's improving performance."
msgstr ""
"我们初始化损失函数和优化器，并将它们传递给``train_loop``和``test_loop``。可以自由增加迭代次数以跟踪模型性能的提升。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Loss Functions <https://pytorch.org/docs/stable/nn.html#loss-functions>`_"
msgstr "`损失函数 <https://pytorch.org/docs/stable/nn.html#loss-functions>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`torch.optim <https://pytorch.org/docs/stable/optim.html>`_"
msgstr "`torch.optim <https://pytorch.org/docs/stable/optim.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Warmstart Training a Model "
"<https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html>`_"
msgstr ""
"`通过使用来自不同模型的参数来热启动训练模型 "
"<https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 4 minutes  56.049 seconds)"
msgstr "**脚本总运行时间：** (4 分钟 56.049 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: optimization_tutorial.py "
"<optimization_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: optimization_tutorial.py "
"<optimization_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: optimization_tutorial.ipynb "
"<optimization_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: optimization_tutorial.ipynb "
"<optimization_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_quickstart_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_quickstart_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || **Quickstart** || `Tensors "
"<tensorqs_tutorial.html>`_ || `Datasets & DataLoaders <data_tutorial.html>`_"
" || `Transforms <transforms_tutorial.html>`_ || `Build Model "
"<buildmodel_tutorial.html>`_ || `Autograd <autogradqs_tutorial.html>`_ || "
"`Optimization <optimization_tutorial.html>`_ || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础知识 <intro.html>`_ || **快速入门** || `张量 <tensorqs_tutorial.html>`_ || "
"`数据集和数据加载器 <data_tutorial.html>`_ || `数据转换 <transforms_tutorial.html>`_ || "
"`构建模型 <buildmodel_tutorial.html>`_ || `自动微分 <autogradqs_tutorial.html>`_ || "
"`优化 <optimization_tutorial.html>`_ || `保存和加载模型 <saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Quickstart"
msgstr "快速入门"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This section runs through the API for common tasks in machine learning. "
"Refer to the links in each section to dive deeper."
msgstr "本节涵盖了机器学习中的常见任务的 API。请参考各节中的链接以深入了解。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Working with data"
msgstr "处理数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch has two `primitives to work with data "
"<https://pytorch.org/docs/stable/data.html>`_: "
"``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``. "
"``Dataset`` stores the samples and their corresponding labels, and "
"``DataLoader`` wraps an iterable around the ``Dataset``."
msgstr ""
"PyTorch 有两个`处理数据的基础组件 "
"<https://pytorch.org/docs/stable/data.html>`_：``torch.utils.data.DataLoader``"
" 和 ``torch.utils.data.Dataset``。``Dataset`` 存储样本及其对应标签，``DataLoader`` "
"为``Dataset``封装一个可迭代对象。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch offers domain-specific libraries such as `TorchText "
"<https://pytorch.org/text/stable/index.html>`_, `TorchVision "
"<https://pytorch.org/vision/stable/index.html>`_, and `TorchAudio "
"<https://pytorch.org/audio/stable/index.html>`_, all of which include "
"datasets. For this tutorial, we  will be using a TorchVision dataset."
msgstr ""
"PyTorch 提供了领域特定的库，如 `TorchText "
"<https://pytorch.org/text/stable/index.html>`_、`TorchVision "
"<https://pytorch.org/vision/stable/index.html>`_ 和 `TorchAudio "
"<https://pytorch.org/audio/stable/index.html>`_，它们都包含数据集。在本教程中，我们将使用 "
"TorchVision 数据集。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``torchvision.datasets`` module contains ``Dataset`` objects for many "
"real-world vision data like CIFAR, COCO (`full list here "
"<https://pytorch.org/vision/stable/datasets.html>`_). In this tutorial, we "
"use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two "
"arguments: ``transform`` and ``target_transform`` to modify the samples and "
"labels respectively."
msgstr ""
"``torchvision.datasets``模块包含了许多真实世界视觉数据的``Dataset``对象，例如 CIFAR, COCO（`完整列表 "
"<https://pytorch.org/vision/stable/datasets.html>`_）。在本教程中，我们使用 FashionMNIST"
" 数据集。每个 TorchVision ``Dataset`` 都包含两个参数：``transform`` 和 ``target_transform``"
" 分别用于修改样本和标签。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an "
"iterable over our dataset, and supports automatic batching, sampling, "
"shuffling and multiprocess data loading. Here we define a batch size of 64, "
"i.e. each element in the dataloader iterable will return a batch of 64 "
"features and labels."
msgstr ""
"我们将``Dataset``作为参数传递给``DataLoader``。它为我们的数据集封装了一个可迭代对象，并支持自动批处理、采样、洗牌和多进程数据加载。在这里我们定义了批次大小为"
" 64，即 dataloader 可迭代对象中的每个元素将返回一批 64 个特征和标签。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Read more about `loading data in PyTorch <data_tutorial.html>`_."
msgstr "了解更多关于 `在 PyTorch 中加载数据 <data_tutorial.html>`_ 的内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Creating Models"
msgstr "创建模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To define a neural network in PyTorch, we create a class that inherits from "
"`nn.Module "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_. We "
"define the layers of the network in the ``__init__`` function and specify "
"how data will pass through the network in the ``forward`` function. To "
"accelerate operations in the neural network, we move it to the `accelerator "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__ such as CUDA, "
"MPS, MTIA, or XPU. If the current accelerator is available, we will use it. "
"Otherwise, we use the CPU."
msgstr ""
"要在 PyTorch 中定义神经网络，我们可以创建一个继承自`nn.Module "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_ "
"的类。在``__init__``函数中定义网络的层级，并在``forward``函数中指定数据如何通过网络传递。为了加速神经网络中的操作，我们将其移动到`加速设备"
" <https://pytorch.org/docs/stable/torch.html#accelerators>`__，例如 "
"CUDA、MPS、MTIA 或 XPU。如果当前加速设备可用，我们将使用它，否则使用 CPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Read more about `building neural networks in PyTorch "
"<buildmodel_tutorial.html>`_."
msgstr "了解更多关于 `在 PyTorch 中构建神经网络 <buildmodel_tutorial.html>`_ 的内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimizing the Model Parameters"
msgstr "优化模型参数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To train a model, we need a `loss function "
"<https://pytorch.org/docs/stable/nn.html#loss-functions>`_ and an `optimizer"
" <https://pytorch.org/docs/stable/optim.html>`_."
msgstr ""
"要训练模型，我们需要一个`损失函数 <https://pytorch.org/docs/stable/nn.html#loss-functions>`_"
" 和一个 `优化器 <https://pytorch.org/docs/stable/optim.html>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In a single training loop, the model makes predictions on the training "
"dataset (fed to it in batches), and backpropagates the prediction error to "
"adjust the model's parameters."
msgstr "在单次训练循环中，模型对训练数据集（以批次形式输入）进行预测，并对预测误差进行回传以调整模型参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We also check the model's performance against the test dataset to ensure it "
"is learning."
msgstr "我们还会通过测试数据集检查模型的性能，以确保它正在学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The training process is conducted over several iterations (*epochs*). During"
" each epoch, the model learns parameters to make better predictions. We "
"print the model's accuracy and loss at each epoch; we'd like to see the "
"accuracy increase and the loss decrease with every epoch."
msgstr ""
"训练过程在多个迭代（*epochs*）中进行。在每个迭代周期中，模型学习参数以更好地进行预测。我们打印每个迭代周期的模型准确率和损失；希望看到准确率随每次迭代提升，损失随每次迭代下降。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Read more about `Training your model <optimization_tutorial.html>`_."
msgstr "了解更多关于 `训练模型 <optimization_tutorial.html>`_ 的内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving Models"
msgstr "保存模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A common way to save a model is to serialize the internal state dictionary "
"(containing the model parameters)."
msgstr "保存模型的一种常见方法是序列化内部状态字典（包含模型参数）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loading Models"
msgstr "加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The process for loading a model includes re-creating the model structure and"
" loading the state dictionary into it."
msgstr "加载模型的过程包括重新创建模型结构，并将状态字典加载到其中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "This model can now be used to make predictions."
msgstr "现在可以使用该模型进行预测。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Read more about `Saving & Loading your model <saveloadrun_tutorial.html>`_."
msgstr "了解更多关于 `保存和加载模型 <saveloadrun_tutorial.html>`_ 的内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  21.879 seconds)"
msgstr "**脚本总运行时间：** (0 分钟 21.879 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: quickstart_tutorial.py "
"<quickstart_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: quickstart_tutorial.py <quickstart_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: quickstart_tutorial.ipynb "
"<quickstart_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: quickstart_tutorial.ipynb "
"<quickstart_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_basics_saveloadrun_tutorial.py>` to download the"
" full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_saveloadrun_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || `Tensors <tensorqs_tutorial.html>`_ || `Datasets & DataLoaders "
"<data_tutorial.html>`_ || `Transforms <transforms_tutorial.html>`_ || `Build"
" Model <buildmodel_tutorial.html>`_ || `Autograd "
"<autogradqs_tutorial.html>`_ || `Optimization <optimization_tutorial.html>`_"
" || **Save & Load Model**"
msgstr ""
"`学习基础知识 <intro.html>`_ || `快速入门 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || `数据集与数据加载器 <data_tutorial.html>`_ || `变换 "
"<transforms_tutorial.html>`_ || `构建模型 <buildmodel_tutorial.html>`_ || `自动微分 "
"<autogradqs_tutorial.html>`_ || `优化 <optimization_tutorial.html>`_ || "
"**保存和加载模型**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save and Load the Model"
msgstr "保存和加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this section we will look at how to persist model state with saving, "
"loading and running model predictions."
msgstr "在本节中，我们将讨论如何通过保存、加载和运行模型预测来持久化模型状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and Loading Model Weights"
msgstr "保存和加载模型权重"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch models store the learned parameters in an internal state dictionary,"
" called ``state_dict``. These can be persisted via the ``torch.save`` "
"method:"
msgstr ""
"PyTorch 模型将学习到的参数存储在一个称为 ``state_dict`` 的内部状态字典中。可以通过 ``torch.save`` "
"方法将其持久化："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To load model weights, you need to create an instance of the same model "
"first, and then load the parameters using ``load_state_dict()`` method."
msgstr "要加载模型权重，您需要首先创建相同模型的一个实例，然后使用 ``load_state_dict()`` 方法加载参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the code below, we set ``weights_only=True`` to limit the functions "
"executed during unpickling to only those necessary for loading weights. "
"Using ``weights_only=True`` is considered a best practice when loading "
"weights."
msgstr ""
"在下面的代码中，我们设置 ``weights_only=True``，以限制在取消序列化过程中仅执行加载权重所需的函数。使用 "
"``weights_only=True`` 是加载权重时的最佳实践。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"be sure to call ``model.eval()`` method before inferencing to set the "
"dropout and batch normalization layers to evaluation mode. Failing to do "
"this will yield inconsistent inference results."
msgstr ""
"在推理之前，一定要调用 ``model.eval()`` 方法，以将 dropout 和批量归一化层设置为评估模式。未进行此操作会导致推理结果不一致。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and Loading Models with Shapes"
msgstr "保存和加载包含结构的模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When loading model weights, we needed to instantiate the model class first, "
"because the class defines the structure of a network. We might want to save "
"the structure of this class together with the model, in which case we can "
"pass ``model`` (and not ``model.state_dict()``) to the saving function:"
msgstr ""
"在加载模型权重时，我们需要先实例化模型类，因为该类定义了网络的结构。我们可能希望将类的结构与模型一起保存，这可以通过将 ``model``（而不是 "
"``model.state_dict()``）传递给保存函数来实现："

#: ../../beginner/vt_tutorial.rst:783
msgid "We can then load the model as demonstrated below."
msgstr "我们可以按照下面的示例代码加载模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As described in `Saving and loading torch.nn.Modules "
"<https://pytorch.org/docs/main/notes/serialization.html#saving-and-loading-"
"torch-nn-modules>`_, saving ``state_dict`` is considered the best practice. "
"However, below we use ``weights_only=False`` because this involves loading "
"the model, which is a legacy use case for ``torch.save``."
msgstr ""
"如 `保存和加载 torch.nn.Modules "
"<https://pytorch.org/docs/main/notes/serialization.html#saving-and-loading-"
"torch-nn-modules>`_ 中所述，保存 ``state_dict`` 被认为是最佳实践。然而，在下面的例子中我们使用 "
"``weights_only=False``，因为这涉及到加载模型，这是对 ``torch.save`` 的一个遗留用例。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This approach uses Python `pickle "
"<https://docs.python.org/3/library/pickle.html>`_ module when serializing "
"the model, thus it relies on the actual class definition to be available "
"when loading the model."
msgstr ""
"此方法使用 Python `pickle <https://docs.python.org/3/library/pickle.html>`_ "
"模块进行模型序列化，因此在加载模型时需要实际的类定义可用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Related Tutorials"
msgstr "相关教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Saving and Loading a General Checkpoint in PyTorch "
"<https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html>`_"
msgstr ""
"`在 PyTorch 中保存和加载通用检查点 "
"<https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Tips for loading an nn.Module from a checkpoint "
"<https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html?highlight=loading%20nn%20module%20from%20checkpoint>`_"
msgstr ""
"`从检查点加载 nn.Module 的技巧 "
"<https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html?highlight=loading%20nn%20module%20from%20checkpoint>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  24.487 seconds)"
msgstr "**脚本总运行时间:** ( 0 分钟 24.487 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: saveloadrun_tutorial.py "
"<saveloadrun_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: saveloadrun_tutorial.py <saveloadrun_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: saveloadrun_tutorial.ipynb "
"<saveloadrun_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: saveloadrun_tutorial.ipynb "
"<saveloadrun_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Computation times"
msgstr "计算时间"

#: ../../beginner/vt_tutorial.rst:783
msgid "**05:54.127** total execution time for **beginner_basics** files:"
msgstr "**05:54.127** 初学者基础文件总执行时间:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_optimization_tutorial.py` "
"(``optimization_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_optimization_tutorial.py` "
"(``optimization_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "04:56.049"
msgstr "04:56.049"

#: ../../beginner/vt_tutorial.rst:783
msgid "0.0 MB"
msgstr "0.0 MB"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_saveloadrun_tutorial.py` "
"(``saveloadrun_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_saveloadrun_tutorial.py` "
"(``saveloadrun_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:24.487"
msgstr "00:24.487"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_quickstart_tutorial.py` "
"(``quickstart_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_quickstart_tutorial.py` "
"(``quickstart_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:21.879"
msgstr "00:21.879"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_transforms_tutorial.py` "
"(``transforms_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_transforms_tutorial.py` "
"(``transforms_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:08.832"
msgstr "00:08.832"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_buildmodel_tutorial.py` "
"(``buildmodel_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_buildmodel_tutorial.py` "
"(``buildmodel_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:01.015"
msgstr "00:01.015"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_tensorqs_tutorial.py` "
"(``tensorqs_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_tensorqs_tutorial.py` "
"(``tensorqs_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.825"
msgstr "00:00.825"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_data_tutorial.py` (``data_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_data_tutorial.py` (``data_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.700"
msgstr "00:00.700"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_basics_autogradqs_tutorial.py` "
"(``autogradqs_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_basics_autogradqs_tutorial.py` "
"(``autogradqs_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.340"
msgstr "00:00.340"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_basics_intro.py` (``intro.py``)"
msgstr ":ref:`sphx_glr_beginner_basics_intro.py` (``intro.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.000"
msgstr "00:00.000"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_tensorqs_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_tensorqs_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || **Tensors** || `Datasets & DataLoaders <data_tutorial.html>`_ || "
"`Transforms <transforms_tutorial.html>`_ || `Build Model "
"<buildmodel_tutorial.html>`_ || `Autograd <autogradqs_tutorial.html>`_ || "
"`Optimization <optimization_tutorial.html>`_ || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础知识 <intro.html>`_ || `快速入门 <quickstart_tutorial.html>`_ || **张量** || "
"`数据集与数据加载器 <data_tutorial.html>`_ || `变换 <transforms_tutorial.html>`_ || "
"`构建模型 <buildmodel_tutorial.html>`_ || `自动微分 <autogradqs_tutorial.html>`_ || "
"`优化 <optimization_tutorial.html>`_ || `保存和加载模型 <saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensors"
msgstr "张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors are a specialized data structure that are very similar to arrays and"
" matrices. In PyTorch, we use tensors to encode the inputs and outputs of a "
"model, as well as the model’s parameters."
msgstr "张量是一种特殊的数据结构，与数组和矩阵非常相似。在 PyTorch 中，我们使用张量对模型的输入和输出以及模型的参数进行编码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors are similar to `NumPy’s <https://numpy.org/>`_ ndarrays, except that"
" tensors can run on GPUs or other hardware accelerators. In fact, tensors "
"and NumPy arrays can often share the same underlying memory, eliminating the"
" need to copy data (see :ref:`bridge-to-np-label`). Tensors are also "
"optimized for automatic differentiation (we'll see more about that later in "
"the `Autograd <autogradqs_tutorial.html>`__ section). If you’re familiar "
"with ndarrays, you’ll be right at home with the Tensor API. If not, follow "
"along!"
msgstr ""
"张量类似于 `NumPy <https://numpy.org/>`_ 的 ndarrays，但张量可以在 GPU "
"或其他硬件加速器上运行。事实上，张量和 NumPy 数组通常可以共享相同的底层内存，从而无需复制数据（请参阅 :ref:`bridge-to-np-"
"label`）。张量还针对自动微分进行了优化（我们稍后会在 `自动微分 <autogradqs_tutorial.html>`__ "
"部分详细讨论）。如果您熟悉 ndarrays，您会对张量 API 感到得心应手。如果不熟悉，请继续阅读！"

#: ../../beginner/vt_tutorial.rst:783
msgid "Initializing a Tensor"
msgstr "初始化张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors can be initialized in various ways. Take a look at the following "
"examples:"
msgstr "张量可以通过多种方式初始化。请看以下示例："

#: ../../beginner/vt_tutorial.rst:783
msgid "**Directly from data**"
msgstr "**直接从数据中生成**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors can be created directly from data. The data type is automatically "
"inferred."
msgstr "张量可以直接从数据创建。数据类型会自动推断。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**From a NumPy array**"
msgstr "**从 NumPy 数组生成**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors can be created from NumPy arrays (and vice versa - see :ref:`bridge-"
"to-np-label`)."
msgstr "张量可以从 NumPy 数组创建（反之亦然 - 请参阅 :ref:`bridge-to-np-label`)。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**From another tensor:**"
msgstr "**从另一张量生成:**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The new tensor retains the properties (shape, datatype) of the argument "
"tensor, unless explicitly overridden."
msgstr "新的张量会保留参数张量的属性（形状、数据类型），除非显式覆盖。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**With random or constant values:**"
msgstr "**使用随机或常量值生成:**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``shape`` is a tuple of tensor dimensions. In the functions below, it "
"determines the dimensionality of the output tensor."
msgstr "``shape`` 是张量维度的元组。在以下函数中，它确定输出张量的维度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Attributes of a Tensor"
msgstr "张量的属性"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensor attributes describe their shape, datatype, and the device on which "
"they are stored."
msgstr "张量属性描述其形状、数据类型以及存储设备。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Operations on Tensors"
msgstr "张量上的操作"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Over 1200 tensor operations, including arithmetic, linear algebra, matrix "
"manipulation (transposing, indexing, slicing), sampling and more are "
"comprehensively described `here "
"<https://pytorch.org/docs/stable/torch.html>`__."
msgstr ""
"超过1200种张量操作，包括算术、线性代数、矩阵操作（转置、索引、切片）、采样等，此处有详细描述：`这里 "
"<https://pytorch.org/docs/stable/torch.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each of these operations can be run on the CPU and `Accelerator "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__ such as CUDA, "
"MPS, MTIA, or XPU. If you’re using Colab, allocate an accelerator by going "
"to Runtime > Change runtime type > GPU."
msgstr ""
"这些操作中的每一个都可以在 CPU 和 `加速器 "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__ 上运行，比如 "
"CUDA、MPS、MTIA 或 XPU。如果你使用 Colab，可以通过转到运行时 > 更改运行时类型 > GPU 来分配加速器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By default, tensors are created on the CPU. We need to explicitly move "
"tensors to the accelerator using ``.to`` method (after checking for "
"accelerator availability). Keep in mind that copying large tensors across "
"devices can be expensive in terms of time and memory!"
msgstr ""
"默认情况下，张量在 CPU 上创建。我们需要显式使用 ``.to`` "
"方法将张量移动到加速器上（在检查加速器可用性之后）。请注意，在设备之间复制大张量在时间和内存方面可能会很昂贵！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Try out some of the operations from the list. If you're familiar with the "
"NumPy API, you'll find the Tensor API a breeze to use."
msgstr "试试列表中的一些操作。如果您熟悉 NumPy 的 API，那么使用张量 API 会非常轻松。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Standard numpy-like indexing and slicing:**"
msgstr "**标准的 numpy 风格的索引和切片:**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of "
"tensors along a given dimension. See also `torch.stack "
"<https://pytorch.org/docs/stable/generated/torch.stack.html>`__, another "
"tensor joining operator that is subtly different from ``torch.cat``."
msgstr ""
"**连接张量** 您可以使用 ``torch.cat`` 沿给定维度连接一系列张量。此外还可以使用 `torch.stack "
"<https://pytorch.org/docs/stable/generated/torch.stack.html>`__，这是另一个张量连接函数，与"
" ``torch.cat`` 有细微差别。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Arithmetic operations**"
msgstr "**算术操作**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Single-element tensors** If you have a one-element tensor, for example by "
"aggregating all values of a tensor into one value, you can convert it to a "
"Python numerical value using ``item()``:"
msgstr ""
"**单元素张量** 如果您有一个单元素张量，比如通过聚合张量的所有值得到一个值，可以使用 ``item()`` 将其转换为 Python 数值："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**In-place operations** Operations that store the result into the operand "
"are called in-place. They are denoted by a ``_`` suffix. For example: "
"``x.copy_(y)``, ``x.t_()``, will change ``x``."
msgstr ""
"**就地操作** 将结果存储到操作数的操作称为就地操作。它们以 ``_`` 后缀命名。例如：``x.copy_(y)``, ``x.t_()``，将更改"
" ``x``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In-place operations save some memory, but can be problematic when computing "
"derivatives because of an immediate loss of history. Hence, their use is "
"discouraged."
msgstr "就地操作可以节省一些内存，但在计算导数时可能会有问题，因为会立即丢失历史记录。因此，不建议使用它们。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Bridge with NumPy"
msgstr "与 NumPy 桥接"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors on the CPU and NumPy arrays can share their underlying memory "
"locations, and changing one will change the other."
msgstr "CPU 上的张量和 NumPy 数组可以共享它们的底层内存位置，改变其中一个会影响另一个。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensor to NumPy array"
msgstr "张量转 NumPy 数组"

#: ../../beginner/vt_tutorial.rst:783
msgid "A change in the tensor reflects in the NumPy array."
msgstr "张量中的更改会反映到 NumPy 数组中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "NumPy array to Tensor"
msgstr "NumPy 数组转张量"

#: ../../beginner/vt_tutorial.rst:783
msgid "Changes in the NumPy array reflects in the tensor."
msgstr "NumPy 数组中的更改会反映到张量中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.825 seconds)"
msgstr "**脚本总运行时间:** ( 0 分钟 0.825 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: tensorqs_tutorial.py "
"<tensorqs_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: tensorqs_tutorial.py <tensorqs_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: tensorqs_tutorial.ipynb "
"<tensorqs_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: tensorqs_tutorial.ipynb "
"<tensorqs_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_basics_transforms_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_basics_transforms_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Learn the Basics <intro.html>`_ || `Quickstart <quickstart_tutorial.html>`_"
" || `Tensors <tensorqs_tutorial.html>`_ || `Datasets & DataLoaders "
"<data_tutorial.html>`_ || **Transforms** || `Build Model "
"<buildmodel_tutorial.html>`_ || `Autograd <autogradqs_tutorial.html>`_ || "
"`Optimization <optimization_tutorial.html>`_ || `Save & Load Model "
"<saveloadrun_tutorial.html>`_"
msgstr ""
"`学习基础知识 <intro.html>`_ || `快速入门 <quickstart_tutorial.html>`_ || `张量 "
"<tensorqs_tutorial.html>`_ || `数据集与数据加载器 <data_tutorial.html>`_ || **变换** ||"
" `构建模型 <buildmodel_tutorial.html>`_ || `自动微分 <autogradqs_tutorial.html>`_ ||"
" `优化 <optimization_tutorial.html>`_ || `保存和加载模型 "
"<saveloadrun_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Transforms"
msgstr "变换"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Data does not always come in its final processed form that is required for "
"training machine learning algorithms. We use **transforms** to perform some "
"manipulation of the data and make it suitable for training."
msgstr "数据并非总是以机器学习算法所需的最终处理形式出现。我们使用 **变换** 对数据进行一些操作，使其适合训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"All TorchVision datasets have two parameters -``transform`` to modify the "
"features and ``target_transform`` to modify the labels - that accept "
"callables containing the transformation logic. The `torchvision.transforms "
"<https://pytorch.org/vision/stable/transforms.html>`_ module offers several "
"commonly-used transforms out of the box."
msgstr ""
"所有 TorchVision 数据集都有两个参数 -``transform`` 用于修改特征，``target_transform`` 用于修改标签 -"
" 接受包含转换逻辑的可调用对象。`torchvision.transforms "
"<https://pytorch.org/vision/stable/transforms.html>`_ 模块提供了几个常用的变换。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The FashionMNIST features are in PIL Image format, and the labels are "
"integers. For training, we need the features as normalized tensors, and the "
"labels as one-hot encoded tensors. To make these transformations, we use "
"``ToTensor`` and ``Lambda``."
msgstr ""
"FashionMNIST 特征为 PIL "
"图像格式，标签为整数。在训练中，我们需要将特征转为归一化张量，并将标签转为独热编码张量。为了完成这些转换，我们使用 ``ToTensor`` 和 "
"``Lambda``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "ToTensor()"
msgstr "ToTensor()"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`ToTensor "
"<https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor>`_"
" converts a PIL image or NumPy ``ndarray`` into a ``FloatTensor``. and "
"scales the image's pixel intensity values in the range [0., 1.]"
msgstr ""
"`ToTensor "
"<https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor>`_"
" 将 PIL 图像或 NumPy ``ndarray`` 转为 ``FloatTensor``，并将图像的像素强度值缩放到范围 [0., 1.]"

#: ../../beginner/vt_tutorial.rst:783
msgid "Lambda Transforms"
msgstr "Lambda 转换"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Lambda transforms apply any user-defined lambda function. Here, we define a "
"function to turn the integer into a one-hot encoded tensor. It first creates"
" a zero tensor of size 10 (the number of labels in our dataset) and calls "
"`scatter_ "
"<https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html>`_ "
"which assigns a ``value=1`` on the index as given by the label ``y``."
msgstr ""
"Lambda 转换应用任何用户定义的 lambda 函数。这里我们定义一个函数，将整数转为独热编码张量。它首先创建一个大小为 "
"10（我们数据集中标签数量）的零张量，并调用 `scatter_ "
"<https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html>`_，在标签"
" ``y`` 给出的索引上分配一个 ``value=1``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`torchvision.transforms API "
"<https://pytorch.org/vision/stable/transforms.html>`_"
msgstr ""
"镜像 API: `torchvision.transforms API "
"<https://pytorch.org/vision/stable/transforms.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  8.832 seconds)"
msgstr "**脚本总运行时间：** (0 分钟 8.832 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: transforms_tutorial.py "
"<transforms_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码： transforms_tutorial.py <transforms_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: transforms_tutorial.ipynb "
"<transforms_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook： transforms_tutorial.ipynb "
"<transforms_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Fast Transformer Inference with Better Transformer"
msgstr "使用更好的 Transformer 实现快速 Transformer 推理"

#: ../../beginner/vt_tutorial.rst:783
msgid "This tutorial has been deprecated."
msgstr "本教程已被弃用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Redirecting in 3 seconds..."
msgstr "3 秒后重定向..."

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_blitz_autograd_tutorial.py>` to"
" download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_blitz_autograd_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "A Gentle Introduction to ``torch.autograd``"
msgstr "``torch.autograd`` 的基础介绍"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.autograd`` is PyTorch’s automatic differentiation engine that powers"
" neural network training. In this section, you will get a conceptual "
"understanding of how autograd helps a neural network train."
msgstr ""
"``torch.autograd`` 是 PyTorch 的自动微分引擎，为神经网络训练提供支持。在本节中，您将从概念上了解 autograd "
"如何帮助神经网络进行训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Background"
msgstr "背景知识"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Neural networks (NNs) are a collection of nested functions that are executed"
" on some input data. These functions are defined by *parameters* (consisting"
" of weights and biases), which in PyTorch are stored in tensors."
msgstr ""
"神经网络 (NN) 是一系列嵌套函数的集合，这些函数在某些输入数据上执行操作。这些函数通过*参数*（包括权重和偏置）来定义，在 PyTorch "
"中，这些参数存储在张量中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training a NN happens in two steps:"
msgstr "训练神经网络分为两个步骤："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Forward Propagation**: In forward prop, the NN makes its best guess about "
"the correct output. It runs the input data through each of its functions to "
"make this guess."
msgstr "**前向传播**：在前向传播中，神经网络对正确的输出进行最佳猜测。它通过每个函数运行输入数据以进行猜测。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Backward Propagation**: In backprop, the NN adjusts its parameters "
"proportionate to the error in its guess. It does this by traversing "
"backwards from the output, collecting the derivatives of the error with "
"respect to the parameters of the functions (*gradients*), and optimizing the"
" parameters using gradient descent. For a more detailed walkthrough of "
"backprop, check out this `video from 3Blue1Brown "
"<https://www.youtube.com/watch?v=tIeHLnjs5U8>`__."
msgstr ""
"**反向传播**：在反向传播中，神经网络根据错误大小调整其参数。它从输出开始反向遍历，收集关于函数参数的误差的导数（*梯度*），并使用梯度下降优化这些参数。有关反向传播更详细的讲解，请查看此"
" `视频 (3Blue1Brown) <https://www.youtube.com/watch?v=tIeHLnjs5U8>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Usage in PyTorch"
msgstr "在 PyTorch 中的使用"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's take a look at a single training step. For this example, we load a "
"pretrained resnet18 model from ``torchvision``. We create a random data "
"tensor to represent a single image with 3 channels, and height & width of "
"64, and its corresponding ``label`` initialized to some random values. Label"
" in pretrained models has shape (1,1000)."
msgstr ""
"让我们看看单次训练步骤。对于此示例，我们从 ``torchvision`` 加载预训练的 resnet18 模型。我们创建一个随机张量数据，代表一张具有"
" 3 个通道，64 高和 64 宽的图像，以及它的对应 ``标签``，初始化为一些随机值。在预训练模型中，标签形状为 (1,1000)。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial works only on the CPU and will not work on GPU devices (even "
"if tensors are moved to CUDA)."
msgstr "本教程仅在 CPU 上使用，不在 GPU 上运行（即便将张量移动到 CUDA 也不支持）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we run the input data through the model through each of its layers to "
"make a prediction. This is the **forward pass**."
msgstr "接下来，我们通过模型的每一层运行输入数据以进行预测。这是 **前向传播**。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We use the model's prediction and the corresponding label to calculate the "
"error (``loss``). The next step is to backpropagate this error through the "
"network. Backward propagation is kicked off when we call ``.backward()`` on "
"the error tensor. Autograd then calculates and stores the gradients for each"
" model parameter in the parameter's ``.grad`` attribute."
msgstr ""
"我们使用模型的预测和对应的标签来计算误差（``损失``）。下一步是通过网络反向传播这个误差。当我们在误差张量上调用 ``.backward()`` "
"时，反向传播开始。Autograd 会计算并存储每个模型参数的梯度，在参数的 ``.grad`` 属性中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we load an optimizer, in this case SGD with a learning rate of 0.01 "
"and `momentum <https://towardsdatascience.com/stochastic-gradient-descent-"
"with-momentum-a84097641a5d>`__ of 0.9. We register all the parameters of the"
" model in the optimizer."
msgstr ""
"接下来，我们加载一个优化器，这里使用学习率为 0.01，`动量 <https://towardsdatascience.com/stochastic-"
"gradient-descent-with-momentum-a84097641a5d>`__ 为 0.9 的 "
"SGD。我们将模型的所有参数注册到该优化器中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we call ``.step()`` to initiate gradient descent. The optimizer "
"adjusts each parameter by its gradient stored in ``.grad``."
msgstr "最后，我们调用 ``.step()`` 来启动梯度下降。优化器会根据保存在 ``.grad`` 中的梯度调整每个参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"At this point, you have everything you need to train your neural network. "
"The below sections detail the workings of autograd - feel free to skip them."
msgstr "到现在为止，您已经掌握了训练神经网络所需的一切。下面的部分详细说明了 autograd 的工作原理，您可以根据需要选择阅读或跳过。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Differentiation in Autograd"
msgstr "在 Autograd 中的求导"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's take a look at how ``autograd`` collects gradients. We create two "
"tensors ``a`` and ``b`` with ``requires_grad=True``. This signals to "
"``autograd`` that every operation on them should be tracked."
msgstr ""
"让我们看看 ``autograd`` 如何收集梯度。我们创建了两个张量 ``a`` 和 ``b`` ，并设置 "
"``requires_grad=True``。这告诉 ``autograd`` 应跟踪所有对它们的操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We create another tensor ``Q`` from ``a`` and ``b``."
msgstr "我们通过 ``a`` 和 ``b`` 创建了另一个张量 ``Q``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Q = 3a^3 - b^2"
msgstr ""
"Q = 3a^3 - b^2\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's assume ``a`` and ``b`` to be parameters of an NN, and ``Q`` to be the "
"error. In NN training, we want gradients of the error w.r.t. parameters, "
"i.e."
msgstr "假设 ``a`` 和 ``b`` 是神经网络的参数，而 ``Q`` 是误差。在神经网络训练中，我们需要误差相对于参数的梯度，即："

#: ../../beginner/vt_tutorial.rst:783
msgid "\\frac{\\partial Q}{\\partial a} = 9a^2"
msgstr ""
"\\frac{\\partial Q}{\\partial a} = 9a^2\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\frac{\\partial Q}{\\partial b} = -2b"
msgstr ""
"\\frac{\\partial Q}{\\partial b} = -2b\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When we call ``.backward()`` on ``Q``, autograd calculates these gradients "
"and stores them in the respective tensors' ``.grad`` attribute."
msgstr ""
"当我们在 ``Q`` 上调用 ``.backward()`` 时，autograd 会计算这些梯度并将其存储在相关张量的 ``.grad`` 属性中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We need to explicitly pass a ``gradient`` argument in ``Q.backward()`` "
"because it is a vector. ``gradient`` is a tensor of the same shape as ``Q``,"
" and it represents the gradient of Q w.r.t. itself, i.e."
msgstr ""
"由于 ``Q`` 是一个向量，因此我们需要在 ``Q.backward()`` 中显式传递一个 ``gradient`` 参数。``gradient``"
" 是一个与 ``Q`` 形状相同的张量，表示 Q 相对于自身的梯度，即："

#: ../../beginner/vt_tutorial.rst:783
msgid "\\frac{dQ}{dQ} = 1"
msgstr ""
"\\frac{dQ}{dQ} = 1\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Equivalently, we can also aggregate Q into a scalar and call backward "
"implicitly, like ``Q.sum().backward()``."
msgstr "或者，我们也可以将 Q 聚合为标量并隐式调用 backward，例如 ``Q.sum().backward()``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Gradients are now deposited in ``a.grad`` and ``b.grad``"
msgstr "现在，梯度会存放在 ``a.grad`` 和 ``b.grad`` 中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optional Reading - Vector Calculus using ``autograd``"
msgstr "可选阅读 - 使用 ``autograd`` 进行向量微积分"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Mathematically, if you have a vector valued function "
":math:`\\vec{y}=f(\\vec{x})`, then the gradient of :math:`\\vec{y}` with "
"respect to :math:`\\vec{x}` is a Jacobian matrix :math:`J`:"
msgstr ""
"从数学上讲，如果您有一个向量值函数 :math:`\\vec{y}=f(\\vec{x})`，那么 :math:`\\vec{y}` 对于 "
":math:`\\vec{x}` 的梯度是一个雅可比矩阵 :math:`J`："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"J\n"
"=\n"
" \\left(\\begin{array}{cc}\n"
" \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n"
" ... &\n"
" \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n"
" \\end{array}\\right)\n"
"=\n"
"\\left(\\begin{array}{ccc}\n"
" \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n"
" \\vdots & \\ddots & \\vdots\\\\\n"
" \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
" \\end{array}\\right)"
msgstr ""
"J\n"
"=\n"
" \\left(\\begin{array}{cc}\n"
" \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n"
" ... &\n"
" \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n"
" \\end{array}\\right)\n"
"=\n"
"\\left(\\begin{array}{ccc}\n"
" \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n"
" \\vdots & \\ddots & \\vdots\\\\\n"
" \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
" \\end{array}\\right)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Generally speaking, ``torch.autograd`` is an engine for computing vector-"
"Jacobian product. That is, given any vector :math:`\\vec{v}`, compute the "
"product :math:`J^{T}\\cdot \\vec{v}`"
msgstr ""
"总的来说，``torch.autograd`` 是一个向量-雅可比积的计算引擎。即给定任何向量 :math:`\\vec{v}`，计算乘积 "
":math:`J^{T}\\cdot \\vec{v}`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If :math:`\\vec{v}` happens to be the gradient of a scalar function "
":math:`l=g\\left(\\vec{y}\\right)`:"
msgstr "如果 :math:`\\vec{v}` 恰好是标量函数 :math:`l=g\\left(\\vec{y}\\right)` 的梯度："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\vec{v}\n"
" =\n"
" \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}"
msgstr ""
"\\vec{v}\n"
" =\n"
" \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"then by the chain rule, the vector-Jacobian product would be the gradient of"
" :math:`l` with respect to :math:`\\vec{x}`:"
msgstr "那么根据链式法则，向量-雅可比积将是 :math:`l` 相对于 :math:`\\vec{x}` 的梯度："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"J^{T}\\cdot \\vec{v} = \\left(\\begin{array}{ccc}\n"
" \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n"
" \\vdots & \\ddots & \\vdots\\\\\n"
" \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
" \\end{array}\\right)\\left(\\begin{array}{c}\n"
" \\frac{\\partial l}{\\partial y_{1}}\\\\\n"
" \\vdots\\\\\n"
" \\frac{\\partial l}{\\partial y_{m}}\n"
" \\end{array}\\right) = \\left(\\begin{array}{c}\n"
" \\frac{\\partial l}{\\partial x_{1}}\\\\\n"
" \\vdots\\\\\n"
" \\frac{\\partial l}{\\partial x_{n}}\n"
" \\end{array}\\right)"
msgstr ""
"J^{T}\\cdot \\vec{v} = \\left(\\begin{array}{ccc}\n"
" \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n"
" \\vdots & \\ddots & \\vdots\\\\\n"
" \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
" \\end{array}\\right)\\left(\\begin{array}{c}\n"
" \\frac{\\partial l}{\\partial y_{1}}\\\\\n"
" \\vdots\\\\\n"
" \\frac{\\partial l}{\\partial y_{m}}\n"
" \\end{array}\\right) = \\left(\\begin{array}{c}\n"
" \\frac{\\partial l}{\\partial x_{1}}\\\\\n"
" \\vdots\\\\\n"
" \\frac{\\partial l}{\\partial x_{n}}\n"
" \\end{array}\\right)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This characteristic of vector-Jacobian product is what we use in the above "
"example; ``external_grad`` represents :math:`\\vec{v}`."
msgstr "向量-雅可比积的这种特性正是我们在上述示例中使用的；``external_grad`` 表示 :math:`\\vec{v}`。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Computational Graph"
msgstr "计算图"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Conceptually, autograd keeps a record of data (tensors) & all executed "
"operations (along with the resulting new tensors) in a directed acyclic "
"graph (DAG) consisting of `Function "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`__ "
"objects. In this DAG, leaves are the input tensors, roots are the output "
"tensors. By tracing this graph from roots to leaves, you can automatically "
"compute the gradients using the chain rule."
msgstr ""
"从概念上讲，autograd 会记录数据（张量）及所有执行的操作（以及产生的新张量），构成一个有向无环图 (DAG)，其中包含 `Function "
"<https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`__ "
"对象。在此图中，叶子是输入张量，根是输出张量。通过从根到叶子的这条路径，您可以使用链式法则自动计算梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "run the requested operation to compute a resulting tensor, and"
msgstr "运行请求操作以计算结果张量，"

#: ../../beginner/vt_tutorial.rst:783
msgid "accumulates them in the respective tensor’s ``.grad`` attribute, and"
msgstr "将梯度累加到相应张量的 ``.grad`` 属性中，"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below is a visual representation of the DAG in our example. In the graph, "
"the arrows are in the direction of the forward pass. The nodes represent the"
" backward functions of each operation in the forward pass. The leaf nodes in"
" blue represent our leaf tensors ``a`` and ``b``."
msgstr ""
"以下是我们的示例中 DAG 的可视化表示。在图中，箭头表示前向传播的方向。节点代表前向传播中每个操作的反向函数。蓝色叶节点表示我们的叶张量 ``a`` "
"和 ``b``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Exclusion from the DAG"
msgstr "从 DAG 中排除"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.autograd`` tracks operations on all tensors which have their "
"``requires_grad`` flag set to ``True``. For tensors that don’t require "
"gradients, setting this attribute to ``False`` excludes it from the gradient"
" computation DAG."
msgstr ""
"``torch.autograd`` 会跟踪所有 ``requires_grad`` 标志设置为 ``True`` "
"的张量上的操作。对于不需要梯度的张量，将此属性设置为 ``False`` 可将其从梯度计算 DAG 中排除。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The output tensor of an operation will require gradients even if only a "
"single input tensor has ``requires_grad=True``."
msgstr "如果仅有一个输入张量设置 ``requires_grad=True``，操作的输出张量仍然需要计算梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In a NN, parameters that don't compute gradients are usually called **frozen"
" parameters**. It is useful to \"freeze\" part of your model if you know in "
"advance that you won't need the gradients of those parameters (this offers "
"some performance benefits by reducing autograd computations)."
msgstr ""
"在神经网络中，不计算梯度的参数通常称为 **冻结参数**。如果您事先知道某些参数不需要梯度，可以“冻结”模型的一部分（通过减少 autograd "
"计算可提高性能）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In finetuning, we freeze most of the model and typically only modify the "
"classifier layers to make predictions on new labels. Let's walk through a "
"small example to demonstrate this. As before, we load a pretrained resnet18 "
"model, and freeze all the parameters."
msgstr ""
"在微调中，我们冻结大部分模型，通常只修改分类层以针对新标签进行预测。让我们通过一个小示例来演示这一点。如前，我们加载了一个预训练的 resnet18 "
"模型，并冻结了所有参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's say we want to finetune the model on a new dataset with 10 labels. In "
"resnet, the classifier is the last linear layer ``model.fc``. We can simply "
"replace it with a new linear layer (unfrozen by default) that acts as our "
"classifier."
msgstr ""
"假设我们想要在 10 个标签的新数据集上微调模型。在 resnet 中，分类器是最后的线性层 "
"``model.fc``。我们只需用一个新的线性层（默认未冻结）替换它，该层充当我们的分类器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now all parameters in the model, except the parameters of ``model.fc``, are "
"frozen. The only parameters that compute gradients are the weights and bias "
"of ``model.fc``."
msgstr "现在，模型中除 ``model.fc`` 的参数外，所有参数都被冻结。唯一计算梯度的参数是 ``model.fc`` 的权重和偏置。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Notice although we register all the parameters in the optimizer, the only "
"parameters that are computing gradients (and hence updated in gradient "
"descent) are the weights and bias of the classifier."
msgstr "请注意，虽然我们将所有参数都注册到优化器中，但唯一计算梯度（并因此在梯度下降中更新）的参数是分类器的权重和偏置。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The same exclusionary functionality is available as a context manager in "
"`torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html>`__"
msgstr ""
"相同的排除功能可以作为上下文管理器 `torch.no_grad() "
"<https://pytorch.org/docs/stable/generated/torch.no_grad.html>`__ 中提供。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Further readings:"
msgstr "进一步阅读："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`In-place operations & Multithreaded Autograd "
"<https://pytorch.org/docs/stable/notes/autograd.html>`__"
msgstr ""
"`原地操作和多线程 Autograd <https://pytorch.org/docs/stable/notes/autograd.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Example implementation of reverse-mode autodiff "
"<https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC>`__"
msgstr ""
"`反向模式自动微分的示例实现 "
"<https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Video: PyTorch Autograd Explained - In-depth Tutorial "
"<https://www.youtube.com/watch?v=MswxJw-8PvE>`__"
msgstr ""
"`视频：深入教程：PyTorch Autograd 说明 "
"<https://www.youtube.com/watch?v=MswxJw-8PvE>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.688 seconds)"
msgstr "**脚本总运行时间：** (0 分钟 0.688 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: autograd_tutorial.py "
"<autograd_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码： autograd_tutorial.py <autograd_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: autograd_tutorial.ipynb "
"<autograd_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook： autograd_tutorial.ipynb "
"<autograd_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_blitz_cifar10_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_blitz_cifar10_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training a Classifier"
msgstr "训练一个分类器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is it. You have seen how to define neural networks, compute loss and "
"make updates to the weights of the network."
msgstr "就是这样。您已经看到了如何定义神经网络、计算损失并更新网络权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now you might be thinking,"
msgstr "现在您可能会想到，"

#: ../../beginner/vt_tutorial.rst:783
msgid "What about data?"
msgstr "数据怎么办？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Generally, when you have to deal with image, text, audio or video data, you "
"can use standard python packages that load data into a numpy array. Then you"
" can convert this array into a ``torch.*Tensor``."
msgstr ""
"通常，当您需要处理图像、文本、音频或视频数据时，可以使用标准的 Python 包将数据加载到一个 numpy 数组中。然后可以将此数组转换为 "
"``torch.*Tensor``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For images, packages such as Pillow, OpenCV are useful"
msgstr "对于图像，可以使用 Pillow、OpenCV 等包"

#: ../../beginner/vt_tutorial.rst:783
msgid "For audio, packages such as scipy and librosa"
msgstr "对于音频，可以使用 scipy 和 librosa"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For text, either raw Python or Cython based loading, or NLTK and SpaCy are "
"useful"
msgstr "对于文本，可以使用原生 Python、基于 Cython 的加载器，或者 NLTK 和 SpaCy"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Specifically for vision, we have created a package called ``torchvision``, "
"that has data loaders for common datasets such as ImageNet, CIFAR10, MNIST, "
"etc. and data transformers for images, viz., ``torchvision.datasets`` and "
"``torch.utils.data.DataLoader``."
msgstr ""
"特别是针对视觉任务，我们创建了一个名为 ``torchvision`` 的包，它具有常用数据集（例如 ImageNet、CIFAR10、MNIST "
"等）的数据加载器和图像数据变换工具，即 ``torchvision.datasets`` 和 "
"``torch.utils.data.DataLoader``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "This provides a huge convenience and avoids writing boilerplate code."
msgstr "这极大地提供了便利，避免了编写样板代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this tutorial, we will use the CIFAR10 dataset. It has the classes: "
"‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, "
"‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel "
"color images of 32x32 pixels in size."
msgstr ""
"在本教程中，我们将使用 CIFAR10 "
"数据集。它包含以下类别：'飞机'、'汽车'、'鸟'、'猫'、'鹿'、'狗'、'青蛙'、'马'、'船'、'卡车'。CIFAR-10 的图像大小为 "
"3x32x32，即 32x32 像素的三通道彩色图像。"

#: ../../beginner/vt_tutorial.rst:783
msgid "cifar10"
msgstr "cifar10"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training an image classifier"
msgstr "训练一个图像分类器"

#: ../../beginner/vt_tutorial.rst:783
msgid "We will do the following steps in order:"
msgstr "我们将按以下步骤进行："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Load and normalize the CIFAR10 training and test datasets using "
"``torchvision``"
msgstr "使用 ``torchvision`` 加载并标准化 CIFAR10 的训练和测试数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define a Convolutional Neural Network"
msgstr "定义一个卷积神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define a loss function"
msgstr "定义一个损失函数"

#: ../../beginner/vt_tutorial.rst:783
msgid "Train the network on the training data"
msgstr "在训练数据上训练网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "Test the network on the test data"
msgstr "在测试数据上测试网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "1. Load and normalize CIFAR10"
msgstr "1. 加载并标准化 CIFAR10"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using ``torchvision``, it’s extremely easy to load CIFAR10."
msgstr "使用 ``torchvision``，加载 CIFAR10 非常简单。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The output of torchvision datasets are PILImage images of range [0, 1]. We "
"transform them to Tensors of normalized range [-1, 1]."
msgstr ""
"torchvision 数据集的输出是范围为 [0, 1] 的 PILImage 图像。我们将其转换为范围为 [-1, 1] 的标准化 Tensor。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If running on Windows and you get a BrokenPipeError, try setting the "
"num_worker of torch.utils.data.DataLoader() to 0."
msgstr ""
"如果在运行 Windows 时出现 BrokenPipeError，尝试将 torch.utils.data.DataLoader() 的 "
"num_worker 设置为 0。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let us show some of the training images, for fun."
msgstr "让我们展示一些训练图像，以增加趣味性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "2. Define a Convolutional Neural Network"
msgstr "2. 定义一个卷积神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Copy the neural network from the Neural Networks section before and modify "
"it to take 3-channel images (instead of 1-channel images as it was defined)."
msgstr "复制之前神经网络部分的神经网络，并将其修改为接受三通道图像（而不是定义为一通道图像）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "3. Define a Loss function and optimizer"
msgstr "3. 定义一个损失函数和优化器"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's use a Classification Cross-Entropy loss and SGD with momentum."
msgstr "我们使用分类交叉熵损失和带动量的 SGD。"

#: ../../beginner/vt_tutorial.rst:783
msgid "4. Train the network"
msgstr "4. 训练网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is when things start to get interesting. We simply have to loop over "
"our data iterator, and feed the inputs to the network and optimize."
msgstr "这时事情开始变得有趣。我们只需要遍历数据迭代器，将输入提供给网络并进行优化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's quickly save our trained model:"
msgstr "快速保存我们的训练模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `here <https://pytorch.org/docs/stable/notes/serialization.html>`_ for "
"more details on saving PyTorch models."
msgstr ""
"更多关于保存 PyTorch 模型的细节请参阅 `这里 "
"<https://pytorch.org/docs/stable/notes/serialization.html>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "5. Test the network on the test data"
msgstr "5. 在测试数据上测试网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have trained the network for 2 passes over the training dataset. But we "
"need to check if the network has learnt anything at all."
msgstr "我们已经对训练数据集进行了两次训练。但我们需要检查网络是否真的学到了东西。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will check this by predicting the class label that the neural network "
"outputs, and checking it against the ground-truth. If the prediction is "
"correct, we add the sample to the list of correct predictions."
msgstr "我们将通过预测神经网络输出的类标签，并将其与真实标签进行比较来检查。如果预测正确，我们将样本添加到正确预测列表中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Okay, first step. Let us display an image from the test set to get familiar."
msgstr "好的，第一步。让我们显示测试集中的一张图片以熟悉它。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, let's load back in our saved model (note: saving and re-loading the "
"model wasn't necessary here, we only did it to illustrate how to do so):"
msgstr "接下来，让我们加载我们保存的模型（注意：这里保存和重新加载模型并不是必须的，我们仅仅是为了展示如何操作）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Okay, now let us see what the neural network thinks these examples above "
"are:"
msgstr "好的，现在让我们看看神经网络对上面的示例的看法："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The outputs are energies for the 10 classes. The higher the energy for a "
"class, the more the network thinks that the image is of the particular "
"class. So, let's get the index of the highest energy:"
msgstr "输出是对 10 个类别的能量值。某类别的能量值越高，网络越认为该图像属于该类别。所以，让我们获取最高能量的索引："

#: ../../beginner/vt_tutorial.rst:783
msgid "The results seem pretty good."
msgstr "结果看起来不错。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let us look at how the network performs on the whole dataset."
msgstr "让我们看看网络在整个数据集上的表现。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That looks way better than chance, which is 10% accuracy (randomly picking a"
" class out of 10 classes). Seems like the network learnt something."
msgstr "这看起来比随机选择（10% 的准确率）要好得多。看起来网络确实学到了一些东西。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hmmm, what are the classes that performed well, and the classes that did not"
" perform well:"
msgstr "嗯，有哪些分类表现良好，而有哪些分类表现不佳："

#: ../../beginner/vt_tutorial.rst:783
msgid "Okay, so what next?"
msgstr "好的，那接下来呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid "How do we run these neural networks on the GPU?"
msgstr "我们如何在 GPU 上运行这些神经网络？"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training on GPU"
msgstr "在 GPU 上训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Just like how you transfer a Tensor onto the GPU, you transfer the neural "
"net onto the GPU."
msgstr "就像将 Tensor 转移到 GPU 一样，可以将神经网络转移到 GPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's first define our device as the first visible cuda device if we have "
"CUDA available:"
msgstr "首先定义设备为第一个可见的 CUDA 设备（如果我们有 CUDA 可用的话）："

#: ../../beginner/vt_tutorial.rst:783
msgid "The rest of this section assumes that ``device`` is a CUDA device."
msgstr "本节其余部分假设 ``device`` 是一个 CUDA 设备。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Then these methods will recursively go over all modules and convert their "
"parameters and buffers to CUDA tensors:"
msgstr "然后这些方法会递归地遍历所有模块并将其参数和缓冲区转换为 CUDA 张量："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Remember that you will have to send the inputs and targets at every step to "
"the GPU too:"
msgstr "请记住，在每一步中还需要将输入和目标发送到 GPU："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Why don't I notice MASSIVE speedup compared to CPU? Because your network is "
"really small."
msgstr "为什么我没有注意到相比 CPU 的大规模加速？因为你的网络确实很小。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Exercise:** Try increasing the width of your network (argument 2 of the "
"first ``nn.Conv2d``, and argument 1 of the second ``nn.Conv2d`` – they need "
"to be the same number), see what kind of speedup you get."
msgstr ""
"**练习：** 尝试增加网络的宽度（第一个 ``nn.Conv2d`` 的参数 2，和第二个 ``nn.Conv2d`` 的参数 "
"1——它们需要是相同的数字），看看你能获得什么样的加速。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Goals achieved**:"
msgstr "**目标达成：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Understanding PyTorch's Tensor library and neural networks at a high level."
msgstr "在高层次上理解 PyTorch 的 Tensor 库和神经网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Train a small neural network to classify images"
msgstr "训练一个小型神经网络来进行图像分类"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training on multiple GPUs"
msgstr "在多个 GPU 上训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you want to see even more MASSIVE speedup using all of your GPUs, please "
"check out :doc:`data_parallel_tutorial`."
msgstr "如果你想使用所有的 GPU 获得更大的加速，请查看 :doc:`data_parallel_tutorial`。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Where do I go next?"
msgstr "接下来去哪儿？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":doc:`Train neural nets to play video games "
"</intermediate/reinforcement_q_learning>`"
msgstr ":doc:`训练神经网络玩电子游戏 </intermediate/reinforcement_q_learning>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Train a state-of-the-art ResNet network on imagenet`_"
msgstr "`在 imagenet 上训练一个先进的 ResNet 网络`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Train a face generator using Generative Adversarial Networks`_"
msgstr "`使用生成对抗网络训练面部生成器`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Train a word-level language model using Recurrent LSTM networks`_"
msgstr "`使用循环 LSTM 网络训练单词级语言模型`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`More examples`_"
msgstr "`更多示例`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`More tutorials`_"
msgstr "`更多教程`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Discuss PyTorch on the Forums`_"
msgstr "`在 PyTorch 论坛上讨论`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Chat with other users on Slack`_"
msgstr "`与其他用户在 Slack 上聊天`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 6 minutes  19.055 seconds)"
msgstr "**脚本总运行时间：**（6 分钟 19.055 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: cifar10_tutorial.py "
"<cifar10_tutorial.py>`"
msgstr ":download:`下载 Python 源代码：cifar10_tutorial.py <cifar10_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: cifar10_tutorial.ipynb "
"<cifar10_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本：cifar10_tutorial.ipynb <cifar10_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_blitz_data_parallel_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_blitz_data_parallel_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optional: Data Parallelism"
msgstr "可选：数据并行化"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Authors**: `Sung Kim <https://github.com/hunkim>`_ and `Jenny Kang "
"<https://github.com/jennykang>`_"
msgstr ""
"**作者**：`Sung Kim <https://github.com/hunkim>`_ 和 `Jenny Kang "
"<https://github.com/jennykang>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we will learn how to use multiple GPUs using "
"``DataParallel``."
msgstr "在本教程中，我们将学习如何使用 ``DataParallel`` 来使用多个 GPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It's very easy to use GPUs with PyTorch. You can put the model on a GPU:"
msgstr "使用 PyTorch 时，使用 GPU 非常简单。你可以将模型放在 GPU 上："

#: ../../beginner/vt_tutorial.rst:783
msgid "Then, you can copy all your tensors to the GPU:"
msgstr "然后，你可以将所有的张量复制到 GPU 上："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Please note that just calling ``my_tensor.to(device)`` returns a new copy of"
" ``my_tensor`` on GPU instead of rewriting ``my_tensor``. You need to assign"
" it to a new tensor and use that tensor on the GPU."
msgstr ""
"请注意，仅调用 ``my_tensor.to(device)`` 会返回 ``my_tensor`` 在 GPU 上的新副本，而不是重写 "
"``my_tensor``。你需要将其赋值给一个新张量，并使用该张量在 GPU 上。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It's natural to execute your forward, backward propagations on multiple "
"GPUs. However, Pytorch will only use one GPU by default. You can easily run "
"your operations on multiple GPUs by making your model run parallelly using "
"``DataParallel``:"
msgstr ""
"在多个 GPU 上执行前向和后向传播是很自然的。然而，PyTorch 默认仅使用一个 GPU。你可以通过使用 ``DataParallel`` "
"让你的模型并行运行来轻松在多个 GPU 上运行操作："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That's the core behind this tutorial. We will explore it in more detail "
"below."
msgstr "这是本教程的核心。我们将在下面更详细地探讨它。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Imports and parameters"
msgstr "导入和参数"

#: ../../beginner/vt_tutorial.rst:783
msgid "Import PyTorch modules and define parameters."
msgstr "导入 PyTorch 模块并定义参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Device"
msgstr "设备"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dummy DataSet"
msgstr "伪数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid "Make a dummy (random) dataset. You just need to implement the getitem"
msgstr "创建一个伪的（随机）数据集。你只需要实现 getitem"

#: ../../beginner/vt_tutorial.rst:783
msgid "Simple Model"
msgstr "简单模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For the demo, our model just gets an input, performs a linear operation, and"
" gives an output. However, you can use ``DataParallel`` on any model (CNN, "
"RNN, Capsule Net etc.)"
msgstr ""
"为了演示，我们的模型仅获取一个输入，执行线性操作后输出结果。不过，你可以在任何模型（如 CNN、RNN、Capsule Net 等）上使用 "
"``DataParallel``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We've placed a print statement inside the model to monitor the size of input"
" and output tensors. Please pay attention to what is printed at batch rank "
"0."
msgstr "我们在模型中放置了一个 print 语句，用于监控输入和输出张量的大小。请注意在 batch 维度为 0 时打印的内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Create Model and DataParallel"
msgstr "创建模型和数据并行"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is the core part of the tutorial. First, we need to make a model "
"instance and check if we have multiple GPUs. If we have multiple GPUs, we "
"can wrap our model using ``nn.DataParallel``. Then we can put our model on "
"GPUs by ``model.to(device)``"
msgstr ""
"这是本教程的核心部分。首先，我们需要创建一个模型实例并检查是否有多个 GPU。如果有多个 GPU，我们可以使用 ``nn.DataParallel`` "
"包装我们的模型。然后我们可以通过 ``model.to(device)`` 将模型放在 GPU 上。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Run the Model"
msgstr "运行模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now we can see the sizes of input and output tensors."
msgstr "现在我们可以看到输入和输出张量的大小。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Results"
msgstr "结果"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the "
"model gets 30 and outputs 30 as expected. But if you have multiple GPUs, "
"then you can get results like this."
msgstr ""
"如果你没有 GPU 或只有一个 GPU，当我们 batch 30 个输入和 30 个输出时，模型会获取 30 并输出预期的 30。但是如果有多个 "
"GPU，你可能会得到像这样的结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid "2 GPUs"
msgstr "2 个 GPU"

#: ../../beginner/vt_tutorial.rst:783
msgid "If you have 2, you will see:"
msgstr "如果有 2 个，你会看到："

#: ../../beginner/vt_tutorial.rst:783
msgid "3 GPUs"
msgstr "3 个 GPU"

#: ../../beginner/vt_tutorial.rst:783
msgid "If you have 3 GPUs, you will see:"
msgstr "如果有 3 个 GPU，你会看到："

#: ../../beginner/vt_tutorial.rst:783
msgid "8 GPUs"
msgstr "8 个 GPU"

#: ../../beginner/vt_tutorial.rst:783
msgid "If you have 8, you will see:"
msgstr "如果有 8 个，你会看到："

#: ../../beginner/vt_tutorial.rst:783
msgid "Summary"
msgstr "总结"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"DataParallel splits your data automatically and sends job orders to multiple"
" models on several GPUs. After each model finishes their job, DataParallel "
"collects and merges the results before returning it to you."
msgstr "数据并行会自动分割你的数据，并将工作任务发送到多个 GPU 上的多个模型。每个模型完成其工作后，数据并行会收集并合并结果，然后返回给你。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information, please check out "
"https://pytorch.org/tutorials/beginner/former\\_torchies/parallelism\\_tutorial.html."
msgstr ""
"更多信息请参阅 "
"https://pytorch.org/tutorials/beginner/former\\_torchies/parallelism\\_tutorial.html。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  6.504 seconds)"
msgstr "**脚本总运行时间：**（0 分钟 6.504 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: data_parallel_tutorial.py "
"<data_parallel_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码：data_parallel_tutorial.py "
"<data_parallel_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: data_parallel_tutorial.ipynb "
"<data_parallel_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本：data_parallel_tutorial.ipynb "
"<data_parallel_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Deep Learning with PyTorch: A 60 Minute Blitz"
msgstr "使用 PyTorch 进行深度学习：60 分钟闪电教程"

#: ../../beginner/vt_tutorial.rst:783
msgid "tensor_tutorial.py"
msgstr "tensor_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What is PyTorch? "
"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html"
msgstr ""
"什么是 "
"PyTorch？https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "autograd_tutorial.py"
msgstr "autograd_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Autograd: Automatic Differentiation "
"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
msgstr ""
"自动微分 https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "neural_networks_tutorial.py"
msgstr "neural_networks_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Neural Networks "
"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#"
msgstr ""
"神经网络 "
"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#"

#: ../../beginner/vt_tutorial.rst:783
msgid "cifar10_tutorial.py"
msgstr "cifar10_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Training a Classifier "
"https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
msgstr ""
"训练分类器 https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "data_parallel_tutorial.py"
msgstr "data_parallel_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Optional: Data Parallelism "
"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"
msgstr ""
"可选：数据并行 "
"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_blitz_autograd_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_blitz_autograd_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_blitz_data_parallel_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_blitz_data_parallel_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_blitz_tensor_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_blitz_tensor_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Neural Networks"
msgstr "神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_blitz_neural_networks_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_blitz_neural_networks_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_blitz_cifar10_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_blitz_cifar10_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_blitz_neural_networks_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_blitz_neural_networks_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Neural networks can be constructed using the ``torch.nn`` package."
msgstr "可以使用 ``torch.nn`` 包构建神经网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that you had a glimpse of ``autograd``, ``nn`` depends on ``autograd`` "
"to define models and differentiate them. An ``nn.Module`` contains layers, "
"and a method ``forward(input)`` that returns the ``output``."
msgstr ""
"现在你已经了解 ``autograd`` 的一些知识，``nn`` 依赖于 ``autograd`` 来定义模型并为它们求导。一个 "
"``nn.Module`` 包含层，以及返回 ``output`` 的方法 ``forward(input)``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For example, look at this network that classifies digit images:"
msgstr "例如，看看这个用于数字图像分类的网络："

#: ../../beginner/vt_tutorial.rst:783
msgid "convnet"
msgstr "convnet"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It is a simple feed-forward network. It takes the input, feeds it through "
"several layers one after the other, and then finally gives the output."
msgstr "这是一个简单的前馈网络。它接收输入，将其逐层传递，最终生成输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid "A typical training procedure for a neural network is as follows:"
msgstr "神经网络的典型训练过程如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Define the neural network that has some learnable parameters (or weights)"
msgstr "定义具有一些可学习参数（或权重）的神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "Iterate over a dataset of inputs"
msgstr "遍历输入的数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid "Process input through the network"
msgstr "通过网络处理输入"

#: ../../beginner/vt_tutorial.rst:783
msgid "Compute the loss (how far is the output from being correct)"
msgstr "计算损失（输出与正确结果的距离）"

#: ../../beginner/vt_tutorial.rst:783
msgid "Propagate gradients back into the network’s parameters"
msgstr "将梯度反向传播到网络的参数中"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Update the weights of the network, typically using a simple update rule: "
"``weight = weight - learning_rate * gradient``"
msgstr "通常使用简单的更新规则更新网络的权重：``weight = weight - learning_rate * gradient``"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define the network"
msgstr "定义网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s define this network:"
msgstr "我们定义这个网络："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You just have to define the ``forward`` function, and the ``backward`` "
"function (where gradients are computed) is automatically defined for you "
"using ``autograd``. You can use any of the Tensor operations in the "
"``forward`` function."
msgstr ""
"你只需定义 ``forward`` 函数，而 ``backward`` 函数（计算梯度）由 ``autograd`` 自动定义。你可以在 "
"``forward`` 函数中使用任何 Tensor 操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The learnable parameters of a model are returned by ``net.parameters()``"
msgstr "模型的可学习参数通过 ``net.parameters()`` 返回"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's try a random 32x32 input. Note: expected input size of this net "
"(LeNet) is 32x32. To use this net on the MNIST dataset, please resize the "
"images from the dataset to 32x32."
msgstr ""
"试试一个随机的 32x32 输入。注意：这个网络（LeNet）的预期输入大小是 32x32。要在 MNIST "
"数据集上使用这个网络，请将数据集中的图像重设为 32x32。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Zero the gradient buffers of all parameters and backprops with random "
"gradients:"
msgstr "清零所有参数的梯度缓存，并使用随机梯度进行反向传播："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.nn`` only supports mini-batches. The entire ``torch.nn`` package "
"only supports inputs that are a mini-batch of samples, and not a single "
"sample."
msgstr "``torch.nn`` 仅支持小批量操作。整个 ``torch.nn`` 包仅支持作为小批量样本的输入，而不是单个样本。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For example, ``nn.Conv2d`` will take in a 4D Tensor of ``nSamples x "
"nChannels x Height x Width``."
msgstr ""
"例如，``nn.Conv2d`` 将接收一个形状为 ``nSamples x nChannels x Height x Width`` 的 4D 张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you have a single sample, just use ``input.unsqueeze(0)`` to add a fake "
"batch dimension."
msgstr "如果你有单个样本，只需使用 ``input.unsqueeze(0)`` 添加一个伪批量维度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before proceeding further, let's recap all the classes you’ve seen so far."
msgstr "在继续之前，让我们回顾一下你到目前为止看到的所有类。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Recap:**"
msgstr "**回顾：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.Tensor`` - A *multi-dimensional array* with support for autograd "
"operations like ``backward()``. Also *holds the gradient* w.r.t. the tensor."
msgstr ""
"``torch.Tensor`` - 一个支持自动微分操作（如 ``backward()``）的*多维数组*，同时*持有与该张量相关的梯度*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``nn.Module`` - Neural network module. *Convenient way of encapsulating "
"parameters*, with helpers for moving them to GPU, exporting, loading, etc."
msgstr "``nn.Module`` - 神经网络模块。*一种封装参数的方便方式*，带有移动到GPU、导出、加载等的辅助功能。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``nn.Parameter`` - A kind of Tensor, that is *automatically registered as a "
"parameter when assigned as an attribute to a* ``Module``."
msgstr "``nn.Parameter`` - 一种特殊的Tensor，*当分配为``Module``的属性时，会自动注册为一个参数*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``autograd.Function`` - Implements *forward and backward definitions of an "
"autograd operation*. Every ``Tensor`` operation creates at least a single "
"``Function`` node that connects to functions that created a ``Tensor`` and "
"*encodes its history*."
msgstr ""
"``autograd.Function`` - "
"实现了*autograd操作的前向和后向定义*。每个``Tensor``操作至少创建一个``Function``节点，这些节点连接到创建``Tensor``的函数并*记录它的历史*。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**At this point, we covered:**"
msgstr "**到目前为止，我们覆盖了：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Defining a neural network"
msgstr "定义神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "Processing inputs and calling backward"
msgstr "处理输入并调用反向传播"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Still Left:**"
msgstr "**仍待学习：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Computing the loss"
msgstr "计算损失"

#: ../../beginner/vt_tutorial.rst:783
msgid "Updating the weights of the network"
msgstr "更新网络的权重"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A loss function takes the (output, target) pair of inputs, and computes a "
"value that estimates how far away the output is from the target."
msgstr "损失函数接收(output, target)这一对输入，并计算一个值，估计输出距离目标有多远。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are several different `loss functions "
"<https://pytorch.org/docs/nn.html#loss-functions>`_ under the nn package . A"
" simple loss is: ``nn.MSELoss`` which computes the mean-squared error "
"between the output and the target."
msgstr ""
"在nn包中有许多不同的`损失函数 <https://pytorch.org/docs/nn.html#loss-"
"functions>`_。一个简单的损失函数是：``nn.MSELoss``，它计算输出与目标之间的均方误差。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For example:"
msgstr "例如："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, if you follow ``loss`` in the backward direction, using its "
"``.grad_fn`` attribute, you will see a graph of computations that looks like"
" this:"
msgstr "现在，如果你沿着``loss``的反向方向，使用它的``.grad_fn``属性，你会看到一个计算图，如下所示："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So, when we call ``loss.backward()``, the whole graph is differentiated "
"w.r.t. the neural net parameters, and all Tensors in the graph that have "
"``requires_grad=True`` will have their ``.grad`` Tensor accumulated with the"
" gradient."
msgstr ""
"因此，当我们调用``loss.backward()``时，整个图相对于神经网络参数进行微分，并且图中特定拥有``requires_grad=True``的所有Tensor都会累积它们的``.grad``参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For illustration, let us follow a few steps backward:"
msgstr "为了说明，我们回溯几个步骤："

#: ../../beginner/vt_tutorial.rst:783
msgid "Backprop"
msgstr "反向传播"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To backpropagate the error all we have to do is to ``loss.backward()``. You "
"need to clear the existing gradients though, else gradients will be "
"accumulated to existing gradients."
msgstr "为了反向传播误差，我们只需调用``loss.backward()``。但你需要清除现有的梯度，否则梯度将会累积到现有的梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we shall call ``loss.backward()``, and have a look at conv1's bias "
"gradients before and after the backward."
msgstr "现在我们将调用``loss.backward()``，看一下conv1的偏置梯度在反向传播之前和之后的变化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now, we have seen how to use loss functions."
msgstr "现在，我们已经了解了如何使用损失函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Read Later:**"
msgstr "**稍后阅读：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The neural network package contains various modules and loss functions that "
"form the building blocks of deep neural networks. A full list with "
"documentation is `here <https://pytorch.org/docs/nn>`_."
msgstr ""
"神经网络包包含了各种模块和损失函数，这些模块构成了深度神经网络的基本构建块。完整的文档和列表在`这里 "
"<https://pytorch.org/docs/nn>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**The only thing left to learn is:**"
msgstr "**我们剩下要学习的唯一内容是：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Update the weights"
msgstr "更新权重"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The simplest update rule used in practice is the Stochastic Gradient Descent"
" (SGD):"
msgstr "实践中最简单的更新规则是随机梯度下降（SGD）："

#: ../../beginner/vt_tutorial.rst:783
msgid "We can implement this using simple Python code:"
msgstr "我们可以用简单的Python代码来实现："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"However, as you use neural networks, you want to use various different "
"update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, "
"we built a small package: ``torch.optim`` that implements all these methods."
" Using it is very simple:"
msgstr ""
"然而，当你使用神经网络时，你可能希望使用各种更新规则，例如SGD，Nesterov-"
"SGD，Adam，RMSProp等。为了实现这一点，我们创建了一个小型包``torch.optim``，它实现了所有这些方法。使用它非常简单："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Observe how gradient buffers had to be manually set to zero using "
"``optimizer.zero_grad()``. This is because gradients are accumulated as "
"explained in the `Backprop`_ section."
msgstr ""
"注意如何必须手动使用``optimizer.zero_grad()``清除梯度缓冲区。这是因为梯度会累积，如`反向传播`_部分所解释的那样。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.387 seconds)"
msgstr "**脚本的总运行时间：** ( 0 分钟  0.387 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: neural_networks_tutorial.py "
"<neural_networks_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: neural_networks_tutorial.py "
"<neural_networks_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: neural_networks_tutorial.ipynb "
"<neural_networks_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: neural_networks_tutorial.ipynb "
"<neural_networks_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**06:27.372** total execution time for **beginner_blitz** files:"
msgstr "**06:27.372** 针对**beginner_blitz**文件的执行总时间："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_blitz_cifar10_tutorial.py` (``cifar10_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_blitz_cifar10_tutorial.py` (``cifar10_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "06:19.055"
msgstr "06:19.055"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_blitz_data_parallel_tutorial.py` "
"(``data_parallel_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_blitz_data_parallel_tutorial.py` "
"(``data_parallel_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:06.504"
msgstr "00:06.504"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_blitz_tensor_tutorial.py` (``tensor_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_blitz_tensor_tutorial.py` (``tensor_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.738"
msgstr "00:00.738"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_blitz_autograd_tutorial.py` "
"(``autograd_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_blitz_autograd_tutorial.py` "
"(``autograd_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.688"
msgstr "00:00.688"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_blitz_neural_networks_tutorial.py` "
"(``neural_networks_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_blitz_neural_networks_tutorial.py` "
"(``neural_networks_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.387"
msgstr "00:00.387"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_blitz_tensor_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里<sphx_glr_download_beginner_blitz_tensor_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs"
" or other specialized hardware to accelerate computing. If you’re familiar "
"with ndarrays, you’ll be right at home with the Tensor API. If not, follow "
"along in this quick API walkthrough."
msgstr ""
"Tensor类似于NumPy的ndarrays，不同之处是Tensor可以在GPU或其他专用硬件上运行以加速计算。如果你熟悉ndarrays，那么Tensor"
" API会让你感觉得心应手。如果没有，请随着这个快速API演练一起学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensor Initialization"
msgstr "张量初始化"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensor Attributes"
msgstr "张量属性"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensor Operations"
msgstr "张量操作"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Over 100 tensor operations, including transposing, indexing, slicing, "
"mathematical operations, linear algebra, random sampling, and more are "
"comprehensively described `here "
"<https://pytorch.org/docs/stable/torch.html>`__."
msgstr ""
"超过100种张量操作，包括转置、索引、切片、数学运算、线性代数、随机采样等，详细描述请见 `这里 "
"<https://pytorch.org/docs/stable/torch.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each of them can be run on the GPU (at typically higher speeds than on a "
"CPU). If you’re using Colab, allocate a GPU by going to Edit > Notebook "
"Settings."
msgstr "它们中的每一个都可以在GPU上运行（通常比在CPU上速度更快）。如果你使用的是Colab，可以通过编辑 > 笔记本设置分配一个GPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of "
"tensors along a given dimension. See also `torch.stack "
"<https://pytorch.org/docs/stable/generated/torch.stack.html>`__, another "
"tensor joining op that is subtly different from ``torch.cat``."
msgstr ""
"**连接张量** 你可以使用``torch.cat``沿着给定维度连接一系列张量。另见`torch.stack "
"<https://pytorch.org/docs/stable/generated/torch.stack.html>`__，这是与``torch.cat``稍有不同的另一个张量连接操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Multiplying tensors**"
msgstr "**张量相乘**"

#: ../../beginner/vt_tutorial.rst:783
msgid "This computes the matrix multiplication between two tensors"
msgstr "这会计算两个张量之间的矩阵乘法"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**In-place operations** Operations that have a ``_`` suffix are in-place. "
"For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``."
msgstr "**就地操作** 后缀为``_``的操作是就地进行的。例如：``x.copy_(y)``、``x.t_()``会改变``x``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.738 seconds)"
msgstr "**脚本的总运行时间：** ( 0 分钟  0.738 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: tensor_tutorial.py "
"<tensor_tutorial.py>`"
msgstr ":download:`下载Python源代码: tensor_tutorial.py <tensor_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: tensor_tutorial.ipynb "
"<tensor_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: tensor_tutorial.ipynb "
"<tensor_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_chatbot_tutorial.py>` to "
"download the full example code"
msgstr "点击 :ref:`这里<sphx_glr_download_beginner_chatbot_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Chatbot Tutorial"
msgstr "聊天机器人教程"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author:** `Matthew Inkawhich <https://github.com/MatthewInkawhich>`_"
msgstr "**作者：** `Matthew Inkawhich <https://github.com/MatthewInkawhich>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we explore a fun and interesting use-case of recurrent "
"sequence-to-sequence models. We will train a simple chatbot using movie "
"scripts from the `Cornell Movie-Dialogs Corpus "
"<https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__."
msgstr ""
"在本教程中，我们探索了一个有趣的回归序列到序列模型的应用场景。我们将使用来自`Cornell Movie-Dialogs Corpus "
"<https://www.cs.cornell.edu/~cristian/Cornell_Movie-"
"Dialogs_Corpus.html>`__的电影脚本训练一个简单的聊天机器人。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Conversational models are a hot topic in artificial intelligence research. "
"Chatbots can be found in a variety of settings, including customer service "
"applications and online helpdesks. These bots are often powered by "
"retrieval-based models, which output predefined responses to questions of "
"certain forms. In a highly restricted domain like a company’s IT helpdesk, "
"these models may be sufficient, however, they are not robust enough for more"
" general use-cases. Teaching a machine to carry out a meaningful "
"conversation with a human in multiple domains is a research question that is"
" far from solved. Recently, the deep learning boom has allowed for powerful "
"generative models like Google’s `Neural Conversational Model "
"<https://arxiv.org/abs/1506.05869>`__, which marks a large step towards "
"multi-domain generative conversational models. In this tutorial, we will "
"implement this kind of model in PyTorch."
msgstr ""
"对话模型是人工智能研究中的一个热点话题。聊天机器人可以在许多环境下找到应用，包括客户服务和在线帮助台。这些机器人通常由基于检索的模型驱动，这些模型根据某些形式的问题输出预定义的响应。在像公司IT帮助台这样高度受限的领域，这些模型可能足够，但它们对于更普通的应用场景来说还不够强大。教机器在多个领域与人类进行有意义的对话仍然是一项尚未解决的研究问题。近年来，深度学习的兴起使得像谷歌的`神经对话模型<https://arxiv.org/abs/1506.05869>`__等强大的生成模型成为可能，这标志着朝多领域生成对话模型迈出了重要的一步。在本教程中，我们将在PyTorch中实现这种模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "bot"
msgstr "机器人"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Tutorial Highlights**"
msgstr "**教程亮点**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Handle loading and preprocessing of `Cornell Movie-Dialogs Corpus "
"<https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__ "
"dataset"
msgstr ""
"处理和预处理`Cornell Movie-Dialogs Corpus "
"<https://www.cs.cornell.edu/~cristian/Cornell_Movie-"
"Dialogs_Corpus.html>`__数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Implement a sequence-to-sequence model with `Luong attention mechanism(s) "
"<https://arxiv.org/abs/1508.04025>`__"
msgstr "实现一个带有`Luong注意力机制<https://arxiv.org/abs/1508.04025>`__的序列到序列模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Jointly train encoder and decoder models using mini-batches"
msgstr "使用小批量共同训练编码器和解码器模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Implement greedy-search decoding module"
msgstr "实现贪婪搜索解码模块"

#: ../../beginner/vt_tutorial.rst:783
msgid "Interact with trained chatbot"
msgstr "与训练好的聊天机器人互动"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Acknowledgments**"
msgstr "**致谢**"

#: ../../beginner/vt_tutorial.rst:783
msgid "This tutorial borrows code from the following sources:"
msgstr "本教程借鉴了以下资源的代码："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Yuan-Kuei Wu’s pytorch-chatbot implementation: "
"https://github.com/ywk991112/pytorch-chatbot"
msgstr ""
"Yuan-Kuei Wu的pytorch-chatbot实现：https://github.com/ywk991112/pytorch-chatbot"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sean Robertson’s practical-pytorch seq2seq-translation example: "
"https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation"
msgstr ""
"Sean Robertson的practical-pytorch序列到序列翻译示例：https://github.com/spro/practical-"
"pytorch/tree/master/seq2seq-translation"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"FloydHub Cornell Movie Corpus preprocessing code: "
"https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus"
msgstr ""
"FloydHub Cornell Movie Corpus预处理代码：https://github.com/floydhub/textutil-"
"preprocess-cornell-movie-corpus"

#: ../../beginner/vt_tutorial.rst:783
msgid "Preparations"
msgstr "准备工作"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To get started, `download "
"<https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-"
"corpus.zip>`__ the Movie-Dialogs Corpus zip file."
msgstr ""
"开始之前，`下载<https://zissou.infosci.cornell.edu/convokit/datasets/movie-"
"corpus/movie-corpus.zip>`__电影对话语料库的zip文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Load & Preprocess Data"
msgstr "加载和预处理数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The next step is to reformat our data file and load the data into structures"
" that we can work with."
msgstr "下一步是重新格式化数据文件并将数据加载到可以使用的结构中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `Cornell Movie-Dialogs Corpus "
"<https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html>`__ "
"is a rich dataset of movie character dialog:"
msgstr ""
"`Cornell Movie-Dialogs Corpus "
"<https://www.cs.cornell.edu/~cristian/Cornell_Movie-"
"Dialogs_Corpus.html>`__是一份丰富的电影角色对话数据集："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"220,579 conversational exchanges between 10,292 pairs of movie characters"
msgstr "220,579条对话交换，涉及10,292对电影角色"

#: ../../beginner/vt_tutorial.rst:783
msgid "9,035 characters from 617 movies"
msgstr "来自617部电影的9,035名角色"

#: ../../beginner/vt_tutorial.rst:783
msgid "304,713 total utterances"
msgstr "总计304,713个语句"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This dataset is large and diverse, and there is a great variation of "
"language formality, time periods, sentiment, etc. Our hope is that this "
"diversity makes our model robust to many forms of inputs and queries."
msgstr "该数据集规模庞大且多样化，语言的形式、时间背景、情感等非常多样。我们希望这种多样性能使我们的模型应对多种形式的输入和查询。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, we’ll take a look at some lines of our datafile to see the original "
"format."
msgstr "首先，我们将查看数据文件中的一些行以了解原始格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Create formatted data file"
msgstr "创建格式化数据文件"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For convenience, we'll create a nicely formatted data file in which each "
"line contains a tab-separated *query sentence* and a *response sentence* "
"pair."
msgstr "为了方便，我们创建一个格式良好的数据文件，每行包含一个通过制表符分隔的*查询句子*和*响应句子*对。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The following functions facilitate the parsing of the raw "
"``utterances.jsonl`` data file."
msgstr "以下函数协助解析原始的``utterances.jsonl``数据文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``loadLinesAndConversations`` splits each line of the file into a dictionary"
" of lines with fields: ``lineID``, ``characterID``, and text and then groups"
" them into conversations with fields: ``conversationID``, ``movieID``, and "
"lines."
msgstr ""
"``loadLinesAndConversations``将文件的每行分割成具有``lineID``、``characterID``和文本字段的字典，然后将其分组成带有``conversationID``、``movieID``和行字段的对话。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``extractSentencePairs`` extracts pairs of sentences from conversations"
msgstr "``extractSentencePairs``从对话中提取句子对"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we’ll call these functions and create the file. We’ll call it "
"``formatted_movie_lines.txt``."
msgstr "现在我们将调用这些函数并创建文件。我们将其命名为``formatted_movie_lines.txt``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Load and trim data"
msgstr "加载并修剪数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our next order of business is to create a vocabulary and load query/response"
" sentence pairs into memory."
msgstr "我们的下一步工作是创建词汇表并将查询/响应句子对加载到内存中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that we are dealing with sequences of **words**, which do not have an "
"implicit mapping to a discrete numerical space. Thus, we must create one by "
"mapping each unique word that we encounter in our dataset to an index value."
msgstr ""
"注意，我们正在处理**单词**序列，这些单词没有隐含的映射到离散的数值空间。因此，我们必须创建一个映射，将我们在数据集中遇到的每个唯一单词映射到一个索引值。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this we define a ``Voc`` class, which keeps a mapping from words to "
"indexes, a reverse mapping of indexes to words, a count of each word and a "
"total word count. The class provides methods for adding a word to the "
"vocabulary (``addWord``), adding all words in a sentence (``addSentence``) "
"and trimming infrequently seen words (``trim``). More on trimming later."
msgstr ""
"为此，我们定义了一个``Voc``类，它维护从单词到索引的映射，索引到单词的反向映射，每个单词的计数和总单词计数。该类提供了添加单词到词汇表（``addWord``）、添加句子中的所有单词（``addSentence``）以及修剪不常见单词（``trim``）的方法。修剪的更多内容稍后介绍。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we can assemble our vocabulary and query/response sentence pairs. Before"
" we are ready to use this data, we must perform some preprocessing."
msgstr "现在我们可以组装我们的词汇表和查询/响应句子对。在我们准备使用这些数据之前，我们必须进行一些预处理。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, we must convert the Unicode strings to ASCII using "
"``unicodeToAscii``. Next, we should convert all letters to lowercase and "
"trim all non-letter characters except for basic punctuation "
"(``normalizeString``). Finally, to aid in training convergence, we will "
"filter out sentences with length greater than the ``MAX_LENGTH`` threshold "
"(``filterPairs``)."
msgstr ""
"首先，我们必须使用``unicodeToAscii``将Unicode字符串转换为ASCII。接着，我们应该将所有字母转换为小写，并修剪所有非字母字符，保留基本标点符号（``normalizeString``）。最后，为了加速训练收敛，我们将过滤掉长度大于``MAX_LENGTH``阈值的句子（``filterPairs``）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Another tactic that is beneficial to achieving faster convergence during "
"training is trimming rarely used words out of our vocabulary. Decreasing the"
" feature space will also soften the difficulty of the function that the "
"model must learn to approximate. We will do this as a two-step process:"
msgstr ""
"另一种有助于在训练期间更快收敛的策略是从词汇表中修剪很少使用的单词。减少特征空间也会减轻模型必须学习逼近的函数的复杂性。我们将通过两步实现这一点："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Trim words used under ``MIN_COUNT`` threshold using the ``voc.trim`` "
"function."
msgstr "使用 ``voc.trim`` 函数修剪使用频率低于 ``MIN_COUNT`` 阈值的单词。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Filter out pairs with trimmed words."
msgstr "过滤掉包含被修剪单词的对。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Prepare Data for Models"
msgstr "为模型准备数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Although we have put a great deal of effort into preparing and massaging our"
" data into a nice vocabulary object and list of sentence pairs, our models "
"will ultimately expect numerical torch tensors as inputs. One way to prepare"
" the processed data for the models can be found in the `seq2seq translation "
"tutorial "
"<https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__."
" In that tutorial, we use a batch size of 1, meaning that all we have to do "
"is convert the words in our sentence pairs to their corresponding indexes "
"from the vocabulary and feed this to the models."
msgstr ""
"尽管我们在准备数据并将其调整为良好的词汇对象和句子对列表上下了很多功夫，但从最终来看，我们的模型需要数值型 torch 张量作为输入。在 "
"`seq2seq 翻译教程 "
"<https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__"
" 中，可以找到一种为这些模型准备处理后数据的方法。在该教程中，我们使用批量大小为 "
"1，这意味着我们只需要将句子对中的单词转换为它们在词汇表中的对应索引，然后将其提供给模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"However, if you’re interested in speeding up training and/or would like to "
"leverage GPU parallelization capabilities, you will need to train with mini-"
"batches."
msgstr "然而，如果您希望加速训练和/或利用 GPU 的并行能力，就需要使用小批量进行训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Using mini-batches also means that we must be mindful of the variation of "
"sentence length in our batches. To accommodate sentences of different sizes "
"in the same batch, we will make our batched input tensor of shape "
"*(max_length, batch_size)*, where sentences shorter than the *max_length* "
"are zero padded after an *EOS_token*."
msgstr ""
"使用小批量还意味着我们必须注意批次中句子长度的变化。为了适应具有不同长度的句子在同一批次，我们将制作一个形状为 *(max_length, "
"batch_size)* 的分批输入张量，其中比 *max_length* 短的句子会在 *EOS_token* 后填充为零。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If we simply convert our English sentences to tensors by converting words to"
" their indexes(\\ ``indexesFromSentence``) and zero-pad, our tensor would "
"have shape *(batch_size, max_length)* and indexing the first dimension would"
" return a full sequence across all time-steps. However, we need to be able "
"to index our batch along time, and across all sequences in the batch. "
"Therefore, we transpose our input batch shape to *(max_length, batch_size)*,"
" so that indexing across the first dimension returns a time step across all "
"sentences in the batch. We handle this transpose implicitly in the "
"``zeroPadding`` function."
msgstr ""
"如果我们通过转换单词为索引（``indexesFromSentence``）并填充零来将英文句子简单地转换为张量，我们的张量将具有 "
"*(batch_size, max_length)* "
"的形状，并按第一维度索引会返回跨所有时间步的一整条序列。然而，我们需要能够沿着时间步并跨批次中的所有序列对批次进行索引。因此，我们将输入批量形状转置为 "
"*(max_length, batch_size)*，以便沿第一维度索引时可以返回批次中所有句子在某个时间步的值。我们在 ``zeroPadding``"
" 函数中隐式处理了这种转置。"

#: ../../beginner/vt_tutorial.rst:783
msgid "batches"
msgstr "批次"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``inputVar`` function handles the process of converting sentences to "
"tensor, ultimately creating a correctly shaped zero-padded tensor. It also "
"returns a tensor of ``lengths`` for each of the sequences in the batch which"
" will be passed to our decoder later."
msgstr ""
"``inputVar`` 函数处理将句子转化为张量的过程，最终会生成一个形状正确的零填充张量。该函数还会返回每个序列的 ``lengths`` "
"张量，这些张量稍后会被传递给我们的解码器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``outputVar`` function performs a similar function to ``inputVar``, but "
"instead of returning a ``lengths`` tensor, it returns a binary mask tensor "
"and a maximum target sentence length. The binary mask tensor has the same "
"shape as the output target tensor, but every element that is a *PAD_token* "
"is 0 and all others are 1."
msgstr ""
"``outputVar`` 函数执行与 ``inputVar`` 类似的功能，但它返回的不是 ``lengths`` "
"张量，而是一个二值掩码张量和最大目标句子长度。二值掩码张量的形状与目标输出张量相同，但每个元素中是 *PAD_token* 的地方值为 0，其他地方值为"
" 1。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``batch2TrainData`` simply takes a bunch of pairs and returns the input and "
"target tensors using the aforementioned functions."
msgstr "``batch2TrainData`` 简单地接收一批对，并使用前述的函数返回输入和目标张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define Models"
msgstr "定义模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Seq2Seq Model"
msgstr "Seq2Seq 模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The "
"goal of a seq2seq model is to take a variable-length sequence as an input, "
"and return a variable-length sequence as an output using a fixed-sized "
"model."
msgstr ""
"我们聊天机器人的大脑是一个序列到序列 (seq2seq) 模型。seq2seq "
"模型的目标是以固定大小的模型为基础，接收一个可变长度的输入序列，并返回一个可变长度的输出序列。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Sutskever et al. <https://arxiv.org/abs/1409.3215>`__ discovered that by "
"using two separate recurrent neural nets together, we can accomplish this "
"task. One RNN acts as an **encoder**, which encodes a variable length input "
"sequence to a fixed-length context vector. In theory, this context vector "
"(the final hidden layer of the RNN) will contain semantic information about "
"the query sentence that is input to the bot. The second RNN is a "
"**decoder**, which takes an input word and the context vector, and returns a"
" guess for the next word in the sequence and a hidden state to use in the "
"next iteration."
msgstr ""
"`Sutskever 等人的发现 <https://arxiv.org/abs/1409.3215>`__ "
"表明，通过使用两个独立的循环神经网络（RNN），可以完成这项任务。一个 RNN 充当 "
"**编码器**，将可变长度的输入序列编码为一个固定长度的上下文向量。理论上，这个上下文向量（RNN "
"的最终隐藏层）将包含关于输入给机器人的查询句子的语义信息。第二个 RNN 是 "
"**解码器**，它接收一个输入单词和上下文向量，并返回序列中下一个单词的预测以及下一次迭代中使用的隐藏状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid "model"
msgstr "模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Image source: https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/"
msgstr "图片来源：https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/"

#: ../../beginner/vt_tutorial.rst:783
msgid "Encoder"
msgstr "编码器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The encoder RNN iterates through the input sentence one token (e.g. word) at"
" a time, at each time step outputting an “output” vector and a “hidden "
"state” vector. The hidden state vector is then passed to the next time step,"
" while the output vector is recorded. The encoder transforms the context it "
"saw at each point in the sequence into a set of points in a high-dimensional"
" space, which the decoder will use to generate a meaningful output for the "
"given task."
msgstr ""
"编码器 RNN "
"通过输入句子逐个标记（例如：单词）进行迭代，在每个时间步输出一个“输出”向量和一个“隐藏状态”向量。隐藏状态向量随后传递到下一时间步，而输出向量被记录下来。编码器将它在序列中各点看到的上下文转换为一个高维空间中的一组点，解码器将使用这些点来为给定任务生成有意义的输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"At the heart of our encoder is a multi-layered Gated Recurrent Unit, "
"invented by `Cho et al. <https://arxiv.org/pdf/1406.1078v3.pdf>`__ in 2014. "
"We will use a bidirectional variant of the GRU, meaning that there are "
"essentially two independent RNNs: one that is fed the input sequence in "
"normal sequential order, and one that is fed the input sequence in reverse "
"order. The outputs of each network are summed at each time step. Using a "
"bidirectional GRU will give us the advantage of encoding both past and "
"future contexts."
msgstr ""
"我们的编码器核心是一个多层门控循环单元 (GRU)，该单元由 `Cho 等人在 2014 年发明 "
"<https://arxiv.org/pdf/1406.1078v3.pdf>`__。我们将使用 GRU 的双向变体，这意味着实际上有两个独立的 "
"RNN：一个以正常顺序接收输入序列，另一个以反向顺序接收输入序列。两个网络的输出在每个时间步相加。使用双向 GRU 将使我们能够编码过去和未来的上下文。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Bidirectional RNN:"
msgstr "双向 RNN："

#: ../../beginner/vt_tutorial.rst:783
msgid "rnn_bidir"
msgstr "rnn_bidir"

#: ../../beginner/vt_tutorial.rst:783
msgid "Image source: https://colah.github.io/posts/2015-09-NN-Types-FP/"
msgstr "图片来源：https://colah.github.io/posts/2015-09-NN-Types-FP/"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that an ``embedding`` layer is used to encode our word indices in an "
"arbitrarily sized feature space. For our models, this layer will map each "
"word to a feature space of size *hidden_size*. When trained, these values "
"should encode semantic similarity between similar meaning words."
msgstr ""
"需要注意的是，“嵌入”层用于将单词索引编码到任意大小的特征空间中。对于我们的模型，此层将每个单词映射到大小为 *hidden_size* "
"的特征空间中。在训练后，这些值应编码相似意义单词之间的语义相似性。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, if passing a padded batch of sequences to an RNN module, we must "
"pack and unpack padding around the RNN pass using "
"``nn.utils.rnn.pack_padded_sequence`` and "
"``nn.utils.rnn.pad_packed_sequence`` respectively."
msgstr ""
"最后，如果将填充过的批次序列传递到 RNN 模块中，则必须使用 ``nn.utils.rnn.pack_padded_sequence`` 和 "
"``nn.utils.rnn.pad_packed_sequence`` 分别对 RNN 过程中的填充值进行打包和解包。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Computation Graph:**"
msgstr "**计算图：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Convert word indexes to embeddings."
msgstr "将单词索引转换为嵌入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Pack padded batch of sequences for RNN module."
msgstr "为 RNN 模块打包填充过的批次序列。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Forward pass through GRU."
msgstr "通过 GRU 的前向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Unpack padding."
msgstr "解包填充值。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Sum bidirectional GRU outputs."
msgstr "对双向 GRU 的输出求和。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Return output and final hidden state."
msgstr "返回输出和最终的隐藏状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Inputs:**"
msgstr "**输入：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``input_seq``: batch of input sentences; shape=\\ *(max_length, batch_size)*"
msgstr "``input_seq``：输入句子的批次；形状=\\ *(max_length, batch_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``input_lengths``: list of sentence lengths corresponding to each sentence "
"in the batch; shape=\\ *(batch_size)*"
msgstr "``input_lengths``：对应批次中每个句子的句子长度列表；形状=\\ *(batch_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``hidden``: hidden state; shape=\\ *(n_layers x num_directions, batch_size, "
"hidden_size)*"
msgstr ""
"``hidden``：隐藏状态；形状=\\ *(n_layers x num_directions, batch_size, hidden_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Outputs:**"
msgstr "**输出：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``outputs``: output features from the last hidden layer of the GRU (sum of "
"bidirectional outputs); shape=\\ *(max_length, batch_size, hidden_size)*"
msgstr ""
"``outputs``：GRU 最后一层隐藏层的输出特征（双向输出的总和）；形状=\\ *(max_length, batch_size, "
"hidden_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``hidden``: updated hidden state from GRU; shape=\\ *(n_layers x "
"num_directions, batch_size, hidden_size)*"
msgstr ""
"``hidden``：GRU 更新后的隐藏状态；形状=\\ *(n_layers x num_directions, batch_size, "
"hidden_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid "Decoder"
msgstr "解码器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The decoder RNN generates the response sentence in a token-by-token fashion."
" It uses the encoder’s context vectors, and internal hidden states to "
"generate the next word in the sequence. It continues generating words until "
"it outputs an *EOS_token*, representing the end of the sentence. A common "
"problem with a vanilla seq2seq decoder is that if we rely solely on the "
"context vector to encode the entire input sequence’s meaning, it is likely "
"that we will have information loss. This is especially the case when dealing"
" with long input sequences, greatly limiting the capability of our decoder."
msgstr ""
"解码器 RNN 以逐标记的方式生成响应句子。它使用编码器的上下文向量和内部隐藏状态生成序列中的下一个单词，直到其输出一个 "
"*EOS_token*，表示句子的结束。一个普通 seq2seq "
"解码器的常见问题是，如果我们仅依赖上下文向量来编码整个输入序列的语义，很可能会导致信息丢失。尤其是在处理长输入序列时，这将极大地限制解码器的能力。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To combat this, `Bahdanau et al. <https://arxiv.org/abs/1409.0473>`__ "
"created an “attention mechanism” that allows the decoder to pay attention to"
" certain parts of the input sequence, rather than using the entire fixed "
"context at every step."
msgstr ""
"为了解决这一问题，`Bahdanau 等人 <https://arxiv.org/abs/1409.0473>`__ "
"创建了一种“注意力机制”，使解码器能够集中关注输入序列的某些部分，而不是在每一步都使用整个固定上下文。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"At a high level, attention is calculated using the decoder’s current hidden "
"state and the encoder’s outputs. The output attention weights have the same "
"shape as the input sequence, allowing us to multiply them by the encoder "
"outputs, giving us a weighted sum which indicates the parts of encoder "
"output to pay attention to. `Sean Robertson’s <https://github.com/spro>`__ "
"figure describes this very well:"
msgstr ""
"从高层次来看，注意力是利用解码器的当前隐藏状态和编码器的输出进行计算的。输出的注意力权重与输入序列具有相同的形状，允许我们将它们与编码器输出相乘，得到加权求和，这表明了要集中注意的编码器输出部分。`Sean"
" Robertson’s <https://github.com/spro>`__ 的图很好地说明了这一点："

#: ../../beginner/vt_tutorial.rst:783
msgid "attn2"
msgstr "attn2"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Luong et al. <https://arxiv.org/abs/1508.04025>`__ improved upon Bahdanau "
"et al.’s groundwork by creating “Global attention”. The key difference is "
"that with “Global attention”, we consider all of the encoder’s hidden "
"states, as opposed to Bahdanau et al.’s “Local attention”, which only "
"considers the encoder’s hidden state from the current time step. Another "
"difference is that with “Global attention”, we calculate attention weights, "
"or energies, using the hidden state of the decoder from the current time "
"step only. Bahdanau et al.’s attention calculation requires knowledge of the"
" decoder’s state from the previous time step. Also, Luong et al. provides "
"various methods to calculate the attention energies between the encoder "
"output and decoder output which are called “score functions”:"
msgstr ""
"`Luong 等人 <https://arxiv.org/abs/1508.04025>`__ 在 Bahdanau "
"等人的基础上进行了改进，创建了“全局注意力”。主要区别在于，“全局注意力”考虑编码器的所有隐藏状态，而 Bahdanau "
"等人的“局部注意力”仅考虑编码器在当前时间步的隐藏状态。另一个区别是，在“全局注意力”中，我们仅使用来自当前时间步的解码器隐藏状态计算注意力权重或能量值。而"
" Bahdanau 等人的注意力计算需要了解解码器从上一个时间步的状态。此外，Luong "
"等人提供了各种方法来计算编码器输出和解码器输出之间的注意力能量值，这些称为“评分函数”："

#: ../../beginner/vt_tutorial.rst:783
msgid "scores"
msgstr "scores"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"where :math:`h_t` = current target decoder state and :math:`\\bar{h}_s` = "
"all encoder states."
msgstr "其中 :math:`h_t` = 当前目标解码器状态，:math:`\\bar{h}_s` = 所有编码器状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Overall, the Global attention mechanism can be summarized by the following "
"figure. Note that we will implement the “Attention Layer” as a separate "
"``nn.Module`` called ``Attn``. The output of this module is a softmax "
"normalized weights tensor of shape *(batch_size, 1, max_length)*."
msgstr ""
"总体来说，全局注意力机制可以用以下图总结。需要注意的是，我们将实现“注意力层”作为一个独立的 ``nn.Module``，称为 "
"``Attn``。此模块的输出是一个形状为 *(batch_size, 1, max_length)* 的 softmax 归一化权重张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "global_attn"
msgstr "global_attn"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have defined our attention submodule, we can implement the "
"actual decoder model. For the decoder, we will manually feed our batch one "
"time step at a time. This means that our embedded word tensor and GRU output"
" will both have shape *(1, batch_size, hidden_size)*."
msgstr ""
"现在我们已经定义了注意力子模块，可以实现实际的解码器模型了。对于解码器，我们将手动一次处理一个时间步的批次。这意味着我们的嵌入单词张量和 GRU "
"输出都将具有形状 *(1, batch_size, hidden_size)*。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Get embedding of current input word."
msgstr "获取当前输入单词的嵌入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Forward through unidirectional GRU."
msgstr "通过单向 GRU 的前向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Calculate attention weights from the current GRU output from (2)."
msgstr "计算来自 (2) 的当前 GRU 输出的注意力权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Multiply attention weights to encoder outputs to get new \"weighted sum\" "
"context vector."
msgstr "将注意力权重与编码器输出相乘，以获取新的“加权求和”上下文向量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Concatenate weighted context vector and GRU output using Luong eq. 5."
msgstr "使用 Luong 方程 5 将加权上下文向量和 GRU 输出连接起来。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Predict next word using Luong eq. 6 (without softmax)."
msgstr "使用 Luong 方程 6（没有 softmax）预测下一个单词。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``input_step``: one time step (one word) of input sequence batch; shape=\\ "
"*(1, batch_size)*"
msgstr "``input_step``：输入序列批次中的一个时间步（一个单词）；形状=\\ *(1, batch_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``last_hidden``: final hidden layer of GRU; shape=\\ *(n_layers x "
"num_directions, batch_size, hidden_size)*"
msgstr ""
"``last_hidden``：GRU 最后一层隐藏层；形状=\\ *(n_layers x num_directions, batch_size, "
"hidden_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``encoder_outputs``: encoder model’s output; shape=\\ *(max_length, "
"batch_size, hidden_size)*"
msgstr ""
"``encoder_outputs``：编码器模型的输出；形状=\\ *(max_length, batch_size, hidden_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``output``: softmax normalized tensor giving probabilities of each word "
"being the correct next word in the decoded sequence; shape=\\ *(batch_size, "
"voc.num_words)*"
msgstr ""
"``output``：softmax 归一化张量，给出每个单词作为解码序列中正确下一个单词的概率；形状=\\ *(batch_size, "
"voc.num_words)*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``hidden``: final hidden state of GRU; shape=\\ *(n_layers x num_directions,"
" batch_size, hidden_size)*"
msgstr ""
"``hidden``：GRU 最终的隐藏状态；形状=\\ *(n_layers x num_directions, batch_size, "
"hidden_size)*"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define Training Procedure"
msgstr "定义训练过程"

#: ../../beginner/vt_tutorial.rst:783
msgid "Masked loss"
msgstr "掩码损失"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Since we are dealing with batches of padded sequences, we cannot simply "
"consider all elements of the tensor when calculating loss. We define "
"``maskNLLLoss`` to calculate our loss based on our decoder’s output tensor, "
"the target tensor, and a binary mask tensor describing the padding of the "
"target tensor. This loss function calculates the average negative log "
"likelihood of the elements that correspond to a *1* in the mask tensor."
msgstr ""
"由于我们处理的是带填充的序列批次，因此在计算损失时不能简单地考虑张量的所有元素。我们定义了 "
"``maskNLLLoss``，以根据解码器的输出张量、目标张量和一个描述目标张量填充的二值掩码张量来计算损失。此损失函数计算掩码张量中值为 *1* "
"的元素的平均负对数似然。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Single training iteration"
msgstr "单次训练迭代"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``train`` function contains the algorithm for a single training "
"iteration (a single batch of inputs)."
msgstr "``train``函数包含单个训练迭代（单个输入批次）的算法。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We will use a couple of clever tricks to aid in convergence:"
msgstr "我们将使用一些巧妙的技巧来帮助收敛："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The first trick is using **teacher forcing**. This means that at some "
"probability, set by ``teacher_forcing_ratio``, we use the current target "
"word as the decoder’s next input rather than using the decoder’s current "
"guess. This technique acts as training wheels for the decoder, aiding in "
"more efficient training. However, teacher forcing can lead to model "
"instability during inference, as the decoder may not have a sufficient "
"chance to truly craft its own output sequences during training. Thus, we "
"must be mindful of how we are setting the ``teacher_forcing_ratio``, and not"
" be fooled by fast convergence."
msgstr ""
"第一个技巧是使用**教师强制**。这意味着，在由``teacher_forcing_ratio``设置的某种概率下，我们使用当前的目标词作为解码器的下一个输入，而不是使用解码器当前的预测。这种技术相当于给解码器戴上了辅助轮，有助于更高效地训练。然而，教师强制可能在推理时导致模型的不稳定性，因为训练期间解码器可能没有足够的机会生成自己的输出序列。因此，我们必须谨慎设置``teacher_forcing_ratio``，并且不要被快速收敛所迷惑。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The second trick that we implement is **gradient clipping**. This is a "
"commonly used technique for countering the “exploding gradient” problem. In "
"essence, by clipping or thresholding gradients to a maximum value, we "
"prevent the gradients from growing exponentially and either overflow (NaN), "
"or overshoot steep cliffs in the cost function."
msgstr ""
"我们实现的第二个技巧是**梯度裁剪**。这是一种常用的技术，用于应对“梯度爆炸”问题。本质上，通过将梯度裁剪或限制在一个最大值内，我们可以防止梯度指数增长，避免溢出（NaN）或在代价函数的陡峭区域过冲。"

#: ../../beginner/vt_tutorial.rst:783
msgid "grad_clip"
msgstr "grad_clip"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Image source: Goodfellow et al. *Deep Learning*. 2016. "
"https://www.deeplearningbook.org/"
msgstr ""
"图片来源: Goodfellow et al. *Deep Learning*. 2016. "
"https://www.deeplearningbook.org/"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Sequence of Operations:**"
msgstr "**操作顺序：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Forward pass entire input batch through encoder."
msgstr "通过编码器前向传播整个输入批次。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Initialize decoder inputs as SOS_token, and hidden state as the encoder's "
"final hidden state."
msgstr "将解码器的输入初始化为SOS_token，隐状态初始化为编码器的最终隐状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Forward input batch sequence through decoder one time step at a time."
msgstr "将输入批次序列一次一步地传递给解码器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If teacher forcing: set next decoder input as the current target; else: set "
"next decoder input as current decoder output."
msgstr "如果使用教师强制：将当前目标设置为解码器的下一个输入；否则：将解码器的当前输出设置为下一个输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Calculate and accumulate loss."
msgstr "计算并累积损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Perform backpropagation."
msgstr "执行反向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Clip gradients."
msgstr "裁剪梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Update encoder and decoder model parameters."
msgstr "更新编码器和解码器模型参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch’s RNN modules (``RNN``, ``LSTM``, ``GRU``) can be used like any "
"other non-recurrent layers by simply passing them the entire input sequence "
"(or batch of sequences). We use the ``GRU`` layer like this in the "
"``encoder``. The reality is that under the hood, there is an iterative "
"process looping over each time step calculating hidden states. "
"Alternatively, you can run these modules one time-step at a time. In this "
"case, we manually loop over the sequences during the training process like "
"we must do for the ``decoder`` model. As long as you maintain the correct "
"conceptual model of these modules, implementing sequential models can be "
"very straightforward."
msgstr ""
"PyTorch的RNN模块（``RNN``，``LSTM``，``GRU``）可以像其他非递归层一样，简单地通过它们传递整个输入序列（或序列的批次）。我们在``encoder``中以这种方式使用了``GRU``层。实际上，在底层，这是一种迭代过程，循环遍历每个时间步计算隐状态。或者，你可以一次一步地运行这些模块。在这种情况下，我们在训练过程中手动循环序列，就像对``decoder``模型所做的一样。只要你正确理解这些模块的概念模型，实现序列模型就会非常简单。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training iterations"
msgstr "训练迭代"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It is finally time to tie the full training procedure together with the "
"data. The ``trainIters`` function is responsible for running "
"``n_iterations`` of training given the passed models, optimizers, data, etc."
" This function is quite self explanatory, as we have done the heavy lifting "
"with the ``train`` function."
msgstr ""
"现在终于可以将完整的训练过程与数据结合起来了。``trainIters``函数负责在传递的模型、优化器、数据等基础上运行``n_iterations``次训练。该函数非常直观，因为我们已经用``train``函数完成了主要工作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One thing to note is that when we save our model, we save a tarball "
"containing the encoder and decoder ``state_dicts`` (parameters), the "
"optimizers’ ``state_dicts``, the loss, the iteration, etc. Saving the model "
"in this way will give us the ultimate flexibility with the checkpoint. After"
" loading a checkpoint, we will be able to use the model parameters to run "
"inference, or we can continue training right where we left off."
msgstr ""
"需要注意的一点是，当我们保存我们的模型时，我们保存了一个包含编码器和解码器``state_dicts``（参数）的压缩包，优化器的``state_dicts``，损失，迭代等。以这种方式保存模型将为我们提供最大的灵活性来使用检查点。加载检查点后，我们可以使用模型参数运行推理，或者从我们中断的地方继续训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define Evaluation"
msgstr "定义评估"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"After training a model, we want to be able to talk to the bot ourselves. "
"First, we must define how we want the model to decode the encoded input."
msgstr "在训练完模型后，我们希望能够亲自与机器人交谈。首先，我们必须定义如何将模型解码为编码输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Greedy decoding"
msgstr "贪心解码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Greedy decoding is the decoding method that we use during training when we "
"are **NOT** using teacher forcing. In other words, for each time step, we "
"simply choose the word from ``decoder_output`` with the highest softmax "
"value. This decoding method is optimal on a single time-step level."
msgstr ""
"贪心解码是训练时**不**使用教师强制时的解码方法。换句话说，对于每个时间步，我们简单地从``decoder_output``中选择softmax值最高的词语。该解码方法在单个时间步层面是最优的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To facilitate the greedy decoding operation, we define a "
"``GreedySearchDecoder`` class. When run, an object of this class takes an "
"input sequence (``input_seq``) of shape *(input_seq length, 1)*, a scalar "
"input length (``input_length``) tensor, and a ``max_length`` to bound the "
"response sentence length. The input sentence is evaluated using the "
"following computational graph:"
msgstr ""
"为了方便贪心解码操作，我们定义了一个``GreedySearchDecoder``类。当运行时，此类的对象接收一个形状为*(input_seq "
"length, "
"1)*的输入序列（``input_seq``），一个标量输入长度（``input_length``）张量，以及一个``max_length``来限制响应句子的长度。输入句子通过以下计算图进行评估："

#: ../../beginner/vt_tutorial.rst:783
msgid "Forward input through encoder model."
msgstr "通过编码器模型前向传播输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Prepare encoder's final hidden layer to be first hidden input to the "
"decoder."
msgstr "准备编码器最终的隐层作为解码器的第一个隐状态输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Initialize decoder's first input as SOS_token."
msgstr "将解码器的第一个输入初始化为SOS_token。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Initialize tensors to append decoded words to."
msgstr "初始化张量以附加解码后的词语。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Iteratively decode one word token at a time:"
msgstr "逐字解码，一个时间步一个时间步地进行："

#: ../../beginner/vt_tutorial.rst:783
msgid "Forward pass through decoder."
msgstr "通过解码器前向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Obtain most likely word token and its softmax score."
msgstr "获取最可能的词语标记及其softmax分数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Record token and score."
msgstr "记录标记和分数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Prepare current token to be next decoder input."
msgstr "准备当前标记作为解码器的下一个输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Return collections of word tokens and scores."
msgstr "返回词语标记和分数的集合。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Evaluate my text"
msgstr "评估我的文本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have our decoding method defined, we can write functions for "
"evaluating a string input sentence. The ``evaluate`` function manages the "
"low-level process of handling the input sentence. We first format the "
"sentence as an input batch of word indexes with *batch_size==1*. We do this "
"by converting the words of the sentence to their corresponding indexes, and "
"transposing the dimensions to prepare the tensor for our models. We also "
"create a ``lengths`` tensor which contains the length of our input sentence."
" In this case, ``lengths`` is scalar because we are only evaluating one "
"sentence at a time (batch_size==1). Next, we obtain the decoded response "
"sentence tensor using our ``GreedySearchDecoder`` object (``searcher``). "
"Finally, we convert the response’s indexes to words and return the list of "
"decoded words."
msgstr ""
"现在既然我们定义了解码方法，我们可以编写函数来评估字符串输入句子。``evaluate``函数管理处理输入句子的底层过程。我们首先将句子格式化为由*batch_size==1*组成的单词索引输入批次。对此，我们将句子的单词转换为对应的索引值，并转置张量的维度以适应我们的模型。我们还创建了一个``lengths``张量，包含我们输入句子的长度。在这种情况下，由于我们一次仅评估一个句子（``batch_size==1``），``lengths``是标量。接下来，我们使用我们的``GreedySearchDecoder``对象（``searcher``）获取解码后的响应句子张量。最后，我们将响应的索引值转换成单词并返回解码后单词的列表。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``evaluateInput`` acts as the user interface for our chatbot. When called, "
"an input text field will spawn in which we can enter our query sentence. "
"After typing our input sentence and pressing *Enter*, our text is normalized"
" in the same way as our training data, and is ultimately fed to the "
"``evaluate`` function to obtain a decoded output sentence. We loop this "
"process, so we can keep chatting with our bot until we enter either “q” or "
"“quit”."
msgstr ""
"``evaluateInput``充当我们聊天机器人的用户界面。当被调用时，将弹出一个输入文本字段，我们可以在其中输入查询句子。在输入句子并按下*Enter*后，我们的文本会以与训练数据相同的方式进行规范化，并最终被传递给``evaluate``函数以获得解码后的输出句子。我们循环此过程，因此可以继续与机器人聊天，直到输入“q”或“quit”。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, if a sentence is entered that contains a word that is not in the "
"vocabulary, we handle this gracefully by printing an error message and "
"prompting the user to enter another sentence."
msgstr "最后，如果输入的句子中包含词汇表中没有的单词，我们会通过打印一条错误信息并提示用户输入另一个句子来优雅地处理。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Run Model"
msgstr "运行模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Finally, it is time to run our model!"
msgstr "最后，到了运行模型的时候了！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Regardless of whether we want to train or test the chatbot model, we must "
"initialize the individual encoder and decoder models. In the following "
"block, we set our desired configurations, choose to start from scratch or "
"set a checkpoint to load from, and build and initialize the models. Feel "
"free to play with different model configurations to optimize performance."
msgstr ""
"无论我们是想训练还是测试聊天机器人模型，都必须初始化单独的编码器和解码器模型。在以下代码块中，我们设置了所需的配置，选择从头开始或设置一个检查点进行加载，并构建和初始化模型。可以自由尝试不同的模型配置以优化性能。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Sample code to load from a checkpoint:"
msgstr "从检查点加载的示例代码："

#: ../../beginner/vt_tutorial.rst:783
msgid "Run Training"
msgstr "运行训练"

#: ../../beginner/vt_tutorial.rst:783
msgid "Run the following block if you want to train the model."
msgstr "如果想要训练模型，请运行以下代码块。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First we set training parameters, then we initialize our optimizers, and "
"finally we call the ``trainIters`` function to run our training iterations."
msgstr "首先，我们设置训练参数，然后初始化优化器，最后调用``trainIters``函数运行训练迭代。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Run Evaluation"
msgstr "运行评估"

#: ../../beginner/vt_tutorial.rst:783
msgid "To chat with your model, run the following block."
msgstr "要与模型聊天，请运行以下代码块。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Conclusion"
msgstr "总结"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That’s all for this one, folks. Congratulations, you now know the "
"fundamentals to building a generative chatbot model! If you’re interested, "
"you can try tailoring the chatbot’s behavior by tweaking the model and "
"training parameters and customizing the data that you train the model on."
msgstr ""
"这就是本教程的全部内容了。恭喜你，现在你已经了解了构建生成式聊天机器人模型的基本知识！如果感兴趣，可以通过调整模型和训练参数以及自定义用于训练模型的数据来尝试改变聊天机器人的行为。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Check out the other tutorials for more cool deep learning applications in "
"PyTorch!"
msgstr "查看其他教程，了解更多关于PyTorch的酷炫深度学习应用！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: chatbot_tutorial.py "
"<chatbot_tutorial.py>`"
msgstr ":download:`下载Python源代码: chatbot_tutorial.py <chatbot_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: chatbot_tutorial.ipynb "
"<chatbot_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: chatbot_tutorial.ipynb <chatbot_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Running Tutorials in Google Colab"
msgstr "在Google Colab中运行教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When you run a tutorial in Google Colab, there might be additional "
"requirements and dependencies that you need to meet in order for the "
"tutorial to work properly. This section contains notes on how to configure "
"various settings in order to successfully run PyTorch tutorials in Google "
"Colab."
msgstr ""
"当您在Google "
"Colab中运行教程时，可能需要满足一些额外的要求和依赖项以确保教程能够正常运行。本节包含有关如何配置各种设置以成功运行PyTorch教程的说明。"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch Version in Google Colab"
msgstr "Google Colab中的PyTorch版本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Wen you are running a tutorial that requires a version of PyTorch that has "
"just been released, that version might not be yet available in Google Colab."
" To check that you have the required ``torch`` and compatible domain "
"libraries installed, run ``!pip list``."
msgstr ""
"当您运行需要最新发布版本的PyTorch的教程时，该版本可能尚未在Google "
"Colab中可用。要检查您是否安装了所需的``torch``及兼容的领域库，请运行``!pip list``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If the installed version of PyTorch is lower than required, uninstall it and"
" reinstall again by running the following commands:"
msgstr "如果安装的PyTorch版本低于要求，请执行以下命令卸载并重新安装："

#: ../../beginner/vt_tutorial.rst:783
msgid "Using Tutorial Data from Google Drive in Colab"
msgstr "在Colab中使用Google驱动器中的教程数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We've added a new feature to tutorials that allows users to open the "
"notebook associated with a tutorial in Google Colab. You may need to copy "
"data to your Google drive account to get the more complex tutorials to work."
msgstr ""
"我们为教程添加了一项新功能，允许用户在Google "
"Colab中打开与教程相关的笔记本。您可能需要将数据复制到您的Google驱动器账号中，以使更复杂的教程工作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example, we'll demonstrate how to change the notebook in Colab to "
"work with the Chatbot Tutorial. To do this, you'll first need to be logged "
"into Google Drive. (For a full description of how to access data in Colab, "
"you can view their example notebook `here "
"<https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd>`__.)"
msgstr ""
"在此示例中，我们将演示如何更改Colab中的笔记本以适应聊天机器人教程。为此，您首先需要登录到Google驱动器。（有关如何在Colab中访问数据的完整说明，可以查看他们的示例笔记本`here"
" "
"<https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd>`__。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To get started open the `Chatbot Tutorial "
"<https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__ in your "
"browser."
msgstr ""
"首先在浏览器中打开`Chatbot Tutorial "
"<https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "At the top of the page click **Run in Google Colab**."
msgstr "在页面顶部点击**Run in Google Colab**。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The file will open in Colab."
msgstr "文件将在Colab中打开。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you select **Runtime**, and then **Run All**, you'll get an error as the "
"file can't be found."
msgstr "如果选择**Runtime**，然后选择**Run All**，您会看到文件无法找到的错误。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To fix this, we'll copy the required file into our Google Drive account."
msgstr "为了解决此问题，我们将需要将所需文件复制到我们的Google驱动器账号中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Log into Google Drive."
msgstr "登录Google驱动器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In Google Drive, make a folder named ``data``, with a subfolder named "
"``cornell``."
msgstr "在Google驱动器中创建一个名为``data``的文件夹，并包含一个名为``cornell``的子文件夹。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Visit the Cornell Movie Dialogs Corpus and download the movie-corpus ZIP "
"file."
msgstr "访问Cornell Movie Dialogs Corpus并下载电影语料库ZIP文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Unzip the file on your local machine."
msgstr "在本地计算机上解压文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Copy the file ``utterances.jsonl`` to the ``data/cornell`` folder that you "
"created in Google Drive."
msgstr "将文件``utterances.jsonl``复制到您在Google驱动器中创建的``data/cornell``文件夹中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we'll need to edit the file in\\_ \\_Colab to point to the file on "
"Google Drive."
msgstr "接下来，我们需要编辑Colab中的文件以指向Google驱动器上的文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In Colab, add the following to top of the code section over the line that "
"begins ``corpus\\_name``:"
msgstr "在Colab中，将以下内容添加到代码部分的顶部，位于``corpus\\_name``所在的行上方："

#: ../../beginner/vt_tutorial.rst:783
msgid "Change the two lines that follow:"
msgstr "更改以下两行："

#: ../../beginner/vt_tutorial.rst:783
msgid "Change the ``corpus\\_name`` value to ``\"cornell\"``."
msgstr "将``corpus\\_name``值更改为``\"cornell\"``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Change the line that begins with ``corpus`` to this:"
msgstr "更改以``corpus``开头的行如下所示："

#: ../../beginner/vt_tutorial.rst:783
msgid "We're now pointing to the file we uploaded to Drive."
msgstr "我们现在指向上传到驱动器的文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now when you click the **Run cell** button for the code section, you'll be "
"prompted to authorize Google Drive and you'll get an authorization code. "
"Paste the code into the prompt in Colab and you should be set."
msgstr ""
"现在当您单击该代码部分的**运行单元格**按钮时，系统会提示您授权Google驱动器，并会为您提供授权代码。将代码粘贴到Colab中的提示框中，设置即可完成。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Rerun the notebook from the **Runtime** / **Run All** menu command and "
"you'll see it process. (Note that this tutorial takes a long time to run.)"
msgstr "从**Runtime** / **Run All**菜单命令再次运行笔记本，您会看到它开始处理。（请注意，这个教程运行时间较长。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hopefully this example will give you a good starting point for running some "
"of the more complex tutorials in Colab. As we evolve our use of Colab on the"
" PyTorch tutorials site, we'll look at ways to make this easier for users."
msgstr ""
"希望这个示例能够为您在 Colab 中运行一些更复杂的教程提供一个良好的起点。随着我们在 PyTorch 教程网站上对 Colab "
"的使用不断发展，我们会考虑为用户提供更简单的使用方式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Enabling CUDA"
msgstr "启用 CUDA"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Some tutorials require a CUDA-enabled device (NVIDIA GPU), which involves "
"changing the Runtime type prior to executing the tutorial. To change the "
"Runtime in Google Colab, on the top drop-down menu select **Runtime**, then "
"select **Change runtime type**. Under **Hardware accelerator**, select ``T4 "
"GPU``, then click ``Save``."
msgstr ""
"一些教程需要使用支持 CUDA 的设备（NVIDIA GPU），这需要在运行教程之前更改运行时类型。在 Google Colab "
"中，可以在顶部下拉菜单中选择 **Runtime**，然后选择 **Change runtime type**。在 **Hardware "
"accelerator** 下选择 ``T4 GPU``，然后点击 ``Save``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_data_loading_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_data_loading_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Writing Custom Datasets, DataLoaders and Transforms"
msgstr "编写自定义数据集、数据加载器和转换"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_"
msgstr "**作者**: `Sasank Chilamkurthy <https://chsasank.github.io>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A lot of effort in solving any machine learning problem goes into preparing "
"the data. PyTorch provides many tools to make data loading easy and "
"hopefully, to make your code more readable. In this tutorial, we will see "
"how to load and preprocess/augment data from a non trivial dataset."
msgstr ""
"解决任何机器学习问题的许多工作都需要数据准备。PyTorch "
"提供了许多工具来简化数据加载并希望使代码更具可读性。在本教程中，我们将学习如何从一个复杂的数据集中加载和预处理/增强数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To run this tutorial, please make sure the following packages are installed:"
msgstr "要运行此教程，请确保安装了以下软件包:"

#: ../../beginner/vt_tutorial.rst:783
msgid "``scikit-image``: For image io and transforms"
msgstr "``scikit-image``: 用于图像输入输出和转换"

#: ../../beginner/vt_tutorial.rst:783
msgid "``pandas``: For easier csv parsing"
msgstr "``pandas``: 用于更方便的 CSV 解析"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The dataset we are going to deal with is that of facial pose. This means "
"that a face is annotated like this:"
msgstr "我们将处理的数据集是面部姿态数据。这意味着一个面部会被如下标注:"

#: ../../beginner/vt_tutorial.rst:783
msgid "Over all, 68 different landmark points are annotated for each face."
msgstr "总共有 68 个不同的关键点为每个面部标注。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Download the dataset from `here "
"<https://download.pytorch.org/tutorial/faces.zip>`_ so that the images are "
"in a directory named 'data/faces/'. This dataset was actually generated by "
"applying excellent `dlib's pose estimation "
"<https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`__ on a "
"few images from imagenet tagged as 'face'."
msgstr ""
"从 `这里 <https://download.pytorch.org/tutorial/faces.zip>`_ 下载数据集，以便图像存储在名为 "
"&apos;data/faces/&apos; 的目录中。实际上，这个数据集是通过对一些来自 imagenet 上标记为 "
"&apos;face&apos; 的图片使用优秀的 `dlib&apos;s 姿态估计 "
"<https://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`__ 生成的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Dataset comes with a ``.csv`` file with annotations which looks like this:"
msgstr "数据集附带一个 ``.csv`` 文件，其中的标注看起来像这样:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's take a single image name and its annotations from the CSV, in this "
"case row index number 65 for person-7.jpg just as an example. Read it, store"
" the image name in ``img_name`` and store its annotations in an (L, 2) array"
" ``landmarks`` where L is the number of landmarks in that row."
msgstr ""
"让我们从 CSV 中取一个图像名称及其标注，例如第 65 行的 person-7.jpg。读取它，将图像名称存储在 ``img_name`` "
"中，并将其标注存储在一个 (L, 2) 的数组 ``landmarks`` 中，其中 L 是该行中的关键点数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's write a simple helper function to show an image and its landmarks and "
"use it to show a sample."
msgstr "让我们编写一个简单的辅助函数来显示图像及其标注，并使用它来显示一个样本。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dataset class"
msgstr "数据集类"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.utils.data.Dataset`` is an abstract class representing a dataset. "
"Your custom dataset should inherit ``Dataset`` and override the following "
"methods:"
msgstr ""
"``torch.utils.data.Dataset`` 是表示数据集的抽象类。自定义数据集应继承 ``Dataset`` 并重写以下方法:"

#: ../../beginner/vt_tutorial.rst:783
msgid "``__len__`` so that ``len(dataset)`` returns the size of the dataset."
msgstr "``__len__`` 方法使得 ``len(dataset)`` 返回数据集的大小。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``__getitem__`` to support the indexing such that ``dataset[i]`` can be used"
" to get :math:`i`\\ th sample."
msgstr "``__getitem__`` 方法支持索引操作，使得可以通过 ``dataset[i]`` 获取第 :math:`i` 个样本。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's create a dataset class for our face landmarks dataset. We will read "
"the csv in ``__init__`` but leave the reading of images to ``__getitem__``. "
"This is memory efficient because all the images are not stored in the memory"
" at once but read as required."
msgstr ""
"让我们为我们的面部关键点数据集创建一个数据集类。在 ``__init__`` 中读取 CSV 文件，但将读取图像的操作留给 "
"``__getitem__``。这种方式更加节省内存，因为所有图像不会一次全部存储在内存中，而是按需读取。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sample of our dataset will be a dict ``{'image': image, 'landmarks': "
"landmarks}``. Our dataset will take an optional argument ``transform`` so "
"that any required processing can be applied on the sample. We will see the "
"usefulness of ``transform`` in the next section."
msgstr ""
"数据集样本将是一个字典 ``{&apos;image&apos;: image, &apos;landmarks&apos;: "
"landmarks}``。我们的数据集将接受一个可选参数 ``transform``，以便对样本应用任何需要的处理。我们将在下一节看到 "
"``transform`` 的有用之处。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's instantiate this class and iterate through the data samples. We will "
"print the sizes of first 4 samples and show their landmarks."
msgstr "让我们初始化此类并迭代数据样本。我们将打印前 4 个样本的大小并显示它们的关键点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One issue we can see from the above is that the samples are not of the same "
"size. Most neural networks expect the images of a fixed size. Therefore, we "
"will need to write some preprocessing code. Let's create three transforms:"
msgstr "从上面可以看出一个问题是，样本的大小不一致。大多数神经网络都要求图像具有固定大小。因此，我们需要编写一些预处理代码。让我们创建三个转换:"

#: ../../beginner/vt_tutorial.rst:783
msgid "``Rescale``: to scale the image"
msgstr "``Rescale``: 用于缩放图像"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``RandomCrop``: to crop from image randomly. This is data augmentation."
msgstr "``RandomCrop``: 用于随机裁剪图像，这是数据增强的一种方式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ToTensor``: to convert the numpy images to torch images (we need to swap "
"axes)."
msgstr "``ToTensor``: 用于将 numpy 图像转换为 torch 图像（需要交换轴）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will write them as callable classes instead of simple functions so that "
"parameters of the transform need not be passed every time it's called. For "
"this, we just need to implement ``__call__`` method and if required, "
"``__init__`` method. We can then use a transform like this:"
msgstr ""
"我们将它们编写为可调用类而不是简单函数，这样可以避免每次调用时都必须传递参数。为此，我们只需要实现 ``__call__`` 方法，如果需要，还可以实现"
" ``__init__`` 方法。然后我们可以这样使用一个转换:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Observe below how these transforms had to be applied both on the image and "
"landmarks."
msgstr "如下观察这些转换是如何同时应用于图像和关键点的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the example above, `RandomCrop` uses an external library's random number "
"generator (in this case, Numpy's `np.random.int`). This can result in "
"unexpected behavior with `DataLoader` (see `here "
"<https://pytorch.org/docs/stable/notes/faq.html#my-data-loader-workers-"
"return-identical-random-numbers>`_). In practice, it is safer to stick to "
"PyTorch's random number generator, e.g. by using `torch.randint` instead."
msgstr ""
"在上面的例子中，`RandomCrop` 使用了一个外部库的随机数生成器（在此案例中为 Numpy 的 `np.random.int`）。这可能会导致与"
" `DataLoader` 一起使用时出现意外行为（参见 `这里 "
"<https://pytorch.org/docs/stable/notes/faq.html#my-data-loader-workers-"
"return-identical-random-numbers>`_）。实际上，更安全的做法是坚持使用 PyTorch 的随机数生成器，例如使用 "
"`torch.randint`。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Compose transforms"
msgstr "组合转换"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now, we apply the transforms on a sample."
msgstr "现在，我们在样本上应用转换。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's say we want to rescale the shorter side of the image to 256 and then "
"randomly crop a square of size 224 from it. i.e, we want to compose "
"``Rescale`` and ``RandomCrop`` transforms. "
"``torchvision.transforms.Compose`` is a simple callable class which allows "
"us to do this."
msgstr ""
"假设我们希望将图像的较短边缩放到 256，然后从中随机裁剪一个大小为 224 的正方形。也就是说，我们希望将 ``Rescale`` 和 "
"``RandomCrop`` 转换组合起来。``torchvision.transforms.Compose`` 是一个简单的可调用类，可以实现这一点。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Iterating through the dataset"
msgstr "迭代数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's put this all together to create a dataset with composed transforms. To"
" summarize, every time this dataset is sampled:"
msgstr "让我们把这些整合起来创建一个带组合转换的数据集。总结如下，每次对这个数据集进行采样:"

#: ../../beginner/vt_tutorial.rst:783
msgid "An image is read from the file on the fly"
msgstr "从文件中即时读取一个图像"

#: ../../beginner/vt_tutorial.rst:783
msgid "Transforms are applied on the read image"
msgstr "对读取的图像应用转换"

#: ../../beginner/vt_tutorial.rst:783
msgid "Since one of the transforms is random, data is augmented on sampling"
msgstr "由于其中一个转换是随机的，因此在采样时自动进行了数据增强"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can iterate over the created dataset with a ``for i in range`` loop as "
"before."
msgstr "我们可以像之前一样使用一个 ``for i in range`` 循环迭代创建的数据集。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"However, we are losing a lot of features by using a simple ``for`` loop to "
"iterate over the data. In particular, we are missing out on:"
msgstr "但是，通过简单的 ``for`` 循环迭代数据，我们失去了许多功能，特别是:"

#: ../../beginner/vt_tutorial.rst:783
msgid "Batching the data"
msgstr "对数据进行批处理"

#: ../../beginner/vt_tutorial.rst:783
msgid "Shuffling the data"
msgstr "对数据进行随机洗牌"

#: ../../beginner/vt_tutorial.rst:783
msgid "Load the data in parallel using ``multiprocessing`` workers."
msgstr "使用 ``multiprocessing`` 工作线程并行加载数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.utils.data.DataLoader`` is an iterator which provides all these "
"features. Parameters used below should be clear. One parameter of interest "
"is ``collate_fn``. You can specify how exactly the samples need to be "
"batched using ``collate_fn``. However, default collate should work fine for "
"most use cases."
msgstr ""
"``torch.utils.data.DataLoader`` 是一个迭代器，它提供了所有这些功能。下面使用的参数应该很清楚。其中一个值得关注的参数是 "
"``collate_fn``。您可以通过 ``collate_fn`` 指定样本如何被精确地批处理。然而，默认的批处理方式应该适用于大多数使用场景。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Afterword: torchvision"
msgstr "后记: torchvision"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we have seen how to write and use datasets, transforms and"
" dataloader. ``torchvision`` package provides some common datasets and "
"transforms. You might not even have to write custom classes. One of the more"
" generic datasets available in torchvision is ``ImageFolder``. It assumes "
"that images are organized in the following way:"
msgstr ""
"在本教程中，我们学习了如何编写和使用数据集、转换和数据加载器。``torchvision`` "
"包提供了一些常见的数据集和转换。您甚至可能不需要编写自定义类。``ImageFolder`` 是 torchvision "
"中一个更通用的数据集。它假定图像被组织成以下形式:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"where 'ants', 'bees' etc. are class labels. Similarly generic transforms "
"which operate on ``PIL.Image`` like  ``RandomHorizontalFlip``, ``Scale``, "
"are also available. You can use these to write a dataloader like this:"
msgstr ""
"其中&apos;ants&apos;, &apos;bees&apos; 等是类别标签。同样，可以对 ``PIL.Image`` 操作的通用转换例如 "
"``RandomHorizontalFlip``、``Scale`` 也是可用的。您可以用这些来编写像这样的数据加载器:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For an example with training code, please see "
":doc:`transfer_learning_tutorial`."
msgstr "有关包含训练代码的示例，请参见 :doc:`transfer_learning_tutorial`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: data_loading_tutorial.py "
"<data_loading_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: data_loading_tutorial.py "
"<data_loading_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: data_loading_tutorial.ipynb "
"<data_loading_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: data_loading_tutorial.ipynb "
"<data_loading_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_dcgan_faces_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_dcgan_faces_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "DCGAN Tutorial"
msgstr "DCGAN 教程"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author**: `Nathan Inkawhich <https://github.com/inkawhich>`__"
msgstr "**作者**: `Nathan Inkawhich <https://github.com/inkawhich>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction"
msgstr "介绍"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial will give an introduction to DCGANs through an example. We "
"will train a generative adversarial network (GAN) to generate new "
"celebrities after showing it pictures of many real celebrities. Most of the "
"code here is from the DCGAN implementation in `pytorch/examples "
"<https://github.com/pytorch/examples>`__, and this document will give a "
"thorough explanation of the implementation and shed light on how and why "
"this model works. But don’t worry, no prior knowledge of GANs is required, "
"but it may require a first-timer to spend some time reasoning about what is "
"actually happening under the hood. Also, for the sake of time it will help "
"to have a GPU, or two. Lets start from the beginning."
msgstr ""
"本教程将通过一个示例介绍 DCGANs。我们将训练一个生成对抗网络（GAN），在向其展示许多真实明星的图片后，让其生成新的明星图片。此处的大部分代码来自"
" `pytorch/examples <https://github.com/pytorch/examples>`__ 中的 DCGAN "
"实现，本文档将详细说明实现过程并阐明该模型如何以及为何能有效工作。不过请放心，不需要提前了解 "
"GANs，但首次接触的人可能需要一些时间来思考实际发生的事情。此外，为节省时间，最好拥有一块或两块 GPU。让我们从头开始。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Generative Adversarial Networks"
msgstr "生成对抗网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is a GAN?"
msgstr "什么是 GAN?"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"GANs are a framework for teaching a deep learning model to capture the "
"training data distribution so we can generate new data from that same "
"distribution. GANs were invented by Ian Goodfellow in 2014 and first "
"described in the paper `Generative Adversarial Nets "
"<https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`__. They"
" are made of two distinct models, a *generator* and a *discriminator*. The "
"job of the generator is to spawn ‘fake’ images that look like the training "
"images. The job of the discriminator is to look at an image and output "
"whether or not it is a real training image or a fake image from the "
"generator. During training, the generator is constantly trying to outsmart "
"the discriminator by generating better and better fakes, while the "
"discriminator is working to become a better detective and correctly classify"
" the real and fake images. The equilibrium of this game is when the "
"generator is generating perfect fakes that look as if they came directly "
"from the training data, and the discriminator is left to always guess at 50%"
" confidence that the generator output is real or fake."
msgstr ""
"GAN 是一种框架，可以教深度学习模型捕获训练数据分布，从而从该分布中生成新数据。GAN 由 Ian Goodfellow 在 2014 "
"年发明，并在论文 `Generative Adversarial Nets "
"<https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`__ "
"中首次描述。它们由两个不同的模型组成，一个是 *生成器*，另一个是 "
"*判别器*。生成器的任务是生成“假”的图像，使其看起来像训练图像。判别器的任务是检验图像并输出该图像是否是真实的训练图像还是来自生成器的假图像。在训练过程中，生成器不断尝试通过生成越来越好的“假”图像来击败判别器，而判别器则努力提高自己的判别能力，正确分类真实图像和假图像。这个游戏的均衡点是生成器生成的假图像看起来好像直接来自于训练数据，而判别器总是以"
" 50% 的信心猜测生成器的输出是真还是假。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, lets define some notation to be used throughout tutorial starting with "
"the discriminator. Let :math:`x` be data representing an image. :math:`D(x)`"
" is the discriminator network which outputs the (scalar) probability that "
":math:`x` came from training data rather than the generator. Here, since we "
"are dealing with images, the input to :math:`D(x)` is an image of CHW size "
"3x64x64. Intuitively, :math:`D(x)` should be HIGH when :math:`x` comes from "
"training data and LOW when :math:`x` comes from the generator. :math:`D(x)` "
"can also be thought of as a traditional binary classifier."
msgstr ""
"现在，让我们定义教程中使用的一些符号，从判别器开始。设 :math:`x` 为一个表示图像的数据。:math:`D(x)` "
"是判别器网络，其输出该图像来自训练数据而不是生成器的概率（标量）。这里，由于我们处理的是图像，:math:`D(x)` 的输入是一个 CHW 尺寸为 "
"3x64x64 的图像。从直观上看，当 :math:`x` 来自训练数据时 :math:`D(x)` 应该高，当 :math:`x` 来自生成器时 "
":math:`D(x)` 应该低。:math:`D(x)` 也可以被看作一个传统的二分类器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For the generator’s notation, let :math:`z` be a latent space vector sampled"
" from a standard normal distribution. :math:`G(z)` represents the generator "
"function which maps the latent vector :math:`z` to data-space. The goal of "
":math:`G` is to estimate the distribution that the training data comes from "
"(:math:`p_{data}`) so it can generate fake samples from that estimated "
"distribution (:math:`p_g`)."
msgstr ""
"对于生成器的符号，设 :math:`z` 为从标准正态分布采样的潜在空间向量。:math:`G(z)` 表示将潜在向量 :math:`z` "
"映射到数据空间的生成器函数。:math:`G` 的目标是估计训练数据的分布 (:math:`p_{data}`)，从而可以从这个估计的分布 "
"(:math:`p_g`) 中生成假样本。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So, :math:`D(G(z))` is the probability (scalar) that the output of the "
"generator :math:`G` is a real image. As described in `Goodfellow’s paper "
"<https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`__, "
":math:`D` and :math:`G` play a minimax game in which :math:`D` tries to "
"maximize the probability it correctly classifies reals and fakes "
"(:math:`logD(x)`), and :math:`G` tries to minimize the probability that "
":math:`D` will predict its outputs are fake (:math:`log(1-D(G(z)))`). From "
"the paper, the GAN loss function is"
msgstr ""
"因此，:math:`D(G(z))` 是判别器 :math:`G` 的输出为真实图像的概率（标量）。如 `Goodfellow 的论文 "
"<https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`__ "
"所述，:math:`D` 和 :math:`G` 玩的是一个极小极大博弈，其中 :math:`D` 尝试最大化其正确分类真实样本和假样本的概率 "
"(:math:`logD(x)`)，而 :math:`G` 尝试最小化 :math:`D` 预测其输出为假的概率 "
"(:math:`log(1-D(G(z)))`)。根据论文中的描述，GAN 的损失函数为"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = "
"\\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim "
"p_{z}(z)}\\big[log(1-D(G(z)))\\big]"
msgstr ""
"\\underset{G}{\\text{min}} \\underset{D}{\\text{max}}V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In theory, the solution to this minimax game is where :math:`p_g = "
"p_{data}`, and the discriminator guesses randomly if the inputs are real or "
"fake. However, the convergence theory of GANs is still being actively "
"researched and in reality models do not always train to this point."
msgstr ""
"理论上，这个极小极大游戏的解是：:math:`p_g = "
"p_{data}`，并且判别器随机猜测输入是真实的还是虚假的。然而，GAN的收敛理论仍在积极研究中，实际上模型不一定总能训练到这个点。"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is a DCGAN?"
msgstr "什么是DCGAN？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A DCGAN is a direct extension of the GAN described above, except that it "
"explicitly uses convolutional and convolutional-transpose layers in the "
"discriminator and generator, respectively. It was first described by Radford"
" et. al. in the paper `Unsupervised Representation Learning With Deep "
"Convolutional Generative Adversarial Networks "
"<https://arxiv.org/pdf/1511.06434.pdf>`__. The discriminator is made up of "
"strided `convolution "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`__ layers, `batch "
"norm <https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d>`__ "
"layers, and `LeakyReLU "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU>`__ activations."
" The input is a 3x64x64 input image and the output is a scalar probability "
"that the input is from the real data distribution. The generator is "
"comprised of `convolutional-transpose "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d>`__ "
"layers, batch norm layers, and `ReLU "
"<https://pytorch.org/docs/stable/nn.html#relu>`__ activations. The input is "
"a latent vector, :math:`z`, that is drawn from a standard normal "
"distribution and the output is a 3x64x64 RGB image. The strided conv-"
"transpose layers allow the latent vector to be transformed into a volume "
"with the same shape as an image. In the paper, the authors also give some "
"tips about how to setup the optimizers, how to calculate the loss functions,"
" and how to initialize the model weights, all of which will be explained in "
"the coming sections."
msgstr ""
"DCGAN是上述GAN的直接扩展，但显式地在判别器和生成器中分别使用了卷积层和反卷积层。它最初由Radford等人在论文《使用深度卷积生成对抗网络进行无监督表示学习"
" <https://arxiv.org/pdf/1511.06434.pdf>`__ 中描述。判别器由步长的`卷积 "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`__层、`批标准化 "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d>`__层和`LeakyReLU"
" "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU>`__激活组成。输入是一个3x64x64的图像，输出是该输入来自真实数据分布的标量概率。生成器由`反卷积"
" "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d>`__层、批标准化层和`ReLU"
" "
"<https://pytorch.org/docs/stable/nn.html#relu>`__激活组成。输入是一个从标准正态分布中抽取的潜向量:math:`z`，输出是一个3x64x64的RGB图像。步长反卷积层允许将潜向量转换为与图像相同形状的体积。在论文中，作者还提供了一些关于设置优化器、计算损失函数和初始化模型权重的提示，这些将在接下来的章节中解释。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Inputs"
msgstr "输入"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s define some inputs for the run:"
msgstr "让我们为运行定义一些输入："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``dataroot`` - the path to the root of the dataset folder. We will talk more"
" about the dataset in the next section."
msgstr "``dataroot`` - 数据集文件夹的根路径。我们将在下一节中详细讨论数据集。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``workers`` - the number of worker threads for loading the data with the "
"``DataLoader``."
msgstr "``workers`` - 使用``DataLoader``加载数据的工作线程数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``batch_size`` - the batch size used in training. The DCGAN paper uses a "
"batch size of 128."
msgstr "``batch_size`` - 训练中使用的批量大小。DCGAN论文使用128的批量大小。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``image_size`` - the spatial size of the images used for training. This "
"implementation defaults to 64x64. If another size is desired, the structures"
" of D and G must be changed. See `here "
"<https://github.com/pytorch/examples/issues/70>`__ for more details."
msgstr ""
"``image_size`` - "
"用于训练的图像的空间大小。此实现默认使用64x64。如果需要其他大小，则必须更改D和G的结构。更多详细信息，请参见`这里 "
"<https://github.com/pytorch/examples/issues/70>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``nc`` - number of color channels in the input images. For color images this"
" is 3."
msgstr "``nc`` - 输入图像中的颜色通道数量。对于彩色图像，这个值是3。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``nz`` - length of latent vector."
msgstr "``nz`` - 潜向量的长度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ngf`` - relates to the depth of feature maps carried through the "
"generator."
msgstr "``ngf`` - 生成器中传播的特征图深度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ndf`` - sets the depth of feature maps propagated through the "
"discriminator."
msgstr "``ndf`` - 判别器中传播的特征图深度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``num_epochs`` - number of training epochs to run. Training for longer will "
"probably lead to better results but will also take much longer."
msgstr "``num_epochs`` - 运行的训练轮数。训练时间越长可能会产生更好的结果，但也需要更长的时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``lr`` - learning rate for training. As described in the DCGAN paper, this "
"number should be 0.0002."
msgstr "``lr`` - 训练的学习率。根据DCGAN论文，该值应为0.0002。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``beta1`` - beta1 hyperparameter for Adam optimizers. As described in paper,"
" this number should be 0.5."
msgstr "``beta1`` - Adam优化器的beta1超参数。根据论文，该值应为0.5。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ngpu`` - number of GPUs available. If this is 0, code will run in CPU "
"mode. If this number is greater than 0 it will run on that number of GPUs."
msgstr "``ngpu`` - 可用的GPU数量。如果是0，代码将在CPU模式下运行。如果这个数字大于0，它将运行相应数量的GPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Data"
msgstr "数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial we will use the `Celeb-A Faces dataset "
"<http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__ which can be "
"downloaded at the linked site, or in `Google Drive "
"<https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`__. "
"The dataset will download as a file named ``img_align_celeba.zip``. Once "
"downloaded, create a directory named ``celeba`` and extract the zip file "
"into that directory. Then, set the ``dataroot`` input for this notebook to "
"the ``celeba`` directory you just created. The resulting directory structure"
" should be:"
msgstr ""
"在本教程中，我们将使用`Celeb-A人脸数据集 "
"<http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__，可以从链接网站或`Google Drive "
"<https://drive.google.com/drive/folders/0B7EVK8r0v71pTUZsaXdaSnZBZzg>`__下载。该数据集将下载为一个名为``img_align_celeba.zip``的文件。下载后，创建一个名为``celeba``的目录，并将压缩文件解压到该目录。然后，将此笔记本的``dataroot``输入设置为您刚刚创建的``celeba``目录。生成的目录结构应该为："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is an important step because we will be using the ``ImageFolder`` "
"dataset class, which requires there to be subdirectories in the dataset root"
" folder. Now, we can create the dataset, create the dataloader, set the "
"device to run on, and finally visualize some of the training data."
msgstr ""
"这是一个重要步骤，因为我们将使用``ImageFolder``数据集类，该类要求数据集根文件夹中有子目录。现在，我们可以创建数据集，创建数据加载器，设置运行设备，并最终可视化一些训练数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Implementation"
msgstr "实现"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"With our input parameters set and the dataset prepared, we can now get into "
"the implementation. We will start with the weight initialization strategy, "
"then talk about the generator, discriminator, loss functions, and training "
"loop in detail."
msgstr "设置好输入参数并准备好数据集后，我们现在可以进入实现过程。我们将从权重初始化策略开始，然后详细讨论生成器、判别器、损失函数和训练循环。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Weight Initialization"
msgstr "权重初始化"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"From the DCGAN paper, the authors specify that all model weights shall be "
"randomly initialized from a Normal distribution with ``mean=0``, "
"``stdev=0.02``. The ``weights_init`` function takes an initialized model as "
"input and reinitializes all convolutional, convolutional-transpose, and "
"batch normalization layers to meet this criteria. This function is applied "
"to the models immediately after initialization."
msgstr ""
"根据DCGAN论文，作者指定所有模型权重应从均值为0、标准差为0.02的正态分布中随机初始化。``weights_init``函数以初始化模型作为输入，并重新初始化所有卷积、反卷积以及批标准化层以满足这些标准。此函数在模型初始化后立即应用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Generator"
msgstr "生成器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The generator, :math:`G`, is designed to map the latent space vector "
"(:math:`z`) to data-space. Since our data are images, converting :math:`z` "
"to data-space means ultimately creating a RGB image with the same size as "
"the training images (i.e. 3x64x64). In practice, this is accomplished "
"through a series of strided two dimensional convolutional transpose layers, "
"each paired with a 2d batch norm layer and a relu activation. The output of "
"the generator is fed through a tanh function to return it to the input data "
"range of :math:`[-1,1]`. It is worth noting the existence of the batch norm "
"functions after the conv-transpose layers, as this is a critical "
"contribution of the DCGAN paper. These layers help with the flow of "
"gradients during training. An image of the generator from the DCGAN paper is"
" shown below."
msgstr ""
"生成器:math:`G`被设计为将潜向量:math:`z`映射到数据空间。由于我们的数据是图像，将:math:`z`转换为数据空间意味着最终生成与训练图像大小相同的RGB图像（即3x64x64）。在实践中，这是通过一系列步长二维反卷积层完成的，每一层都与一个二维批标准化层和一个ReLU激活函数配对。生成器的输出通过一个tanh函数最终返回到输入数据范围:math:`[-1,1]`。值得注意的是，反卷积层后的批标准化函数是DCGAN论文的重要贡献之一。这些层有助于训练期间的梯度流动。以下是DCGAN论文中生成器的示意图。"

#: ../../beginner/vt_tutorial.rst:783
msgid "dcgan_generator"
msgstr "dcgan_generator"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Notice, how the inputs we set in the input section (``nz``, ``ngf``, and "
"``nc``) influence the generator architecture in code. ``nz`` is the length "
"of the z input vector, ``ngf`` relates to the size of the feature maps that "
"are propagated through the generator, and ``nc`` is the number of channels "
"in the output image (set to 3 for RGB images). Below is the code for the "
"generator."
msgstr ""
"请注意，我们在输入章节中设置的输入（``nz``、``ngf``和``nc``）如何影响生成器架构代码。``nz``是z输入向量的长度，``ngf``与生成器中传播的特征图大小有关，``nc``是输出图像中的通道数量（对于RGB图像为3）。下面是生成器的代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, we can instantiate the generator and apply the ``weights_init`` "
"function. Check out the printed model to see how the generator object is "
"structured."
msgstr "现在，我们可以实例化生成器并应用``weights_init``函数。检查打印出的模型以查看生成器对象的结构。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Discriminator"
msgstr "判别器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As mentioned, the discriminator, :math:`D`, is a binary classification "
"network that takes an image as input and outputs a scalar probability that "
"the input image is real (as opposed to fake). Here, :math:`D` takes a "
"3x64x64 input image, processes it through a series of Conv2d, BatchNorm2d, "
"and LeakyReLU layers, and outputs the final probability through a Sigmoid "
"activation function. This architecture can be extended with more layers if "
"necessary for the problem, but there is significance to the use of the "
"strided convolution, BatchNorm, and LeakyReLUs. The DCGAN paper mentions it "
"is a good practice to use strided convolution rather than pooling to "
"downsample because it lets the network learn its own pooling function. Also "
"batch norm and leaky relu functions promote healthy gradient flow which is "
"critical for the learning process of both :math:`G` and :math:`D`."
msgstr ""
"如前所述，判别器:math:`D`是一个二进制分类网络，它以图像作为输入并输出该输入图像是真实（而非虚假）的标量概率。在这里，:math:`D`接受一个3x64x64的输入图像，通过一系列Conv2d、BatchNorm2d和LeakyReLU层进行处理，并通过Sigmoid激活函数输出最终概率。如果必要，可以扩展此架构以包含更多层，但使用步长卷积、BatchNorm和LeakyReLU功能具有重要意义。DCGAN论文提到，使用步长卷积而不是池化来下采样是一个好的实践，因为它允许网络学习自己的池化功能。此外，批标准化和LeakyReLU函数促进了健康梯度流动，这对于:math:`G`和:math:`D`的学习过程至关重要。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Discriminator Code"
msgstr "判别器代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, as with the generator, we can create the discriminator, apply the "
"``weights_init`` function, and print the model’s structure."
msgstr "现在，与生成器一样，我们可以创建判别器，应用``weights_init``函数，并打印模型的结构。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loss Functions and Optimizers"
msgstr "损失函数和优化器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"With :math:`D` and :math:`G` setup, we can specify how they learn through "
"the loss functions and optimizers. We will use the Binary Cross Entropy loss"
" (`BCELoss "
"<https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss>`__)"
" function which is defined in PyTorch as:"
msgstr ""
"设置好:math:`D`和:math:`G`后，我们可以通过损失函数和优化器指定它们如何学习。我们将使用二进制交叉熵损失(`BCELoss "
"<https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss>`__)函数，该函数在PyTorch中定义为："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n "
"\\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]"
msgstr ""
"\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Notice how this function provides the calculation of both log components in "
"the objective function (i.e. :math:`log(D(x))` and :math:`log(1-D(G(z)))`). "
"We can specify what part of the BCE equation to use with the :math:`y` "
"input. This is accomplished in the training loop which is coming up soon, "
"but it is important to understand how we can choose which component we wish "
"to calculate just by changing :math:`y` (i.e. GT labels)."
msgstr ""
"注意此函数如何提供目标函数中两个对数成分的计算（即:math:`log(D(x))`和:math:`log(1-D(G(z)))`）。我们可以通过:math:`y`输入指定要使用BCE公式的哪个部分。这将在即将到来的训练循环中完成，但重要的是理解我们可以仅通过更改:math:`y`（即GT标签）来选择我们希望计算的组件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we define our real label as 1 and the fake label as 0. These labels "
"will be used when calculating the losses of :math:`D` and :math:`G`, and "
"this is also the convention used in the original GAN paper. Finally, we set "
"up two separate optimizers, one for :math:`D` and one for :math:`G`. As "
"specified in the DCGAN paper, both are Adam optimizers with learning rate "
"0.0002 and Beta1 = 0.5. For keeping track of the generator’s learning "
"progression, we will generate a fixed batch of latent vectors that are drawn"
" from a Gaussian distribution (i.e. fixed_noise) . In the training loop, we "
"will periodically input this fixed_noise into :math:`G`, and over the "
"iterations we will see images form out of the noise."
msgstr ""
"接下来，我们定义真实标签为1，虚假标签为0。这些标签将在计算:math:`D`和:math:`G`的损失时使用，这也是原始GAN论文中使用的约定。最后，我们设置两个独立的优化器，一个用于:math:`D`，一个用于:math:`G`。按照DCGAN论文中的规定，两者均使用Adam优化器，学习率为0.0002，Beta1为0.5。为了跟踪生成器的学习进度，我们将生成一批固定的潜向量，这些向量是从高斯分布中抽取的（即固定噪声）。在训练循环中，我们将定期将此固定噪声输入至:math:`G`，随着迭代次数的增加，我们会看到从噪声中形成图像。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training"
msgstr "训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, now that we have all of the parts of the GAN framework defined, we "
"can train it. Be mindful that training GANs is somewhat of an art form, as "
"incorrect hyperparameter settings lead to mode collapse with little "
"explanation of what went wrong. Here, we will closely follow Algorithm 1 "
"from the `Goodfellow’s paper <https://papers.nips.cc/paper/5423-generative-"
"adversarial-nets.pdf>`__, while abiding by some of the best practices shown "
"in `ganhacks <https://github.com/soumith/ganhacks>`__. Namely, we will "
"“construct different mini-batches for real and fake” images, and also adjust"
" G’s objective function to maximize :math:`log(D(G(z)))`. Training is split "
"up into two main parts. Part 1 updates the Discriminator and Part 2 updates "
"the Generator."
msgstr ""
"最终，现在我们已经定义了GAN框架的所有部分，我们可以对其进行训练。请注意，训练GAN在某种程度上是一门艺术，错误的超参数设置可能会导致模式崩溃，而难以解释问题的原因。在这里，我们将严格遵循`Goodfellow论文中的算法1"
" <https://papers.nips.cc/paper/5423-generative-adversarial-"
"nets.pdf>`__，同时遵守`ganhacks "
"<https://github.com/soumith/ganhacks>`__中提供的一些最佳实践。即，我们将“为真实和虚假图像构建不同的小批量”，还调整生成器的目标函数以最大化:math:`log(D(G(z)))`。训练分为两个主要部分。第一部分更新判别器，第二部分更新生成器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Part 1 - Train the Discriminator**"
msgstr "**第1部分 - 训练判别器**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Recall, the goal of training the discriminator is to maximize the "
"probability of correctly classifying a given input as real or fake. In terms"
" of Goodfellow, we wish to “update the discriminator by ascending its "
"stochastic gradient”. Practically, we want to maximize :math:`log(D(x)) + "
"log(1-D(G(z)))`. Due to the separate mini-batch suggestion from `ganhacks "
"<https://github.com/soumith/ganhacks>`__, we will calculate this in two "
"steps. First, we will construct a batch of real samples from the training "
"set, forward pass through :math:`D`, calculate the loss (:math:`log(D(x))`),"
" then calculate the gradients in a backward pass. Secondly, we will "
"construct a batch of fake samples with the current generator, forward pass "
"this batch through :math:`D`, calculate the loss (:math:`log(1-D(G(z)))`), "
"and *accumulate* the gradients with a backward pass. Now, with the gradients"
" accumulated from both the all-real and all-fake batches, we call a step of "
"the Discriminator’s optimizer."
msgstr ""
"回顾一下，训练判别器的目标是最大化正确分类给定输入是真实还是虚假样本的概率。根据Goodfellow的说法，我们希望“通过其随机梯度的上升来更新判别器”。实际上，我们想最大化"
" :math:`log(D(x)) + log(1-D(G(z)))`。根据`ganhacks "
"<https://github.com/soumith/ganhacks>`__ "
"的分离小批次建议，我们将分两步计算。首先，我们将从训练集构建一个真实样本的批次，正向通过 :math:`D`，计算损失 "
"(:math:`log(D(x))`)，然后通过反向传播计算梯度。其次，我们将用当前生成器构建一个假样本的批次，正向通过 :math:`D`，计算损失 "
"(:math:`log(1-D(G(z)))`)，并*累积*梯度通过反向传播。现在，利用从全真实和全虚假的批次累积的梯度，我们调用判别器优化器的一步。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Part 2 - Train the Generator**"
msgstr "**第2部分 - 训练生成器**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As stated in the original paper, we want to train the Generator by "
"minimizing :math:`log(1-D(G(z)))` in an effort to generate better fakes. As "
"mentioned, this was shown by Goodfellow to not provide sufficient gradients,"
" especially early in the learning process. As a fix, we instead wish to "
"maximize :math:`log(D(G(z)))`. In the code we accomplish this by: "
"classifying the Generator output from Part 1 with the Discriminator, "
"computing G’s loss *using real labels as GT*, computing G’s gradients in a "
"backward pass, and finally updating G’s parameters with an optimizer step. "
"It may seem counter-intuitive to use the real labels as GT labels for the "
"loss function, but this allows us to use the :math:`log(x)` part of the "
"``BCELoss`` (rather than the :math:`log(1-x)` part) which is exactly what we"
" want."
msgstr ""
"如原论文所述，我们希望通过最小化 :math:`log(1-D(G(z)))` "
"来训练生成器，以生成更好的虚假样本。如前所述，Goodfellow证明了这在学习早期不能提供足够的梯度。作为一种修正，我们转而希望最大化 "
":math:`log(D(G(z)))`。在代码中，我们通过以下方式实现：用判别器对第1部分生成器的输出进行分类，使用真实标签作为GT计算生成器的损失，用反向传播计算生成器的梯度，最后用优化器步骤更新生成器的参数。使用真实标签作为损失函数的GT标签看起来可能有悖直觉，但这允许我们使用``BCELoss``的"
" :math:`log(x)` 部分（而不是 :math:`log(1-x)` 部分），这正是我们需要的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we will do some statistic reporting and at the end of each epoch we"
" will push our fixed_noise batch through the generator to visually track the"
" progress of G’s training. The training statistics reported are:"
msgstr "最后，我们将进行一些统计报告，并在每个训练周期结束时通过生成器推送固定噪声批次，以可视化生成器训练的进展。报告的训练统计包括："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Loss_D** - discriminator loss calculated as the sum of losses for the all "
"real and all fake batches (:math:`log(D(x)) + log(1 - D(G(z)))`)."
msgstr ""
"**Loss_D** - 判别器损失，计算为全真实和全虚假批次的损失总和 (:math:`log(D(x)) + log(1 - D(G(z)))`)。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Loss_G** - generator loss calculated as :math:`log(D(G(z)))`"
msgstr "**Loss_G** - 生成器损失，计算为 :math:`log(D(G(z)))`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**D(x)** - the average output (across the batch) of the discriminator for "
"the all real batch. This should start close to 1 then theoretically converge"
" to 0.5 when G gets better. Think about why this is."
msgstr ""
"**D(x)** - 判别器的平均输出（跨越批次）针对所有真实批次。这应该一开始接近1，然后理论上当生成器变得更好时收敛到0.5。思考为什么会这样。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**D(G(z))** - average discriminator outputs for the all fake batch. The "
"first number is before D is updated and the second number is after D is "
"updated. These numbers should start near 0 and converge to 0.5 as G gets "
"better. Think about why this is."
msgstr ""
"**D(G(z))** - "
"针对所有虚假批次的判别器平均输出。第一个数字是在判别器更新之前，第二个数字是在判别器更新之后。这些数字一开始应该接近0，随着生成器变得更好收敛到0.5。思考为什么会这样。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Note:** This step might take a while, depending on how many epochs you run"
" and if you removed some data from the dataset."
msgstr "**注意:** 根据你运行的训练周期数量以及是否从数据集中删除了一些数据，这一步可能需要一段时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, lets check out how we did. Here, we will look at three different "
"results. First, we will see how D and G’s losses changed during training. "
"Second, we will visualize G’s output on the fixed_noise batch for every "
"epoch. And third, we will look at a batch of real data next to a batch of "
"fake data from G."
msgstr ""
"最后，让我们看看效果如何。在这里，我们将查看三个不同的结果。首先，我们将查看判别器和生成器的损失在训练期间的变化。其次，我们将可视化生成每个训练周期固定噪声批次的生成器输出的进展。第三，我们将查看一批真实数据旁边的生成器生成的虚假数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Loss versus training iteration**"
msgstr "**损失与训练迭代**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Below is a plot of D & G’s losses versus training iterations."
msgstr "以下是判别器和生成器的损失与训练迭代的关系图。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Visualization of G’s progression**"
msgstr "**生成器进展的可视化**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Remember how we saved the generator’s output on the fixed_noise batch after "
"every epoch of training. Now, we can visualize the training progression of G"
" with an animation. Press the play button to start the animation."
msgstr "还记得我们如何在每个训练周期后保存生成器在固定噪声批次上的输出吗？现在，我们可以通过动画可视化生成器的训练进展。按播放按钮开始动画。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Real Images vs. Fake Images**"
msgstr "**真实图像 vs. 虚假图像**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, lets take a look at some real images and fake images side by side."
msgstr "最后，让我们并排看看一些真实图像和虚假图像。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Where to Go Next"
msgstr "下一步去哪里"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have reached the end of our journey, but there are several places you "
"could go from here. You could:"
msgstr "我们已经到达旅程的终点，但这里还有几个方向可以探索。你可以："

#: ../../beginner/vt_tutorial.rst:783
msgid "Train for longer to see how good the results get"
msgstr "训练更长时间以观察效果能达到多好"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Modify this model to take a different dataset and possibly change the size "
"of the images and the model architecture"
msgstr "修改此模型以适应不同的数据集，并可能更改图像大小和模型架构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Check out some other cool GAN projects `here "
"<https://github.com/nashory/gans-awesome-applications>`__"
msgstr ""
"查看一些其他很酷的GAN项目 `这里 <https://github.com/nashory/gans-awesome-applications>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Create GANs that generate `music <https://www.deepmind.com/blog/wavenet-a-"
"generative-model-for-raw-audio/>`__"
msgstr ""
"创建生成 `音乐 <https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-"
"audio/>`__ 的GANs"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 5 minutes  49.207 seconds)"
msgstr "**脚本总运行时间:** (5分钟49.207秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: dcgan_faces_tutorial.py "
"<dcgan_faces_tutorial.py>`"
msgstr ""
":download:`下载Python源码: dcgan_faces_tutorial.py <dcgan_faces_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: dcgan_faces_tutorial.ipynb "
"<dcgan_faces_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: dcgan_faces_tutorial.ipynb "
"<dcgan_faces_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <ddp_series_intro.html>`__ \\|\\| `What is DDP "
"<ddp_series_theory.html>`__ \\|\\| `Single-Node Multi-GPU Training "
"<ddp_series_multigpu.html>`__ \\|\\| **Fault Tolerance** \\|\\| `Multi-Node "
"training <../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT "
"Training <../intermediate/ddp_series_minGPT.html>`__"
msgstr ""
"`简介 <ddp_series_intro.html>`__ \\|\\| `什么是DDP <ddp_series_theory.html>`__ "
"\\|\\| `单节点多GPU训练 <ddp_series_multigpu.html>`__ \\|\\| **故障容错** \\|\\| "
"`多节点训练 <../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT训练 "
"<../intermediate/ddp_series_minGPT.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Fault-tolerant Distributed Training with ``torchrun``"
msgstr "使用``torchrun``进行故障容错分布式训练"

#: ../../beginner/vt_tutorial.rst:783
msgid "Authors: `Suraj Subramanian <https://github.com/subramen>`__"
msgstr "作者: `Suraj Subramanian <https://github.com/subramen>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What you will learn"
msgstr ""
"你将学到什么"

#: ../../beginner/vt_tutorial.rst:783
msgid "Launching multi-GPU training jobs with ``torchrun``"
msgstr "使用``torchrun``启动多GPU训练作业"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and loading snapshots of your training job"
msgstr "保存和加载训练作业的快照"

#: ../../beginner/vt_tutorial.rst:783
msgid "Structuring your training script for graceful restarts"
msgstr "为优雅重启结构化训练脚本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":octicon:`code-square;1.0em;` View the code used in this tutorial on `GitHub"
" <https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu_torchrun.py>`__"
msgstr ""
":octicon:`code-square;1.0em;` 在 `GitHub "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu_torchrun.py>`__ 上查看此教程使用的代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Prerequisites"
msgstr ""
"前提条件"

#: ../../beginner/vt_tutorial.rst:783
msgid "High-level `overview <ddp_series_theory.html>`__ of DDP"
msgstr "DDP的高级 `概述 <ddp_series_theory.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Familiarity with `DDP code <ddp_series_multigpu.html>`__"
msgstr "熟悉 `DDP代码 <ddp_series_multigpu.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A machine with multiple GPUs (this tutorial uses an AWS p3.8xlarge instance)"
msgstr "具有多GPU的机器（本教程使用AWS p3.8xlarge实例）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch `installed <https://pytorch.org/get-started/locally/>`__ with CUDA"
msgstr "安装CUDA版本的PyTorch `安装 <https://pytorch.org/get-started/locally/>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch/9kIvQOiwYzg>`__."
msgstr ""
"根据以下视频或在 `YouTube <https://www.youtube.com/watch/9kIvQOiwYzg>`__ 上进行学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In distributed training, a single process failure can disrupt the entire "
"training job. Since the susceptibility for failure can be higher here, "
"making your training script robust is particularly important here. You might"
" also prefer your training job to be *elastic*, for example, compute "
"resources can join and leave dynamically over the course of the job."
msgstr ""
"在分布式训练中，单个进程的故障可能会破坏整个训练作业。由于故障的可能性在这里更高，使训练脚本鲁棒性尤为重要。此外，您可能更喜欢使训练作业具有*弹性*，例如计算资源可以在作业期间动态加入和退出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch offers a utility called ``torchrun`` that provides fault-tolerance "
"and elastic training. When a failure occurs, ``torchrun`` logs the errors "
"and attempts to automatically restart all the processes from the last saved "
"“snapshot” of the training job."
msgstr ""
"PyTorch提供了一种名为``torchrun``的工具，支持容错和弹性训练。当发生故障时，``torchrun``记录错误，并尝试从上次保存的训练作业“快照”自动重新启动所有进程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The snapshot saves more than just the model state; it can include details "
"about the number of epochs run, optimizer states or any other stateful "
"attribute of the training job necessary for its continuity."
msgstr "快照不仅仅保存模型状态；它可以包括关于运行周期数、优化器状态或训练作业必要的其他属性的详细信息，以确保其连续性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Why use ``torchrun``"
msgstr "为什么使用``torchrun``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torchrun`` handles the minutiae of distributed training so that you don't "
"need to. For instance,"
msgstr "``torchrun``处理分布式训练的细节，让您不必费心。例如，"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You don't need to set environment variables or explicitly pass the ``rank`` "
"and ``world_size``; ``torchrun`` assigns this along with several other "
"`environment variables "
"<https://pytorch.org/docs/stable/elastic/run.html#environment-variables>`__."
msgstr ""
"您不需要设置环境变量或显式传递``rank``和``world_size``；``torchrun``会分配这些以及其他 `环境变量 "
"<https://pytorch.org/docs/stable/elastic/run.html#environment-variables>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"No need to call ``mp.spawn`` in your script; you only need a generic "
"``main()`` entry point, and launch the script with ``torchrun``. This way "
"the same script can be run in non-distributed as well as single-node and "
"multinode setups."
msgstr ""
"无需在脚本中调用``mp.spawn``；您只需要一个通用的``main()``入口点，并使用``torchrun``启动脚本。这样，相同的脚本可以在非分布式、多节点和单节点设置中运行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Gracefully restarting training from the last saved training snapshot."
msgstr "优雅地从最后保存的训练快照重新启动训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Graceful restarts"
msgstr "优雅重启"

#: ../../beginner/vt_tutorial.rst:783
msgid "For graceful restarts, you should structure your train script like:"
msgstr "为优雅重启，您应如以下方式结构化训练脚本："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If a failure occurs, ``torchrun`` will terminate all the processes and "
"restart them. Each process entry point first loads and initializes the last "
"saved snapshot, and continues training from there. So at any failure, you "
"only lose the training progress from the last saved snapshot."
msgstr ""
"如果出现故障，``torchrun``将终止所有进程并重新启动它们。每个进程入口点首先加载并初始化最后保存的快照，然后从那里继续训练。因此，在任何故障发生时，您只会失去最后保存快照以来的训练进度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In elastic training, whenever there are any membership changes (adding or "
"removing nodes), ``torchrun`` will terminate and spawn processes on "
"available devices. Having this structure ensures your training job can "
"continue without manual intervention."
msgstr ""
"在弹性训练中，每当发生成员更改（添加或移除节点时），``torchrun``将终止并在可用设备上生成进程。具有这种结构可以确保您的训练作业能够继续进行，无需人工干预。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Diff for `multigpu.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu.py>`__ v/s `multigpu_torchrun.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu_torchrun.py>`__"
msgstr ""
"`multigpu.py <https://github.com/pytorch/examples/blob/main/distributed/ddp-"
"tutorial-series/multigpu.py>`__ 与 `multigpu_torchrun.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu_torchrun.py>`__ 的差异"

#: ../../beginner/vt_tutorial.rst:783
msgid "Process group initialization"
msgstr "进程组初始化"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torchrun`` assigns ``RANK`` and ``WORLD_SIZE`` automatically, among `other"
" envvariables <https://pytorch.org/docs/stable/elastic/run.html#environment-"
"variables>`__"
msgstr ""
"``torchrun``自动分配``RANK``和``WORLD_SIZE``，以及 `其他环境变量 "
"<https://pytorch.org/docs/stable/elastic/run.html#environment-variables>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Use torchrun-provided environment variables"
msgstr "使用torchrun提供的环境变量"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and loading snapshots"
msgstr "保存和加载快照"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Regularly storing all the relevant information in snapshots allows our "
"training job to seamlessly resume after an interruption."
msgstr "定期将所有相关信息存储在快照中，允许我们的训练作业在中断后无缝恢复。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loading a snapshot in the Trainer constructor"
msgstr "在Trainer构造函数中加载快照"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When restarting an interrupted training job, your script will first try to "
"load a snapshot to resume training from."
msgstr "当重新启动中断的训练作业时，您的脚本将首先尝试加载快照以继续训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Resuming training"
msgstr "恢复训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Training can resume from the last epoch run, instead of starting all over "
"from scratch."
msgstr "训练可以从最后运行的周期继续，而不是从零开始。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Running the script"
msgstr "运行脚本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Simply call your entry point function as you would for a non-multiprocessing"
" script; ``torchrun`` automatically spawns the processes."
msgstr "只需像非多进程脚本一样调用入口点函数即可；``torchrun``会自动生成进程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Multi-Node training with DDP <../intermediate/ddp_series_multinode.html>`__"
"  (next tutorial in this series)"
msgstr ""
"`使用DDP进行多节点训练 <../intermediate/ddp_series_multinode.html>`__  （本系列下一教程）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Multi-GPU Training with DDP <ddp_series_multigpu.html>`__ (previous "
"tutorial in this series)"
msgstr "`使用DDP进行多GPU训练 <ddp_series_multigpu.html>`__ （本系列上一教程）"

#: ../../beginner/vt_tutorial.rst:783
msgid "`torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__"
msgstr "`torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Torchrun launch options "
"<https://github.com/pytorch/pytorch/blob/bbe803cb35948df77b46a2d38372910c96693dcd/torch/distributed/run.py#L401>`__"
msgstr ""
"`Torchrun启动选项 "
"<https://github.com/pytorch/pytorch/blob/bbe803cb35948df77b46a2d38372910c96693dcd/torch/distributed/run.py#L401>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Migrating from torch.distributed.launch to torchrun "
"<https://pytorch.org/docs/stable/elastic/train_script.html#elastic-train-"
"script>`__"
msgstr ""
"`从torch.distributed.launch迁移到torchrun "
"<https://pytorch.org/docs/stable/elastic/train_script.html#elastic-train-"
"script>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Introduction** \\|\\| `What is DDP <ddp_series_theory.html>`__ \\|\\| "
"`Single-Node Multi-GPU Training <ddp_series_multigpu.html>`__ \\|\\| `Fault "
"Tolerance <ddp_series_fault_tolerance.html>`__ \\|\\| `Multi-Node training "
"<../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT Training "
"<../intermediate/ddp_series_minGPT.html>`__"
msgstr ""
"**简介** \\|\\| `什么是DDP <ddp_series_theory.html>`__ \\|\\| `单节点多GPU训练 "
"<ddp_series_multigpu.html>`__ \\|\\| `故障容忍 "
"<ddp_series_fault_tolerance.html>`__ \\|\\| `多节点训练 "
"<../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT训练 "
"<../intermediate/ddp_series_minGPT.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Distributed Data Parallel in PyTorch - Video Tutorials"
msgstr "PyTorch中的分布式数据并行 - 视频教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch/-K3bZYHYHEA>`__."
msgstr ""
"根据以下视频或在 `YouTube <https://www.youtube.com/watch/-K3bZYHYHEA>`__ 上进行学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This series of video tutorials walks you through distributed training in "
"PyTorch via DDP."
msgstr "这一系列视频教程通过PyTorch中的DDP为您讲解分布式训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The series starts with a simple non-distributed training job, and ends with "
"deploying a training job across several machines in a cluster. Along the "
"way, you will also learn about `torchrun "
"<https://pytorch.org/docs/stable/elastic/run.html>`__ for fault-tolerant "
"distributed training."
msgstr ""
"该系列教程从一个简单的非分布式训练任务开始，最终完成跨集群中的多台机器部署训练任务。在此过程中，您还将学习如何使用 `torchrun "
"<https://pytorch.org/docs/stable/elastic/run.html>`__ 实现容错的分布式训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The tutorial assumes a basic familiarity with model training in PyTorch."
msgstr "该教程假设您对 PyTorch 模型训练有基本的了解。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Running the code"
msgstr "运行代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You will need multiple CUDA GPUs to run the tutorial code. Typically, this "
"can be done on a cloud instance with multiple GPUs (the tutorials use an "
"Amazon EC2 P3 instance with 4 GPUs)."
msgstr ""
"您需要多块 CUDA GPU 来运行教程代码。通常可以在拥有多块 GPU 的云实例上完成（教程使用了亚马逊 EC2 的 P3 实例，配备了4块 "
"GPU）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The tutorial code is hosted in this `github repo "
"<https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-"
"series>`__. Clone the repository and follow along!"
msgstr ""
"教程代码托管在这个 `GitHub 仓库 "
"<https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-"
"series>`__ 中。克隆该仓库并跟随教程一起学习！"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tutorial sections"
msgstr "教程部分"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction (this page)"
msgstr "简介（当前页面）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`What is DDP? <ddp_series_theory.html>`__ Gently introduces what DDP is "
"doing under the hood"
msgstr "`DDP 是什么? <ddp_series_theory.html>`__ 温和地介绍了 DDP 的底层原理"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Single-Node Multi-GPU Training <ddp_series_multigpu.html>`__ Training "
"models using multiple GPUs on a single machine"
msgstr "`单节点多 GPU 训练 <ddp_series_multigpu.html>`__ 使用单机上的多块 GPU 训练模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Fault-tolerant distributed training <ddp_series_fault_tolerance.html>`__ "
"Making your distributed training job robust with torchrun"
msgstr ""
"`容错分布式训练 <ddp_series_fault_tolerance.html>`__ 使用 torchrun 提高分布式训练任务的健壮性"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Multi-Node training <../intermediate/ddp_series_multinode.html>`__ Training"
" models using multiple GPUs on multiple machines"
msgstr ""
"`多节点训练 <../intermediate/ddp_series_multinode.html>`__ 使用多台机器上的多块 GPU 训练模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Training a GPT model with DDP <../intermediate/ddp_series_minGPT.html>`__ "
"“Real-world” example of training a `minGPT "
"<https://github.com/karpathy/minGPT>`__ model with DDP"
msgstr ""
"`使用 DDP 训练 GPT 模型 <../intermediate/ddp_series_minGPT.html>`__ 使用 DDP 训练 "
"`minGPT <https://github.com/karpathy/minGPT>`__ 模型的“真实场景”示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <ddp_series_intro.html>`__ \\|\\| `What is DDP "
"<ddp_series_theory.html>`__ \\|\\| **Single-Node Multi-GPU Training** \\|\\|"
" `Fault Tolerance <ddp_series_fault_tolerance.html>`__ \\|\\| `Multi-Node "
"training <../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT "
"Training <../intermediate/ddp_series_minGPT.html>`__"
msgstr ""
"`简介 <ddp_series_intro.html>`__ \\|\\| `DDP 是什么? <ddp_series_theory.html>`__ "
"\\|\\| **单节点多 GPU 训练** \\|\\| `容错能力 <ddp_series_fault_tolerance.html>`__ "
"\\|\\| `多节点训练 <../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT "
"训练 <../intermediate/ddp_series_minGPT.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Multi GPU training with DDP"
msgstr "使用 DDP 进行多 GPU 训练"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to migrate a single-GPU training script to multi-GPU via DDP"
msgstr "如何将单 GPU 训练脚本迁移到通过 DDP 实现的多 GPU 训练"

#: ../../beginner/vt_tutorial.rst:783
msgid "Setting up the distributed process group"
msgstr "设置分布式进程组"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and loading models in a distributed setup"
msgstr "在分布式环境中保存和加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":octicon:`code-square;1.0em;` View the code used in this tutorial on `GitHub"
" <https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu.py>`__"
msgstr ""
":octicon:`code-square;1.0em;` 查看教程中使用的代码，托管于 `GitHub "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu.py>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "High-level overview of `how DDP works  <ddp_series_theory.html>`__"
msgstr "了解 `DDP 如何工作 <ddp_series_theory.html>`__ 的高层概述"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch/-LAtx9Q6DA8>`__."
msgstr ""
"通过以下视频或在 `YouTube <https://www.youtube.com/watch/-LAtx9Q6DA8>`__ 上观看教程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the `previous tutorial <ddp_series_theory.html>`__, we got a high-level "
"overview of how DDP works; now we see how to use DDP in code. In this "
"tutorial, we start with a single-GPU training script and migrate that to "
"running it on 4 GPUs on a single node. Along the way, we will talk through "
"important concepts in distributed training while implementing them in our "
"code."
msgstr ""
"在 `上一节教程 <ddp_series_theory.html>`__ 中，我们了解了 DDP 工作原理的高层概述；现在我们来看如何在代码中使用 "
"DDP。在本教程中，我们从一个单 GPU 训练脚本开始，迁移到在单节点上的 4 块 GPU "
"上运行。在此过程中，我们将讨论分布式训练中的重要概念并在代码中实现它们。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If your model contains any ``BatchNorm`` layers, it needs to be converted to"
" ``SyncBatchNorm`` to sync the running stats of ``BatchNorm`` layers across "
"replicas."
msgstr ""
"如果您的模型包含任何 ``BatchNorm`` 层，则需要将其转换为 ``SyncBatchNorm``，以同步 ``BatchNorm`` "
"层跨副本的运行状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use the helper function "
"`torch.nn.SyncBatchNorm.convert_sync_batchnorm(model) "
"<https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm>`__"
" to convert all ``BatchNorm`` layers in the model to ``SyncBatchNorm``."
msgstr ""
"使用辅助函数 `torch.nn.SyncBatchNorm.convert_sync_batchnorm(model) "
"<https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm>`__"
" 将模型中的所有 ``BatchNorm`` 层转换为 ``SyncBatchNorm``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Diff for `single_gpu.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/single_gpu.py>`__ v/s `multigpu.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu.py>`__"
msgstr ""
"`single_gpu.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/single_gpu.py>`__ 与 `multigpu.py "
"<https://github.com/pytorch/examples/blob/main/distributed/ddp-tutorial-"
"series/multigpu.py>`__ 的差异"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"These are the changes you typically make to a single-GPU training script to "
"enable DDP."
msgstr "以下是为启用 DDP 通常需要对单 GPU 训练脚本进行的更改。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Imports"
msgstr "导入"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.multiprocessing`` is a PyTorch wrapper around Python's native "
"multiprocessing"
msgstr ""
"``torch.multiprocessing`` 是 PyTorch 针对 Python 原生 `multiprocessing` 的封装。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The distributed process group contains all the processes that can "
"communicate and synchronize with each other."
msgstr "分布式进程组包含可以相互通信和同步的所有进程。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Constructing the process group"
msgstr "构建进程组"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, before initializing the group process, call `set_device "
"<https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html?highlight=set_device#torch.cuda.set_device>`__,"
" which sets the default GPU for each process. This is important to prevent "
"hangs or excessive memory utilization on `GPU:0`"
msgstr ""
"首先，在初始化进程组之前，调用 `set_device "
"<https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html?highlight=set_device#torch.cuda.set_device>`__，为每个进程设置默认"
" GPU。这很重要，以防止在 `GPU:0` 上出现死锁或过度内存利用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The process group can be initialized by TCP (default) or from a shared file-"
"system. Read more on `process group initialization "
"<https://pytorch.org/docs/stable/distributed.html#tcp-initialization>`__"
msgstr ""
"进程组可以通过 TCP（默认）或来自共享文件系统进行初始化。阅读更多相关内容请访问 `进程组初始化 "
"<https://pytorch.org/docs/stable/distributed.html#tcp-initialization>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`init_process_group "
"<https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group>`__"
" initializes the distributed process group."
msgstr ""
"`init_process_group "
"<https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group>`__"
" 用于初始化分布式进程组。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Read more about `choosing a DDP backend "
"<https://pytorch.org/docs/stable/distributed.html#which-backend-to-use>`__"
msgstr ""
"阅读更多关于 `选择 DDP 后端 <https://pytorch.org/docs/stable/distributed.html#which-"
"backend-to-use>`__ 的信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Constructing the DDP model"
msgstr "构建 DDP 模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Distributing input data"
msgstr "分布输入数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DistributedSampler "
"<https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler>`__"
" chunks the input data across all distributed processes."
msgstr ""
"`DistributedSampler "
"<https://pytorch.org/docs/stable/data.html?highlight=distributedsampler#torch.utils.data.distributed.DistributedSampler>`__"
" 将输入数据划分到所有分布式进程中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `DataLoader "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>`__ "
"combines a dataset and a"
msgstr ""
"`DataLoader "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>`__ "
"将数据集和"

#: ../../beginner/vt_tutorial.rst:783
msgid "sampler, and provides an iterable over the given dataset."
msgstr "采样器组合，并提供一个数据集的迭代器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each process will receive an input batch of 32 samples; the effective batch "
"size is ``32 * nprocs``, or 128 when using 4 GPUs."
msgstr "每个进程将接收一个包含 32 个样本的输入批次；有效的批次大小为 ``32 * nprocs``，使用 4 块 GPU 时为 128。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Calling the ``set_epoch()`` method on the ``DistributedSampler`` at the "
"beginning of each epoch is necessary to make shuffling work properly across "
"multiple epochs. Otherwise, the same ordering will be used in each epoch."
msgstr ""
"在每个 epoch 开始时调用 ``DistributedSampler`` 的 ``set_epoch()`` 方法是必需的，以确保跨多个 epoch"
" 的随机打乱正确工作，否则，每个 epoch 的数据顺序将相同。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving model checkpoints"
msgstr "保存模型检查点"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We only need to save model checkpoints from one process. Without this "
"condition, each process would save its copy of the identical mode. Read more"
" on saving and loading models with DDP `here "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-"
"checkpoints>`__"
msgstr ""
"我们只需要从一个进程保存模型检查点。否则，每个进程都会保存它自己的一份相同的模型。阅读更多关于以 DDP 保存和加载模型的信息，访问 `此处 "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-"
"checkpoints>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Collective calls "
"<https://pytorch.org/docs/stable/distributed.html#collective-functions>`__ "
"are functions that run on all the distributed processes, and they are used "
"to gather certain states or values to a specific process. Collective calls "
"require all ranks to run the collective code. In this example, "
"`_save_checkpoint` should not have any collective calls because it is only "
"run on the ``rank:0`` process. If you need to make any collective calls, it "
"should be before the ``if self.gpu_id == 0`` check."
msgstr ""
"`Collective 函数调用 "
"<https://pytorch.org/docs/stable/distributed.html#collective-functions>`__ "
"是运行于所有分布式进程上的函数，用于汇总某些状态或值到特定进程。Collective 函数调用需要所有 rank "
"运行此代码。在此示例中，`_save_checkpoint` 不应该包含 Collective 函数调用，因为它仅在 ``rank:0`` "
"进程上运行。如果需要使用 Collective 调用，应在 ``if self.gpu_id == 0`` 检查之前完成。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Running the distributed training job"
msgstr "运行分布式训练任务"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Include new arguments ``rank`` (replacing ``device``) and ``world_size``."
msgstr "包括新的参数 ``rank``（替代 ``device``）和 ``world_size``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``rank`` is auto-allocated by DDP when calling `mp.spawn "
"<https://pytorch.org/docs/stable/multiprocessing.html#spawning-"
"subprocesses>`__."
msgstr ""
"``rank`` 是在调用 `mp.spawn "
"<https://pytorch.org/docs/stable/multiprocessing.html#spawning-"
"subprocesses>`__ 时由 DDP 自动分配。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``world_size`` is the number of processes across the training job. For GPU "
"training, this corresponds to the number of GPUs in use, and each process "
"works on a dedicated GPU."
msgstr ""
"``world_size`` 是训练任务的进程总数。对于 GPU 训练而言，这对应于使用的 GPU 数量，每个进程在专属的 GPU 上运行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Here's what the code looks like:"
msgstr "代码如下所示："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Fault Tolerant distributed training <ddp_series_fault_tolerance.html>`__  "
"(next tutorial in this series)"
msgstr "`容错分布式训练 <ddp_series_fault_tolerance.html>`__ （本系列教程的下一节）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Intro to DDP <ddp_series_theory.html>`__ (previous tutorial in this series)"
msgstr "`DDP 基础介绍 <ddp_series_theory.html>`__ （本系列教程的上一节）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Getting Started with DDP "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`__"
msgstr ""
"`DDP 入门 <https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Process Group Initialization "
"<https://pytorch.org/docs/stable/distributed.html#tcp-initialization>`__"
msgstr ""
"`进程组初始化 <https://pytorch.org/docs/stable/distributed.html#tcp-"
"initialization>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <ddp_series_intro.html>`__ \\|\\| **What is DDP** \\|\\| "
"`Single-Node Multi-GPU Training <ddp_series_multigpu.html>`__ \\|\\| `Fault "
"Tolerance <ddp_series_fault_tolerance.html>`__ \\|\\| `Multi-Node training "
"<../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT Training "
"<../intermediate/ddp_series_minGPT.html>`__"
msgstr ""
"`简介 <ddp_series_intro.html>`__ \\|\\| **DDP 是什么** \\|\\| `单节点多 GPU 训练 "
"<ddp_series_multigpu.html>`__ \\|\\| `容错能力 "
"<ddp_series_fault_tolerance.html>`__ \\|\\| `多节点训练 "
"<../intermediate/ddp_series_multinode.html>`__ \\|\\| `minGPT 训练 "
"<../intermediate/ddp_series_minGPT.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is Distributed Data Parallel (DDP)"
msgstr "什么是分布式数据并行（DDP）"

#: ../../beginner/vt_tutorial.rst:783
msgid "How DDP works under the hood"
msgstr "DDP 的底层工作原理"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is ``DistributedSampler``"
msgstr "什么是 ``DistributedSampler``"

#: ../../beginner/vt_tutorial.rst:783
msgid "How gradients are synchronized across GPUs"
msgstr "如何跨 GPU 同步梯度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Familiarity with `basic non-distributed training  "
"<https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html>`__ "
"in PyTorch"
msgstr ""
"熟悉 PyTorch 中 `基本的非分布式训练 "
"<https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch/Cvdhwx-OBBo>`__."
msgstr ""
"通过以下视频或在 `YouTube <https://www.youtube.com/watch/Cvdhwx-OBBo>`__ 上观看教程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is a gentle introduction to PyTorch `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
" (DDP) which enables data parallel training in PyTorch. Data parallelism is "
"a way to process multiple data batches across multiple devices "
"simultaneously to achieve better performance. In PyTorch, the "
"`DistributedSampler "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler>`__"
" ensures each device gets a non-overlapping input batch. The model is "
"replicated on all the devices; each replica calculates gradients and "
"simultaneously synchronizes with the others using the `ring all-reduce "
"algorithm <https://tech.preferred.jp/en/blog/technologies-behind-"
"distributed-deep-learning-allreduce/>`__."
msgstr ""
"本教程是对 PyTorch 中支持数据并行训练的 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
" (DDP) 的一个简单介绍。数据并行是一种同时在多个设备上处理多个数据批次以获得更好性能的方式。在 PyTorch "
"中，`DistributedSampler "
"<https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler>`__"
" 确保每个设备获得非重叠的数据输入批次。模型会在所有设备上进行复制；每个副本计算梯度并使用 `环状全归约算法 "
"<https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-"
"learning-allreduce/>`__ 同步。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This `illustrative tutorial "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html#>`__ provides a "
"more in-depth python view of the mechanics of DDP."
msgstr ""
"这个 `说明性教程 <https://pytorch.org/tutorials/intermediate/dist_tuto.html#>`__ "
"提供了关于 DDP 工作机制更深入的 Python 视角。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Why you should prefer DDP over ``DataParallel`` (DP)"
msgstr "为什么应优先选择 DDP 而不是 ``DataParallel`` (DP)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html>`__ is"
" an older approach to data parallelism. DP is trivially simple (with just "
"one extra line of code) but it is much less performant. DDP improves upon "
"the architecture in a few ways:"
msgstr ""
"`DataParallel "
"<https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html>`__ "
"是数据并行的一种较旧的方法。DP 非常简单（只需一行额外代码）但性能较差。DDP 在架构上改善了几个方面："

#: ../../beginner/vt_tutorial.rst:783
msgid "``DataParallel``"
msgstr "``DataParallel``"

#: ../../beginner/vt_tutorial.rst:783
msgid "``DistributedDataParallel``"
msgstr "``DistributedDataParallel``"

#: ../../beginner/vt_tutorial.rst:783
msgid "More overhead; model is replicated and destroyed at each forward pass"
msgstr "更多开销；模型在每次前向传播时都会被复制和销毁"

#: ../../beginner/vt_tutorial.rst:783
msgid "Model is replicated only once"
msgstr "模型仅被复制一次"

#: ../../beginner/vt_tutorial.rst:783
msgid "Only supports single-node parallelism"
msgstr "仅支持单节点并行"

#: ../../beginner/vt_tutorial.rst:783
msgid "Supports scaling to multiple machines"
msgstr "支持扩展到多台机器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Slower; uses multithreading on a single process and runs into Global "
"Interpreter Lock (GIL) contention"
msgstr "速度较慢；使用单个进程的多线程并受到全局解释器锁 (GIL) 争用的影响"

#: ../../beginner/vt_tutorial.rst:783
msgid "Faster (no GIL contention) because it uses multiprocessing"
msgstr "速度更快（无 GIL 争用），因为它使用多进程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Multi-GPU training with DDP <ddp_series_multigpu.html>`__ (next tutorial in"
" this series)"
msgstr "`使用 DDP 的多 GPU 训练 <ddp_series_multigpu.html>`__ （本系列教程的下一节）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DDP API "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
msgstr ""
"`DDP API "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DDP Internal Design "
"<https://pytorch.org/docs/master/notes/ddp.html#internal-design>`__"
msgstr ""
"`DDP 内部设计 <https://pytorch.org/docs/master/notes/ddp.html#internal-"
"design>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DDP Mechanics Tutorial "
"<https://pytorch.org/tutorials/intermediate/dist_tuto.html#>`__"
msgstr ""
"`DDP 机制教程 <https://pytorch.org/tutorials/intermediate/dist_tuto.html#>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author**: `Soumith Chintala <http://soumith.ch>`_"
msgstr "**作者**: `Soumith Chintala <http://soumith.ch>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is PyTorch?"
msgstr "什么是 PyTorch?"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch is a Python-based scientific computing package serving two broad "
"purposes:"
msgstr "PyTorch 是一个基于 Python 的科学计算包，服务于两个主要目的："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A replacement for NumPy to use the power of GPUs and other accelerators."
msgstr "作为 NumPy 的替代品，利用 GPU 和其他加速器的性能。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"An automatic differentiation library that is useful to implement neural "
"networks."
msgstr "一个自动微分库，用于实现神经网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Goal of this tutorial:"
msgstr "本教程的目标："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Understand PyTorch’s Tensor library and neural networks at a high level."
msgstr "从高层理解 PyTorch 的张量库和神经网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To run the tutorials below, make sure you have the `torch`_, `torchvision`_,"
" and `matplotlib`_ packages installed."
msgstr "要运行以下教程，确保安装了 `torch`_, `torchvision`_ 和 `matplotlib`_ 包。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors"
msgstr ""
"张量"

#: ../../beginner/vt_tutorial.rst:783
msgid "In this tutorial, you will learn the basics of PyTorch tensors."
msgstr "在本教程中，您将学习 PyTorch 张量的基础知识。"

#: ../../beginner/vt_tutorial.rst:783
msgid ":octicon:`code;1em` Code"
msgstr ":octicon:`code;1em` 代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A Gentle Introduction to torch.autograd"
msgstr ""
"torch.autograd 简介"

#: ../../beginner/vt_tutorial.rst:783
msgid "Learn about autograd."
msgstr "了解 autograd。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Neural Networks"
msgstr ""
"神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial demonstrates how you can train neural networks in PyTorch."
msgstr "本教程展示如何在 PyTorch 中训练神经网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Training a Classifier"
msgstr ""
"训练分类器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Learn how to train an image classifier in PyTorch by using the CIFAR10 "
"dataset."
msgstr "学习如何使用 CIFAR10 数据集在 PyTorch 中训练图像分类器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Image Segmentation DeepLabV3 on Android"
msgstr "在 Android 上进行图像分割 DeepLabV3"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch Mobile is no longer actively supported. Please check out `ExecuTorch"
" <https://github.com/pytorch/executorch>`__."
msgstr ""
"PyTorch Mobile 不再被积极支持。请查看 `ExecuTorch "
"<https://github.com/pytorch/executorch>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Image Segmentation DeepLabV3 on iOS"
msgstr "在 iOS 上进行图像分割 DeepLabV3"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_deploy_seq2seq_hybrid_frontend_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_beginner_deploy_seq2seq_hybrid_frontend_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Deploying a Seq2Seq Model with TorchScript"
msgstr "使用 TorchScript 部署 Seq2Seq 模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial will walk through the process of transitioning a sequence-to-"
"sequence model to TorchScript using the TorchScript API. The model that we "
"will convert is the chatbot model from the `Chatbot tutorial "
"<https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__. You can "
"either treat this tutorial as a “Part 2” to the Chatbot tutorial and deploy "
"your own pretrained model, or you can start with this document and use a "
"pretrained model that we host. In the latter case, you can reference the "
"original Chatbot tutorial for details regarding data preprocessing, model "
"theory and definition, and model training."
msgstr ""
"本教程将逐步讲解如何使用 TorchScript API 将序列到序列模型迁移到 TorchScript。我们将转换的模型是来自 `Chatbot 教程"
" <https://pytorch.org/tutorials/beginner/chatbot_tutorial.html>`__ "
"的聊天机器人模型。您可以将此教程视为 Chatbot "
"教程的“第二部分”并部署您自己的预训练模型，或者您可以从本文档开始，并使用我们托管的预训练模型。在后一种情况下，您可以参考原始 Chatbot "
"教程，以了解数据预处理、模型理论及定义和模型训练的详细信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is TorchScript?"
msgstr "什么是 TorchScript？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"During the research and development phase of a deep learning-based project, "
"it is advantageous to interact with an **eager**, imperative interface like "
"PyTorch’s. This gives users the ability to write familiar, idiomatic Python,"
" allowing for the use of Python data structures, control flow operations, "
"print statements, and debugging utilities. Although the eager interface is a"
" beneficial tool for research and experimentation applications, when it "
"comes time to deploy the model in a production environment, having a "
"**graph**-based model representation is very beneficial. A deferred graph "
"representation allows for optimizations such as out-of-order execution, and "
"the ability to target highly optimized hardware architectures. Also, a "
"graph-based representation enables framework-agnostic model exportation. "
"PyTorch provides mechanisms for incrementally converting eager-mode code "
"into TorchScript, a statically analyzable and optimizable subset of Python "
"that Torch uses to represent deep learning programs independently from the "
"Python runtime."
msgstr ""
"在基于深度学习的项目的研究和开发阶段，使用像 PyTorch 这样直接而命令式的**实时**界面非常有利。这使用户能够编写熟悉的、符合 Python "
"规范的代码，可以使用 Python "
"数据结构、控制流操作、打印语句和调试工具。虽然实时界面对研究和实验有益，但当需要在生产环境中部署模型时，具有**图**形式的模型表示形式非常有用。推迟的图形式表示可以实现优化，例如乱序执行，并能够适配高效的硬件架构。此外，基于图的表示形式还可以实现框架无关的模型导出。PyTorch"
" 提供了将实时模式代码逐步转换为 TorchScript 的机制，TorchScript 是独立于 Python 运行时的可静态分析和优化的 "
"Python 子集，Torch 使用它来表示深度学习程序。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The API for converting eager-mode PyTorch programs into TorchScript is found"
" in the ``torch.jit`` module. This module has two core modalities for "
"converting an eager-mode model to a TorchScript graph representation: "
"**tracing** and **scripting**. The ``torch.jit.trace`` function takes a "
"module or function and a set of example inputs. It then runs the example "
"input through the function or module while tracing the computational steps "
"that are encountered, and outputs a graph-based function that performs the "
"traced operations. **Tracing** is great for straightforward modules and "
"functions that do not involve data-dependent control flow, such as standard "
"convolutional neural networks. However, if a function with data-dependent if"
" statements and loops is traced, only the operations called along the "
"execution route taken by the example input will be recorded. In other words,"
" the control flow itself is not captured. To convert modules and functions "
"containing data-dependent control flow, a **scripting** mechanism is "
"provided. The ``torch.jit.script`` function/decorator takes a module or "
"function and does not requires example inputs. Scripting then explicitly "
"converts the module or function code to TorchScript, including all control "
"flows. One caveat with using scripting is that it only supports a subset of "
"Python, so you might need to rewrite the code to make it compatible with the"
" TorchScript syntax."
msgstr ""
"将实时模式的 PyTorch 程序转换为 TorchScript 的 API 位于 ``torch.jit`` 模块中。该模块为将实时模式模型转换为 "
"TorchScript "
"图形表示提供了两种核心模式：**跟踪**（Tracing）和**脚本**（Scripting）。``torch.jit.trace`` "
"函数接受一个模块或函数以及一组示例输入。然后，它会通过函数或模块运行示例输入，同时记录遇到的计算步骤，并输出一个基于图形的函数，用于执行记录的操作。**跟踪**非常适合那些没有数据依赖控制流的常规模块和函数，例如标准卷积神经网络。然而，如果对包含数据依赖的"
" if "
"语句和循环的函数进行跟踪，那么只有示例输入所采取的执行路径上的操作会被记录。换句话说，控制流本身不会被捕获。为了转换包含数据依赖控制流的模块和函数，还提供了一种**脚本**机制。``torch.jit.script``"
" 函数/装饰器接受一个模块或函数，不需要示例输入。脚本会显式地将模块或函数代码转换为 "
"TorchScript，包括所有控制流。脚本使用的一个警告是它仅支持 Python 的一个子集，因此您可能需要重写代码以使其兼容 TorchScript"
" 语法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For all details relating to the supported features, see the `TorchScript "
"language reference <https://pytorch.org/docs/master/jit.html>`__. To provide"
" the maximum flexibility, you can also mix tracing and scripting modes "
"together to represent your whole program, and these techniques can be "
"applied incrementally."
msgstr ""
"有关所有支持功能的详细信息，请参阅 `TorchScript 语言参考 "
"<https://pytorch.org/docs/master/jit.html>`__。为了提供最大灵活性，您还可以混合使用跟踪和脚本模式来表示整个程序，可以渐进式地应用这些技术。"

#: ../../beginner/vt_tutorial.rst:783
msgid "workflow"
msgstr "工作流"

#: ../../beginner/vt_tutorial.rst:783
msgid "Acknowledgments"
msgstr "归功与致谢"

#: ../../beginner/vt_tutorial.rst:783
msgid "This tutorial was inspired by the following sources:"
msgstr "本教程受以下来源的启发："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Yuan-Kuei Wu's pytorch-chatbot implementation: "
"https://github.com/ywk991112/pytorch-chatbot"
msgstr ""
"Yuan-Kuei Wu 的 pytorch-chatbot 实现: https://github.com/ywk991112/pytorch-"
"chatbot"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sean Robertson's practical-pytorch seq2seq-translation example: "
"https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation"
msgstr ""
"Sean Robertson 的 practical-pytorch 序列到序列翻译示例: "
"https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"FloydHub's Cornell Movie Corpus preprocessing code: "
"https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus"
msgstr ""
"FloydHub 的 Cornell Movie Corpus 数据预处理代码: "
"https://github.com/floydhub/textutil-preprocess-cornell-movie-corpus"

#: ../../beginner/vt_tutorial.rst:783
msgid "Prepare Environment"
msgstr "准备环境"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, we will import the required modules and set some constants. If you "
"are planning on using your own model, be sure that the ``MAX_LENGTH`` "
"constant is set correctly. As a reminder, this constant defines the maximum "
"allowed sentence length during training and the maximum length output that "
"the model is capable of producing."
msgstr ""
"首先，我们将导入所需的模块并设置一些常量。如果您打算使用自己的模型，请确保 ``MAX_LENGTH`` "
"常量设置正确。请记住，这个常量定义了训练期间允许的最大句子长度，以及模型能够生成的最大输出长度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Model Overview"
msgstr "模型概述"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As mentioned, the model that we are using is a `sequence-to-sequence "
"<https://arxiv.org/abs/1409.3215>`__ (seq2seq) model. This type of model is "
"used in cases when our input is a variable-length sequence, and our output "
"is also a variable length sequence that is not necessarily a one-to-one "
"mapping of the input. A seq2seq model is comprised of two recurrent neural "
"networks (RNNs) that work cooperatively: an **encoder** and a **decoder**."
msgstr ""
"如前所述，我们使用的模型是一个 `序列到序列 "
"<https://arxiv.org/abs/1409.3215>`__（seq2seq）模型。这种类型的模型用于输入是可变长度序列，并且输出也是可变的长度序列，且不一定是一对一的映射。一个"
" seq2seq 模型包含两个循环神经网络（RNN），它们协同工作：一个 **编码器** 和一个 **解码器**。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The decoder RNN generates the response sentence in a token-by-token fashion."
" It uses the encoder’s context vectors, and internal hidden states to "
"generate the next word in the sequence. It continues generating words until "
"it outputs an *EOS_token*, representing the end of the sentence. We use an "
"`attention mechanism <https://arxiv.org/abs/1409.0473>`__ in our decoder to "
"help it to “pay attention” to certain parts of the input when generating the"
" output. For our model, we implement `Luong et al. "
"<https://arxiv.org/abs/1508.04025>`__\\ ’s “Global attention” module, and "
"use it as a submodule in our decode model."
msgstr ""
"解码器 RNN 以逐个令牌的方式生成响应句子。它使用编码器的上下文向量和内部隐藏状态来生成序列中的下一个单词。它继续生成单词，直到输出一个 "
"*EOS_token*，表示句子的结束。我们在解码器中使用了一种 `注意力机制 "
"<https://arxiv.org/abs/1409.0473>`__，以帮助它在生成输出时“注意”输入的某些部分。对于我们的模型，我们实现了 "
"`Luong 等人 <https://arxiv.org/abs/1508.04025>`__ 的“全局注意力”模块，并将其用作解码模型的子模块。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Data Handling"
msgstr "数据处理"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Although our models conceptually deal with sequences of tokens, in reality, "
"they deal with numbers like all machine learning models do. In this case, "
"every word in the model’s vocabulary, which was established before training,"
" is mapped to an integer index. We use a ``Voc`` object to contain the "
"mappings from word to index, as well as the total number of words in the "
"vocabulary. We will load the object later before we run the model."
msgstr ""
"尽管我们的模型在概念上处理的是令牌序列，但实际中，它们像所有机器学习模型一样处理数字。在这种情况下，模型的词汇中的每个单词（在训练之前被建立）都映射到一个整数索引中。我们使用一个"
" ``Voc`` 对象来包含从单词到索引的映射以及词汇表中的总词数。稍后在运行模型之前我们会加载该对象。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Also, in order for us to be able to run evaluations, we must provide a tool "
"for processing our string inputs. The ``normalizeString`` function converts "
"all characters in a string to lowercase and removes all non-letter "
"characters. The ``indexesFromSentence`` function takes a sentence of words "
"and returns the corresponding sequence of word indexes."
msgstr ""
"而且，为了能够运行评估，我们必须提供一个工具来处理我们的字符串输入。``normalizeString`` "
"函数将字符串中的所有字符转换为小写并移除所有非字母字符。``indexesFromSentence`` "
"函数接收一个由单词组成的句子并返回对应的单词索引序列。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define Encoder"
msgstr "定义编码器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We implement our encoder’s RNN with the ``torch.nn.GRU`` module which we "
"feed a batch of sentences (vectors of word embeddings) and it internally "
"iterates through the sentences one token at a time calculating the hidden "
"states. We initialize this module to be bidirectional, meaning that we have "
"two independent GRUs: one that iterates through the sequences in "
"chronological order, and another that iterates in reverse order. We "
"ultimately return the sum of these two GRUs’ outputs. Since our model was "
"trained using batching, our ``EncoderRNN`` model’s ``forward`` function "
"expects a padded input batch. To batch variable-length sentences, we allow a"
" maximum of *MAX_LENGTH* tokens in a sentence, and all sentences in the "
"batch that have less than *MAX_LENGTH* tokens are padded at the end with our"
" dedicated *PAD_token* tokens. To use padded batches with a PyTorch RNN "
"module, we must wrap the forward pass call with "
"``torch.nn.utils.rnn.pack_padded_sequence`` and "
"``torch.nn.utils.rnn.pad_packed_sequence`` data transformations. Note that "
"the ``forward`` function also takes an ``input_lengths`` list, which "
"contains the length of each sentence in the batch. This input is used by the"
" ``torch.nn.utils.rnn.pack_padded_sequence`` function when padding."
msgstr ""
"我们使用 ``torch.nn.GRU`` 模块实现了编码器的 "
"RNN，该模块接收一个句子批次（单词嵌入的向量），并内部按顺序逐个标记计算隐藏状态。我们将该模块初始化为双向模式，这意味着我们有两个独立的 "
"GRU：一个按照时间顺序迭代序列，另一个按照反向顺序迭代序列。最终，我们返回这两个 GRU 输出的总和。由于我们的模型是通过批处理训练的，因此 "
"``EncoderRNN`` 模型的 ``forward`` 方法期望接收到一个填充过的输入批次。为了批量化可变长度的句子，我们允许句子中最多包含 "
"*MAX_LENGTH* 个标记，所有少于 *MAX_LENGTH* 个标记的句子将在结尾处用我们专门的 *PAD_token* "
"标记进行填充。为了在使用 PyTorch RNN 模块时处理填充批次，我们需要使用 "
"``torch.nn.utils.rnn.pack_padded_sequence`` 和 "
"``torch.nn.utils.rnn.pad_packed_sequence`` 数据转换包装前向传递调用。注意，``forward`` "
"方法还接受一个 ``input_lengths`` 列表，其中包含批次中每个句子的长度。此输入由 "
"``torch.nn.utils.rnn.pack_padded_sequence`` 函数在进行填充时使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "TorchScript Notes:"
msgstr "TorchScript 注意事项："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Since the encoder’s ``forward`` function does not contain any data-dependent"
" control flow, we will use **tracing** to convert it to script mode. When "
"tracing a module, we can leave the module definition as-is. We will "
"initialize all models towards the end of this document before we run "
"evaluations."
msgstr ""
"由于编码器的 ``forward`` 方法不包含任何数据相关的控制流，因此我们将使用 **tracing** "
"来将其转换为脚本模式。在跟踪模块时，我们可以保持模块定义不变。我们将在本文档的末尾初始化所有模型，然后再运行评估。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define Decoder’s Attention Module"
msgstr "定义解码器的注意力模块"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we’ll define our attention module (``Attn``). Note that this module "
"will be used as a submodule in our decoder model. Luong et al. consider "
"various “score functions”, which take the current decoder RNN output and the"
" entire encoder output, and return attention “energies”. This attention "
"energies tensor is the same size as the encoder output, and the two are "
"ultimately multiplied, resulting in a weighted tensor whose largest values "
"represent the most important parts of the query sentence at a particular "
"time-step of decoding."
msgstr ""
"接下来，我们将定义注意力模块（``Attn``）。注意，此模块将用作解码器模型中的子模块。Luong "
"等人考虑了各种“评分函数”，这些函数会接收当前解码器 RNN "
"的输出和整个编码器的输出，并返回注意力“能量”。此注意力能量张量与编码器输出的大小相同，两者最终相乘，结果是一个加权张量，其最大值表示查询句子在解码的特定时间步中最重要的部分。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define Decoder"
msgstr "定义解码器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Similarly to the ``EncoderRNN``, we use the ``torch.nn.GRU`` module for our "
"decoder’s RNN. This time, however, we use a unidirectional GRU. It is "
"important to note that unlike the encoder, we will feed the decoder RNN one "
"word at a time. We start by getting the embedding of the current word and "
"applying a `dropout "
"<https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout>`__."
" Next, we forward the embedding and the last hidden state to the GRU and "
"obtain a current GRU output and hidden state. We then use our ``Attn`` "
"module as a layer to obtain the attention weights, which we multiply by the "
"encoder’s output to obtain our attended encoder output. We use this attended"
" encoder output as our ``context`` tensor, which represents a weighted sum "
"indicating what parts of the encoder’s output to pay attention to. From "
"here, we use a linear layer and softmax normalization to select the next "
"word in the output sequence."
msgstr ""
"与 ``EncoderRNN`` 类似，我们使用 ``torch.nn.GRU`` 模块作为解码器的 RNN。然而，这次我们只使用单向 "
"GRU。需要注意的是，不同于编码器，我们会一次喂给解码器 RNN 一个单词。首先我们获取当前单词的嵌入，并应用 `dropout "
"<https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout>`__。接下来，我们将嵌入和最后的隐藏状态传递给"
" GRU，并获取当前的 GRU 输出和隐藏状态。然后我们使用 ``Attn`` "
"模块作为一层来获得注意力权重，这些权重与编码器的输出相乘以获得我们关注的编码器输出。我们使用此关注的编码器输出作为 ``context`` "
"张量，该张量表示一个加权和，用于指示编码器输出的哪些部分需要关注。从这里，我们使用线性层和 softmax 正规化来选择输出序列中的下一个单词。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Greedy Search Decoder"
msgstr "贪婪搜索解码器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As in the chatbot tutorial, we use a ``GreedySearchDecoder`` module to "
"facilitate the actual decoding process. This module has the trained encoder "
"and decoder models as attributes, and drives the process of encoding an "
"input sentence (a vector of word indexes), and iteratively decoding an "
"output response sequence one word (word index) at a time."
msgstr ""
"与聊天机器人教程一样，我们使用 ``GreedySearchDecoder`` "
"模块来方便实际解码过程。此模块将训练的编码器和解码器模型作为属性，并驱动将输入句子（单词索引的向量）编码，以及迭代地解码输出响应序列的过程，一次一个单词（单词索引）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Encoding the input sequence is straightforward: simply forward the entire "
"sequence tensor and its corresponding lengths vector to the ``encoder``. It "
"is important to note that this module only deals with one input sequence at "
"a time, **NOT** batches of sequences. Therefore, when the constant **1** is "
"used for declaring tensor sizes, this corresponds to a batch size of 1. To "
"decode a given decoder output, we must iteratively run forward passes "
"through our decoder model, which outputs softmax scores corresponding to the"
" probability of each word being the correct next word in the decoded "
"sequence. We initialize the ``decoder_input`` to a tensor containing an "
"*SOS_token*. After each pass through the ``decoder``, we *greedily* append "
"the word with the highest softmax probability to the ``decoded_words`` list."
" We also use this word as the ``decoder_input`` for the next iteration. The "
"decoding process terminates either if the ``decoded_words`` list has reached"
" a length of *MAX_LENGTH* or if the predicted word is the *EOS_token*."
msgstr ""
"编码输入序列很简单：只需将整个序列张量及其对应的长度向量传递到 "
"``encoder``。需要注意此模块一次只处理一个输入序列，**而不是**序列批次。因此，当常数 **1** 用于声明张量大小时，它对应于批量大小为 "
"1。要解码给定解码器的输出，我们必须迭代地通过解码器模型运行前向传递，该模型输出与每个单词是解码序列中正确的下一个单词的概率相对应的 softmax "
"分数。我们将 ``decoder_input`` 初始化为一个包含 *SOS_token* 的张量。每次通过解码器后，我们将具有最高 softmax "
"概率的单词“贪婪地”附加到 ``decoded_words`` 列表中。我们还将此单词用作下一次迭代的 "
"``decoder_input``。解码过程的结束条件是 ``decoded_words`` 列表已达到 *MAX_LENGTH* 长度，或预测的单词为"
" *EOS_token*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``forward`` method of this module involves iterating over the range of "
":math:`[0, max\\_length)` when decoding an output sequence one word at a "
"time. Because of this, we should use **scripting** to convert this module to"
" TorchScript. Unlike with our encoder and decoder models, which we can "
"trace, we must make some necessary changes to the ``GreedySearchDecoder`` "
"module in order to initialize an object without error. In other words, we "
"must ensure that our module adheres to the rules of the TorchScript "
"mechanism, and does not utilize any language features outside of the subset "
"of Python that TorchScript includes."
msgstr ""
"该模块的 ``forward`` 方法在解码输出序列时涉及对范围 :math:`[0, max\\_length)` "
"的迭代，因为我们一次解码一个单词。因此，我们应该使用 **scripting** 将此模块转换为 "
"TorchScript。不像我们的编码器和解码器模型，我们可以跟踪，在初始化对象时，我们需要对 ``GreedySearchDecoder`` "
"模块进行一些必要的更改，以确保不会报错。换句话说，我们必须确保模块遵守 TorchScript 机制的规则，并且不会使用任何超出 TorchScript"
" 所支持的 Python 子集的语言功能。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To get an idea of some manipulations that may be required, we will go over "
"the diffs between the ``GreedySearchDecoder`` implementation from the "
"chatbot tutorial and the implementation that we use in the cell below. Note "
"that the lines highlighted in red are lines removed from the original "
"implementation and the lines highlighted in green are new."
msgstr ""
"为了了解一些可能需要的操作，我们将比较聊天机器人教程中的 ``GreedySearchDecoder`` "
"实现和我们在以下单元中使用的实现的异同。请注意，红色高亮的行是从原始实现中删除的行，绿色高亮的行是新添加的行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "diff"
msgstr "diff"

#: ../../beginner/vt_tutorial.rst:783
msgid "Changes:"
msgstr "更改："

#: ../../beginner/vt_tutorial.rst:783
msgid "Added ``decoder_n_layers`` to the constructor arguments"
msgstr "向构造函数参数添加 ``decoder_n_layers``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This change stems from the fact that the encoder and decoder models that we "
"pass to this module will be a child of ``TracedModule`` (not ``Module``). "
"Therefore, we cannot access the decoder’s number of layers with "
"``decoder.n_layers``. Instead, we plan for this, and pass this value in "
"during module construction."
msgstr ""
"此更改源于我们传递给此模块的编码器和解码器模型将是 ``TracedModule`` 的子模块（而不是 ``Module``）。因此，我们无法通过 "
"``decoder.n_layers`` 来访问解码器的层数。相反，我们对此进行了规划，并在模块构建过程中传入此值。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Store away new attributes as constants"
msgstr "将新的属性存储为常量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the original implementation, we were free to use variables from the "
"surrounding (global) scope in our ``GreedySearchDecoder``\\ ’s ``forward`` "
"method. However, now that we are using scripting, we do not have this "
"freedom, as the assumption with scripting is that we cannot necessarily hold"
" on to Python objects, especially when exporting. An easy solution to this "
"is to store these values from the global scope as attributes to the module "
"in the constructor, and add them to a special list called ``__constants__`` "
"so that they can be used as literal values when constructing the graph in "
"the ``forward`` method. An example of this usage is on NEW line 19, where "
"instead of using the ``device`` and ``SOS_token`` global values, we use our "
"constant attributes ``self._device`` and ``self._SOS_token``."
msgstr ""
"在原始实现中，我们可以自由使用 ``GreedySearchDecoder`` 的 ``forward`` "
"方法中周围（全局）范围的变量。然而现在我们使用 scripting，因为 scripting 假设我们不能必然地保留 Python "
"对象，尤其是在导出时。一个简单的解决方案是将这些全局范围的值作为属性存储到模块中，并将它们添加到一个特殊列表 ``__constants__`` "
"中，以便在 ``forward`` 方法构造图时将其用作字面值。在 NEW 的第 19 行中，我们不仅使用了 ``device`` 和 "
"``SOS_token`` 的全局值，而且使用了作为常量属性的 ``self._device`` 和 ``self._SOS_token``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Enforce types of ``forward`` method arguments"
msgstr "强制规定 ``forward`` 方法参数的类型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By default, all parameters to a TorchScript function are assumed to be "
"Tensor. If we need to pass an argument of a different type, we can use "
"function type annotations as introduced in `PEP 3107 "
"<https://www.python.org/dev/peps/pep-3107/>`__. In addition, it is possible "
"to declare arguments of different types using Mypy-style type annotations "
"(see `doc <https://pytorch.org/docs/master/jit.html#types>`__)."
msgstr ""
"默认情况下，所有 TorchScript 函数的参数都假定为 Tensor。如果需要传递不同类型的参数，可以使用 PEP 3107 "
"中介绍的函数类型注解。此外，还可以使用 Mypy 样式的类型注解来声明不同类型的参数（详情见 `文档 "
"<https://pytorch.org/docs/master/jit.html#types>`__）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Change initialization of ``decoder_input``"
msgstr "更改 ``decoder_input`` 的初始化"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the original implementation, we initialized our ``decoder_input`` tensor "
"with ``torch.LongTensor([[SOS_token]])``. When scripting, we are not allowed"
" to initialize tensors in a literal fashion like this. Instead, we can "
"initialize our tensor with an explicit torch function such as "
"``torch.ones``. In this case, we can easily replicate the scalar "
"``decoder_input`` tensor by multiplying 1 by our SOS_token value stored in "
"the constant ``self._SOS_token``."
msgstr ""
"在原始实现中，我们通过 ``torch.LongTensor([[SOS_token]])`` 初始化了 ``decoder_input`` "
"张量。当使用脚本时，不允许以字面方式初始化张量。相反，我们可以使用显式的 torch 函数（如 "
"``torch.ones``）来初始化张量。在这种情况下，我们可以通过将常量 ``self._SOS_token`` 的 SOS_token 值乘以 "
"1，轻松复制标量 ``decoder_input`` 张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Evaluating an Input"
msgstr "评估输入"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we define some functions for evaluating an input. The ``evaluate`` "
"function takes a normalized string sentence, processes it to a tensor of its"
" corresponding word indexes (with batch size of 1), and passes this tensor "
"to a ``GreedySearchDecoder`` instance called ``searcher`` to handle the "
"encoding/decoding process. The searcher returns the output word index vector"
" and a scores tensor corresponding to the softmax scores for each decoded "
"word token. The final step is to convert each word index back to its string "
"representation using ``voc.index2word``."
msgstr ""
"接下来，我们定义一些函数来评估输入。``evaluate`` 函数接收一个规范化的字符串句子，将其处理为对应单词索引的张量（批量大小为 "
"1），并将该张量传递给一个名为 ``searcher`` 的 ``GreedySearchDecoder`` "
"实例来处理编码/解码流程。搜索器返回输出单词索引向量和一个对应于每个解码单词标记的 softmax 分数张量。最后一步是使用 "
"``voc.index2word`` 将每个单词索引转换回其字符串表示形式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We also define two functions for evaluating an input sentence. The "
"``evaluateInput`` function prompts a user for an input, and evaluates it. It"
" will continue to ask for another input until the user enters ‘q’ or ‘quit’."
msgstr ""
"我们还定义了两个函数来评估输入句子。``evaluateInput`` 函数会提示用户输入内容并进行评估。它会不断要求输入，直到用户输入 'q' 或 "
"'quit'。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``evaluateExample`` function simply takes a string input sentence as an "
"argument, normalizes it, evaluates it, and prints the response."
msgstr "``evaluateExample`` 函数只需要一个字符串输入句子作为参数，规范化后进行评估，并打印响应。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Load Pretrained Parameters"
msgstr "加载预训练参数"

#: ../../beginner/vt_tutorial.rst:783
msgid "No, let's load our model!"
msgstr "现在，让我们加载模型！"

#: ../../beginner/vt_tutorial.rst:783
msgid "Use hosted model"
msgstr "使用托管模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "To load the hosted model:"
msgstr "要加载托管模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Download the model `here "
"<https://download.pytorch.org/models/tutorials/4000_checkpoint.tar>`__."
msgstr ""
"在 `这里 <https://download.pytorch.org/models/tutorials/4000_checkpoint.tar>`__"
" 下载模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Set the ``loadFilename`` variable to the path to the downloaded checkpoint "
"file."
msgstr "将 ``loadFilename`` 变量设置为下载的检查点文件的路径。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Leave the ``checkpoint = torch.load(loadFilename)`` line uncommented, as the"
" hosted model was trained on CPU."
msgstr "保持 ``checkpoint = torch.load(loadFilename)`` 行未注释，因为托管模型是在 CPU 上训练的。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Use your own model"
msgstr "使用您自己的模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "To load your own pretrained model:"
msgstr "要加载您自己的预训练模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Set the ``loadFilename`` variable to the path to the checkpoint file that "
"you wish to load. Note that if you followed the convention for saving the "
"model from the chatbot tutorial, this may involve changing the "
"``model_name``, ``encoder_n_layers``, ``decoder_n_layers``, ``hidden_size``,"
" and ``checkpoint_iter`` (as these values are used in the model path)."
msgstr ""
"将 ``loadFilename`` 变量设置为您希望加载的检查点文件的路径。注意，如果您遵循聊天机器人教程中保存模型的约定，这可能需要更改 "
"``model_name``、``encoder_n_layers``、``decoder_n_layers``、``hidden_size`` 和 "
"``checkpoint_iter``（因为这些值在模型路径中使用）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you trained the model on a CPU, make sure that you are opening the "
"checkpoint with the ``checkpoint = torch.load(loadFilename)`` line. If you "
"trained the model on a GPU and are running this tutorial on a CPU, uncomment"
" the ``checkpoint = torch.load(loadFilename, "
"map_location=torch.device('cpu'))`` line."
msgstr ""
"如果您在 CPU 上训练模型，请确保您使用 ``checkpoint = torch.load(loadFilename)`` 行打开检查点。如果您在 "
"GPU 上训练模型，而正在使用此教程的 CPU，请取消注释 ``checkpoint = torch.load(loadFilename, "
"map_location=torch.device('cpu'))`` 行。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Notice that we initialize and load parameters into our encoder and decoder "
"models as usual. If you are using tracing mode(``torch.jit.trace``) for some"
" part of your models, you must call ``.to(device)`` to set the device "
"options of the models and ``.eval()`` to set the dropout layers to test mode"
" **before** tracing the models. `TracedModule` objects do not inherit the "
"``to`` or ``eval`` methods. Since in this tutorial we are only using "
"scripting instead of tracing, we only need to do this before we do "
"evaluation (which is the same as we normally do in eager mode)."
msgstr ""
"注意，我们像往常一样初始化并加载参数到我们的编码器和解码器模型中。如果您对某些模型部分使用了跟踪模式（``torch.jit.trace``），在跟踪模型之前必须调用"
" ``.to(device)`` 设置模型的设备选项，以及调用 ``.eval()`` 将 dropout "
"层设置为测试模式。`TracedModule` 对象不继承 ``to`` 或 ``eval`` 方法。由于在本教程中我们仅使用 scripting 而非"
" tracing，我们只需要在评估之前进行这些操作（与通常的 eager 模式相同）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Convert Model to TorchScript"
msgstr "将模型转换为 TorchScript"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As previously mentioned, to convert the encoder model to TorchScript, we use"
" **scripting**. The encoder model takes an input sequence and a "
"corresponding lengths tensor. Therefore, we create an example input sequence"
" tensor ``test_seq``, which is of appropriate size (MAX_LENGTH, 1), contains"
" numbers in the appropriate range :math:`[0, voc.num\\_words)`, and is of "
"the appropriate type (int64). We also create a ``test_seq_length`` scalar "
"which realistically contains the value corresponding to how many words are "
"in the ``test_seq``. The next step is to use the ``torch.jit.trace`` "
"function to trace the model. Notice that the first argument we pass is the "
"module that we want to trace, and the second is a tuple of arguments to the "
"module’s ``forward`` method."
msgstr ""
"如前所述，要将编码器模型转换为 TorchScript，我们使用 "
"**scripting**。编码器模型接收一个输入序列和一个对应的长度张量。因此，我们创建一个样例输入序列张量 "
"``test_seq``，这个张量具有适当的大小（MAX_LENGTH, 1），包含适当范围 :math:`[0, voc.num\\_words)` "
"的数字，且具有适当的类型（int64）。我们还创建了一个 ``test_seq_length`` 标量，该标量实际上包含与 ``test_seq`` "
"中单词数量对应的值。下一步是使用 ``torch.jit.trace`` 函数跟踪模型。注意我们传递的第一个参数是我们想要跟踪的模块，第二个参数是模块 "
"``forward`` 方法的参数元组。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We perform the same process for tracing the decoder as we did for the "
"encoder. Notice that we call forward on a set of random inputs to the "
"traced_encoder to get the output that we need for the decoder. This is not "
"required, as we could also simply manufacture a tensor of the correct shape,"
" type, and value range. This method is possible because in our case we do "
"not have any constraints on the values of the tensors because we do not have"
" any operations that could fault on out-of-range inputs."
msgstr ""
"我们对解码器进行的跟踪过程与对编码器进行的过程相同。注意，我们通过在 traced_encoder 上使用一组随机输入调用 forward "
"方法来获取解码器所需的输出。这不是必须的，因为我们同样可以简单地制造一个具有正确形状、类型和值范围的张量。该方法可行，因为在我们的情况下，张量的值没有任何限制，因为我们没有任何可能因输入值超出范围而出错的操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid "GreedySearchDecoder"
msgstr "贪婪搜索解码器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Recall that we scripted our searcher module due to the presence of data-"
"dependent control flow. In the case of scripting, we do necessary language "
"changes to make sure the implementation complies with TorchScript. We "
"initialize the scripted searcher the same way that we would initialize an "
"unscripted variant."
msgstr ""
"记得我们因为存在数据依赖的控制流而对搜索模块进行了脚本化。在脚本化的情况下，我们需要做必要的语言更改，以确保实现符合 TorchScript "
"的要求。我们初始化脚本化的搜索器与初始化未脚本化的变体方式相同。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Print Graphs"
msgstr "打印图表"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that our models are in TorchScript form, we can print the graphs of each"
" to ensure that we captured the computational graph appropriately. Since "
"TorchScript allow us to recursively compile the whole model hierarchy and "
"inline the ``encoder`` and ``decoder`` graph into a single graph, we just "
"need to print the `scripted_searcher` graph"
msgstr ""
"现在我们的模型已转换为 TorchScript 形式，我们可以打印每个模型的图表，以确保我们正确捕获了计算图。由于 TorchScript "
"允许我们递归编译整个模型层级，并将 `encoder` 和 `decoder` 的图嵌入到一个单一图中，我们只需要打印 "
"`scripted_searcher` 的图表即可。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we will run evaluation of the chatbot model using the TorchScript "
"models. If converted correctly, the models will behave exactly as they would"
" in their eager-mode representation."
msgstr "最后，我们将使用 TorchScript 模型运行聊天机器人模型的评估。如果转换正确，模型的行为将与它们在急迫模式的表示中完全一致。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By default, we evaluate a few common query sentences. If you want to chat "
"with the bot yourself, uncomment the ``evaluateInput`` line and give it a "
"spin."
msgstr "默认情况下，我们会评估几个常见的查询语句。如果你想亲自与机器人聊天，可以取消注释 ``evaluateInput`` 行并尝试一下。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save Model"
msgstr "保存模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have successfully converted our model to TorchScript, we will "
"serialize it for use in a non-Python deployment environment. To do this, we "
"can simply save our ``scripted_searcher`` module, as this is the user-facing"
" interface for running inference against the chatbot model. When saving a "
"Script module, use script_module.save(PATH) instead of torch.save(model, "
"PATH)."
msgstr ""
"现在我们已经成功将模型转换为 TorchScript，我们将对其进行序列化，以便在非 Python 的部署环境中使用。为此，我们可以简单地保存 "
"``scripted_searcher`` 模块，因为这是进行聊天机器人模型推理时的用户接口。在保存 Script 模块时，应使用 "
"script_module.save(PATH) 而不是 torch.save(model, PATH)。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: "
"deploy_seq2seq_hybrid_frontend_tutorial.py "
"<deploy_seq2seq_hybrid_frontend_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: deploy_seq2seq_hybrid_frontend_tutorial.py "
"<deploy_seq2seq_hybrid_frontend_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: "
"deploy_seq2seq_hybrid_frontend_tutorial.ipynb "
"<deploy_seq2seq_hybrid_frontend_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: "
"deploy_seq2seq_hybrid_frontend_tutorial.ipynb "
"<deploy_seq2seq_hybrid_frontend_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch Distributed Overview"
msgstr "PyTorch 分布式概览"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author**: `Will Constable <https://github.com/wconstab/>`_"
msgstr "**作者**: `Will Constable <https://github.com/wconstab/>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/beginner_source/dist_overview.rst>`__."
msgstr ""
"|edit| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/beginner_source/dist_overview.rst>`__"
" 上查看并编辑本教程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is the overview page for the ``torch.distributed`` package. The goal of"
" this page is to categorize documents into different topics and briefly "
"describe each of them. If this is your first time building distributed "
"training applications using PyTorch, it is recommended to use this document "
"to navigate to the technology that can best serve your use case."
msgstr ""
"这是 ``torch.distributed`` 包的概览页面。此页面的目标是将文档分类为不同主题并简要描述每个主题。如果这是你第一次使用 "
"PyTorch 创建分布式训练应用程序，建议你使用本文档来导航到最适合的技术。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The PyTorch Distributed library includes a collective of parallelism "
"modules, a communications layer, and infrastructure for launching and "
"debugging large training jobs."
msgstr "PyTorch 分布式库包括并行模块集合、通信层以及用于启动和调试大型训练任务的基础设施。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Parallelism APIs"
msgstr "并行性 API"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"These Parallelism Modules offer high-level functionality and compose with "
"existing models:"
msgstr "这些并行模块提供高级功能，并与现有模型相互配合："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Distributed Data-Parallel (DDP) "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"
msgstr ""
"`分布式数据并行 (DDP) "
"<https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Fully Sharded Data-Parallel Training (FSDP) "
"<https://pytorch.org/docs/stable/fsdp.html>`__"
msgstr "`完全分片数据并行 (FSDP) <https://pytorch.org/docs/stable/fsdp.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Tensor Parallel (TP) "
"<https://pytorch.org/docs/stable/distributed.tensor.parallel.html>`__"
msgstr ""
"`张量并行 (TP) "
"<https://pytorch.org/docs/stable/distributed.tensor.parallel.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Pipeline Parallel (PP) "
"<https://pytorch.org/docs/main/distributed.pipelining.html>`__"
msgstr ""
"`管道并行 (PP) <https://pytorch.org/docs/main/distributed.pipelining.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Sharding primitives"
msgstr "分片原语"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``DTensor`` and ``DeviceMesh`` are primitives used to build parallelism in "
"terms of sharded or replicated tensors on N-dimensional process groups."
msgstr "``DTensor`` 和 ``DeviceMesh`` 是用于在N维进程组上构建分片或复制张量的并行性原语。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DTensor "
"<https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md>`__"
" represents a tensor that is sharded and/or replicated, and communicates "
"automatically to reshard tensors as needed by operations."
msgstr ""
"`DTensor "
"<https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md>`__"
" 表示一个被分片和/或复制的张量，并根据操作的需要自动通信以重新分片张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`DeviceMesh <https://pytorch.org/docs/stable/distributed.html#devicemesh>`__"
" abstracts the accelerator device communicators into a multi-dimensional "
"array, which manages the underlying ``ProcessGroup`` instances for "
"collective communications in multi-dimensional parallelisms.  Try out our "
"`Device Mesh Recipe "
"<https://pytorch.org/tutorials/recipes/distributed_device_mesh.html>`__ to "
"learn more."
msgstr ""
"`DeviceMesh <https://pytorch.org/docs/stable/distributed.html#devicemesh>`__"
" 将加速器设备通信抽象为一个多维数组，它管理多维并行中用于集合通信的基础 ``ProcessGroup`` 实例。尝试我们的 `设备网格配方 "
"<https://pytorch.org/tutorials/recipes/distributed_device_mesh.html>`__ "
"以了解更多信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Communications APIs"
msgstr "通信 API"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `PyTorch distributed communication layer (C10D) "
"<https://pytorch.org/docs/stable/distributed.html>`__ offers both collective"
" communication APIs (e.g., `all_reduce "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce>`__"
msgstr ""
"`PyTorch 分布式通信层 (C10D) <https://pytorch.org/docs/stable/distributed.html>`__"
" 提供集合通信 API（例如 `all_reduce "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_reduce>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"and `all_gather "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather>`__)"
" and P2P communication APIs (e.g., `send "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.send>`__"
" and `isend "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.isend>`__),"
" which are used under the hood in all of the parallelism implementations. "
"`Writing Distributed Applications with PyTorch "
"<../intermediate/dist_tuto.html>`__ shows examples of using c10d "
"communication APIs."
msgstr ""
"和 `all_gather "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.all_gather>`__）以及点对点通信"
" API（例如 `send "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.send>`__"
" 和 `isend "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.isend>`__），它们在所有并行实现中被底层使用。`使用"
" PyTorch 编写分布式应用程序 <../intermediate/dist_tuto.html>`__ 显示了使用 c10d 通信 API "
"的示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Launcher"
msgstr "启动器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__ is a widely-"
"used launcher script, which spawns processes on the local and remote "
"machines for running distributed PyTorch programs."
msgstr ""
"`torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__ "
"是一个广泛使用的启动脚本，可在本地和远程机器上生成进程以运行分布式 PyTorch 程序。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Applying Parallelism To Scale Your Model"
msgstr "将并行性应用于扩展模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Data Parallelism is a widely adopted single-program multiple-data training "
"paradigm where the model is replicated on every process, every model replica"
" computes local gradients for a different set of input data samples, "
"gradients are averaged within the data-parallel communicator group before "
"each optimizer step."
msgstr ""
"数据并行是一种广泛采用的单程序多数据训练范式，其中模型在每个进程上复制，每个模型副本为一组不同的输入数据样本计算本地梯度，在每次优化器步骤前，梯度在数据并行通信组中进行平均。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Model Parallelism techniques (or Sharded Data Parallelism) are required when"
" a model doesn't fit in GPU, and can be combined together to form multi-"
"dimensional (N-D) parallelism techniques."
msgstr "当模型无法适应 GPU 时，需要使用模型并行技术（或分片数据并行），并可以结合起来形成多维（N-D）并行技术。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When deciding what parallelism techniques to choose for your model, use "
"these common guidelines:"
msgstr "在选择模型的并行技术时，可遵循以下常见准则："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use `DistributedDataParallel (DDP) "
"<https://pytorch.org/docs/stable/notes/ddp.html>`__, if your model fits in a"
" single GPU but you want to easily scale up training using multiple GPUs."
msgstr ""
"如果模型适合单个 GPU，但你想轻松通过多个 GPU 扩展训练，请使用 `分布式数据并行 (DDP) "
"<https://pytorch.org/docs/stable/notes/ddp.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use `torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__, to "
"launch multiple pytorch processes if you are using more than one node."
msgstr ""
"如果使用多个节点，请使用 `torchrun <https://pytorch.org/docs/stable/elastic/run.html>`__"
" 启动多个 PyTorch 进程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See also: `Getting Started with Distributed Data Parallel "
"<../intermediate/ddp_tutorial.html>`__"
msgstr "另请参阅：`分布式数据并行入门指南 <../intermediate/ddp_tutorial.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use `FullyShardedDataParallel (FSDP) "
"<https://pytorch.org/docs/stable/fsdp.html>`__ when your model cannot fit on"
" one GPU."
msgstr ""
"当模型无法适应单个 GPU 时，请使用 `完全分片数据并行 (FSDP) "
"<https://pytorch.org/docs/stable/fsdp.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See also: `Getting Started with FSDP "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__"
msgstr ""
"另请参阅：`FSDP 入门指南 "
"<https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use `Tensor Parallel (TP) "
"<https://pytorch.org/docs/stable/distributed.tensor.parallel.html>`__ and/or"
" `Pipeline Parallel (PP) "
"<https://pytorch.org/docs/main/distributed.pipelining.html>`__ if you reach "
"scaling limitations with FSDP."
msgstr ""
"如果达到 FSDP 的扩展限制，请使用 `张量并行 (TP) "
"<https://pytorch.org/docs/stable/distributed.tensor.parallel.html>`__ 和/或 "
"`管道并行 (PP) <https://pytorch.org/docs/main/distributed.pipelining.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Try our `Tensor Parallelism Tutorial "
"<https://pytorch.org/tutorials/intermediate/TP_tutorial.html>`__"
msgstr ""
"尝试我们的 `张量并行教程 "
"<https://pytorch.org/tutorials/intermediate/TP_tutorial.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See also: `TorchTitan end to end example of 3D parallelism "
"<https://github.com/pytorch/torchtitan>`__"
msgstr ""
"另请参阅：`TorchTitan 三维并行的端到端示例 <https://github.com/pytorch/torchtitan>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Data-parallel training also works with `Automatic Mixed Precision (AMP) "
"<https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-"
"multiple-gpus>`__."
msgstr ""
"数据并行训练也支持 `自动混合精度 (AMP) "
"<https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-"
"multiple-gpus>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch Distributed Developers"
msgstr "PyTorch 分布式开发者"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you'd like to contribute to PyTorch Distributed, refer to our `Developer "
"Guide "
"<https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md>`_."
msgstr ""
"如果你想对 PyTorch 分布式进行贡献，请参考我们的 `开发者指南 "
"<https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Tensors and autograd"
msgstr "PyTorch：张量和自动梯度"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_autograd_polynomial_autograd.py`"
msgstr ":ref:`sphx_glr_beginner_examples_autograd_polynomial_autograd.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Defining New autograd Functions"
msgstr "PyTorch：定义新的自动梯度函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_autograd_polynomial_custom_function.py`"
msgstr ""
":ref:`sphx_glr_beginner_examples_autograd_polynomial_custom_function.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_examples_autograd_polynomial_autograd.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_beginner_examples_autograd_polynomial_autograd.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A third order polynomial, trained to predict :math:`y=\\sin(x)` from "
":math:`-\\pi` to :math:`\\pi` by minimizing squared Euclidean distance."
msgstr ""
"一个三阶多项式，经训练从 :math:`-\\pi` 到 :math:`\\pi` 预测 "
":math:`y=\\sin(x)`，通过最小化平方欧几里得距离。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This implementation computes the forward pass using operations on PyTorch "
"Tensors, and uses PyTorch autograd to compute gradients."
msgstr "此实现使用 PyTorch 张量上的操作计算前向传播，并使用 PyTorch 自动梯度计算梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A PyTorch Tensor represents a node in a computational graph. If ``x`` is a "
"Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor "
"holding the gradient of ``x`` with respect to some scalar value."
msgstr ""
"PyTorch 张量代表计算图中的一个节点。如果 ``x`` 是一个 Tensor 并且设置了 ``x.requires_grad=True``，那么 "
"``x.grad`` 是另一个 Tensor，保存了 ``x`` 相对于某个标量值的梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_autograd.py "
"<polynomial_autograd.py>`"
msgstr ""
":download:`下载 Python 源代码: polynomial_autograd.py <polynomial_autograd.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_autograd.ipynb "
"<polynomial_autograd.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_autograd.ipynb "
"<polynomial_autograd.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_examples_autograd_polynomial_custom_function.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_beginner_examples_autograd_polynomial_custom_function.py>`"
" 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A third order polynomial, trained to predict :math:`y=\\sin(x)` from "
":math:`-\\pi` to :math:`\\pi` by minimizing squared Euclidean distance. "
"Instead of writing the polynomial as :math:`y=a+bx+cx^2+dx^3`, we write the "
"polynomial as :math:`y=a+b P_3(c+dx)` where "
":math:`P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)` is the `Legendre "
"polynomial`_ of degree three."
msgstr ""
"一个三阶多项式，经训练从 :math:`-\\pi` 到 :math:`\\pi` 预测 "
":math:`y=\\sin(x)`，通过最小化平方欧几里得距离。我们不是采用 :math:`y=a+bx+cx^2+dx^3` 的形式，而是采用 "
":math:`y=a+b P_3(c+dx)` 的形式，其中 "
":math:`P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)` 是三阶 `勒让德多项式`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this implementation we implement our own custom autograd function to "
"perform :math:`P_3'(x)`. By mathematics, "
":math:`P_3'(x)=\\frac{3}{2}\\left(5x^2-1\\right)`"
msgstr ""
"在该实现中，我们实现了自定义的自动梯度函数来执行 :math:`P_3&apos;(x)` 的操作。根据数学公式， "
":math:`P_3&apos;(x)=\\frac{3}{2}\\left(5x^2-1\\right)`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_custom_function.py "
"<polynomial_custom_function.py>`"
msgstr ""
":download:`下载 Python 源代码: polynomial_custom_function.py "
"<polynomial_custom_function.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_custom_function.ipynb "
"<polynomial_custom_function.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_custom_function.ipynb "
"<polynomial_custom_function.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_examples_nn_dynamic_net.py>` to"
" download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_examples_nn_dynamic_net.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Control Flow + Weight Sharing"
msgstr "PyTorch：控制流 + 权重共享"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To showcase the power of PyTorch dynamic graphs, we will implement a very "
"strange model: a third-fifth order polynomial that on each forward pass "
"chooses a random number between 4 and 5 and uses that many orders, reusing "
"the same weights multiple times to compute the fourth and fifth order."
msgstr ""
"为了展示 PyTorch 动态图的能力，我们将实现一个非常特别的模型：一个三到五阶的多项式，在每次前向传播中随机选择 4 到 5 "
"之间的一个数字，并使用这么多阶数，同时多次重复使用相同的权重来计算四阶和五阶。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: dynamic_net.py <dynamic_net.py>`"
msgstr ":download:`下载 Python 源代码: dynamic_net.py <dynamic_net.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: dynamic_net.ipynb <dynamic_net.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: dynamic_net.ipynb <dynamic_net.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: optim"
msgstr "PyTorch：优化器"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_nn_polynomial_optim.py`"
msgstr ":ref:`sphx_glr_beginner_examples_nn_polynomial_optim.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Custom nn Modules"
msgstr "PyTorch：自定义 nn 模块"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_nn_polynomial_module.py`"
msgstr ":ref:`sphx_glr_beginner_examples_nn_polynomial_module.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_nn_dynamic_net.py`"
msgstr ":ref:`sphx_glr_beginner_examples_nn_dynamic_net.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: nn"
msgstr "PyTorch：nn"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_nn_polynomial_nn.py`"
msgstr ":ref:`sphx_glr_beginner_examples_nn_polynomial_nn.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_examples_nn_polynomial_module.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_examples_nn_polynomial_module.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This implementation defines the model as a custom Module subclass. Whenever "
"you want a model more complex than a simple sequence of existing Modules you"
" will need to define your model this way."
msgstr "此实现将模型定义为自定义 Module 子类。每当你需要比简单的现有模块序列更复杂的模型时，都需要这样定义模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_module.py "
"<polynomial_module.py>`"
msgstr ""
":download:`下载 Python 源代码: polynomial_module.py <polynomial_module.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_module.ipynb "
"<polynomial_module.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_module.ipynb "
"<polynomial_module.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_examples_nn_polynomial_nn.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里<sphx_glr_download_beginner_examples_nn_polynomial_nn.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A third order polynomial, trained to predict :math:`y=\\sin(x)` from "
":math:`-\\pi` to :math:`pi` by minimizing squared Euclidean distance."
msgstr ""
"一个三阶多项式，训练来预测 :math:`y=\\sin(x)`，范围为 :math:`-\\pi` 到 "
":math:`pi`，通过最小化平方欧几里得距离。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This implementation uses the nn package from PyTorch to build the network. "
"PyTorch autograd makes it easy to define computational graphs and take "
"gradients, but raw autograd can be a bit too low-level for defining complex "
"neural networks; this is where the nn package can help. The nn package "
"defines a set of Modules, which you can think of as a neural network layer "
"that produces output from input and may have some trainable weights."
msgstr ""
"此实现使用 PyTorch 的 nn 包构建网络。PyTorch "
"的自动梯度使定义计算图和计算梯度变得简单，但直接使用原始的自动梯度可能对定义复杂的神经网络来说太低级了；这时 nn 包可以帮助解决问题。nn "
"包定义了一组模块，你可以将这些模块看作是神经网络的一层，它从输入生成输出，并可能拥有一些可训练的权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.560 seconds)"
msgstr "**脚本的总运行时间:**（0 分钟 0.560 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_nn.py <polynomial_nn.py>`"
msgstr ":download:`下载 Python 源代码: polynomial_nn.py <polynomial_nn.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_nn.ipynb "
"<polynomial_nn.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_nn.ipynb <polynomial_nn.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_examples_nn_polynomial_optim.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里<sphx_glr_download_beginner_examples_nn_polynomial_optim.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This implementation uses the nn package from PyTorch to build the network."
msgstr "此实现使用 PyTorch 的 nn 包构建网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Rather than manually updating the weights of the model as we have been "
"doing, we use the optim package to define an Optimizer that will update the "
"weights for us. The optim package defines many optimization algorithms that "
"are commonly used for deep learning, including SGD+momentum, RMSProp, Adam, "
"etc."
msgstr ""
"与我们之前手动更新模型权重相比，此处我们使用 optim 包定义一个优化器，由它来为我们更新权重。optim 包定义了许多常用于深度学习的优化算法，包括"
" SGD+动量、RMSProp、Adam 等。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_optim.py "
"<polynomial_optim.py>`"
msgstr ":download:`下载 Python 源代码: polynomial_optim.py <polynomial_optim.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_optim.ipynb "
"<polynomial_optim.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_optim.ipynb "
"<polynomial_optim.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**00:00.560** total execution time for **beginner_examples_nn** files:"
msgstr "**00:00.560** 的总执行时间对于**beginner_examples_nn**文件："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_nn_polynomial_nn.py` (``polynomial_nn.py``)"
msgstr ""
":ref:`sphx_glr_beginner_examples_nn_polynomial_nn.py` （`polynomial_nn.py`）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.560"
msgstr "00:00.560"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_nn_dynamic_net.py` (``dynamic_net.py``)"
msgstr ""
":ref:`sphx_glr_beginner_examples_nn_dynamic_net.py` （`dynamic_net.py`）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_nn_polynomial_module.py` "
"(``polynomial_module.py``)"
msgstr ""
":ref:`sphx_glr_beginner_examples_nn_polynomial_module.py` "
"（`polynomial_module.py`）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_nn_polynomial_optim.py` "
"(``polynomial_optim.py``)"
msgstr ""
":ref:`sphx_glr_beginner_examples_nn_polynomial_optim.py` "
"（`polynomial_optim.py`）"

#: ../../beginner/vt_tutorial.rst:783
msgid "Warm-up: numpy"
msgstr "热身：numpy"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_tensor_polynomial_numpy.py`"
msgstr ":ref:`sphx_glr_beginner_examples_tensor_polynomial_numpy.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Tensors"
msgstr "PyTorch：张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_examples_tensor_polynomial_tensor.py`"
msgstr ":ref:`sphx_glr_beginner_examples_tensor_polynomial_tensor.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_examples_tensor_polynomial_numpy.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里<sphx_glr_download_beginner_examples_tensor_polynomial_numpy.py>`"
" 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This implementation uses numpy to manually compute the forward pass, loss, "
"and backward pass."
msgstr "此实现使用 numpy 手动计算前向传播、损失和反向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A numpy array is a generic n-dimensional array; it does not know anything "
"about deep learning or gradients or computational graphs, and is just a way "
"to perform generic numeric computations."
msgstr "一个 numpy 数组是一个通用的 n 维数组；它不知道任何关于深度学习、梯度或计算图的信息，仅仅是进行通用数值计算的工具。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.555 seconds)"
msgstr "**脚本的总运行时间:**（0 分钟 0.555 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_numpy.py "
"<polynomial_numpy.py>`"
msgstr ":download:`下载 Python 源代码: polynomial_numpy.py <polynomial_numpy.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_numpy.ipynb "
"<polynomial_numpy.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_numpy.ipynb "
"<polynomial_numpy.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_examples_tensor_polynomial_tensor.py>` to "
"download the full example code"
msgstr ""
"点击 "
":ref:`这里<sphx_glr_download_beginner_examples_tensor_polynomial_tensor.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This implementation uses PyTorch tensors to manually compute the forward "
"pass, loss, and backward pass."
msgstr "此实现使用 PyTorch 张量手动计算前向传播、损失和反向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A PyTorch Tensor is basically the same as a numpy array: it does not know "
"anything about deep learning or computational graphs or gradients, and is "
"just a generic n-dimensional array to be used for arbitrary numeric "
"computation."
msgstr "PyTorch 张量基本与 numpy 数组相同：它不知道深度学习、计算图或梯度，仅仅是一个通用的 n 维数组可用于任意数值计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The biggest difference between a numpy array and a PyTorch Tensor is that a "
"PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, "
"just cast the Tensor to a cuda datatype."
msgstr ""
"numpy 数组与 PyTorch 张量最大的不同在于 PyTorch 张量既可以运行在 CPU 上，也可以运行在 GPU 上。要在 GPU "
"上运行操作，只需将张量转换为 cuda 数据类型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.144 seconds)"
msgstr "**脚本的总运行时间:**（0 分钟 0.144 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: polynomial_tensor.py "
"<polynomial_tensor.py>`"
msgstr ""
":download:`下载 Python 源代码: polynomial_tensor.py <polynomial_tensor.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: polynomial_tensor.ipynb "
"<polynomial_tensor.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: polynomial_tensor.ipynb "
"<polynomial_tensor.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**00:00.698** total execution time for **beginner_examples_tensor** files:"
msgstr "**00:00.698** 的总执行时间对于**beginner_examples_tensor**文件："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_tensor_polynomial_numpy.py` "
"(``polynomial_numpy.py``)"
msgstr ""
":ref:`sphx_glr_beginner_examples_tensor_polynomial_numpy.py` "
"（`polynomial_numpy.py`）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.555"
msgstr "00:00.555"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_examples_tensor_polynomial_tensor.py` "
"(``polynomial_tensor.py``)"
msgstr ""
":ref:`sphx_glr_beginner_examples_tensor_polynomial_tensor.py` "
"（`polynomial_tensor.py`）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.144"
msgstr "00:00.144"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_fgsm_tutorial.py>` to download "
"the full example code"
msgstr "点击 :ref:`这里<sphx_glr_download_beginner_fgsm_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Adversarial Example Generation"
msgstr "对抗样本生成"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author:** `Nathan Inkawhich <https://github.com/inkawhich>`__"
msgstr "**作者:** `Nathan Inkawhich <https://github.com/inkawhich>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you are reading this, hopefully you can appreciate how effective some "
"machine learning models are. Research is constantly pushing ML models to be "
"faster, more accurate, and more efficient. However, an often overlooked "
"aspect of designing and training models is security and robustness, "
"especially in the face of an adversary who wishes to fool the model."
msgstr ""
"如果你正在阅读这篇文章，希望你能够欣赏一些机器学习模型的强大。研究正在不断推动机器学习模型变得更快、更准、更高效。然而，在设计和训练模型时，被忽视的一个方面是安全性和鲁棒性，特别是在面对想欺骗模型的攻击者时。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial will raise your awareness to the security vulnerabilities of "
"ML models, and will give insight into the hot topic of adversarial machine "
"learning. You may be surprised to find that adding imperceptible "
"perturbations to an image *can* cause drastically different model "
"performance. Given that this is a tutorial, we will explore the topic via "
"example on an image classifier. Specifically, we will use one of the first "
"and most popular attack methods, the Fast Gradient Sign Attack (FGSM), to "
"fool an MNIST classifier."
msgstr ""
"本教程将提高你对机器学习模型的安全漏洞的认识，给予对抗性机器学习这一热门话题的洞见。你可能会惊讶地发现，对图像加入几乎察觉不到的扰动会大幅改变模型表现。由于这是一个教程，我们将通过一个图像分类器的示例来探索这一主题。具体来说，我们将使用一种最早且最流行的攻击方法——快速梯度符号攻击（FGSM）来欺骗"
" MNIST 分类器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Threat Model"
msgstr "威胁模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For context, there are many categories of adversarial attacks, each with a "
"different goal and assumption of the attacker’s knowledge. However, in "
"general the overarching goal is to add the least amount of perturbation to "
"the input data to cause the desired misclassification. There are several "
"kinds of assumptions of the attacker’s knowledge, two of which are: **white-"
"box** and **black-box**. A *white-box* attack assumes the attacker has full "
"knowledge and access to the model, including architecture, inputs, outputs, "
"and weights. A *black-box* attack assumes the attacker only has access to "
"the inputs and outputs of the model, and knows nothing about the underlying "
"architecture or weights. There are also several types of goals, including "
"**misclassification** and **source/target misclassification**. A goal of "
"*misclassification* means the adversary only wants the output classification"
" to be wrong but does not care what the new classification is. A "
"*source/target misclassification* means the adversary wants to alter an "
"image that is originally of a specific source class so that it is classified"
" as a specific target class."
msgstr ""
"从背景上看，对抗性攻击有许多类别，每种类别有不同的目标和对攻击者知识的假设。然而，一般来说，总体目标是对输入数据添加尽可能少的扰动以实现期望的错误分类。攻击者知识的假设有几种类型，其中两种是：**白盒**和**黑盒**。*白盒攻击*假设攻击者对模型拥有完全的知识和访问权限，包括架构、输入、输出和权重。*黑盒攻击*假设攻击者只能访问模型的输入和输出，对底层架构和权重一无所知。攻击目标也有几种类型，包括**错误分类**和**源/目标错误分类**。目标为*错误分类*意味着攻击者只希望输出分类是错误的，而不关心新的分类是什么。*源/目标错误分类*则意味着攻击者希望将原本属于某个特定源类的图像修改为被分类为某个特定目标类。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this case, the FGSM attack is a *white-box* attack with the goal of "
"*misclassification*. With this background information, we can now discuss "
"the attack in detail."
msgstr "在这种情况下，FGSM 攻击是一种*白盒*攻击，目标是*错误分类*。有了这些背景知识，现在可以详细讨论攻击方式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Fast Gradient Sign Attack"
msgstr "快速梯度符号攻击"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One of the first and most popular adversarial attacks to date is referred to"
" as the *Fast Gradient Sign Attack (FGSM)* and is described by Goodfellow "
"et. al. in `Explaining and Harnessing Adversarial Examples "
"<https://arxiv.org/abs/1412.6572>`__. The attack is remarkably powerful, and"
" yet intuitive. It is designed to attack neural networks by leveraging the "
"way they learn, *gradients*. The idea is simple, rather than working to "
"minimize the loss by adjusting the weights based on the backpropagated "
"gradients, the attack *adjusts the input data to maximize the loss* based on"
" the same backpropagated gradients. In other words, the attack uses the "
"gradient of the loss w.r.t the input data, then adjusts the input data to "
"maximize the loss."
msgstr ""
"迄今为止最早且最流行的对抗性攻击之一被称为*快速梯度符号攻击（FGSM）*，由 Goodfellow 等人在 `解释和利用对抗性样本 "
"<https://arxiv.org/abs/1412.6572>`__ "
"中描述。该攻击非常强大，同时也很直观。它旨在通过利用神经网络的学习方式（即梯度）来攻击神经网络。其思路简单，与通过根据反向传播的梯度调整权重以最小化损失相反，该攻击是使用相同的反向传播梯度来*调整输入数据以最大化损失*。换句话说，该攻击使用损失相对于输入数据的梯度，然后调整输入数据以最大化损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before we jump into the code, let’s look at the famous `FGSM "
"<https://arxiv.org/abs/1412.6572>`__ panda example and extract some "
"notation."
msgstr ""
"在我们进入代码之前，让我们看一下著名的 `FGSM <https://arxiv.org/abs/1412.6572>`__ 熊猫示例并提取一些符号。"

#: ../../beginner/vt_tutorial.rst:783
msgid "fgsm_panda_image"
msgstr "fgsm_panda_image"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"From the figure, :math:`\\mathbf{x}` is the original input image correctly "
"classified as a “panda”, :math:`y` is the ground truth label for "
":math:`\\mathbf{x}`, :math:`\\mathbf{\\theta}` represents the model "
"parameters, and :math:`J(\\mathbf{\\theta}, \\mathbf{x}, y)` is the loss "
"that is used to train the network. The attack backpropagates the gradient "
"back to the input data to calculate :math:`\\nabla_{x} J(\\mathbf{\\theta}, "
"\\mathbf{x}, y)`. Then, it adjusts the input data by a small step "
"(:math:`\\epsilon` or :math:`0.007` in the picture) in the direction (i.e. "
":math:`sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))`) that will "
"maximize the loss. The resulting perturbed image, :math:`x'`, is then "
"*misclassified* by the target network as a “gibbon” when it is still clearly"
" a “panda”."
msgstr ""
"从图中，:math:`\\mathbf{x}` 是被正确分类为“熊猫”的原始输入图像，:math:`y` 是 :math:`\\mathbf{x}` "
"的真实标签，:math:`\\mathbf{\\theta}` 表示模型的参数，而 :math:`J(\\mathbf{\\theta}, "
"\\mathbf{x}, y)` 是用于训练网络的损失。攻击将梯度反向传播回输入数据以计算 :math:`\\nabla_{x} "
"J(\\mathbf{\\theta}, \\mathbf{x}, y)`。然后它通过一个小步长（图中的 :math:`\\epsilon` 或 "
":math:`0.007`）调整输入数据，沿着方向（即 :math:`sign(\\nabla_{x} J(\\mathbf{\\theta}, "
"\\mathbf{x}, y))`）以最大化损失。最终生成的扰动图像 :math:`x&apos;` "
"被目标网络*错误分类*为“长臂猿”，尽管它仍显然是一只“熊猫”。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hopefully now the motivation for this tutorial is clear, so lets jump into "
"the implementation."
msgstr "希望现在本教程的动机已经很清晰了，那我们开始进行实现吧。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this section, we will discuss the input parameters for the tutorial, "
"define the model under attack, then code the attack and run some tests."
msgstr "在本节中，我们将讨论教程的输入参数，定义受攻击的模型，然后编写攻击代码并运行一些测试。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are only three inputs for this tutorial, and are defined as follows:"
msgstr "本教程只有三个输入参数，定义如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``epsilons`` - List of epsilon values to use for the run. It is important to"
" keep 0 in the list because it represents the model performance on the "
"original test set. Also, intuitively we would expect the larger the epsilon,"
" the more noticeable the perturbations but the more effective the attack in "
"terms of degrading model accuracy. Since the data range here is "
":math:`[0,1]`, no epsilon value should exceed 1."
msgstr ""
"``epsilons`` - 用于运行的 epsilon 值列表。确保列表中包含 0，这表示模型在原始测试集上的表现。另外，直观上我们可以预期 "
"epsilon 越大，扰动越明显，但攻击效果越显著（模型准确率下降）。由于数据范围为 :math:`[0,1]`，epsilon 值不得超过 1。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``pretrained_model`` - path to the pretrained MNIST model which was trained "
"with `pytorch/examples/mnist "
"<https://github.com/pytorch/examples/tree/master/mnist>`__. For simplicity, "
"download the pretrained model `here "
"<https://drive.google.com/file/d/1HJV2nUHJqclXQ8flKvcWmjZ-"
"OU5DGatl/view?usp=drive_link>`__."
msgstr ""
"``pretrained_model`` - 训练好的 MNIST 模型的路径，该模型使用 `pytorch/examples/mnist "
"<https://github.com/pytorch/examples/tree/master/mnist>`__ 训练。为了简便，可以从 `这里 "
"<https://drive.google.com/file/d/1HJV2nUHJqclXQ8flKvcWmjZ-"
"OU5DGatl/view?usp=drive_link>`__ 下载预训练模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Model Under Attack"
msgstr "受攻击的模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As mentioned, the model under attack is the same MNIST model from "
"`pytorch/examples/mnist "
"<https://github.com/pytorch/examples/tree/master/mnist>`__. You may train "
"and save your own MNIST model or you can download and use the provided "
"model. The *Net* definition and test dataloader here have been copied from "
"the MNIST example. The purpose of this section is to define the model and "
"dataloader, then initialize the model and load the pretrained weights."
msgstr ""
"如前所述，受攻击的模型是 `pytorch/examples/mnist "
"<https://github.com/pytorch/examples/tree/master/mnist>`__ 中的 MNIST "
"模型。你可以自己训练并保存 MNIST 模型，也可以下载并使用提供的模型。此部分的目的在于定义模型和数据加载器，然后初始化模型并加载训练好的权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid "FGSM Attack"
msgstr "FGSM 攻击"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, we can define the function that creates the adversarial examples by "
"perturbing the original inputs. The ``fgsm_attack`` function takes three "
"inputs, *image* is the original clean image (:math:`x`), *epsilon* is the "
"pixel-wise perturbation amount (:math:`\\epsilon`), and *data_grad* is "
"gradient of the loss w.r.t the input image (:math:`\\nabla_{x} "
"J(\\mathbf{\\theta}, \\mathbf{x}, y)`). The function then creates perturbed "
"image as"
msgstr ""
"现在，我们可以定义创建对抗样本的函数，通过扰动原始输入来生成对抗样本。``fgsm_attack`` 函数接收三个输入，*image* 是原始干净图片 "
"(:math:`x`)，*epsilon* 是逐像素的扰动量 (:math:`\\epsilon`)，*data_grad* 是损失相对于输入图片的梯度"
" (:math:`\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)`)。函数将生成扰动图像为："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * "
"sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))"
msgstr ""
"perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, in order to maintain the original range of the data, the perturbed "
"image is clipped to range :math:`[0,1]`."
msgstr "最后，为了保持数据的原始范围，对扰动图像进行裁剪，使其范围在 :math:`[0,1]`。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Testing Function"
msgstr "测试功能"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, the central result of this tutorial comes from the ``test`` "
"function. Each call to this test function performs a full test step on the "
"MNIST test set and reports a final accuracy. However, notice that this "
"function also takes an *epsilon* input. This is because the ``test`` "
"function reports the accuracy of a model that is under attack from an "
"adversary with strength :math:`\\epsilon`. More specifically, for each "
"sample in the test set, the function computes the gradient of the loss w.r.t"
" the input data (:math:`data\\_grad`), creates a perturbed image with "
"``fgsm_attack`` (:math:`perturbed\\_data`), then checks to see if the "
"perturbed example is adversarial. In addition to testing the accuracy of the"
" model, the function also saves and returns some successful adversarial "
"examples to be visualized later."
msgstr ""
"最后，本教程的核心结果来自``test``函数。每次调用此测试函数都会对MNIST测试集执行完整的测试步骤并报告最终准确率。然而，请注意，该函数还接受一个"
" *epsilon* 输入。这是因为``test``函数会报告模型在由强度 :math:`\\epsilon` "
"的对抗性攻击下的准确率。具体来说，对于测试集中的每个样本，该函数计算损失相对于输入数据的梯度 "
"(:math:`data\\_grad`)，然后使用``fgsm_attack``创建一个扰动图像 "
"(:math:`perturbed\\_data`)，接着检查这个扰动样本是否为对抗样本。除了测试模型的准确率外，函数还保存并返回一些成功的对抗样本，以便后续可视化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Run Attack"
msgstr "运行攻击"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The last part of the implementation is to actually run the attack. Here, we "
"run a full test step for each epsilon value in the *epsilons* input. For "
"each epsilon we also save the final accuracy and some successful adversarial"
" examples to be plotted in the coming sections. Notice how the printed "
"accuracies decrease as the epsilon value increases. Also, note the "
":math:`\\epsilon=0` case represents the original test accuracy, with no "
"attack."
msgstr ""
"实现过程的最后部分是实际运行攻击。在这里，我们对每个 *epsilons* "
"输入中的epsilon值运行完整的测试步骤。对于每个epsilon值，我们还保存最终准确率以及一些成功的对抗样本以便在后续部分绘制。注意打印出的准确率随着epsilon值的增加而下降。此外，请注意"
" :math:`\\epsilon=0` 的情况代表原始测试准确率，没有受到攻击。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Accuracy vs Epsilon"
msgstr "准确率与Epsilon"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The first result is the accuracy versus epsilon plot. As alluded to earlier,"
" as epsilon increases we expect the test accuracy to decrease. This is "
"because larger epsilons mean we take a larger step in the direction that "
"will maximize the loss. Notice the trend in the curve is not linear even "
"though the epsilon values are linearly spaced. For example, the accuracy at "
":math:`\\epsilon=0.05` is only about 4% lower than :math:`\\epsilon=0`, but "
"the accuracy at :math:`\\epsilon=0.2` is 25% lower than "
":math:`\\epsilon=0.15`. Also, notice the accuracy of the model hits random "
"accuracy for a 10-class classifier between :math:`\\epsilon=0.25` and "
":math:`\\epsilon=0.3`."
msgstr ""
"第一个结果是准确率对epsilon的曲线图。此前提到过，当epsilon增加时，我们预计测试准确率会下降。这是因为较大的epsilon意味着我们在最大化损失方向上采取了较大的步骤。注意曲线中的趋势并非线性，尽管epsilon值是线性间隔。例如，:math:`\\epsilon=0.05`"
" 的准确率比 :math:`\\epsilon=0` 仅低约4%，但 :math:`\\epsilon=0.2` 的准确率比 "
":math:`\\epsilon=0.15` 低25%。此外，注意在10类别分类器中，模型的随机准确率出现在 "
":math:`\\epsilon=0.25` 和 :math:`\\epsilon=0.3` 之间。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Sample Adversarial Examples"
msgstr "示例对抗样本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Remember the idea of no free lunch? In this case, as epsilon increases the "
"test accuracy decreases **BUT** the perturbations become more easily "
"perceptible. In reality, there is a tradeoff between accuracy degradation "
"and perceptibility that an attacker must consider. Here, we show some "
"examples of successful adversarial examples at each epsilon value. Each row "
"of the plot shows a different epsilon value. The first row is the "
":math:`\\epsilon=0` examples which represent the original “clean” images "
"with no perturbation. The title of each image shows the “original "
"classification -> adversarial classification.” Notice, the perturbations "
"start to become evident at :math:`\\epsilon=0.15` and are quite evident at "
":math:`\\epsilon=0.3`. However, in all cases humans are still capable of "
"identifying the correct class despite the added noise."
msgstr ""
"还记得“没有免费的午餐”的说法吗？在这种情况下，当epsilon增加时，测试准确率下降 **但** "
"扰动变得更加容易被感知。实际上，攻击者必须考虑准确率下降和可感知性之间的权衡。在这里，我们展示了每个epsilon值的成功对抗样本示例。每行图中的展示对应一个不同的epsilon值。第一行是"
" :math:`\\epsilon=0` 的示例，代表未经扰动的“干净”图像。每张图的标题显示“原始分类 -> 对抗分类”。注意 "
":math:`\\epsilon=0.15` 时扰动开始变得明显，而 :math:`\\epsilon=0.3` "
"时非常明显。然而，在所有情况下，人类仍然能够在加入噪声后识别正确类别。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Where to go next?"
msgstr "接下来去哪儿？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hopefully this tutorial gives some insight into the topic of adversarial "
"machine learning. There are many potential directions to go from here. This "
"attack represents the very beginning of adversarial attack research and "
"since there have been many subsequent ideas for how to attack and defend ML "
"models from an adversary. In fact, at NIPS 2017 there was an adversarial "
"attack and defense competition and many of the methods used in the "
"competition are described in this paper: `Adversarial Attacks and Defences "
"Competition <https://arxiv.org/pdf/1804.00097.pdf>`__. The work on defense "
"also leads into the idea of making machine learning models more *robust* in "
"general, to both naturally perturbed and adversarially crafted inputs."
msgstr ""
"希望本教程能够为对抗性机器学习领域提供一些洞见。从这里可以有很多潜在的方向。这种攻击代表了对抗性攻击研究的开端，之后人们针对如何攻击和防御机器学习模型提出了众多后续方案。事实上，在2017年NIPS大会上，有一个对抗攻击和防御的竞赛，竞赛所使用的许多方法都被描述在这篇论文中："
" `Adversarial Attacks and Defences Competition "
"<https://arxiv.org/pdf/1804.00097.pdf>`__ "
"。关于防御的工作也引出了一个思路，即使机器学习模型可以更加泛化**鲁棒性**，无论是在自然扰动输入还是对抗生成输入的情况下。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Another direction to go is adversarial attacks and defense in different "
"domains. Adversarial research is not limited to the image domain, check out "
"`this <https://arxiv.org/pdf/1801.01944.pdf>`__ attack on speech-to-text "
"models. But perhaps the best way to learn more about adversarial machine "
"learning is to get your hands dirty. Try to implement a different attack "
"from the NIPS 2017 competition, and see how it differs from FGSM. Then, try "
"to defend the model from your own attacks."
msgstr ""
"另一个方向是研究不同领域的对抗攻击与防御。对抗性研究并不仅限于图像领域，看看 `这个 "
"<https://arxiv.org/pdf/1801.01944.pdf>`__ "
"对语音到文本模型的攻击。但或许了解对抗性机器学习最好的方式就是亲身实践。试着用2017年NIPS竞赛中的方法实现一种不同的攻击，并观察与FGSM的差异。然后，尝试防御你自己的攻击。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A further direction to go, depending on available resources, is to modify "
"the code to support processing work in batch, in parallel, and or "
"distributed vs working on one attack at a time in the above for each "
"``epsilon test()`` loop."
msgstr ""
"进一步的方向取决于可用资源，可以修改代码支持批量处理、并行处理或者分布式处理，而不是像上面的各个``epsilon "
"test()``循环一次完成一个攻击。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: fgsm_tutorial.py <fgsm_tutorial.py>`"
msgstr ":download:`下载Python源码: fgsm_tutorial.py <fgsm_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: fgsm_tutorial.ipynb "
"<fgsm_tutorial.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: fgsm_tutorial.ipynb <fgsm_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Finetuning Torchvision Models"
msgstr "微调Torchvision模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial has been moved to "
"https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
msgstr ""
"本教程已移至 https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_flava_finetuning_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_flava_finetuning_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "TorchMultimodal Tutorial: Finetuning FLAVA"
msgstr "TorchMultimodal教程：微调FLAVA"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Multimodal AI has recently become very popular owing to its ubiquitous "
"nature, from use cases like image captioning and visual search to more "
"recent applications like image generation from text. **TorchMultimodal is a "
"library powered by Pytorch consisting of building blocks and end to end "
"examples, aiming to enable and accelerate research in multimodality**."
msgstr ""
"多模态AI因其普遍特性最近变得非常流行，从图像字幕和视觉搜索等用例到最近基于文本生成图像的应用。**TorchMultimodal是一个由PyTorch支持的库，包含构建模块和端到端示例，旨在启用和加速多模态研究。**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we will demonstrate how to use a **pretrained SoTA model "
"called** `FLAVA <https://arxiv.org/pdf/2112.04482.pdf>`__ **from "
"TorchMultimodal library to finetune on a multimodal task i.e. visual "
"question answering** (VQA). The model consists of two unimodal transformer "
"based encoders for text and image and a multimodal encoder to combine the "
"two embeddings. It is pretrained using contrastive, image text matching and "
"text, image and multimodal masking losses."
msgstr ""
"在本教程中，我们将展示如何使用 **TorchMultimodal库中的预训练的SoTA模型** `FLAVA "
"<https://arxiv.org/pdf/2112.04482.pdf>`__ **在一个多模态任务上进行微调，即视觉问答** "
"(VQA)。该模型由两个基于文本和图像的单模态Transformer编码器以及一个将两个嵌入结合的多模态编码器组成。它通过对比学习、图像文本匹配以及图像、文本和多模态掩码损失进行了预训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Installation"
msgstr "安装"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will use TextVQA dataset and ``bert tokenizer`` from Hugging Face for "
"this tutorial. So you need to install datasets and transformers in addition "
"to TorchMultimodal."
msgstr ""
"在本教程中，我们将使用来自Hugging Face的TextVQA数据集和``bert "
"tokenizer``。因此你需要安装datasets和transformers以及TorchMultimodal。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When running this tutorial in Google Colab, install the required packages by"
" creating a new cell and running the following commands:"
msgstr "如果在Google Colab中运行本教程，通过创建一个新单元格并运行以下命令来安装所需的包："

#: ../../beginner/vt_tutorial.rst:783
msgid "Steps"
msgstr "步骤"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Download the Hugging Face dataset to a directory on your computer by running"
" the following command:"
msgstr "运行以下命令，下载Hugging Face数据集到你的电脑目录："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you are running this tutorial in Google Colab, run these commands in a "
"new cell and prepend these commands with an exclamation mark (!)"
msgstr "如果在Google Colab中运行本教程，请在新单元格中运行这些命令，并在命令前加上感叹号(!)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this tutorial, we treat VQA as a classification task where the inputs "
"are images and question (text) and the output is an answer class. So we need"
" to download the vocab file with answer classes and create the answer to "
"label mapping."
msgstr ""
"对于本教程，我们将VQA视为一个分类任务，其中输入是图像和问题（文本），输出是一个答案类别。因此我们需要下载带有答案类别的词汇文件并创建答案到标签的映射。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We also load the `textvqa dataset <https://arxiv.org/pdf/1904.08920.pdf>`__ "
"containing 34602 training samples (images,questions and answers) from "
"Hugging Face"
msgstr ""
"我们还从Hugging Face加载了 `textvqa数据集 "
"<https://arxiv.org/pdf/1904.08920.pdf>`__，其中包含34602个训练样本（图像、问题和答案）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We see there are 3997 answer classes including a class representing unknown "
"answers."
msgstr "我们发现有3997个答案类别，包括一个表示未知答案的类别。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Lets display a sample entry from the dataset:"
msgstr "让我们显示数据集中的一个示例条目："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"3. Next, we write the transform function to convert the image and text into "
"Tensors consumable by our model - For images, we use the transforms from "
"torchvision to convert to Tensor and resize to uniform sizes - For text, we "
"tokenize (and pad) them using the ``BertTokenizer`` from Hugging Face - For "
"answers (i.e. labels), we take the most frequently occurring answer as the "
"label to train with:"
msgstr ""
"3. 接下来，我们编写转换函数，将图像和文本转换为模型可消耗的Tensor - "
"对于图像，我们使用torchvision中的transforms来转换为Tensor并调整为统一大小 - 对于文本，我们使用Hugging "
"Face中的``BertTokenizer``进行标记化（和填充） - 对于答案（即标签），我们采用最频繁出现的答案作为训练标签："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"4. Finally, we import the ``flava_model_for_classification`` from "
"``torchmultimodal``. It loads the pretrained FLAVA checkpoint by default and"
" includes a classification head."
msgstr ""
"4. "
"最后，我们从``torchmultimodal``导入``flava_model_for_classification``。它默认加载预训练FLAVA检查点并包含一个分类头。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The model forward function passes the image through the visual encoder and "
"the question through the text encoder. The image and question embeddings are"
" then passed through the multimodal encoder. The final embedding "
"corresponding to the CLS token is passed through a MLP head which finally "
"gives the probability distribution over each possible answers."
msgstr ""
"该模型的前向函数将图像通过视觉编码器，问题通过文本编码器。图像和问题嵌入然后通过多模态编码器。最终对应CLS标记的嵌入通过MLP头，最终提供每个可能答案的概率分布。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"5. We put together the dataset and model in a toy training loop to "
"demonstrate how to train the model for 3 iterations:"
msgstr "5. 我们将数据集和模型整合到一个简单的训练循环中，以演示如何训练模型3次迭代："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial introduced the basics around how to finetune on a multimodal "
"task using FLAVA from TorchMultimodal. Please also check out other examples "
"from the library like `MDETR "
"<https://github.com/facebookresearch/multimodal/tree/main/torchmultimodal/models/mdetr>`__"
" which is a multimodal model for object detection and `Omnivore "
"<https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/models/omnivore.py>`__"
" which is multitask model spanning image, video and 3d classification."
msgstr ""
"本教程介绍了如何使用TorchMultimodal中的FLAVA微调多模态任务的基础内容。另外，请查看库中的其他示例，如 `MDETR "
"<https://github.com/facebookresearch/multimodal/tree/main/torchmultimodal/models/mdetr>`__"
" ，这是一个用于目标检测的多模态模型，以及 `Omnivore "
"<https://github.com/facebookresearch/multimodal/blob/main/torchmultimodal/models/omnivore.py>`__"
" ，它适用于图像、视频和3D分类的多任务模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: flava_finetuning_tutorial.py "
"<flava_finetuning_tutorial.py>`"
msgstr ""
":download:`下载Python源码: flava_finetuning_tutorial.py "
"<flava_finetuning_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: flava_finetuning_tutorial.ipynb "
"<flava_finetuning_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: flava_finetuning_tutorial.ipynb "
"<flava_finetuning_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Autograd"
msgstr "自动梯度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is out of date. You'll be redirected to the new tutorial in 3 "
"seconds: "
"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
msgstr ""
"本教程已过时。您将在3秒后被重定向到新的教程: "
"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "nn Package"
msgstr "nn包"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is out of date. You'll be redirected to the new tutorial in 3 "
"seconds: https://pytorch.org/tutorials/beginner/nn_tutorial.html"
msgstr ""
"本教程已过时。您将在3秒后被重定向到新的教程: "
"https://pytorch.org/tutorials/beginner/nn_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "Multi-GPU Examples"
msgstr "多GPU示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is out of date. You'll be redirected to the new tutorial in 3 "
"seconds: "
"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"
msgstr ""
"本教程已过时。您将在3秒后被重定向到新的教程: "
"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is out of date. You'll be redirected to the new tutorial in 3 "
"seconds: "
"https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
msgstr ""
"本教程已过时。您将在3秒后被重定向到新的教程: "
"https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch for Former Torch Users"
msgstr "给前Torch用户的PyTorch教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial is out of date. Please check out the PyTorch tutorials here: "
"https://pytorch.org/tutorials/"
msgstr "本教程已过时。请查看PyTorch教程: https://pytorch.org/tutorials/"

#: ../../beginner/vt_tutorial.rst:783
msgid "You will be redirected in 3 seconds."
msgstr "您将在3秒后被重定向。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to Holistic Trace Analysis"
msgstr "全局追踪分析简介"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author:** `Anupam Bhatnagar <https://github.com/anupambhatnagar>`_"
msgstr "**作者:** `Anupam Bhatnagar <https://github.com/anupambhatnagar>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we demonstrate how to use Holistic Trace Analysis (HTA) to"
" analyze traces from a distributed training job. To get started follow the "
"steps below."
msgstr "在本教程中，我们展示了如何使用全局追踪分析(HTA)来分析分布式训练工作的追踪数据。要开始，请按照以下步骤操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Installing HTA"
msgstr "安装HTA"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We recommend using a Conda environment to install HTA. To install Anaconda, "
"see `the official Anaconda documentation "
"<https://docs.anaconda.com/anaconda/install/index.html>`_."
msgstr ""
"我们建议使用Conda环境来安装HTA。欲安装Anaconda，请参阅 `官方Anaconda文档 "
"<https://docs.anaconda.com/anaconda/install/index.html>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Install HTA using pip:"
msgstr "使用pip安装HTA："

#: ../../beginner/vt_tutorial.rst:783
msgid "(Optional and recommended) Set up a Conda environment:"
msgstr "(可选且推荐) 设置Conda环境："

#: ../../beginner/vt_tutorial.rst:783
msgid "Getting Started"
msgstr "快速开始"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Launch a Jupyter notebook and set the ``trace_dir`` variable to the location"
" of the traces."
msgstr "启动Jupyter笔记本，并将``trace_dir``变量设置为追踪数据的位置。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Temporal Breakdown"
msgstr "时间分解"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To effectively utilize the GPUs, it is crucial to understand how they are "
"spending time for a specific job. Are they primarily engaged in computation,"
" communication, memory events, or are they idle? The temporal breakdown "
"feature provides a detailed analysis of the time spent in these three "
"categories."
msgstr ""
"为了有效利用GPU，关键是了解它们在特定任务中如何花费时间。它们主要用于计算、通信、内存事件还是处于空闲状态？时间分布功能提供了这三个类别中花费时间的详细分析。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Idle time - GPU is idle."
msgstr "空闲时间 - GPU处于空闲状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Compute time - GPU is being used for matrix multiplications or vector "
"operations."
msgstr "计算时间 - GPU用于矩阵乘法或向量操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Non-compute time - GPU is being used for communication or memory events."
msgstr "非计算时间 - GPU用于通信或内存事件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To achieve high training efficiency, the code should maximize compute time "
"and minimize idle time and non-compute time. The following function "
"generates a dataframe that provides a detailed breakdown of the temporal "
"usage for each rank."
msgstr "为了实现高效的训练效率，代码应最大化计算时间，同时最小化空闲时间和非计算时间。以下函数生成一个数据框，提供每个排名时间使用的详细分解。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When the ``visualize`` argument is set to ``True`` in the "
"`get_temporal_breakdown "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_temporal_breakdown>`_"
" function it also generates a bar graph representing the breakdown by rank."
msgstr ""
"当`get_temporal_breakdown "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_temporal_breakdown>`_函数中的``visualize``参数设置为``True``时，它还会生成一个按排名表示分解比例的条形图。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Idle Time Breakdown"
msgstr "空闲时间分解"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Gaining insight into the amount of time the GPU spends idle and the reasons "
"behind it can help guide optimization strategies. A GPU is considered idle "
"when no kernel is running on it. We have developed an algorithm to "
"categorize the `Idle` time into three distinct categories:"
msgstr ""
"了解GPU空闲时间及其背后的原因，有助于制定优化策略。当GPU上没有内核运行时，我们认为它是空闲的。我们开发了一种算法，将`空闲`时间划分为三种不同类别："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Host wait:** refers to the idle time on the GPU that is caused by the CPU "
"not enqueuing kernels quickly enough to keep the GPU fully utilized. These "
"types of inefficiencies can be addressed by examining the CPU operators that"
" are contributing to the slowdown, increasing the batch size and applying "
"operator fusion."
msgstr ""
"**主机等待：**指由于CPU未能快速写入内核以保持GPU充分利用，导致的GPU空闲时间。这类低效可以通过检查导致减速的CPU操作、增加批量大小和应用操作融合来解决。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Kernel wait:** This refers to brief overhead associated with launching "
"consecutive kernels on the GPU. The idle time attributed to this category "
"can be minimized by using CUDA Graph optimizations."
msgstr "**内核等待：**这是指在GPU上启动连续内核时的短暂开销。可以通过使用CUDA图优化来尽量减少归因于该类别的空闲时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Other wait:** This category includes idle time that cannot currently be "
"attributed due to insufficient information. The likely causes include "
"synchronization among CUDA streams using CUDA events and delays in launching"
" kernels."
msgstr "**其他等待：**此类别包括由于信息不足无法归因的空闲时间。可能的原因包括使用CUDA事件在CUDA流之间进行同步以及启动内核的延迟。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The host wait time can be interpreted as the time when the GPU is stalling "
"due to the CPU. To attribute the idle time as kernel wait we use the "
"following heuristic:"
msgstr "主机等待时间可解释为GPU因CPU的阻塞而停滞的时间。为了将空闲时间归因于内核等待，我们使用以下启发式方法："

#: ../../beginner/vt_tutorial.rst:783
msgid "**gap between consecutive kernels < threshold**"
msgstr "**连续内核之间的间隙 < 阈值**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The default threshold value is 30 nanoseconds and can be configured using "
"the ``consecutive_kernel_delay`` argument. By default, the idle time "
"breakdown is computed for rank 0 only. In order to calculate the breakdown "
"for other ranks, use the ``ranks`` argument in the `get_idle_time_breakdown "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_idle_time_breakdown>`_"
" function. The idle time breakdown can be generated as follows:"
msgstr ""
"默认的阈值为30纳秒，可通过``consecutive_kernel_delay``参数进行配置。默认情况下，空闲时间分解仅针对排名0进行计算。为了计算其他排名的分解，请在`get_idle_time_breakdown"
" "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_idle_time_breakdown>`_函数中使用``ranks``参数。可按以下方式生成空闲时间分解："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The function returns a tuple of dataframes. The first dataframe contains the"
" idle time by category on each stream for each rank."
msgstr "该函数返回一个数据框元组。第一个数据框包含每个排名在每个流上的分类空闲时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The second dataframe is generated when ``show_idle_interval_stats`` is set "
"to ``True``. It contains the summary statistics of the idle time for each "
"stream on each rank."
msgstr ""
"当``show_idle_interval_stats``设置为``True``时，会生成第二个数据框，包含每个流在每个排名上的空闲时间的概述统计信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By default, the idle time breakdown presents the percentage of each of the "
"idle time categories. Setting the ``visualize_pctg`` argument to ``False``, "
"the function renders with absolute time on the y-axis."
msgstr ""
"默认情况下，空闲时间分解显示每个空闲时间类别的百分比。如果将``visualize_pctg``参数设置为``False``，函数将在y轴上以绝对时间呈现。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Kernel Breakdown"
msgstr "内核分解"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The kernel breakdown feature breaks down the time spent for each kernel "
"type, such as communication (COMM), computation (COMP), and memory (MEM), "
"across all ranks and presents the proportion of time spent in each category."
" Here is the percentage of time spent in each category as a pie chart:"
msgstr ""
"内核分解功能将各排名的通信（COMM）、计算（COMP）和内存（MEM）等内核类型的时间进行分解，并展示每种类别的时间比例。以下是按类别花费时间比例的饼状图："

#: ../../beginner/vt_tutorial.rst:783
msgid "The kernel breakdown can be calculated as follows:"
msgstr "可以按以下方式计算内核分解："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The first dataframe returned by the function contains the raw values used to"
" generate the pie chart."
msgstr "函数返回的第一个数据框包含生成饼图所用的原始值。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Kernel Duration Distribution"
msgstr "内核时长分布"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The second dataframe returned by `get_gpu_kernel_breakdown "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_gpu_kernel_breakdown>`_"
" contains duration summary statistics for each kernel. In particular, this "
"includes the count, min, max, average, standard deviation, sum, and kernel "
"type for each kernel on each rank."
msgstr ""
"`get_gpu_kernel_breakdown "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_gpu_kernel_breakdown>`_返回的第二个数据框包含每个内核的时长统计数据，特别是每个排名上的内核的计数、最小值、最大值、平均值、标准偏差、总和及内核类型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Using this data HTA creates many visualizations to identify performance "
"bottlenecks."
msgstr "使用这些数据，HTA生成许多可视化图表以识别性能瓶颈。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Pie charts of the top kernels for each kernel type for each rank."
msgstr "每个排名的每种内核类型的前几个内核的饼状图。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Bar graphs of the average duration across all ranks for each of the top "
"kernels and for each kernel type."
msgstr "每种内核类型的平均时长条形图，跨越所有排名。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"All images are generated using plotly. Hovering on the graph shows the mode "
"bar on the top right which allows the user to zoom, pan, select, and "
"download the graph."
msgstr "所有图像均使用plotly生成。鼠标悬停在图表上可显示右上角的模式栏，允许用户缩放、平移、选择并下载图表。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The pie charts above show the top 5 computation, communication, and memory "
"kernels. Similar pie charts are generated for each rank. The pie charts can "
"be configured to show the top k kernels using the ``num_kernels`` argument "
"passed to the `get_gpu_kernel_breakdown` function. Additionally, the "
"``duration_ratio`` argument can be used to tune the percentage of time that "
"needs to be analyzed. If both ``num_kernels`` and ``duration_ratio`` are "
"specified, then ``num_kernels`` takes precedence."
msgstr ""
"上面的饼状图显示了前5个计算、通信和内存内核。对每个排名生成类似的饼状图。可以使用传递给`get_gpu_kernel_breakdown`函数的``num_kernels``参数配置饼状图以显示前k个内核。此外，还可以使用``duration_ratio``参数调整需要分析的时间比例。如果同时指定``num_kernels``和``duration_ratio``，则``num_kernels``优先。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The bar graph above shows the average duration of the NCCL AllReduce kernel "
"across all the ranks. The black lines indicate the minimum and maximum time "
"taken on each rank."
msgstr "上方的条形图显示了跨所有排名的NCCL AllReduce内核的平均时长。黑线表示每个排名上的最小和最大时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When using jupyter-lab set the \"image_renderer\" argument value to "
"\"jupyterlab\" otherwise the graphs will not render in the notebook."
msgstr ""
"在使用jupyter-lab时，将``image_renderer``参数值设置为``jupyterlab``，否则图表无法在笔记本中渲染。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For a detailed walkthrough of this feature see the `gpu_kernel_breakdown "
"notebook "
"<https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/kernel_breakdown_demo.ipynb>`_"
" in the examples folder of the repo."
msgstr ""
"有关此功能的详细讲解，请参阅仓库示例文件夹中的`gpu_kernel_breakdown notebook "
"<https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/kernel_breakdown_demo.ipynb>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Communication Computation Overlap"
msgstr "通信与计算重叠"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In distributed training, a significant amount of time is spent in "
"communication and synchronization events between GPUs. To achieve high GPU "
"efficiency (such as TFLOPS/GPU), it is crucial to keep the GPU "
"oversubscribed with computation kernels. In other words, the GPU should not "
"be blocked due to unresolved data dependencies. One way to measure the "
"extent to which computation is blocked by data dependencies is to calculate "
"the communication computation overlap. Higher GPU efficiency is observed if "
"communication events overlap computation events. Lack of communication and "
"computation overlap will lead to the GPU being idle, resulting in low "
"efficiency. To sum up, a higher communication computation overlap is "
"desirable. To calculate the overlap percentage for each rank, we measure the"
" following ratio:"
msgstr ""
"在分布式训练中，GPU之间的通信和同步事件占据了大量时间。为了实现高效的GPU效率（如每GPU "
"TFLOPS），关键是让GPU充分利用计算内核换挡。换句话说，GPU不应因未解析的数据依赖关系而被阻塞。评估计算是否因数据依赖关系而受阻的一种方法是计算通信和计算的重叠。通信事件与计算事件的重叠越高，GPU效率越高。缺乏通信和计算的重叠会导致GPU空闲，从而降低效率。总而言之，较高的通信计算重叠是理想的。为了计算每个排名的重叠百分比，我们测量以下比率："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**(time spent in computation while communicating) / (time spent in "
"communication)**"
msgstr "**（进行通信的同时用于计算的时间） / （通信花费的时间）**"

#: ../../beginner/vt_tutorial.rst:783
msgid "The communication computation overlap can be calculated as follows:"
msgstr "可以按以下方式计算通信计算重叠："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The function returns a dataframe containing the overlap percentage for each "
"rank."
msgstr "该函数返回一个数据框，包含每个排名的重叠百分比。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When the ``visualize`` argument is set to True, the `get_comm_comp_overlap "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_comm_comp_overlap>`_"
" function also generates a bar graph representing the overlap by rank."
msgstr ""
"当参数``visualize``设置为True时，函数`get_comm_comp_overlap "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_comm_comp_overlap>`_还会生成一张条形图，按排名表示重叠程度。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Augmented Counters"
msgstr "增强计数器"

#: ../../beginner/vt_tutorial.rst:783
msgid "Memory Bandwidth & Queue Length Counters"
msgstr "内存带宽与队列长度计数器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Memory bandwidth counters measure the memory copy bandwidth used while "
"copying the data from H2D, D2H and D2D by memory copy (memcpy) and memory "
"set (memset) events. HTA also computes the number of outstanding operations "
"on each CUDA stream. We refer to this as **queue length**. When the queue "
"length on a stream is 1024 or larger new events cannot be scheduled on that "
"stream and the CPU will stall until the events on the GPU stream have "
"processed."
msgstr ""
"内存带宽计数器测量了将数据从H2D、D2H和D2D复制时内存拷贝（memcpy）和内存置零（memset）事件的内存拷贝带宽。HTA还计算了每个CUDA流上的未完成操作数，我们将其称为**队列长度**。当流上的队列长度达到1024或更大时，流上无法调度新事件，CPU将在GPU流上的事件处理完之前暂停。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `generate_trace_with_counters "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.generate_trace_with_counters>`_"
" API outputs a new trace file with the memory bandwidth and queue length "
"counters. The new trace file contains tracks which indicate the memory "
"bandwidth used by memcpy/memset operations and tracks for the queue length "
"on each stream. By default, these counters are generated using the rank 0 "
"trace file, and the new file contains the suffix ``_with_counters`` in its "
"name. Users have the option to generate the counters for multiple ranks by "
"using the ``ranks`` argument in the ``generate_trace_with_counters`` API."
msgstr ""
"`generate_trace_with_counters "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.generate_trace_with_counters>`_"
" "
"API输出一个带有内存带宽和队列长度计数器的新跟踪文件。新文件包含指示memcpy/memset操作所用内存带宽的轨迹以及每个流上的队列长度轨迹。默认情况下，这些计数器基于排名0的跟踪文件生成，新文件名包含后缀``_with_counters``。用户可以使用``generate_trace_with_counters``API中的``ranks``参数为多个排名生成计数器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "A screenshot of the generated trace file with augmented counters."
msgstr "生成的带有增强计数器的跟踪文件截图。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"HTA also provides a summary of the memory copy bandwidth and queue length "
"counters as well as the time series of the counters for the profiled portion"
" of the code using the following API:"
msgstr "HTA还提供了内存拷贝带宽和队列长度计数器的汇总以及代码分析部分计数器时间序列，使用以下API："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`get_memory_bw_summary "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_memory_bw_summary>`_"
msgstr ""
"`get_memory_bw_summary "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_memory_bw_summary>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`get_queue_length_summary "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_queue_length_summary>`_"
msgstr ""
"`get_queue_length_summary "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_queue_length_summary>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`get_memory_bw_time_series "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_memory_bw_time_series>`_"
msgstr ""
"`get_memory_bw_time_series "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_memory_bw_time_series>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`get_queue_length_time_series "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_queue_length_time_series>`_"
msgstr ""
"`get_queue_length_time_series "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_queue_length_time_series>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "To view the summary and time series, use:"
msgstr "要查看汇总和时间序列，请使用："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The summary contains the count, min, max, mean, standard deviation, 25th, "
"50th, and 75th percentile."
msgstr "汇总包含计数、最小值、最大值、平均值、标准偏差、第25、第50和第75百分位数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The time series only contains the points when a value changes. Once a value "
"is observed the time series stays constant until the next update. The memory"
" bandwidth and queue length time series functions return a dictionary whose "
"key is the rank and the value is the time series for that rank. By default, "
"the time series is computed for rank 0 only."
msgstr ""
"时间序列仅包含值更改时的点。一旦观测到某个值，时间序列保持不变，直到下一次更新。内存带宽和队列长度时间序列函数返回的字典的键是排名，值是相应排名的时间序列。默认情况下，时间序列仅为排名0计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid "CUDA Kernel Launch Statistics"
msgstr "CUDA内核启动统计"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For each event launched on the GPU, there is a corresponding scheduling "
"event on the CPU, such as ``CudaLaunchKernel``, ``CudaMemcpyAsync``, "
"``CudaMemsetAsync``. These events are linked by a common correlation ID in "
"the trace - see the figure above. This feature computes the duration of the "
"CPU runtime event, its corresponding GPU kernel and the launch delay, for "
"example, the difference between GPU kernel starting and CPU operator ending."
" The kernel launch info can be generated as follows:"
msgstr ""
"对于GPU上启动的每个事件，CPU上有相应的调度事件，例如``CudaLaunchKernel``、``CudaMemcpyAsync``、``CudaMemsetAsync``。这些事件通过跟踪中的公共关联ID链接——参见上图。此功能计算了CPU运行时事件、相应的GPU内核时长及启动延迟，例如GPU内核启动与CPU操作结束之间的时间差。可以按以下方式生成内核启动信息："

#: ../../beginner/vt_tutorial.rst:783
msgid "A screenshot of the generated dataframe is given below."
msgstr "下方提供了生成的数据框截屏。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The duration of the CPU op, GPU kernel, and the launch delay allow us to "
"find the following:"
msgstr "CPU操作、GPU内核的时长以及启动延迟允许我们确定以下内容："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Short GPU kernels** - GPU kernels with duration less than the "
"corresponding CPU runtime event."
msgstr "**短时GPU内核** - GPU内核运行的时长短于相应的CPU运行时事件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Runtime event outliers** - CPU runtime events with excessive duration."
msgstr "**运行时事件异常值** - CPU运行事件的持续时间过长。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Launch delay outliers** - GPU kernels which take too long to be scheduled."
msgstr "**启动延迟异常值** - GPU内核调度时间过长。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"HTA generates distribution plots for each of the aforementioned three "
"categories."
msgstr "HTA为上述三类每一类生成分布图。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Short GPU kernels**"
msgstr "**短GPU内核**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Typically, the launch time on the CPU side ranges from 5-20 microseconds. In"
" some cases, the GPU execution time is lower than the launch time itself. "
"The graph below helps us to find how frequently such instances occur in the "
"code."
msgstr ""
"通常情况下，CPU端的启动时间范围是5-20微秒。在某些情况下，GPU执行时间甚至低于启动时间本身。下图帮助我们找到在代码中这种情况的发生频率。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Runtime event outliers**"
msgstr "**运行时事件异常值**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The runtime outliers depend on the cutoff used to classify the outliers, "
"hence the `get_cuda_kernel_launch_stats "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_cuda_kernel_launch_stats>`_"
" API provides the ``runtime_cutoff`` argument to configure the value."
msgstr ""
"运行时异常值取决于用于分类异常值的阈值，因此`get_cuda_kernel_launch_stats "
"<https://hta.readthedocs.io/en/latest/source/api/trace_analysis_api.html#hta.trace_analysis.TraceAnalysis.get_cuda_kernel_launch_stats>`_"
" API提供了``runtime_cutoff``参数用于配置该值。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Launch delay outliers**"
msgstr "**启动延迟异常值**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The launch delay outliers depend on the cutoff used to classify the "
"outliers, hence the `get_cuda_kernel_launch_stats` API provides the "
"``launch_delay_cutoff`` argument to configure the value."
msgstr ""
"启动延迟异常值取决于用于分类异常值的阈值，因此`get_cuda_kernel_launch_stats` "
"API提供了``launch_delay_cutoff``参数用于配置该值。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, you have learned how to install and use HTA, a performance"
" tool that enables you analyze bottlenecks in your distributed training "
"workflows. To learn how you can use the HTA tool to perform trace diff "
"analysis, see `Trace Diff using Holistic Trace Analysis "
"<https://pytorch.org/tutorials/beginner/hta_trace_diff_tutorial.html>`__."
msgstr ""
"在本教程中，您已经学习到如何安装和使用HTA，这是一种性能工具，可以用于分析分布式训练工作流程中的瓶颈。要了解如何使用HTA工具进行追踪差异分析，请参见`使用整体追踪分析进行追踪差异"
" <https://pytorch.org/tutorials/beginner/hta_trace_diff_tutorial.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Trace Diff using Holistic Trace Analysis"
msgstr "使用整体追踪分析进行追踪差异"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Occasionally, users need to identify the changes in PyTorch operators and "
"CUDA kernels resulting from a code change. To support this requirement, HTA "
"provides a trace comparison feature. This feature allows the user to input "
"two sets of trace files where the first can be thought of as the *control "
"group* and the second as the *test group*, similar to an A/B test. The "
"``TraceDiff`` class provides functions to compare the differences between "
"traces and functionality to visualize these differences. In particular, "
"users can find operators and kernels that were added and removed from each "
"group, along with the frequency of each operator/kernel and the cumulative "
"time taken by the operator/kernel."
msgstr ""
"用户有时需要识别因代码更改而导致的PyTorch操作符和CUDA内核的变化。为支持此需求，HTA提供了一种追踪比较功能。该功能允许用户输入两组追踪文件，其中第一组可视为*控制组*，而第二组为*测试组*，类似于A/B测试。``TraceDiff``类提供了比较追踪差异的函数以及可视化这些差异的功能。具体而言，用户可以找到在每组中增加和移除的操作符和内核，并查看每个操作符/内核的频率及其累计时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `TraceDiff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html>`_ "
"class has the following methods:"
msgstr ""
"`TraceDiff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html>`_类具有以下方法："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`compare_traces "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.compare_traces>`_:"
" Compare the frequency and total duration of CPU operators and GPU kernels "
"from two sets of traces."
msgstr ""
"`compare_traces "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.compare_traces>`_:"
" 比较两组追踪中的CPU操作符和GPU内核的频率及总持续时间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`ops_diff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.ops_diff>`_:"
" Get the operators and kernels which have been:"
msgstr ""
"`ops_diff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.ops_diff>`_:"
" 获取以下操作符和内核："

#: ../../beginner/vt_tutorial.rst:783
msgid "**added** to the test trace and are absent in the control trace"
msgstr "**添加**到测试追踪中，且控制追踪中不存在"

#: ../../beginner/vt_tutorial.rst:783
msgid "**deleted** from the test trace and are present in the control trace"
msgstr "**删除**从测试追踪中，且控制追踪中存在"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**increased** in frequency in the test trace and exist in the control trace"
msgstr "**频率增加**在测试追踪中，且在控制追踪中存在"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**decreased** in frequency in the test trace and exist in the control trace"
msgstr "**频率减少**在测试追踪中，且在控制追踪中存在"

#: ../../beginner/vt_tutorial.rst:783
msgid "**unchanged** between the two sets of traces"
msgstr "**未变化**在两组追踪中均未变化"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`visualize_counts_diff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.visualize_counts_diff>`_"
msgstr ""
"`visualize_counts_diff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.visualize_counts_diff>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`visualize_duration_diff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.visualize_duration_diff>`_"
msgstr ""
"`visualize_duration_diff "
"<https://hta.readthedocs.io/en/latest/source/api/trace_diff_api.html#hta.trace_diff.TraceDiff.visualize_duration_diff>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The last two methods can be used to visualize various changes in frequency "
"and duration of CPU operators and GPU kernels, using the output of the "
"``compare_traces`` method."
msgstr "最后两种方法可以用于可视化CPU操作符和GPU内核的频率和持续时间的各种变化，使用``compare_traces``方法的输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For example, the top ten operators with increase in frequency can be "
"computed as follows:"
msgstr "例如，可以计算频率增加最多的前十个操作符如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Similarly, the top ten operators with the largest change in duration can be "
"computed as follows:"
msgstr "类似地，可以计算持续时间变化最大的前十个操作符如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For a detailed example of this feature see the `trace_diff_demo notebook "
"<https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_diff_demo.ipynb>`_"
" in the examples folder of the repository."
msgstr ""
"有关此功能的详细示例，请参见存储库示例文件夹中的`trace_diff_demo notebook "
"<https://github.com/facebookresearch/HolisticTraceAnalysis/blob/main/examples/trace_diff_demo.ipynb>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "learning_hybrid_frontend_through_example_tutorial.py"
msgstr "learning_hybrid_frontend_through_example_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Learning Hybrid Frontend Through Example "
"https://pytorch.org/tutorials/beginner/hybrid_frontend/learning_hybrid_frontend_through_example_tutorial.html"
msgstr ""
"通过示例学习混合前端 "
"https://pytorch.org/tutorials/beginner/hybrid_frontend/learning_hybrid_frontend_through_example_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "introduction_to_hybrid_frontend_tutorial.py"
msgstr "introduction_to_hybrid_frontend_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Introduction to Hybrid Frontend "
"https://pytorch.org/tutorials/beginner/hybrid_frontend/introduction_to_hybrid_frontend_tutorial.html"
msgstr ""
"混合前端介绍 "
"https://pytorch.org/tutorials/beginner/hybrid_frontend/introduction_to_hybrid_frontend_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "Learning Hybrid Frontend Syntax Through Example"
msgstr "通过示例学习混合前端语法"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_hybrid_frontend_learning_hybrid_frontend_through_example_tutorial.py`"
msgstr ""
":ref:`sphx_glr_beginner_hybrid_frontend_learning_hybrid_frontend_through_example_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_hybrid_frontend_learning_hybrid_frontend_through_example_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_beginner_hybrid_frontend_learning_hybrid_frontend_through_example_tutorial.py>`"
" 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author:** `Nathan Inkawhich <https://github.com/inkawhich>`_"
msgstr "**作者:** `Nathan Inkawhich <https://github.com/inkawhich>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This document is meant to highlight the syntax of the Hybrid Frontend "
"through a non-code intensive example. The Hybrid Frontend is one of the new "
"shiny features of Pytorch 1.0 and provides an avenue for developers to "
"transition their models from **eager** to **graph** mode. PyTorch users are "
"very familiar with eager mode as it provides the ease-of-use and flexibility"
" that we all enjoy as researchers. Caffe2 users are more aquainted with "
"graph mode which has the benefits of speed, optimization opportunities, and "
"functionality in C++ runtime environments. The hybrid frontend bridges the "
"gap between the the two modes by allowing researchers to develop and refine "
"their models in eager mode (i.e. PyTorch), then gradually transition the "
"proven model to graph mode for production, when speed and resouce "
"consumption become critical."
msgstr ""
"本文件旨在通过非代码密集型示例突出混合前端的语法。混合前端是Pytorch "
"1.0的一项新功能，为开发者提供了从**即时模式**到**图模式**转换模型的途径。PyTorch用户非常熟悉即时模式，因为它提供了我们作为研究人员所享受的易操作性和灵活性。Caffe2用户更熟悉图模式，因为它具备速度、优化机会以及在C++运行时环境中的功能优势。混合前端通过允许研究人员在即时模式（即PyTorch）中开发和完善模型，然后逐步将经过验证的模型转换为生产时的图模式，从而缩短了两种模式之间的差距，当速度和资源消耗变得至关重要时，可以进行生产。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Hybrid Frontend Information"
msgstr "混合前端信息"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The process for transitioning a model to graph mode is as follows. First, "
"the developer constructs, trains, and tests the model in eager mode. Then "
"they incrementally **trace** and **script** each function/module of the "
"model with the Just-In-Time (JIT) compiler, at each step verifying that the "
"output is correct. Finally, when each of the components of the top-level "
"model have been traced and scripted, the model itself is traced. At which "
"point the model has been transitioned to graph mode, and has a complete "
"python-free representation. With this representation, the model runtime can "
"take advantage of high-performance Caffe2 operators and graph based "
"optimizations."
msgstr ""
"将模型转换为图模式的过程如下。首先，开发者在即时模式下构建、训练和测试模型。然后，他们逐步使用即时编译器（Just-In-"
"Time，JIT）通过**追踪**和**脚本**逐步转换模型的每个函数/模块，并验证每步的输出是否正确。最后，当顶层模型的每个组件都已被追踪和脚本化时，模型本身就会被追踪到图模式，此时模型已完成转换，并具有完整的无Python表示。在这种表示下，模型运行时可以利用高性能的Caffe2操作符和基于图的优化。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before we continue, it is important to understand the idea of tracing and "
"scripting, and why they are separate. The goal of **trace** and **script** "
"is the same, and that is to create a graph representation of the operations "
"taking place in a given function. The discrepency comes from the flexibility"
" of eager mode that allows for **data-dependent control flows** within the "
"model architecture. When a function does NOT have a data-dependent control "
"flow, it may be *traced* with ``torch.jit.trace``. However, when the "
"function *has* a data-dependent control flow it must be *scripted* with "
"``torch.jit.script``. We will leave the details of the interworkings of the "
"hybrid frontend for another document, but the code example below will show "
"the syntax of how to trace and script different pure python functions and "
"torch Modules. Hopefully, you will find that using the hybrid frontend is "
"non-intrusive as it mostly involves adding decorators to the existing "
"function and class definitions."
msgstr ""
"在继续之前，理解追踪和脚本的概念以及为何它们分开是重要的。**追踪**和**脚本**的目标是一样的，即创建给定函数中操作的图表示。差异在于即时模式的灵活性允许在模型架构中出现**数据依赖的控制流**。当一个函数没有数据依赖控制流时，它可以被``torch.jit.trace``追踪。然而，当函数含有数据依赖控制流时，它必须通过``torch.jit.script``脚本化。虽然混合前端的工作原理细节将在另一篇文档中介绍，但下面的代码示例将展示如何让不同的纯Python函数和torch模块实现追踪与脚本化的语法。希望您发现使用混合前端几乎不会打扰现有代码，只需在函数和类定义中添加修饰器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Motivating Example"
msgstr "动机示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example we will implement a strange math function that may be "
"logically broken up into four parts that do, and do not contain data-"
"dependent control flows. The purpose here is to show a non-code intensive "
"example where the use of the JIT is highlighted. This example is a stand-in "
"representation of a useful model, whose implementation has been divided into"
" various pure python functions and modules."
msgstr ""
"在此示例中，我们将实现一个奇特的数学函数，它可以逻辑上分为四部分，其内容分别包含和不包含数据依赖控制流。目的是展示一个代码量不多的示例，突出JIT使用的场景。此示例是一个代表有用模型的替代实现，对其划分为多个纯Python函数和模块。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The function we seek to implement, :math:`Y(x)`, is defined for :math:`x "
"\\epsilon \\mathbb{N}` as"
msgstr "我们要实现的函数，:math:`Y(x)`，对于:math:`x \\epsilon \\mathbb{N}`定义为"

#: ../../beginner/vt_tutorial.rst:783
msgid "z(x) = \\Biggl \\lfloor \\frac{\\sqrt{\\prod_{i=1}^{|2 x|}i}}{5} \\Biggr \\rfloor"
msgstr "z(x) = \\Biggl \\lfloor \\frac{\\sqrt{\\prod_{i=1}^{|2 x|}i}}{5} \\Biggr \\rfloor"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Y(x) = \\begin{cases}\n"
"  \\frac{z(x)}{2}  &  \\text{if } z(x)\\%2 == 0, \\\\\n"
"  z(x)             &  \\text{otherwise}\n"
"\\end{cases}"
msgstr ""
"Y(x) = \\begin{cases}\n"
"  \\frac{z(x)}{2}  &  \\text{if } z(x)\\%2 == 0, \\\\\n"
"  z(x)             &  \\text{otherwise}\n"
"\\end{cases}"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\begin{array}{| r  | r |} \\hline\n"
"x &1 &2 &3 &4 &5 &6 &7 \\\\ \\hline\n"
"Y(x) &0 &0 &-5 &20 &190 &-4377 &-59051 \\\\ \\hline\n"
"\\end{array}"
msgstr ""
"\\begin{array}{| r  | r |} \\hline\n"
"x &1 &2 &3 &4 &5 &6 &7 \\\\ \\hline\n"
"Y(x) &0 &0 &-5 &20 &190 &-4377 &-59051 \\\\ \\hline\n"
"\\end{array}"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As mentioned, the computation is split into four parts. Part one is the "
"simple tensor calculation of :math:`|2x|`, which can be traced. Part two is "
"the iterative product calculation that represents a data dependent control "
"flow to be scripted (the number of loop iteration depends on the input at "
"runtime). Part three is a trace-able :math:`\\lfloor \\sqrt{a/5} \\rfloor` "
"calculation. Finally, part 4 handles the output cases depending on the value"
" of :math:`z(x)` and must be scripted due to the data dependency. Now, let's"
" see how this looks in code."
msgstr ""
"如前所述，计算被分为四个部分。第一部分是简单的张量运算，计算:math:`|2x|`，可以被追踪。第二部分是迭代乘积计算，表示数据依赖的控制流，需要脚本化（循环迭代次数取决于运行时输入值）。第三部分是可追踪的:math:`\\lfloor"
" \\sqrt{a/5} "
"\\rfloor`计算。最后，第四部分根据:math:`z(x)`值处理输出的不同情况，由于数据依赖性，需要脚本化。现在，让我们看看代码中这些部分的表现形式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Part 1 - Tracing a pure python function"
msgstr "部分1 - 追踪纯Python函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can implement part one as a pure python function as below. Notice, to "
"trace this function we call ``torch.jit.trace`` and pass in the function to "
"be traced. Since the trace requires a dummy input of the expected runtime "
"type and shape, we also include the ``torch.rand`` to generate a single "
"valued torch tensor."
msgstr ""
"我们可以如下形式实现第一部分，作为一个纯Python函数。注意，为追踪此函数，我们调用``torch.jit.trace``并传入需要追踪的函数。由于追踪需要具有预期运行时类型和形状的测试输入，我们还包括了``torch.rand``以生成一个单值torch张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Part 2 - Scripting a pure python function"
msgstr "部分2 - 脚本化纯Python函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can also implement part 2 as a pure python function where we iteratively "
"compute the product. Since the number of iterations depends on the value of "
"the input, we have a data dependent control flow, so the function must be "
"scripted. We can script python functions simply with the "
"``@torch.jit.script`` decorator."
msgstr ""
"我们也可以将第二部分实现为一个纯Python函数，其中我们迭代计算乘积。由于迭代次数取决于输入值，因此我们具有一个数据依赖控制流，所以必须脚本化此函数。通过``@torch.jit.script``修饰符可以轻松地实现纯Python函数的脚本化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Part 3 - Tracing a nn.Module"
msgstr "部分3 - 追踪nn.Module"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we will implement part 3 of the computation within the forward "
"function of a ``torch.nn.Module``. This module may be traced, but rather "
"than adding a decorator here, we will handle the tracing where the Module is"
" constructed. Thus, the class definition is not changed at all."
msgstr ""
"接下来，我们将在``torch.nn.Module``的forward函数中实现第三部分的计算。此模块可以被追踪，但与这里直接添加修饰符不同，我们将在模块的构造阶段处理追踪。因此，类定义完全不变。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Part 4 - Scripting a nn.Module"
msgstr "部分4 - 脚本化nn.Module"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the final part of the computation we have a ``torch.nn.Module`` that must"
" be scripted. To accomodate this, we inherit from ``torch.jit.ScriptModule``"
" and add the ``@torch.jit.script_method`` decorator to the forward function."
msgstr ""
"在计算的最后部分中，我们有一个需要脚本化的``torch.nn.Module``。为此，我们继承了``torch.jit.ScriptModule``并为forward函数添加了``@torch.jit.script_method``修饰符。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Top-Level Module"
msgstr "顶层模块"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we will put together the pieces of the computation via a top level "
"module called ``Net``. In the constructor, we will instantiate the "
"``TracedModule`` and ``ScriptModule`` as attributes. This must be done "
"because we ultimately want to trace/script the top level module, and having "
"the traced/scripted modules as attributes allows the Net to inherit the "
"required submodules' parameters. Notice, this is where we actually trace the"
" ``TracedModule`` by calling ``torch.jit.trace()`` and providing the "
"necessary dummy input. Also notice that the ``ScriptModule`` is constructed "
"as normal because we handled the scripting in the class definition."
msgstr ""
"现在我们将通过一个名为“Net”的顶级模块来组合计算的各个部分。在构造函数中，我们将实例化“TracedModule”和“ScriptModule”并将它们作为属性。必须这样做，因为我们最终希望跟踪/脚本化顶级模块，并且将跟踪/脚本化的模块作为属性允许Net继承所需子模块的参数。注意，这里我们实际上通过调用“torch.jit.trace()”并提供必要的虚拟输入来跟踪“TracedModule”。还要注意，“ScriptModule”是正常构造的，因为我们在类定义中处理了脚本化部分。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here we can also print the graphs created for each individual part of the "
"computation. The printed graphs allows us to see how the JIT ultimately "
"interpreted the functions as graph computations."
msgstr "在这里，我们还可以打印为计算的每个单独部分创建的图。打印的图允许我们看到JIT最终如何将函数解释为图计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we define the ``forward`` function for the Net module where we run "
"the input data ``x`` through the four parts of the computation. There is no "
"strange syntax here and we call the traced and scripted modules and "
"functions as expected."
msgstr ""
"最后，我们为Net模块定义“forward”函数，运行输入数据“x”通过计算的四个部分。这里没有奇怪的语法，我们按预期调用跟踪和脚本化的模块及函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Running the Model"
msgstr "运行模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"All that's left to do is construct the Net and compute the output through "
"the forward function. Here, we use :math:`x=5` as the test input value and "
"expect :math:`Y(x)=190.` Also, check out the graphs that were printed during"
" the construction of the Net."
msgstr ""
"剩下的就是构造Net并通过forward函数计算输出。在这里，我们使用:math:`x=5`作为测试输入值并预计:math:`Y(x)=190.` "
"另外，请查看在Net构造过程中打印的图。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tracing the Top-Level Model"
msgstr "跟踪顶级模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The last part of the example is to trace the top-level module, ``Net``. As "
"mentioned previously, since the traced/scripted modules are attributes of "
"Net, we are able to trace ``Net`` as it inherits the parameters of the "
"traced/scripted submodules. Note, the syntax for tracing Net is identical to"
" the syntax for tracing ``TracedModule``. Also, check out the graph that is "
"created."
msgstr ""
"示例的最后部分是跟踪顶级模块“Net”。如前所述，由于跟踪/脚本化的模块是Net的属性，我们能够跟踪“Net”，因为它继承了跟踪/脚本化子模块的参数。注意，跟踪Net的语法与跟踪“TracedModule”的语法相同。此外，还可以查看所创建的图。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hopefully, this document can serve as an introduction to the hybrid frontend"
" as well as a syntax reference guide for more experienced users. Also, there"
" are a few things to keep in mind when using the hybrid frontend. There is a"
" constraint that traced/scripted methods must be written in a restricted "
"subset of python, as features like generators, defs, and Python data "
"structures are not supported. As a workaround, the scripting model *is* "
"designed to work with both traced and non-traced code which means you can "
"call non-traced code from traced functions. However, such a model may not be"
" exported to ONNX."
msgstr ""
"希望本文档可以作为对混合前端的介绍以及作为经验丰富用户的语法参考指南。此外，使用混合前端时需要记住一些事项。有一个约束是跟踪/脚本化的方法必须写在受限的Python子集里，因为像生成器、defs和Python数据结构这样的特性是不支持的。作为一种解决方法，脚本化模型*确实*设计为可以与跟踪和非跟踪代码一起使用，这意味着您可以从跟踪函数中调用非跟踪代码。然而，这样的模型可能无法导出到ONNX。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.068 seconds)"
msgstr "**脚本总运行时间:**（0分 0.068秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: "
"learning_hybrid_frontend_through_example_tutorial.py "
"<learning_hybrid_frontend_through_example_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: learning_hybrid_frontend_through_example_tutorial.py"
" <learning_hybrid_frontend_through_example_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: "
"learning_hybrid_frontend_through_example_tutorial.ipynb "
"<learning_hybrid_frontend_through_example_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook: "
"learning_hybrid_frontend_through_example_tutorial.ipynb "
"<learning_hybrid_frontend_through_example_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**00:00.068** total execution time for **beginner_hybrid_frontend** files:"
msgstr "**00:00.068** 文件**beginner_hybrid_frontend**的总执行时间:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_hybrid_frontend_learning_hybrid_frontend_through_example_tutorial.py`"
" (``learning_hybrid_frontend_through_example_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_hybrid_frontend_learning_hybrid_frontend_through_example_tutorial.py`"
" (`learning_hybrid_frontend_through_example_tutorial.py`)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.068"
msgstr "00:00.068"

#: ../../beginner/vt_tutorial.rst:783
msgid "Hybrid Frontend Tutorials"
msgstr "混合前端教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Authors**: `Nathan Inkawhich <https://google.com>`_ and `Matthew Inkawhich"
" <https://github.com/MatthewInkawhich>`_"
msgstr ""
"**作者**：`Nathan Inkawhich <https://google.com>` 和 `Matthew Inkawhich "
"<https://github.com/MatthewInkawhich>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "In this set of tutorials, you will learn the following:"
msgstr "在这一系列教程中，您将学习以下内容："

#: ../../beginner/vt_tutorial.rst:783
msgid "What the hybrid frontend is and the suggested workflow"
msgstr "什么是混合前端及建议的工作流程"

#: ../../beginner/vt_tutorial.rst:783
msgid "Basic syntax"
msgstr "基础语法"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to transition an eager model to graph mode"
msgstr "如何将即时模式模型转换为图模式"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_hyperparameter_tuning_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_hyperparameter_tuning_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Hyperparameter tuning with Ray Tune"
msgstr "使用Ray Tune进行超参数调优"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Hyperparameter tuning can make the difference between an average model and a"
" highly accurate one. Often simple things like choosing a different learning"
" rate or changing a network layer size can have a dramatic impact on your "
"model performance."
msgstr "超参数调优可以使普通模型和高精度模型之间产生差异。通常，选择不同的学习率或更改网络层大小等简单操作可能对模型性能产生巨大影响。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Fortunately, there are tools that help with finding the best combination of "
"parameters. `Ray Tune <https://docs.ray.io/en/latest/tune.html>`_ is an "
"industry standard tool for distributed hyperparameter tuning. Ray Tune "
"includes the latest hyperparameter search algorithms, integrates with "
"various analysis libraries, and natively supports distributed training "
"through `Ray's distributed machine learning engine <https://ray.io/>`_."
msgstr ""
"幸运的是，有一些工具可以帮助找到最佳参数组合。`Ray Tune <https://docs.ray.io/zh/latest/tune.html>` "
"是一个用于分布式超参数调优的行业标准工具。Ray Tune包含最新的超参数搜索算法，与各种分析库集成，并原生支持`Ray&apos;s分布式机器学习引擎"
" <https://ray.io/>`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we will show you how to integrate Ray Tune into your "
"PyTorch training workflow. We will extend `this tutorial from the PyTorch "
"documentation "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`_ for "
"training a CIFAR10 image classifier."
msgstr ""
"在本教程中，我们将向您展示如何将Ray Tune集成到您的PyTorch训练工作流中。我们将扩展`PyTorch 文档中的这个教程 "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`用于训练CIFAR10图像分类器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As you will see, we only need to add some slight modifications. In "
"particular, we need to"
msgstr "如您所见，我们只需要做一些略微修改。特别是，我们需要"

#: ../../beginner/vt_tutorial.rst:783
msgid "wrap data loading and training in functions,"
msgstr "将数据加载和训练包装进函数，"

#: ../../beginner/vt_tutorial.rst:783
msgid "make some network parameters configurable,"
msgstr "使一些网络参数可配置，"

#: ../../beginner/vt_tutorial.rst:783
msgid "add checkpointing (optional),"
msgstr "添加检查点功能（可选），"

#: ../../beginner/vt_tutorial.rst:783
msgid "and define the search space for the model tuning"
msgstr "并定义模型调优的搜索空间"

#: ../../beginner/vt_tutorial.rst:783
msgid "``ray[tune]``: Distributed hyperparameter tuning library"
msgstr "``ray[tune]``：分布式超参数调优库"

#: ../../beginner/vt_tutorial.rst:783
msgid "``torchvision``: For the data transformers"
msgstr "``torchvision``：用于数据转换"

#: ../../beginner/vt_tutorial.rst:783
msgid "Setup / Imports"
msgstr "设置/导入"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's start with the imports:"
msgstr "让我们从导入开始："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Most of the imports are needed for building the PyTorch model. Only the last"
" imports are for Ray Tune."
msgstr "大多数导入是构建PyTorch模型所需的。只有最后的导入是为Ray Tune准备的。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Data loaders"
msgstr "数据加载器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We wrap the data loaders in their own function and pass a global data "
"directory. This way we can share a data directory between different trials."
msgstr "我们将数据加载器包装进它们自己的函数，并传递一个全局数据目录。通过这种方式，我们可以在不同试验间共享一个数据目录。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Configurable neural network"
msgstr "可配置神经网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can only tune those parameters that are configurable. In this example, we"
" can specify the layer sizes of the fully connected layers:"
msgstr "我们只能调优那些可配置的参数。在此示例中，我们可以指定全连接层的层大小："

#: ../../beginner/vt_tutorial.rst:783
msgid "The train function"
msgstr "训练函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now it gets interesting, because we introduce some changes to the example "
"`from the PyTorch documentation "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`_."
msgstr ""
"现在变得有趣了，因为我们对`PyTorch文档中的示例 "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`做了一些更改。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We wrap the training script in a function ``train_cifar(config, "
"data_dir=None)``. The ``config`` parameter will receive the hyperparameters "
"we would like to train with. The ``data_dir`` specifies the directory where "
"we load and store the data, so that multiple runs can share the same data "
"source. We also load the model and optimizer state at the start of the run, "
"if a checkpoint is provided. Further down in this tutorial you will find "
"information on how to save the checkpoint and what it is used for."
msgstr ""
"我们将训练脚本包装在一个函数``train_cifar(config, "
"data_dir=None)``中。``config``参数将接收我们想要训练的超参数配置。``data_dir``指定我们加载和存储数据的目录，以便多个运行可以共享相同的数据源。如果提供了检查点，我们还会在运行开始时加载模型和优化器状态。在本教程的后面部分，您将找到有关如何保存检查点及其用途的信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The learning rate of the optimizer is made configurable, too:"
msgstr "优化器的学习率也被设置为可配置项:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We also split the training data into a training and validation subset. We "
"thus train on 80% of the data and calculate the validation loss on the "
"remaining 20%. The batch sizes with which we iterate through the training "
"and test sets are configurable as well."
msgstr ""
"我们还将训练数据分为训练集和验证集。我们因此使用80%的数据进行训练，并计算剩余20%的数据上的验证损失。迭代训练和测试集时的批量大小也是可配置的。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Adding (multi) GPU support with DataParallel"
msgstr "使用DataParallel添加（多）GPU支持"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Image classification benefits largely from GPUs. Luckily, we can continue to"
" use PyTorch's abstractions in Ray Tune. Thus, we can wrap our model in "
"``nn.DataParallel`` to support data parallel training on multiple GPUs:"
msgstr ""
"图像分类在很大程度上受益于GPU。幸运的是，我们可以继续在Ray "
"Tune中使用PyTorch的抽象。因此，我们可以使用``nn.DataParallel``包装我们的模型以支持多GPU上的数据并行训练："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By using a ``device`` variable we make sure that training also works when we"
" have no GPUs available. PyTorch requires us to send our data to the GPU "
"memory explicitly, like this:"
msgstr "通过使用``device``变量，我们确保即使没有GPU也可以进行训练。PyTorch要求我们明确地将数据发送到GPU内存，如这样："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The code now supports training on CPUs, on a single GPU, and on multiple "
"GPUs. Notably, Ray also supports `fractional GPUs "
"<https://docs.ray.io/en/master/using-ray-with-gpus.html#fractional-gpus>`_ "
"so we can share GPUs among trials, as long as the model still fits on the "
"GPU memory. We'll come back to that later."
msgstr ""
"此代码现在支持在CPU、单个GPU和多个GPU上的训练。值得注意的是，Ray还支持`部分GPU "
"<https://docs.ray.io/zh/latest/using-ray-with-gpus.html#fractional-gpus>` "
"，因此我们可以在试验之间共享GPU，只要模型仍能适应GPU内存。我们稍后会回到这个问题。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Communicating with Ray Tune"
msgstr "与Ray Tune通信"

#: ../../beginner/vt_tutorial.rst:783
msgid "The most interesting part is the communication with Ray Tune:"
msgstr "最有趣的部分是与Ray Tune的通信："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here we first save a checkpoint and then report some metrics back to Ray "
"Tune. Specifically, we send the validation loss and accuracy back to Ray "
"Tune. Ray Tune can then use these metrics to decide which hyperparameter "
"configuration lead to the best results. These metrics can also be used to "
"stop bad performing trials early in order to avoid wasting resources on "
"those trials."
msgstr ""
"在这里，我们首先保存一个检查点，然后将一些指标报告回Ray Tune。具体而言，我们将验证损失和准确性发送回Ray Tune。Ray "
"Tune可以使用这些指标来决定哪个超参数配置带来了最佳结果。这些指标还可以用于及早停止表现较差的试验，以避免在这些试验上浪费资源。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The checkpoint saving is optional, however, it is necessary if we wanted to "
"use advanced schedulers like `Population Based Training "
"<https://docs.ray.io/en/latest/tune/examples/pbt_guide.html>`_. Also, by "
"saving the checkpoint we can later load the trained models and validate them"
" on a test set. Lastly, saving checkpoints is useful for fault tolerance, "
"and it allows us to interrupt training and continue training later."
msgstr ""
"虽然检查点保存是可选的，但如果我们想使用高级调度程序，例如`基于群体训练 "
"<https://docs.ray.io/zh/latest/tune/examples/pbt_guide.html>`，则是必要的。此外，通过保存检查点，我们稍后可以加载训练过的模型并在测试集上验证它们。最后，保存检查点对于容错很有用，它允许我们中断训练并稍后继续训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Full training function"
msgstr "完整训练函数"

#: ../../beginner/vt_tutorial.rst:783
msgid "The full code example looks like this:"
msgstr "完整代码示例如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As you can see, most of the code is adapted directly from the original "
"example."
msgstr "正如您所见，大部分代码直接采用自原始示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Test set accuracy"
msgstr "测试集准确率"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Commonly the performance of a machine learning model is tested on a hold-out"
" test set with data that has not been used for training the model. We also "
"wrap this in a function:"
msgstr "通常，机器学习模型的性能在未用于训练模型的保留测试集上进行测试。我们也将其包装在一个函数中："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The function also expects a ``device`` parameter, so we can do the test set "
"validation on a GPU."
msgstr "该函数还期待一个``device``参数，因此我们可以在GPU上进行测试集验证。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Configuring the search space"
msgstr "配置搜索空间"

#: ../../beginner/vt_tutorial.rst:783
msgid "Lastly, we need to define Ray Tune's search space. Here is an example:"
msgstr "最后，我们需要定义Ray Tune的搜索空间。以下是一个示例："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``tune.choice()`` accepts a list of values that are uniformly sampled "
"from. In this example, the ``l1`` and ``l2`` parameters should be powers of "
"2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256. The ``lr`` "
"(learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly, "
"the batch size is a choice between 2, 4, 8, and 16."
msgstr ""
"``tune.choice()``接受一个值列表，并从中均匀抽样。在此示例中，``l1``和``l2``参数应为4到256之间的2的幂次，所以是4、8、16、32、64、128或256。``lr``（学习率）应在0.0001到0.1之间均匀抽样。最后，批量大小是2、4、8和16之间的选项。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"At each trial, Ray Tune will now randomly sample a combination of parameters"
" from these search spaces. It will then train a number of models in parallel"
" and find the best performing one among these. We also use the "
"``ASHAScheduler`` which will terminate bad performing trials early."
msgstr ""
"在每次试验中，Ray "
"Tune现在将随机抽样来自这些搜索空间的参数组合。然后，它将并行训练多个模型，并在这些模型中找到性能最佳的一个。我们还使用了``ASHAScheduler``，它将提前终止表现较差的试验。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We wrap the ``train_cifar`` function with ``functools.partial`` to set the "
"constant ``data_dir`` parameter. We can also tell Ray Tune what resources "
"should be available for each trial:"
msgstr ""
"我们使用``functools.partial``包装``train_cifar``函数来设置常量``data_dir``参数。我们还可以告诉Ray "
"Tune为每次试验提供哪些资源："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can specify the number of CPUs, which are then available e.g. to "
"increase the ``num_workers`` of the PyTorch ``DataLoader`` instances. The "
"selected number of GPUs are made visible to PyTorch in each trial. Trials do"
" not have access to GPUs that haven't been requested for them - so you don't"
" have to care about two trials using the same set of resources."
msgstr ""
"您可以指定CPU数量，例如增加PyTorch的``DataLoader``实例的``num_workers``。所选的GPU数量在每次试验中对PyTorch可见。试验无法访问未请求的GPU资源，因此您无需担心两个试验使用相同的资源集。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here we can also specify fractional GPUs, so something like "
"``gpus_per_trial=0.5`` is completely valid. The trials will then share GPUs "
"among each other. You just have to make sure that the models still fit in "
"the GPU memory."
msgstr ""
"在这里，我们还可以指定部分GPU，例如``gpus_per_trial=0.5``是完全有效的。试验之间将共享GPU。您只需要确保模型仍能适应GPU内存。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"After training the models, we will find the best performing one and load the"
" trained network from the checkpoint file. We then obtain the test set "
"accuracy and report everything by printing."
msgstr "训练模型后，我们将找到性能最好的一个并从检查点文件加载训练网络。然后我们获取测试集上的准确性并通过打印报告所有内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The full main function looks like this:"
msgstr "完整的主函数如下："

#: ../../beginner/vt_tutorial.rst:783
msgid "If you run the code, an example output could look like this:"
msgstr "如果您运行代码，示例输出可能如下所示："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Most trials have been stopped early in order to avoid wasting resources. The"
" best performing trial achieved a validation accuracy of about 47%, which "
"could be confirmed on the test set."
msgstr "大多数试验已提前停止以避免浪费资源。表现最佳的试验在验证集上取得约47%的准确率，这可以在测试集中得到确认。"

#: ../../beginner/vt_tutorial.rst:783
msgid "So that's it! You can now tune the parameters of your PyTorch models."
msgstr "就是这样！现在您可以调优PyTorch模型的参数了。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 33 minutes  57.026 seconds)"
msgstr "**脚本总运行时间:**（33分 57.026秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: hyperparameter_tuning_tutorial.py "
"<hyperparameter_tuning_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: hyperparameter_tuning_tutorial.py "
"<hyperparameter_tuning_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: hyperparameter_tuning_tutorial.ipynb "
"<hyperparameter_tuning_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook: hyperparameter_tuning_tutorial.ipynb "
"<hyperparameter_tuning_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to PyTorch - YouTube Series"
msgstr "PyTorch介绍 - YouTube系列"

#: ../../beginner/vt_tutorial.rst:783
msgid "This page has been moved."
msgstr "此页面已移动。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Redirecting now..."
msgstr "正在重定向..."

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_introyt_autogradyt_tutorial.py>` to download the"
" full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_introyt_autogradyt_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || `Tensors "
"<tensors_deeper_tutorial.html>`_ || **Autograd** || `Building Models "
"<modelsyt_tutorial.html>`_ || `TensorBoard Support "
"<tensorboardyt_tutorial.html>`_ || `Training Models <trainingyt.html>`_ || "
"`Model Understanding <captumyt.html>`_"
msgstr ""
"`介绍 <introyt1_tutorial.html>`_ || `张量 <tensors_deeper_tutorial.html>`_ || "
"**自动求导** || `构建模型 <modelsyt_tutorial.html>`_ || `TensorBoard支持 "
"<tensorboardyt_tutorial.html>`_ || `训练模型 <trainingyt.html>`_ || `模型理解 "
"<captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "The Fundamentals of Autograd"
msgstr "自动求导的基础知识"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=M0fX15_-xrY>`__."
msgstr ""
"请观看下面的视频或在 `YouTube <https://www.youtube.com/watch?v=M0fX15_-xrY>`__ 上观看。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch’s *Autograd* feature is part of what make PyTorch flexible and fast "
"for building machine learning projects. It allows for the rapid and easy "
"computation of multiple partial derivatives (also referred to as "
"*gradients)* over a complex computation. This operation is central to "
"backpropagation-based neural network learning."
msgstr ""
"PyTorch的 *自动求导* 功能是使PyTorch在构建机器学习项目时灵活且快速的原因之一。它允许对复杂计算进行快速且简单的多重偏导数（也称为 "
"*梯度*）计算。这一操作对于基于反向传播的神经网络学习至关重要。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The power of autograd comes from the fact that it traces your computation "
"dynamically *at runtime,* meaning that if your model has decision branches, "
"or loops whose lengths are not known until runtime, the computation will "
"still be traced correctly, and you’ll get correct gradients to drive "
"learning. This, combined with the fact that your models are built in Python,"
" offers far more flexibility than frameworks that rely on static analysis of"
" a more rigidly-structured model for computing gradients."
msgstr ""
"自动求导的强大之处在于它在运行时动态记录计算，这意味着如果您的模型包含决策分支或者长度在运行时才能确定的循环，计算仍会被正确地记录，您将获得能够驱动学习的正确梯度。这一特性，加上模型使用Python构建，比依赖静态分析的更加僵化的框架提供了更大的灵活性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "What Do We Need Autograd For?"
msgstr "我们为什么需要自动求导？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A machine learning model is a *function*, with inputs and outputs. For this "
"discussion, we’ll treat the inputs as an *i*-dimensional vector "
":math:`\\vec{x}`, with elements :math:`x_{i}`. We can then express the "
"model, *M*, as a vector-valued function of the input: :math:`\\vec{y} = "
"\\vec{M}(\\vec{x})`. (We treat the value of M’s output as a vector because "
"in general, a model may have any number of outputs.)"
msgstr ""
"机器学习模型是一个 *函数*，具有输入和输出。在本次讨论中，我们将输入视为一个 *i*-维向量 :math:`\\vec{x}`，元素为 "
":math:`x_{i}`。然后我们可以将模型 *M* 表示为输入的向量值函数：:math:`\\vec{y} = "
"\\vec{M}(\\vec{x})`。（我们将M的输出值视为一个向量，因为一般情况下，模型可能有任意数量的输出。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Since we’ll mostly be discussing autograd in the context of training, our "
"output of interest will be the model’s loss. The *loss function* "
"L(:math:`\\vec{y}`) = L(:math:`\\vec{M}`\\ (:math:`\\vec{x}`)) is a single-"
"valued scalar function of the model’s output. This function expresses how "
"far off our model’s prediction was from a particular input’s *ideal* output."
" *Note: After this point, we will often omit the vector sign where it should"
" be contextually clear - e.g.,* :math:`y` instead of :math:`\\vec y`."
msgstr ""
"由于我们主要是在训练上下文中讨论自动求导，因此我们感兴趣的输出将是模型的损失。*损失函数* L(:math:`\\vec{y}`) = "
"L(:math:`\\vec{M}`\\ (:math:`\\vec{x}`)) 是模型输出的单值标量函数。该函数表明我们模型的预测与某个输入的 "
"*理想* 输出之间的误差距离。*注意：之后在上下文清晰的情况下，我们将省略向量符号，例如使用* :math:`y` *代替* :math:`\\vec "
"y`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In training a model, we want to minimize the loss. In the idealized case of "
"a perfect model, that means adjusting its learning weights - that is, the "
"adjustable parameters of the function - such that loss is zero for all "
"inputs. In the real world, it means an iterative process of nudging the "
"learning weights until we see that we get a tolerable loss for a wide "
"variety of inputs."
msgstr ""
"在训练模型时，我们希望最小化损失。在理想情况下，对于一个完美模型，这意味着调整它的学习权重 - 即函数的可调参数 - "
"使得所有输入的损失为零。在现实情况下，这意味着一种迭代的过程，通过不断调整学习权重，直到在各种输入上得到可接受的损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How do we decide how far and in which direction to nudge the weights? We "
"want to *minimize* the loss, which means making its first derivative with "
"respect to the input equal to 0: :math:`\\frac{\\partial L}{\\partial x} = "
"0`."
msgstr ""
"我们如何决定用多大幅度以及向哪个方向调整权重呢？我们想要 *最小化* "
"损失，这意味着使它相对于输入的一阶导数等于零：:math:`\\frac{\\partial L}{\\partial x} = 0`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Recall, though, that the loss is not *directly* derived from the input, but "
"a function of the model’s output (which is a function of the input "
"directly), :math:`\\frac{\\partial L}{\\partial x}` = "
":math:`\\frac{\\partial {L({\\vec y})}}{\\partial x}`. By the chain rule of "
"differential calculus, we have :math:`\\frac{\\partial {L({\\vec "
"y})}}{\\partial x}` = :math:`\\frac{\\partial L}{\\partial "
"y}\\frac{\\partial y}{\\partial x}` = :math:`\\frac{\\partial L}{\\partial "
"y}\\frac{\\partial M(x)}{\\partial x}`."
msgstr ""
"然而，请记住，损失并不是 *直接* 从输入导出的，而是模型输出的函数（直接是输入的函数），:math:`\\frac{\\partial "
"L}{\\partial x}` = :math:`\\frac{\\partial {L({\\vec y})}}{\\partial "
"x}`。通过微积分中的链式法则，我们得到 :math:`\\frac{\\partial {L({\\vec y})}}{\\partial x}` ="
" :math:`\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}` = "
":math:`\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":math:`\\frac{\\partial M(x)}{\\partial x}` is where things get complex. The"
" partial derivatives of the model’s outputs with respect to its inputs, if "
"we were to expand the expression using the chain rule again, would involve "
"many local partial derivatives over every multiplied learning weight, every "
"activation function, and every other mathematical transformation in the "
"model. The full expression for each such partial derivative is the sum of "
"the products of the local gradient of *every possible path* through the "
"computation graph that ends with the variable whose gradient we are trying "
"to measure."
msgstr ""
":math:`\\frac{\\partial M(x)}{\\partial x}` "
"是复杂的地方。模型输出相对于输入的偏导数，如果我们再次应用链式法则展开表达式，将涉及模型中每个乘法学习权重、每个激活函数以及其他每个数学变换的局部偏导数。每个这样的偏导数的完整表达式是通过计算图中每条可能路径的局部梯度之积的总和，该路径最终以我们试图测量梯度的变量结束。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In particular, the gradients over the learning weights are of interest to us"
" - they tell us *what direction to change each weight* to get the loss "
"function closer to zero."
msgstr "特别是，学习权重上的梯度对我们很重要 - 它们告诉我们把每个权重调整到哪个方向，以使损失函数更接近零。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Since the number of such local derivatives (each corresponding to a separate"
" path through the model’s computation graph) will tend to go up "
"exponentially with the depth of a neural network, so does the complexity in "
"computing them. This is where autograd comes in: It tracks the history of "
"every computation. Every computed tensor in your PyTorch model carries a "
"history of its input tensors and the function used to create it. Combined "
"with the fact that PyTorch functions meant to act on tensors each have a "
"built-in implementation for computing their own derivatives, this greatly "
"speeds the computation of the local derivatives needed for learning."
msgstr ""
"因为这样的局部导数的数量（每个都对应模型计算图中的不同路径）通常会随着神经网络的深度指数增加，计算它们的复杂性也如此。这就是自动求导的作用：它跟踪每次计算的历史。PyTorch模型中每个计算的张量都会携带输入张量的历史以及用于创建它的函数的记录。加上PyTorch中的用于执行张量操作的函数都内置了计算其自身导数的实现，这极大地加速了学习所需局部导数的计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid "A Simple Example"
msgstr "简单示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That was a lot of theory - but what does it look like to use autograd in "
"practice?"
msgstr "理论讲了很多 - 实际使用自动求导是什么样子的呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s start with a straightforward example. First, we’ll do some imports to "
"let us graph our results:"
msgstr "让我们从一个简单的示例开始。首先，我们导入一些库以便绘制结果："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we’ll create an input tensor full of evenly spaced values on the "
"interval :math:`[0, 2{\\pi}]`, and specify ``requires_grad=True``. (Like "
"most functions that create tensors, ``torch.linspace()`` accepts an optional"
" ``requires_grad`` option.) Setting this flag means that in every "
"computation that follows, autograd will be accumulating the history of the "
"computation in the output tensors of that computation."
msgstr ""
"接下来，我们创建一个充满区间 :math:`[0, 2{\\pi}]` 内均匀间隔值的输入张量，并指定 "
"``requires_grad=True``。（像其他许多创建张量的函数一样，``torch.linspace()`` 接受一个可选 "
"``requires_grad`` 参数。）设置此标志意味着在后续计算中，自动求导将积累计算的历史记录到输出张量中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we’ll perform a computation, and plot its output in terms of its "
"inputs:"
msgstr "接下来，我们进行一些计算，并在输入与输出上绘制其关系："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s have a closer look at the tensor ``b``. When we print it, we see an "
"indicator that it is tracking its computation history:"
msgstr "让我们仔细看看张量 ``b``。当我们打印它时，可以看到它正在跟踪其计算历史的指示符："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This ``grad_fn`` gives us a hint that when we execute the backpropagation "
"step and compute gradients, we’ll need to compute the derivative of "
":math:`\\sin(x)` for all this tensor’s inputs."
msgstr ""
"这个 ``grad_fn`` 提示我们，当我们执行反向传播步骤并计算梯度时，我们需要计算 :math:`\\sin(x)` 对此张量所有输入的导数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s perform some more computations:"
msgstr "让我们进行更多计算："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, let’s compute a single-element output. When you call "
"``.backward()`` on a tensor with no arguments, it expects the calling tensor"
" to contain only a single element, as is the case when computing a loss "
"function."
msgstr ""
"最后，我们计算一个单元素输出。当您调用没有参数的 ``.backward()`` 时，该张量必须只包含一个元素，就如我们在计算损失函数时的情况。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each ``grad_fn`` stored with our tensors allows you to walk the computation "
"all the way back to its inputs with its ``next_functions`` property. We can "
"see below that drilling down on this property on ``d`` shows us the gradient"
" functions for all the prior tensors. Note that ``a.grad_fn`` is reported as"
" ``None``, indicating that this was an input to the function with no history"
" of its own."
msgstr ""
"每个存储在张量中的 ``grad_fn`` 都允许您通过其 ``next_functions`` 属性回溯到输入。我们可以看到以下对 ``d`` "
"的属性的详细检查显示了所有先前张量的梯度函数。注意 ``a.grad_fn`` 显示为 ``None``，表明它是无历史记录输入的函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"With all this machinery in place, how do we get derivatives out? You call "
"the ``backward()`` method on the output, and check the input’s ``grad`` "
"property to inspect the gradients:"
msgstr ""
"准备好所有这些机制后，我们如何获取导数呢？您可以调用输出上的 ``backward()`` 方法，并检查输入的 ``grad`` 属性以查看梯度："

#: ../../beginner/vt_tutorial.rst:783
msgid "Recall the computation steps we took to get here:"
msgstr "回想我们所执行的计算步骤："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Adding a constant, as we did to compute ``d``, does not change the "
"derivative. That leaves :math:`c = 2 * b = 2 * \\sin(a)`, the derivative of "
"which should be :math:`2 * \\cos(a)`. Looking at the graph above, that’s "
"just what we see."
msgstr ""
"添加常数（如我们用于计算``d``的操作）不会改变导数。这就只剩下 :math:`c = 2 * b = 2 * \\sin(a)`，它的导数应该是 "
":math:`2 * \\cos(a)`。观察上图，这恰好是我们所看到的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Be aware that only *leaf nodes* of the computation have their gradients "
"computed. If you tried, for example, ``print(c.grad)`` you’d get back "
"``None``. In this simple example, only the input is a leaf node, so only it "
"has gradients computed."
msgstr ""
"请注意只有 *计算的叶节点* 有梯度被计算。如果您尝试 ``print(c.grad)``，您会得到 ``None`` "
"。在这个简单的示例中，只有输入是叶节点，所以只有它的梯度被计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Autograd in Training"
msgstr "训练中的自动求导"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ve had a brief look at how autograd works, but how does it look when it’s"
" used for its intended purpose? Let’s define a small model and examine how "
"it changes after a single training batch. First, define a few constants, our"
" model, and some stand-ins for inputs and outputs:"
msgstr ""
"我们简单介绍了自动求导的工作原理，但在其预期目的下，它如何工作呢？让我们定义一个小模型并检查在单个训练批次后它会发生什么变化。首先定义几个常量，我们的模型，以及一些输入和输出的替代值："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One thing you might notice is that we never specify ``requires_grad=True`` "
"for the model’s layers. Within a subclass of ``torch.nn.Module``, it’s "
"assumed that we want to track gradients on the layers’ weights for learning."
msgstr ""
"您可能会注意到，我们从未特意为模型层指定 ``requires_grad=True``。在 ``torch.nn.Module`` "
"的子类中，默认假定我们希望对学习的层权重进行梯度跟踪。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If we look at the layers of the model, we can examine the values of the "
"weights, and verify that no gradients have been computed yet:"
msgstr "如果我们查看模型的各层，可以检查权重的值，并验证尚未计算梯度："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s see how this changes when we run through one training batch. For a "
"loss function, we’ll just use the square of the Euclidean distance between "
"our ``prediction`` and the ``ideal_output``, and we’ll use a basic "
"stochastic gradient descent optimizer."
msgstr ""
"让我们看看当我们训练一个批次后会发生什么变化。对于损失函数，我们将简单使用预测和理想输出之间的欧几里得距离的平方，并使用一个基本的随机梯度下降优化器。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now, let’s call ``loss.backward()`` and see what happens:"
msgstr "现在，让我们调用 ``loss.backward()`` 并看到会发生什么："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can see that the gradients have been computed for each learning weight, "
"but the weights remain unchanged, because we haven’t run the optimizer yet. "
"The optimizer is responsible for updating model weights based on the "
"computed gradients."
msgstr "我们看到每个学习权重都计算出了梯度，但权重保持不变，因为我们还没有运行优化器。优化器负责根据计算出的梯度更新模型权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid "You should see that ``layer2``\\ ’s weights have changed."
msgstr "您应该看到 ``layer2`` 的权重发生了变化。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One important thing about the process: After calling ``optimizer.step()``, "
"you need to call ``optimizer.zero_grad()``, or else every time you run "
"``loss.backward()``, the gradients on the learning weights will accumulate:"
msgstr ""
"重要的一点是：调用 ``optimizer.step()`` 之后，您需要调用 ``optimizer.zero_grad()``，否则每次您运行 "
"``loss.backward()`` 时，学习权重的梯度都会积累："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"After running the cell above, you should see that after running "
"``loss.backward()`` multiple times, the magnitudes of most of the gradients "
"will be much larger. Failing to zero the gradients before running your next "
"training batch will cause the gradients to blow up in this manner, causing "
"incorrect and unpredictable learning results."
msgstr ""
"运行上面的代码后，您会看到多次运行 ``loss.backward()`` "
"后，大多数梯度的幅值会变得更大。如果在运行下一训练批次之前没有清零梯度，这些梯度就会因这种现象而爆炸，导致学习结果不正确且不可预测。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Turning Autograd Off and On"
msgstr "打开和关闭自动求导"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are situations where you will need fine-grained control over whether "
"autograd is enabled. There are multiple ways to do this, depending on the "
"situation."
msgstr "有些情况下，您需要对是否启用自动求导进行精细控制。根据具体情况，有多种方法可以做到这一点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The simplest is to change the ``requires_grad`` flag on a tensor directly:"
msgstr "最简单的方法是直接修改张量上的 ``requires_grad`` 标志："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the cell above, we see that ``b1`` has a ``grad_fn`` (i.e., a traced "
"computation history), which is what we expect, since it was derived from a "
"tensor, ``a``, that had autograd turned on. When we turn off autograd "
"explicitly with ``a.requires_grad = False``, computation history is no "
"longer tracked, as we see when we compute ``b2``."
msgstr ""
"在上面的代码中，我们看到 ``b1`` 有一个 "
"``grad_fn``（即一个已经跟踪的计算历史），这是预期的，因为它来源于开启自动求导的张量``a``。当我们通过 ``a.requires_grad"
" = False`` 显式关闭自动求导时，计算历史就不再跟踪了。计算得到的 ``b2`` 证实了这一点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you only need autograd turned off temporarily, a better way is to use the"
" ``torch.no_grad()``:"
msgstr "如果您只需要暂时关闭自动求导，最好使用 ``torch.no_grad()``："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.no_grad()`` can also be used as a function or method decorator:"
msgstr "``torch.no_grad()`` 也可以用作函数或方法的装饰器："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There’s a corresponding context manager, ``torch.enable_grad()``, for "
"turning autograd on when it isn’t already. It may also be used as a "
"decorator."
msgstr "有一个对应的上下文管理器 ``torch.enable_grad()``，用于在自动求导未启用时将其打开。它也可以用作装饰器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, you may have a tensor that requires gradient tracking, but you want"
" a copy that does not. For this we have the ``Tensor`` object’s ``detach()``"
" method - it creates a copy of the tensor that is *detached* from the "
"computation history:"
msgstr ""
"最后，可能会有一种情况：一个张量需要跟踪梯度，但你需要一个不跟踪梯度的副本。这时可以使用 ``Tensor`` 对象的 ``detach()`` "
"方法——它会创建一个与计算历史 *分离* 的张量副本："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We did this above when we wanted to graph some of our tensors. This is "
"because ``matplotlib`` expects a NumPy array as input, and the implicit "
"conversion from a PyTorch tensor to a NumPy array is not enabled for tensors"
" with requires_grad=True. Making a detached copy lets us move forward."
msgstr ""
"我们之前在绘制某些张量的时候已经用过这种方法。这是因为 ``matplotlib`` 需要一个 NumPy 数组作为输入，而从 PyTorch 张量到 "
"NumPy 数组的隐式转换对带有 requires_grad=True 的张量是不可用的。创建一个分离的副本可以让我们继续前进。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Autograd and In-place Operations"
msgstr "自动微分与就地操作"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In every example in this notebook so far, we’ve used variables to capture "
"the intermediate values of a computation. Autograd needs these intermediate "
"values to perform gradient computations. *For this reason, you must be "
"careful about using in-place operations when using autograd.* Doing so can "
"destroy information you need to compute derivatives in the ``backward()`` "
"call. PyTorch will even stop you if you attempt an in-place operation on "
"leaf variable that requires autograd, as shown below."
msgstr ""
"到目前为止，在本笔记本中的每个示例中，我们都使用变量来捕获计算的中间值。自动微分需要这些中间值来执行梯度计算。*因此，使用自动微分时必须谨慎使用就地操作。*进行就地操作可能会破坏调用"
" ``backward()`` 时计算导数所需的信息。如果您尝试对需要自动微分的叶变量进行就地操作，PyTorch 会阻止您，如下所示。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The following code cell throws a runtime error. This is expected."
msgstr "以下代码单元会引发运行时错误。这是预期的行为。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Autograd Profiler"
msgstr "自动微分性能分析器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Autograd tracks every step of your computation in detail. Such a computation"
" history, combined with timing information, would make a handy profiler - "
"and autograd has that feature baked in. Here’s a quick example usage:"
msgstr "自动微分会详细跟踪您计算的每一步。这种计算历史结合时间信息，可用作一个有用的性能分析器——而自动微分内置了这一功能。以下是快速示例："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The profiler can also label individual sub-blocks of code, break out the "
"data by input tensor shape, and export data as a Chrome tracing tools file. "
"For full details of the API, see the `documentation "
"<https://pytorch.org/docs/stable/autograd.html#profiler>`__."
msgstr ""
"性能分析器还可以标记代码的单独子块，根据输入张量形状分解数据，并将数据导出为 Chrome 跟踪工具文件。有关 API 的完整详细信息，请参阅 `文档 "
"<https://pytorch.org/docs/stable/autograd.html#profiler>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Advanced Topic: More Autograd Detail and the High-Level API"
msgstr "高级主题：更多自动微分细节和高级API"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you have a function with an n-dimensional input and m-dimensional output,"
" :math:`\\vec{y}=f(\\vec{x})`, the complete gradient is a matrix of the "
"derivative of every output with respect to every input, called the "
"*Jacobian:*"
msgstr ""
"如果您有一个 n 维输入和 m 维输出的函数 :math:`\\vec{y}=f(\\vec{x})`，完整的梯度是一个输出与每个输入的衍生矩阵，称为 "
"*雅可比矩阵*："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"J\n"
"=\n"
"\\left(\\begin{array}{ccc}\n"
"\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n"
"\\vdots & \\ddots & \\vdots\\\\\n"
"\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
"\\end{array}\\right)"
msgstr ""
"J\n"
"=\n"
"\\left(\\begin{array}{ccc}\n"
"\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n"
"\\vdots & \\ddots & \\vdots\\\\\n"
"\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
"\\end{array}\\right)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you have a second function, :math:`l=g\\left(\\vec{y}\\right)` that takes"
" m-dimensional input (that is, the same dimensionality as the output above),"
" and returns a scalar output, you can express its gradients with respect to "
":math:`\\vec{y}` as a column vector, "
":math:`v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & "
"\\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}` - "
"which is really just a one-column Jacobian."
msgstr ""
"如果您有第二个函数 :math:`l=g\\left(\\vec{y}\\right)`，它接受 m "
"维输入（即与上述输出具有相同维数）并返回一个标量输出，您可以将它相对于 :math:`\\vec{y}` 的梯度表示为列向量 "
":math:`v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & "
"\\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}` - "
"它实际上是一个单列雅可比矩阵。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"More concretely, imagine the first function as your PyTorch model (with "
"potentially many inputs and many outputs) and the second function as a loss "
"function (with the model’s output as input, and the loss value as the scalar"
" output)."
msgstr ""
"更具体地，将第一个函数想象为您的 PyTorch 模型（可能有许多输入和输出），第二个函数为损失函数（模型输出作为输入，损失值作为标量输出）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If we multiply the first function’s Jacobian by the gradient of the second "
"function, and apply the chain rule, we get:"
msgstr "如果我们把第一个函数的雅可比矩阵与第二个函数的梯度相乘并应用链式法则，我们得到："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n"
"\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n"
"\\vdots & \\ddots & \\vdots\\\\\n"
"\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
"\\end{array}\\right)\\left(\\begin{array}{c}\n"
"\\frac{\\partial l}{\\partial y_{1}}\\\\\n"
"\\vdots\\\\\n"
"\\frac{\\partial l}{\\partial y_{m}}\n"
"\\end{array}\\right)=\\left(\\begin{array}{c}\n"
"\\frac{\\partial l}{\\partial x_{1}}\\\\\n"
"\\vdots\\\\\n"
"\\frac{\\partial l}{\\partial x_{n}}\n"
"\\end{array}\\right)"
msgstr ""
"J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n"
"\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n"
"\\vdots & \\ddots & \\vdots\\\\\n"
"\\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n"
"\\end{array}\\right)\\left(\\begin{array}{c}\n"
"\\frac{\\partial l}{\\partial y_{1}}\\\\\n"
"\\vdots\\\\\n"
"\\frac{\\partial l}{\\partial y_{m}}\n"
"\\end{array}\\right)=\\left(\\begin{array}{c}\n"
"\\frac{\\partial l}{\\partial x_{1}}\\\\\n"
"\\vdots\\\\\n"
"\\frac{\\partial l}{\\partial x_{n}}\n"
"\\end{array}\\right)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note: You could also use the equivalent operation :math:`v^{T}\\cdot J`, and"
" get back a row vector."
msgstr "注意：您也可以使用等效的操作 :math:`v^{T}\\cdot J`，得到一个行向量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The resulting column vector is the *gradient of the second function with "
"respect to the inputs of the first* - or in the case of our model and loss "
"function, the gradient of the loss with respect to the model inputs."
msgstr "所得到的列向量是 *第二个函数对第一个函数输入的梯度*——在我们的模型和损失函数的情况下，就是损失相对于模型输入的梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**``torch.autograd`` is an engine for computing these products.** This is "
"how we accumulate the gradients over the learning weights during the "
"backward pass."
msgstr "**``torch.autograd`` 是计算这些乘积的引擎。** 这就是我们在反向传播过程中积累学习权重上的梯度的方式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this reason, the ``backward()`` call can *also* take an optional vector "
"input. This vector represents a set of gradients over the tensor, which are "
"multiplied by the Jacobian of the autograd-traced tensor that precedes it. "
"Let’s try a specific example with a small vector:"
msgstr ""
"因此，``backward()`` "
"调用还可以接收一个可选的向量输入。该向量表示张量上的一组梯度，这些梯度会与自动微分跟踪的前一个张量的雅可比矩阵相乘。让我们用一个小向量试试具体的例子："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If we tried to call ``y.backward()`` now, we’d get a runtime error and a "
"message that gradients can only be *implicitly* computed for scalar outputs."
" For a multi-dimensional output, autograd expects us to provide gradients "
"for those three outputs that it can multiply into the Jacobian:"
msgstr ""
"如果我们现在尝试调用 ``y.backward()``，会得到一个运行时错误并显示消息，表明仅可为标量输出 *隐式地* "
"计算梯度。对于多维输出，自动微分希望我们提供这三个输出的梯度，以便它可以与雅可比矩阵相乘："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"(Note that the output gradients are all related to powers of two - which "
"we’d expect from a repeated doubling operation.)"
msgstr "（注意，输出梯度都与二次方相关——这是符合重复倍增操作预期的结果。）"

#: ../../beginner/vt_tutorial.rst:783
msgid "The High-Level API"
msgstr "高级API"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There is an API on autograd that gives you direct access to important "
"differential matrix and vector operations. In particular, it allows you to "
"calculate the Jacobian and the *Hessian* matrices of a particular function "
"for particular inputs. (The Hessian is like the Jacobian, but expresses all "
"partial *second* derivatives.) It also provides methods for taking vector "
"products with these matrices."
msgstr ""
"在自动微分上有一个API，它允许您直接访问重要的微分矩阵和向量操作。特别是，它允许您计算特定输入和函数的雅可比矩阵和 "
"*海森矩阵*（海森矩阵类似于雅可比矩阵，但表示所有的二阶导数）。它还提供了与这些矩阵进行向量积的方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s take the Jacobian of a simple function, evaluated for a 2 single-"
"element inputs:"
msgstr "让我们对一个简单函数计算雅可比矩阵，并使用两个单元素输入进行评估："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you look closely, the first output should equal :math:`2e^x` (since the "
"derivative of :math:`e^x` is :math:`e^x`), and the second value should be 3."
msgstr ""
"如果仔细看，第一个输出应该等于 :math:`2e^x`（因为 :math:`e^x` 的导数是 :math:`e^x`），第二个值应该是 3。"

#: ../../beginner/vt_tutorial.rst:783
msgid "You can, of course, do this with higher-order tensors:"
msgstr "当然，您可以对高阶张量执行此操作："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``torch.autograd.functional.hessian()`` method works identically "
"(assuming your function is twice differentiable), but returns a matrix of "
"all second derivatives."
msgstr ""
"``torch.autograd.functional.hessian()`` 方法完全相同（假设您的函数是二次可微的），但会返回所有二阶导数的矩阵。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There is also a function to directly compute the vector-Jacobian product, if"
" you provide the vector:"
msgstr "如果提供一个向量，该函数还可以直接计算向量-雅可比矩阵乘积："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``torch.autograd.functional.jvp()`` method performs the same matrix "
"multiplication as ``vjp()`` with the operands reversed. The ``vhp()`` and "
"``hvp()`` methods do the same for a vector-Hessian product."
msgstr ""
"``torch.autograd.functional.jvp()`` 方法执行与 ``vjp()`` "
"相同的矩阵乘法，但操作数是反转的。``vhp()`` 和 ``hvp()`` 方法对向量-海森矩阵乘积也有相同作用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information, including performance notes on the `docs for the "
"functional API <https://pytorch.org/docs/stable/autograd.html#functional-"
"higher-level-api>`__"
msgstr ""
"有关详细信息，包括性能注意事项，请参阅 `功能API文档 "
"<https://pytorch.org/docs/stable/autograd.html#functional-higher-level-"
"api>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  1.311 seconds)"
msgstr "**脚本总运行时间：**（ 0 分钟 1.311 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: autogradyt_tutorial.py "
"<autogradyt_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: autogradyt_tutorial.py <autogradyt_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: autogradyt_tutorial.ipynb "
"<autogradyt_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本: autogradyt_tutorial.ipynb "
"<autogradyt_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_introyt_captumyt.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_introyt_captumyt.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || `Tensors "
"<tensors_deeper_tutorial.html>`_ || `Autograd <autogradyt_tutorial.html>`_ "
"|| `Building Models <modelsyt_tutorial.html>`_ || `TensorBoard Support "
"<tensorboardyt_tutorial.html>`_ || `Training Models <trainingyt.html>`_ || "
"**Model Understanding**"
msgstr ""
"`简介 <introyt1_tutorial.html>`_ || `张量 <tensors_deeper_tutorial.html>`_ || "
"`自动微分 <autogradyt_tutorial.html>`_ || `构建模型 <modelsyt_tutorial.html>`_ || "
"`TensorBoard支持 <tensorboardyt_tutorial.html>`_ || `训练模型 <trainingyt.html>`_ "
"|| **模型理解**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Model Understanding with Captum"
msgstr "利用 Captum 理解模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=Am2EF9CLu-g>`__. Download the notebook and "
"corresponding files `here <https://pytorch-tutorial-"
"assets.s3.amazonaws.com/youtube-series/video7.zip>`__."
msgstr ""
"可以观看下面的视频或在 `YouTube <https://www.youtube.com/watch?v=Am2EF9CLu-g>`__ 上观看。点击"
" `这里 <https://pytorch-tutorial-assets.s3.amazonaws.com/youtube-"
"series/video7.zip>`__ 下载笔记本和对应文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Captum <https://captum.ai/>`__ (“comprehension” in Latin) is an open "
"source, extensible library for model interpretability built on PyTorch."
msgstr ""
"`Captum <https://captum.ai/>`__ （在拉丁语中意为“理解”）是一个基于 PyTorch "
"构建的开放源代码、可扩展的模型可解释性库。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"With the increase in model complexity and the resulting lack of "
"transparency, model interpretability methods have become increasingly "
"important. Model understanding is both an active area of research as well as"
" an area of focus for practical applications across industries using machine"
" learning. Captum provides state-of-the-art algorithms, including Integrated"
" Gradients, to provide researchers and developers with an easy way to "
"understand which features are contributing to a model’s output."
msgstr ""
"随着模型复杂性的增加及由此导致的透明性缺失，模型可解释性方法变得日益重要。模型理解既是一个活跃的研究领域，也是机器学习在行业中的实践应用的重点领域。Captum"
" 提供包括集成梯度算法在内的最先进算法，为研究人员和开发人员提供一种易于理解哪些特征对模型输出有贡献的方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Full documentation, an API reference, and a suite of tutorials on specific "
"topics are available at the `captum.ai <https://captum.ai/>`__ website."
msgstr "完整文档、API 参考和一系列特定主题的教程可在 `captum.ai <https://captum.ai/>`__ 网站上找到。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Captum’s approach to model interpretability is in terms of *attributions.* "
"There are three kinds of attributions available in Captum:"
msgstr "Captum 的模型可解释性方法是基于 *归因* 的。Captum 有三种归因类型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Feature Attribution** seeks to explain a particular output in terms of "
"features of the input that generated it. Explaining whether a movie review "
"was positive or negative in terms of certain words in the review is an "
"example of feature attribution."
msgstr ""
"**特征归因** 试图根据生成特定输出的输入特征来解释该输出。比如，用某些评论中的词语来解释一篇电影评论是积极的还是消极的就是特征归因的一个例子。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Layer Attribution** examines the activity of a model’s hidden layer "
"subsequent to a particular input. Examining the spatially-mapped output of a"
" convolutional layer in response to an input image in an example of layer "
"attribution."
msgstr "**层归因** 检查模型隐藏层在某个特定输入之后的活动情况。比如，检查卷积层对输入图像的空间映射输出是层归因的一个例子。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Neuron Attribution** is analagous to layer attribution, but focuses on the"
" activity of a single neuron."
msgstr "**神经元归因** 类似于层归因，但关注于单个神经元的活动。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this interactive notebook, we’ll look at Feature Attribution and Layer "
"Attribution."
msgstr "在此交互式笔记本中，我们将研究特征归因和层归因。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each of the three attribution types has multiple **attribution algorithms** "
"associated with it. Many attribution algorithms fall into two broad "
"categories:"
msgstr "三个归因类型中的每一个都有多个 **归因算法** 与之相关。许多归因算法分为两大类："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Gradient-based algorithms** calculate the backward gradients of a model "
"output, layer output, or neuron activation with respect to the input. "
"**Integrated Gradients** (for features), **Layer Gradient \\* Activation**, "
"and **Neuron Conductance** are all gradient-based algorithms."
msgstr ""
"**基于梯度的算法** 计算模型输出、层输出或神经元激活相对于输入的反向梯度。**集成梯度**（针对特征）、**层梯度 \\* 激活** 和 "
"**神经元导通值** 都是基于梯度的算法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Perturbation-based algorithms** examine the changes in the output of a "
"model, layer, or neuron in response to changes in the input. The input "
"perturbations may be directed or random. **Occlusion,** **Feature "
"Ablation,** and **Feature Permutation** are all perturbation-based "
"algorithms."
msgstr ""
"**基于扰动的算法** 检查模型、层或神经元的输出随输入变化的变化。输入扰动可以是定向的或随机的。**遮挡**、**特征消融** 和 **特征置换** "
"都是基于扰动的算法。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We’ll be examining algorithms of both types below."
msgstr "我们将在下面研究上述两种类型的算法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Especially where large models are involved, it can be valuable to visualize "
"attribution data in ways that relate it easily to the input features being "
"examined. While it is certainly possible to create your own visualizations "
"with Matplotlib, Plotly, or similar tools, Captum offers enhanced tools "
"specific to its attributions:"
msgstr ""
"特别是对于大型模型，可视化归因数据以便于将其与被检查的输入特征关联起来可能非常有价值。虽然可以使用 Matplotlib、Plotly "
"或类似工具自行创建可视化，但 Captum 提供了特有的增强工具以针对其归因进行可视化："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``captum.attr.visualization`` module (imported below as ``viz``) "
"provides helpful functions for visualizing attributions related to images."
msgstr "``captum.attr.visualization`` 模块（以下导入为 ``viz``）提供了用于可视化与图像相关归因的实用函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Captum Insights** is an easy-to-use API on top of Captum that provides a "
"visualization widget with ready-made visualizations for image, text, and "
"arbitrary model types."
msgstr ""
"**Captum Insights** 是 Captum 之上的易用API，其提供一个可视化小部件，包括用于图像、文本以及任意模型类型的预制可视化功能。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Both of these visualization toolsets will be demonstrated in this notebook. "
"The first few examples will focus on computer vision use cases, but the "
"Captum Insights section at the end will demonstrate visualization of "
"attributions in a multi-model, visual question-and-answer model."
msgstr ""
"这两种可视化工具集都将在本笔记本中演示。前几个示例将侧重于计算机视觉的用例，而最后的 Captum Insights "
"部分将演示多模型视觉问答模型中的归因可视化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Before you get started, you need to have a Python environment with:"
msgstr "在开始之前，您需要一个安装有以下内容的 Python 环境："

#: ../../beginner/vt_tutorial.rst:783
msgid "Python version 3.6 or higher"
msgstr "Python 版本 3.6 或更高版本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For the Captum Insights example, Flask 1.1 or higher and Flask-Compress (the"
" latest version is recommended)"
msgstr "对于Captum Insights示例，需要Flask 1.1或更高版本以及Flask-Compress（推荐使用最新版本）"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch version 1.2 or higher (the latest version is recommended)"
msgstr "PyTorch版本1.2或更高版本（推荐使用最新版本）"

#: ../../beginner/vt_tutorial.rst:783
msgid "TorchVision version 0.6 or higher (the latest version is recommended)"
msgstr "TorchVision版本0.6或更高版本（推荐使用最新版本）"

#: ../../beginner/vt_tutorial.rst:783
msgid "Captum (the latest version is recommended)"
msgstr "Captum（推荐使用最新版本）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Matplotlib version 3.3.4, since Captum currently uses a Matplotlib function "
"whose arguments have been renamed in later versions"
msgstr "Matplotlib版本3.3.4，因为Captum目前使用了Matplotlib中的一个函数，其参数在更高版本中已经被重命名"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To install Captum in an Anaconda or pip virtual environment, use the "
"appropriate command for your environment below:"
msgstr "要在Anaconda或pip虚拟环境中安装Captum，请使用以下适合您的环境的命令："

#: ../../beginner/vt_tutorial.rst:783
msgid "With ``conda``:"
msgstr "使用``conda``："

#: ../../beginner/vt_tutorial.rst:783
msgid "With ``pip``:"
msgstr "使用``pip``："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Restart this notebook in the environment you set up, and you’re ready to go!"
msgstr "在您设置的环境中重新启动此笔记本，您就准备好了！"

#: ../../beginner/vt_tutorial.rst:783
msgid "A First Example"
msgstr "第一个示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To start, let’s take a simple, visual example. We’ll start with a ResNet "
"model pretrained on the ImageNet dataset. We’ll get a test input, and use "
"different **Feature Attribution** algorithms to examine how the input images"
" affect the output, and see a helpful visualization of this input "
"attribution map for some test images."
msgstr ""
"首先，让我们来看一个简单的可视化示例。我们将从一个在ImageNet数据集上预训练的ResNet模型开始。我们会获得一个测试输入，并使用不同的**特征归因**算法来研究输入图像如何影响输出，并为一些测试图像看到一个有用的输入归因图可视化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "First, some imports:"
msgstr "首先，一些导入："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we’ll use the TorchVision model library to download a pretrained ResNet."
" Since we’re not training, we’ll place it in evaluation mode for now."
msgstr "现在我们使用TorchVision模型库下载一个预训练的ResNet。由于我们不是在进行训练，目前会将其置于评估模式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The place where you got this interactive notebook should also have an "
"``img`` folder with a file ``cat.jpg`` in it."
msgstr "您获取此交互式笔记本的地方还应该有一个``img``文件夹，其中包含一个名为``cat.jpg``的文件。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our ResNet model was trained on the ImageNet dataset, and expects images to "
"be of a certain size, with the channel data normalized to a specific range "
"of values. We’ll also pull in the list of human-readable labels for the "
"categories our model recognizes - that should be in the ``img`` folder as "
"well."
msgstr ""
"我们的ResNet模型是在ImageNet数据集上训练的，并且预期图像需要特定大小，同时通道数据需要归一化到一个特定的值范围。我们也会列出模型识别的类别的人类可读标签列表——应该也在``img``文件夹中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, we can ask the question: What does our model think this image "
"represents?"
msgstr "现在，我们可以问一个问题：我们的模型认为这张图像表示什么？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ve confirmed that ResNet thinks our image of a cat is, in fact, a cat. "
"But *why* does the model think this is an image of a cat?"
msgstr "我们已经确认ResNet认为我们的猫图像确实是一只猫。但这个模型为什么认为这是猫的图像呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid "For the answer to that, we turn to Captum."
msgstr "对于该问题的答案，我们转向Captum。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Feature Attribution with Integrated Gradients"
msgstr "使用集成梯度进行特征归因"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Feature attribution** attributes a particular output to features of the "
"input. It uses a specific input - here, our test image - to generate a map "
"of the relative importance of each input feature to a particular output "
"feature."
msgstr ""
"**特征归因**将特定输出归因到输入的特征。它使用一个特定输入——在这里是我们的测试图像——生成一个相对重要性的地图，显示每个输入特征对于特定输出特征的重要性。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Integrated Gradients <https://captum.ai/api/integrated_gradients.html>`__ "
"is one of the feature attribution algorithms available in Captum. Integrated"
" Gradients assigns an importance score to each input feature by "
"approximating the integral of the gradients of the model’s output with "
"respect to the inputs."
msgstr ""
"`集成梯度 "
"<https://captum.ai/api/integrated_gradients.html>`__是Captum提供的特征归因算法之一。集成梯度通过与输入有关的模型输出梯度的积分来赋予每个输入特征一个重要性分数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In our case, we’re going to be taking a specific element of the output "
"vector - that is, the one indicating the model’s confidence in its chosen "
"category - and use Integrated Gradients to understand what parts of the "
"input image contributed to this output."
msgstr "在我们的案例中，我们会获取输出向量的一个特定元素——即模型对所选类别的置信度——并使用集成梯度来理解输入图像的哪些部分促成了该输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once we have the importance map from Integrated Gradients, we’ll use the "
"visualization tools in Captum to give a helpful representation of the "
"importance map. Captum’s ``visualize_image_attr()`` function provides a "
"variety of options for customizing display of your attribution data. Here, "
"we pass in a custom Matplotlib color map."
msgstr ""
"一旦我们拥有了集成梯度的重要性地图，我们会使用Captum中的可视化工具提供一个有助的表达此重要性地图的表示。Captum的``visualize_image_attr()``函数提供了多种选项，可以自定义属性数据的显示方式。在这里，我们传递了一个自定义的Matplotlib颜色地图。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Running the cell with the ``integrated_gradients.attribute()`` call will "
"usually take a minute or two."
msgstr "运行包含``integrated_gradients.attribute()``调用的单元格通常需要一到两分钟。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the image above, you should see that Integrated Gradients gives us the "
"strongest signal around the cat’s location in the image."
msgstr "在上面的图像中，您应该可以看到集成梯度在猫所在的图像区域给出了最强的信号。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Feature Attribution with Occlusion"
msgstr "使用遮挡进行特征归因"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Gradient-based attribution methods help to understand the model in terms of "
"directly computing out the output changes with respect to the input. "
"*Perturbation-based attribution* methods approach this more directly, by "
"introducing changes to the input to measure the effect on the output. "
"`Occlusion <https://captum.ai/api/occlusion.html>`__ is one such method. It "
"involves replacing sections of the input image, and examining the effect on "
"the output signal."
msgstr ""
"基于梯度的归因方法帮助理解模型，直观地计算输出如何随输入变化。*基于扰动的归因*方法更直接，通过改变输入来测量对输出的影响。`遮挡 "
"<https://captum.ai/api/occlusion.html>`__ "
"是一种这样的方法。它包含替换输入图像的某些部分，并检查对输出信号的影响。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below, we set up Occlusion attribution. Similarly to configuring a "
"convolutional neural network, you can specify the size of the target region,"
" and a stride length to determine the spacing of individual measurements. "
"We’ll visualize the output of our Occlusion attribution with "
"``visualize_image_attr_multiple()``, showing heat maps of both positive and "
"negative attribution by region, and by masking the original image with the "
"positive attribution regions. The masking gives a very instructive view of "
"what regions of our cat photo the model found to be most “cat-like”."
msgstr ""
"下面，我们设置遮挡归因。类似于配置卷积神经网络，您可以指定目标区域的大小以及步长以确定单个测量值的间距。我们会使用``visualize_image_attr_multiple()``可视化遮挡归因的输出，显示区域的正负归因热图，并通过屏蔽原始图像的正归因区域提供视图。这种遮罩提供了一个非常有启发性的视图，展示了模型对我们猫照片中的哪些区域认为最“像猫”。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Again, we see greater significance placed on the region of the image that "
"contains the cat."
msgstr "同样，我们看到模型对包含猫的图像区域赋予了更大的重要性。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Layer Attribution with Layer GradCAM"
msgstr "使用Layer GradCAM进行层归因"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Layer Attribution** allows you to attribute the activity of hidden layers "
"within your model to features of your input. Below, we’ll use a layer "
"attribution algorithm to examine the activity of one of the convolutional "
"layers within our model."
msgstr "**层归因**允许您将模型中的隐藏层活动归因到输入的特征。下面，我们会使用一个层归因算法来检查模型中的一个卷积隐藏层的活动。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"GradCAM computes the gradients of the target output with respect to the "
"given layer, averages for each output channel (dimension 2 of output), and "
"multiplies the average gradient for each channel by the layer activations. "
"The results are summed over all channels. GradCAM is designed for convnets; "
"since the activity of convolutional layers often maps spatially to the "
"input, GradCAM attributions are often upsampled and used to mask the input."
msgstr ""
"GradCAM计算目标输出对指定层的梯度，对于每个输出通道（输出的维度2）进行平均，并将每个通道的平均梯度与层激活相乘。结果会对所有通道求和。GradCAM是为卷积网络设计的；由于卷积层的活动通常在空间上映射到输入，GradCAM归因通常会被上采样并用于屏蔽输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Layer attribution is set up similarly to input attribution, except that in "
"addition to the model, you must specify a hidden layer within the model that"
" you wish to examine. As above, when we call ``attribute()``, we specify the"
" target class of interest."
msgstr ""
"层归因的设置与输入归因类似，不同之处在于除了模型本身之外，您还必须指定希望检查的模型中的隐藏层。同样，当我们调用``attribute()``时，会指定关注的目标类别。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ll use the convenience method ``interpolate()`` in the `LayerAttribution "
"<https://captum.ai/api/base_classes.html?highlight=layerattribution#captum.attr.LayerAttribution>`__"
" base class to upsample this attribution data for comparison to the input "
"image."
msgstr ""
"我们会使用`LayerAttribution "
"<https://captum.ai/api/base_classes.html?highlight=layerattribution#captum.attr.LayerAttribution>`__基类中的便捷方法``interpolate()``对归因数据进行上采样，以便与输入图像进行比较。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Visualizations such as this can give you novel insights into how your hidden"
" layers respond to your input."
msgstr "这样的可视化可以让您对隐藏层如何响应输入有新的洞察。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Visualization with Captum Insights"
msgstr "使用Captum Insights进行可视化"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Captum Insights is an interpretability visualization widget built on top of "
"Captum to facilitate model understanding. Captum Insights works across "
"images, text, and other features to help users understand feature "
"attribution. It allows you to visualize attribution for multiple "
"input/output pairs, and provides visualization tools for image, text, and "
"arbitrary data."
msgstr ""
"Captum Insights是一个基于Captum构建的可解释性可视化工具，可以帮助用户理解模型。Captum "
"Insights支持图像、文本和其他特征的可视化归因，允许用户查看模型对多个输入/输出对的归因，并提供图像、文本和任意数据的可视化工具。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this section of the notebook, we’ll visualize multiple image "
"classification inferences with Captum Insights."
msgstr "在笔记本的这一部分中，我们会使用Captum Insights可视化多次图像分类推断。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, let’s gather some image and see what the model thinks of them. For "
"variety, we’ll take our cat, a teapot, and a trilobite fossil:"
msgstr "首先，让我们收集一些图像并查看模型对它们的判断。为了增加些多样性，我们会选取一只猫，一个茶壶，以及一个三叶虫化石："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"…and it looks like our model is identifying them all correctly - but of "
"course, we want to dig deeper. For that we’ll use the Captum Insights "
"widget, which we configure with an ``AttributionVisualizer`` object, "
"imported below. The ``AttributionVisualizer`` expects batches of data, so "
"we’ll bring in Captum’s ``Batch`` helper class. And we’ll be looking at "
"images specifically, so well also import ``ImageFeature``."
msgstr ""
"...看起来我们的模型对它们的判断都正确——但当然，我们想要深入探究。为此我们会使用Captum "
"Insights小工具，该工具通过一个``AttributionVisualizer``对象进行配置，下面导入。``AttributionVisualizer``需要数据批处理，因此我们会使用Captum的``Batch``辅助类。我们还会特别关注图像，因此会导入``ImageFeature``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We configure the ``AttributionVisualizer`` with the following arguments:"
msgstr "我们用以下参数配置``AttributionVisualizer``："

#: ../../beginner/vt_tutorial.rst:783
msgid "An array of models to be examined (in our case, just the one)"
msgstr "一个待检查的模型数组（在我们的例子中只有一个）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A scoring function, which allows Captum Insights to pull out the top-k "
"predictions from a model"
msgstr "一个评分函数，Captum Insights可以用它来提取模型的前k个预测"

#: ../../beginner/vt_tutorial.rst:783
msgid "An ordered, human-readable list of classes our model is trained on"
msgstr "一个按顺序排列的、人类可读的模型训练类别列表"

#: ../../beginner/vt_tutorial.rst:783
msgid "A list of features to look for - in our case, an ``ImageFeature``"
msgstr "一个要查找的特征列表——在我们的案例中是``ImageFeature``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A dataset, which is an iterable object returning batches of inputs and "
"labels - just like you’d use for training"
msgstr "一个数据集，这是一个返回输入和标签批次的可迭代对象——就像您用于训练的那样"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that running the cell above didn’t take much time at all, unlike our "
"attributions above. That’s because Captum Insights lets you configure "
"different attribution algorithms in a visual widget, after which it will "
"compute and display the attributions. *That* process will take a few "
"minutes."
msgstr ""
"注意，运行上面的单元格不会花费太多时间，不像之前的归因操作。因为Captum "
"Insights允许您在一个视觉小工具中配置不同的归因算法，然后才计算和显示归因。*那个*过程会花费几分钟。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Running the cell below will render the Captum Insights widget. You can then "
"choose attributions methods and their arguments, filter model responses "
"based on predicted class or prediction correctness, see the model’s "
"predictions with associated probabilities, and view heatmaps of the "
"attribution compared with the original image."
msgstr ""
"运行下面的单元格会渲染Captum "
"Insights小工具。然后您可以选择归因方法及其参数，基于预测类别或预测正确性过滤模型响应，查看模型预测及其关联概率，并查看归因的热图与原始图像的比较。"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Python source code: captumyt.py <captumyt.py>`"
msgstr ":download:`下载Python源码: captumyt.py <captumyt.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Jupyter notebook: captumyt.ipynb <captumyt.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: captumyt.ipynb <captumyt.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to PyTorch on YouTube"
msgstr "在YouTube上了解PyTorch"

#: ../../beginner/vt_tutorial.rst:783
msgid "introyt.py"
msgstr "introyt.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Introduction to PyTorch - Youtube Series "
"https://pytorch.org/tutorials/beginner/introyt/introyt.html"
msgstr ""
"PyTorch入门-YouTube系列 "
"https://pytorch.org/tutorials/beginner/introyt/introyt.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "introyt1_tutorial.py"
msgstr "introyt1_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Introduction to PyTorch "
"https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html"
msgstr ""
"PyTorch入门 "
"https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "tensors_deeper_tutorial.py"
msgstr "tensors_deeper_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch Tensors "
"https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html"
msgstr ""
"PyTorch张量 "
"https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "autogradyt_tutorial.py"
msgstr "autogradyt_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The Fundamentals of Autograd "
"https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
msgstr ""
"自动梯度的基础知识 "
"https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "modelsyt_tutorial.py"
msgstr "modelsyt_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Building Models with PyTorch "
"https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html"
msgstr ""
"用PyTorch构建模型 "
"https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "tensorboardyt_tutorial.py"
msgstr "tensorboardyt_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch TensorBoard Support "
"https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html"
msgstr ""
"PyTorch TensorBoard 支持 "
"https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "trainingyt_tutorial.py"
msgstr "trainingyt_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Training with PyTorch "
"https://pytorch.org/tutorials/beginner/introyt/trainingyt_tutorial.html"
msgstr ""
"用PyTorch进行训练 "
"https://pytorch.org/tutorials/beginner/introyt/trainingyt_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "captumyt_tutorial.py"
msgstr "captumyt_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Model Understanding with Captum "
"https://pytorch.org/tutorials/beginner/introyt/captumyt_tutorial.html"
msgstr ""
"使用Captum了解模型 "
"https://pytorch.org/tutorials/beginner/introyt/captumyt_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Introduction <introyt1_tutorial.html>`_ ||"
msgstr "`入门 <introyt1_tutorial.html>`_ ||"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_introyt_index.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_introyt_index.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_modelsyt_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_modelsyt_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_autogradyt_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_autogradyt_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_trainingyt.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_trainingyt.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_tensorboardyt_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_tensorboardyt_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_captumyt.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_captumyt.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_tensors_deeper_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_tensors_deeper_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Introduction** ||"
msgstr "**入门** ||"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_introyt1_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_introyt_introyt1_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_introyt_introyt1_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击:ref:`这里 "
"<sphx_glr_download_beginner_introyt_introyt1_tutorial.py>`下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Introduction** || `Tensors <tensors_deeper_tutorial.html>`_ || `Autograd "
"<autogradyt_tutorial.html>`_ || `Building Models <modelsyt_tutorial.html>`_ "
"|| `TensorBoard Support <tensorboardyt_tutorial.html>`_ || `Training Models "
"<trainingyt.html>`_ || `Model Understanding <captumyt.html>`_"
msgstr ""
"**入门** || `张量 <tensors_deeper_tutorial.html>`_ || `自动梯度 "
"<autogradyt_tutorial.html>`_ || `构建模型 <modelsyt_tutorial.html>`_ || "
"`TensorBoard支持 <tensorboardyt_tutorial.html>`_ || `训练模型 <trainingyt.html>`_ "
"|| `模型理解 <captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to PyTorch"
msgstr "PyTorch入门"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=IC0_FRiX-sw>`__."
msgstr ""
"可在以下视频中或在`YouTube上 <https://www.youtube.com/watch?v=IC0_FRiX-sw>`__观看。"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch Tensors"
msgstr "PyTorch张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video beginning at `03:50 "
"<https://www.youtube.com/watch?v=IC0_FRiX-sw&t=230s>`__."
msgstr ""
"从`03:50 <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=230s>`__开始，观看以下视频。"

#: ../../beginner/vt_tutorial.rst:783
msgid "First, we’ll import pytorch."
msgstr "首先，我们将导入 PyTorch。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s see a few basic tensor manipulations. First, just a few of the ways to"
" create tensors:"
msgstr "让我们看看几个基本的张量操作。首先，是创建张量的几种方法："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Above, we create a 5x3 matrix filled with zeros, and query its datatype to "
"find out that the zeros are 32-bit floating point numbers, which is the "
"default PyTorch."
msgstr "上面，我们创建了一个 5x3 的矩阵，填充为零，并查询其数据类型以发现这些零是 32 位浮点数，这是 PyTorch 的默认设置。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What if you wanted integers instead? You can always override the default:"
msgstr "如果您想要整数怎么办？您始终可以覆盖默认设置："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can see that when we do change the default, the tensor helpfully reports"
" this when printed."
msgstr "您可以看到，当我们更改默认设置时，张量在打印时会显式地报告这一点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It’s common to initialize learning weights randomly, often with a specific "
"seed for the PRNG for reproducibility of results:"
msgstr "在初始化学习权重时，通常会随机分配，通常带有特定的伪随机数种子以便于结果重现："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch tensors perform arithmetic operations intuitively. Tensors of "
"similar shapes may be added, multiplied, etc. Operations with scalars are "
"distributed over the tensor:"
msgstr "PyTorch 张量直观地执行算术操作。形状相似的张量可以进行加法、乘法等操作。同标量的操作会分布到整个张量上："

#: ../../beginner/vt_tutorial.rst:783
msgid "Here’s a small sample of the mathematical operations available:"
msgstr "以下是部分可用数学操作的简单示例："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There’s a good deal more to know about the power of PyTorch tensors, "
"including how to set them up for parallel computations on GPU - we’ll be "
"going into more depth in another video."
msgstr "关于 PyTorch 张量的强大功能还有很多需要了解，包括如何配置它们以在 GPU 上进行并行计算——我们将在另一部视频中深入探讨。"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch Models"
msgstr "PyTorch 模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video beginning at `10:00 "
"<https://www.youtube.com/watch?v=IC0_FRiX-sw&t=600s>`__."
msgstr ""
"从视频 `10:00 <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=600s>`__ 开始跟随。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s talk about how we can express models in PyTorch"
msgstr "让我们讨论如何在 PyTorch 中表示模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "le-net-5 diagram"
msgstr "LeNet-5 图示"

#: ../../beginner/vt_tutorial.rst:783
msgid "*Figure: LeNet-5*"
msgstr "*图：LeNet-5*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Above is a diagram of LeNet-5, one of the earliest convolutional neural "
"nets, and one of the drivers of the explosion in Deep Learning. It was built"
" to read small images of handwritten numbers (the MNIST dataset), and "
"correctly classify which digit was represented in the image."
msgstr ""
"上图是 LeNet-5 的示意图，这是最早的卷积神经网络之一，也是深度学习爆发的推动者之一。它用于读取小型手写数字图像（MNIST "
"数据集），并正确分类图像中表示的数字。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Here’s the abridged version of how it works:"
msgstr "以下是其工作的简要说明："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Layer C1 is a convolutional layer, meaning that it scans the input image for"
" features it learned during training. It outputs a map of where it saw each "
"of its learned features in the image. This “activation map” is downsampled "
"in layer S2."
msgstr ""
"C1 层是一个卷积层，意味着它扫描输入图像中在训练过程中学习到的特征。它输出一个激活图，表示图像中每个学习特征的位置。该“激活图”在 S2 "
"层中被下采样。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Layer C3 is another convolutional layer, this time scanning C1’s activation "
"map for *combinations* of features. It also puts out an activation map "
"describing the spatial locations of these feature combinations, which is "
"downsampled in layer S4."
msgstr ""
"C3 层是另一个卷积层，这次扫描 C1 的激活图以寻找*特征组合*。它还输出一个激活图，描述这些特征组合的空间位置，该图在 S4 层被下采样。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, the fully-connected layers at the end, F5, F6, and OUTPUT, are a "
"*classifier* that takes the final activation map, and classifies it into one"
" of ten bins representing the 10 digits."
msgstr "最后，末端的全连接层 F5、F6 和 OUTPUT 是*分类器*，它将最终的激活图分类为表示 10 个数字之一的十个分类。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How do we express this simple neural network in code?"
msgstr "我们如何用代码表示这个简单的神经网络？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Looking over this code, you should be able to spot some structural "
"similarities with the diagram above."
msgstr "看一下这段代码，你应该能够发现它与上方示意图的一些结构相似之处。"

#: ../../beginner/vt_tutorial.rst:783
msgid "This demonstrates the structure of a typical PyTorch model:"
msgstr "这展示了一个典型 PyTorch 模型的结构："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It inherits from ``torch.nn.Module`` - modules may be nested - in fact, even"
" the ``Conv2d`` and ``Linear`` layer classes inherit from "
"``torch.nn.Module``."
msgstr ""
"它继承自 ``torch.nn.Module`` - 模块可以嵌套 - 实际上，连 ``Conv2d`` 和 ``Linear`` 层类也继承自 "
"``torch.nn.Module``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A model will have an ``__init__()`` function, where it instantiates its "
"layers, and loads any data artifacts it might need (e.g., an NLP model might"
" load a vocabulary)."
msgstr ""
"一个模型将拥有一个 ``__init__()`` 函数，在这个函数中实例化其层，并加载任何它可能需要的数据资源（例如，一个 NLP "
"模型可能会加载词汇表）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A model will have a ``forward()`` function. This is where the actual "
"computation happens: An input is passed through the network layers and "
"various functions to generate an output."
msgstr "一个模型还会有一个 ``forward()`` 函数。这是实际计算发生的地方：输入经过网络层和各种函数处理以生成输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Other than that, you can build out your model class like any other Python "
"class, adding whatever properties and methods you need to support your "
"model’s computation."
msgstr "除此之外，你可以像构建任何 Python 类一样构建你的模型类，添加任何支持模型计算所需的属性和方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s instantiate this object and run a sample input through it."
msgstr "让我们实例化这个对象并运行一个示例输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "There are a few important things happening above:"
msgstr "上述操作中有几个关键点："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, we instantiate the ``LeNet`` class, and we print the ``net`` object. "
"A subclass of ``torch.nn.Module`` will report the layers it has created and "
"their shapes and parameters. This can provide a handy overview of a model if"
" you want to get the gist of its processing."
msgstr ""
"首先，我们实例化了 ``LeNet`` 类，并打印了 ``net`` 对象。``torch.nn.Module`` "
"的子类会报告它已创建的层及其形状和参数。如果您想了解模型的处理过程概要，这可以提供一个方便的概述。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below that, we create a dummy input representing a 32x32 image with 1 color "
"channel. Normally, you would load an image tile and convert it to a tensor "
"of this shape."
msgstr "在下方，我们创建了一个表示 32x32 图像且有一个色彩通道的虚拟输入。通常情况下，你会加载一个图像块并将其转换为这种形状的张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You may have noticed an extra dimension to our tensor - the *batch "
"dimension.* PyTorch models assume they are working on *batches* of data - "
"for example, a batch of 16 of our image tiles would have the shape ``(16, 1,"
" 32, 32)``. Since we’re only using one image, we create a batch of 1 with "
"shape ``(1, 1, 32, 32)``."
msgstr ""
"你可能已经注意到我们的张量中有额外的维度 - 即批量维度。PyTorch 模型假设它们在处理数据的*批量*——例如，我们的图像块的一个包含 16 "
"个样本的批量会有形状 ``(16, 1, 32, 32)``。由于我们只使用一张图像，所以我们创建了一个批量大小为 1 的数据，形状为 ``(1, 1,"
" 32, 32)``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We ask the model for an inference by calling it like a function: "
"``net(input)``. The output of this call represents the model’s confidence "
"that the input represents a particular digit. (Since this instance of the "
"model hasn’t learned anything yet, we shouldn’t expect to see any signal in "
"the output.) Looking at the shape of ``output``, we can see that it also has"
" a batch dimension, the size of which should always match the input batch "
"dimension. If we had passed in an input batch of 16 instances, ``output`` "
"would have a shape of ``(16, 10)``."
msgstr ""
"我们通过调用模型来请求推理，就像调用一个函数一样：``net(input)``。这个调用的输出表示模型对输入表示某个特定数字的信心。（由于这个模型的实例尚未学到任何东西，所以我们不应该期望在输出中看到任何显著信号。）查看"
" ``output`` 的形状，我们可以看到它也有一个批量维度，其大小应该始终与输入批量维度匹配。如果我们传入一个包含 16 "
"个实例的输入批量，``output`` 的形状将为 ``(16, 10)``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Datasets and Dataloaders"
msgstr "数据集和数据加载器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video beginning at `14:00 "
"<https://www.youtube.com/watch?v=IC0_FRiX-sw&t=840s>`__."
msgstr ""
"从视频 `14:00 <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=840s>`__ 开始跟随。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below, we’re going to demonstrate using one of the ready-to-download, open-"
"access datasets from TorchVision, how to transform the images for "
"consumption by your model, and how to use the DataLoader to feed batches of "
"data to your model."
msgstr ""
"下面，我们将演示如何使用 TorchVision 中的一个可下载的开源数据集，如何转换图像以供模型使用，以及如何使用 DataLoader "
"将数据批量传递到模型中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The first thing we need to do is transform our incoming images into a "
"PyTorch tensor."
msgstr "我们需要做的第一件事是将输入图像转换为 PyTorch 张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Here, we specify two transformations for our input:"
msgstr "这里，我们为输入指定了两个转换："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``transforms.ToTensor()`` converts images loaded by Pillow into PyTorch "
"tensors."
msgstr "``transforms.ToTensor()`` 将通过 Pillow 加载的图像转换为 PyTorch 张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``transforms.Normalize()`` adjusts the values of the tensor so that their "
"average is zero and their standard deviation is 1.0. Most activation "
"functions have their strongest gradients around x = 0, so centering our data"
" there can speed learning. The values passed to the transform are the means "
"(first tuple) and the standard deviations (second tuple) of the rgb values "
"of the images in the dataset. You can calculate these values yourself by "
"running these few lines of code::"
msgstr ""
"``transforms.Normalize()`` 调整张量的值，使其平均值为零，标准差为 1.0。大多数激活函数在 x = 0 "
"附近具有最强的梯度，因此将数据居中可以加速学习。传递给该转换的值是数据集中图像 rgb "
"值的平均值（第一个元组）和标准差（第二个元组）。您可以运行以下几行代码自己计算这些值："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are many more transforms available, including cropping, centering, "
"rotation, and reflection."
msgstr "还有许多其他可用的转换，包括裁剪、居中、旋转和反射。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we’ll create an instance of the CIFAR10 dataset. This is a set of "
"32x32 color image tiles representing 10 classes of objects: 6 of animals "
"(bird, cat, deer, dog, frog, horse) and 4 of vehicles (airplane, automobile,"
" ship, truck):"
msgstr ""
"接下来，我们将创建一个 CIFAR10 数据集实例。这是一个包含 32x32 彩色图像块的数据集，表示十种类别的对象：6 "
"种动物（鸟、猫、鹿、狗、青蛙、马）和 4 种车辆（飞机、汽车、船、卡车）："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When you run the cell above, it may take a little time for the dataset to "
"download."
msgstr "运行上面的单元可能需要一点时间来下载数据集。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is an example of creating a dataset object in PyTorch. Downloadable "
"datasets (like CIFAR-10 above) are subclasses of "
"``torch.utils.data.Dataset``. ``Dataset`` classes in PyTorch include the "
"downloadable datasets in TorchVision, Torchtext, and TorchAudio, as well as "
"utility dataset classes such as ``torchvision.datasets.ImageFolder``, which "
"will read a folder of labeled images. You can also create your own "
"subclasses of ``Dataset``."
msgstr ""
"这是在 PyTorch 中创建数据集对象的一个示例。可下载的数据集（例如上方的 CIFAR-10）是 "
"``torch.utils.data.Dataset`` 的子类。PyTorch 中的 ``Dataset`` 类包括 "
"TorchVision、Torchtext 和 TorchAudio 中的可下载数据集，以及诸如 "
"``torchvision.datasets.ImageFolder`` 之类的实用数据集类，它可以读取带标签的图像文件夹。您还可以创建自己的 "
"``Dataset`` 子类。"

#: ../../beginner/vt_tutorial.rst:783
msgid "When we instantiate our dataset, we need to tell it a few things:"
msgstr "当我们实例化数据集时，需要告诉它几件事："

#: ../../beginner/vt_tutorial.rst:783
msgid "The filesystem path to where we want the data to go."
msgstr "我们希望数据传输到的文件系统路径。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Whether or not we are using this set for training; most datasets will be "
"split into training and test subsets."
msgstr "我们是否将此集用于训练；大多数数据集将被分为训练集和测试集。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Whether we would like to download the dataset if we haven’t already."
msgstr "如果我们尚未下载数据集，是否希望下载它。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The transformations we want to apply to the data."
msgstr "我们希望对数据应用的转换。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Once your dataset is ready, you can give it to the ``DataLoader``:"
msgstr "一旦数据集准备好后，您可以将其提供给 ``DataLoader``："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A ``Dataset`` subclass wraps access to the data, and is specialized to the "
"type of data it’s serving. The ``DataLoader`` knows *nothing* about the "
"data, but organizes the input tensors served by the ``Dataset`` into batches"
" with the parameters you specify."
msgstr ""
"``Dataset`` 子类封装了对数据的访问，并专注于它所提供的数据类型。而 ``DataLoader`` 对数据一无所知，但会按照您指定的参数将 "
"``Dataset`` 提供的输入张量组织为批量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the example above, we’ve asked a ``DataLoader`` to give us batches of 4 "
"images from ``trainset``, randomizing their order (``shuffle=True``), and we"
" told it to spin up two workers to load data from disk."
msgstr ""
"在上面的示例中，我们要求 ``DataLoader`` 从 ``trainset`` 中给出批量大小为 4 "
"的图像，随机化其顺序（``shuffle=True``），并让它启动两个工作线程从磁盘加载数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It’s good practice to visualize the batches your ``DataLoader`` serves:"
msgstr "可视化您的 ``DataLoader`` 提供的批次是一个好习惯："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Running the above cell should show you a strip of four images, and the "
"correct label for each."
msgstr "运行上面的单元格应显示四个图像的条形图，以及每个图像的正确标签。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training Your PyTorch Model"
msgstr "训练您的 PyTorch 模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video beginning at `17:10 "
"<https://www.youtube.com/watch?v=IC0_FRiX-sw&t=1030s>`__."
msgstr ""
"从视频 `17:10 <https://www.youtube.com/watch?v=IC0_FRiX-sw&t=1030s>`__ 开始跟随。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s put all the pieces together, and train a model:"
msgstr "让我们将所有部分组合在一起，训练一个模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, we’ll need training and test datasets. If you haven’t already, run "
"the cell below to make sure the dataset is downloaded. (It may take a "
"minute.)"
msgstr "首先，我们需要训练和测试数据集。如果您尚未下载，请运行下面的单元格以确保数据集已下载。（可能需要一分钟。）"

#: ../../beginner/vt_tutorial.rst:783
msgid "We’ll run our check on the output from ``DataLoader``:"
msgstr "我们将在 ``DataLoader`` 的输出上运行检查："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is the model we’ll train. If it looks familiar, that’s because it’s a "
"variant of LeNet - discussed earlier in this video - adapted for 3-color "
"images."
msgstr "这是我们将训练的模型。如果它看起来很熟悉，那是因为它是 LeNet 的一个变体 - 在本视频前面讨论过 - 适配了三色图像。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The last ingredients we need are a loss function and an optimizer:"
msgstr "我们需要的最后两个元素是损失函数和优化器："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The loss function, as discussed earlier in this video, is a measure of how "
"far from our ideal output the model’s prediction was. Cross-entropy loss is "
"a typical loss function for classification models like ours."
msgstr "如本视频前面讨论的那样，损失函数是衡量模型预测结果与理想输出之间距离的指标。交叉熵损失是像我们这样的分类模型的典型损失函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The **optimizer** is what drives the learning. Here we have created an "
"optimizer that implements *stochastic gradient descent,* one of the more "
"straightforward optimization algorithms. Besides parameters of the "
"algorithm, like the learning rate (``lr``) and momentum, we also pass in "
"``net.parameters()``, which is a collection of all the learning weights in "
"the model - which is what the optimizer adjusts."
msgstr ""
"**优化器** "
"驱动学习。在这里，我们创建了一个实现*随机梯度下降*的优化器，这是一个更为直接的优化算法之一。除算法的参数（如学习率（``lr``）和动量）外，我们还传入了"
" ``net.parameters()``，它是模型中所有学习权重的集合——这是优化器调整的部分。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, all of this is assembled into the training loop. Go ahead and run "
"this cell, as it will likely take a few minutes to execute:"
msgstr "最后，所有这些被组装成了训练循环。运行这段代码，因为它可能需要几分钟来执行："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here, we are doing only **2 training epochs** (line 1) - that is, two passes"
" over the training dataset. Each pass has an inner loop that **iterates over"
" the training data** (line 4), serving batches of transformed input images "
"and their correct labels."
msgstr ""
"这里我们只进行了**两个训练轮次**（第 1 行）——即训练数据集的两次遍历。每次遍历都有一个内循环，**迭代训练数据**（第 4 "
"行），提供转换后的输入图像和其正确标签的批次。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Zeroing the gradients** (line 9) is an important step. Gradients are "
"accumulated over a batch; if we do not reset them for every batch, they will"
" keep accumulating, which will provide incorrect gradient values, making "
"learning impossible."
msgstr ""
"**清零梯度**（第 9 "
"行）是一个重要步骤。梯度在一个批次中累积；如果我们对每个批次未重置它们，它们将继续累积，这会提供不正确的梯度值，使学习变得不可能。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In line 12, we **ask the model for its predictions** on this batch. In the "
"following line (13), we compute the loss - the difference between "
"``outputs`` (the model prediction) and ``labels`` (the correct output)."
msgstr ""
"在第 12 行，我们**向模型请求预测**该批次。在接下来的行（第 13 行），我们计算损失——``outputs``（模型预测）与 "
"``labels``（正确输出）之间的差距。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In line 14, we do the ``backward()`` pass, and calculate the gradients that "
"will direct the learning."
msgstr "在第 14 行，我们进行了 ``backward()`` 传播，计算将指导学习的梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In line 15, the optimizer performs one learning step - it uses the gradients"
" from the ``backward()`` call to nudge the learning weights in the direction"
" it thinks will reduce the loss."
msgstr "在第 15 行，优化器执行了一次学习步骤——它使用来自 ``backward()`` 调用的梯度，将学习权重朝它认为会减少损失的方向推进。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The remainder of the loop does some light reporting on the epoch number, how"
" many training instances have been completed, and what the collected loss is"
" over the training loop."
msgstr "循环的其余部分对轮次编号、已完成的训练实例数量以及训练循环中的累计损失进行了一些轻度报告。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**When you run the cell above,** you should see something like this:"
msgstr "**当你运行上方的单元格时，**你应该看到类似以下内容："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that the loss is monotonically descending, indicating that our model is"
" continuing to improve its performance on the training dataset."
msgstr "注意，损失值是单调下降的，这表明我们的模型在训练数据集上的性能持续提升。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As a final step, we should check that the model is actually doing *general* "
"learning, and not simply “memorizing” the dataset. This is called "
"**overfitting,** and usually indicates that the dataset is too small (not "
"enough examples for general learning), or that the model has more learning "
"parameters than it needs to correctly model the dataset."
msgstr ""
"作为最后一步，我们需要检查模型是否确实在进行*泛化*学习，而不是简单地“记住”数据集。这被称为**过拟合**，通常意味着数据集太小（没有足够的示例进行泛化学习），或者模型拥有比正确建模数据集所需更多的学习参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is the reason datasets are split into training and test subsets - to "
"test the generality of the model, we ask it to make predictions on data it "
"hasn’t trained on:"
msgstr "这就是为什么数据集会被划分为训练和测试子集——为了测试模型的泛化能力，我们让它对未训练过的数据进行预测："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you followed along, you should see that the model is roughly 50% accurate"
" at this point. That’s not exactly state-of-the-art, but it’s far better "
"than the 10% accuracy we’d expect from a random output. This demonstrates "
"that some general learning did happen in the model."
msgstr ""
"如果你跟随操作，你应该看到模型在这一点上的准确率大约是50%。这虽然不是最先进的水平，但远远好于随机输出的10%准确率。这表明模型确实发生了一些泛化学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 5 minutes  40.759 seconds)"
msgstr "**脚本运行总时间：**（5分钟 40.759秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: introyt1_tutorial.py "
"<introyt1_tutorial.py>`"
msgstr ":download:`下载Python源码: introyt1_tutorial.py <introyt1_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: introyt1_tutorial.ipynb "
"<introyt1_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: introyt1_tutorial.ipynb "
"<introyt1_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_introyt_introyt_index.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_introyt_introyt_index.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || `Tensors "
"<tensors_deeper_tutorial.html>`_ || `Autograd <autogradyt_tutorial.html>`_ "
"|| `Building Models <modelsyt_tutorial.html>`_ || `TensorBoard Support "
"<tensorboardyt_tutorial.html>`_ || `Training Models <trainingyt.html>`_ || "
"`Model Understanding <captumyt.html>`_"
msgstr ""
"`简介 <introyt1_tutorial.html>`_ || `张量 <tensors_deeper_tutorial.html>`_ || "
"`Autograd <autogradyt_tutorial.html>`_ || `构建模型 <modelsyt_tutorial.html>`_ "
"|| `TensorBoard支持 <tensorboardyt_tutorial.html>`_ || `训练模型 "
"<trainingyt.html>`_ || `模型理解 <captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Authors: `Brad Heintz <https://github.com/fbbradheintz>`_"
msgstr "作者: `Brad Heintz <https://github.com/fbbradheintz>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial follows along with the `PyTorch Beginner Series "
"<https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN>`_"
" on YouTube."
msgstr ""
"本教程与YouTube上的`PyTorch初学者系列 "
"<https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN>`_配套学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**On the cloud**: This is the easiest way to get started! Each section has a"
" Colab link at the top, which opens a notebook with the code in a fully-"
"hosted environment. Pro tip: Use Colab with a GPU runtime to speed up "
"operations *Runtime > Change runtime type > GPU*"
msgstr ""
"**在云端**：这是开始学习的最简单方式！每个部分顶部都有一个Colab链接，它会打开一个包含代码的完全托管环境的笔记本。专业提示：使用带有GPU运行时的Colab以加速操作"
" *Runtime > Change runtime type > GPU*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Locally**: This option requires you to set up PyTorch and torchvision on "
"your local machine (`installation instructions <https://pytorch.org/get-"
"started/locally/>`_). Download the notebook or copy the code into your "
"favorite IDE."
msgstr ""
"**本地**：此选项需要在你的本地机器上配置PyTorch和torchvision（`安装说明 <https://pytorch.org/get-"
"started/locally/>`_）。下载笔记本或将代码复制到你喜欢的IDE中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: introyt_index.py <introyt_index.py>`"
msgstr ":download:`下载Python源码: introyt_index.py <introyt_index.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: introyt_index.ipynb "
"<introyt_index.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: introyt_index.ipynb <introyt_index.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_introyt_modelsyt_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_introyt_modelsyt_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || `Tensors "
"<tensors_deeper_tutorial.html>`_ || `Autograd <autogradyt_tutorial.html>`_ "
"|| **Building Models** || `TensorBoard Support "
"<tensorboardyt_tutorial.html>`_ || `Training Models <trainingyt.html>`_ || "
"`Model Understanding <captumyt.html>`_"
msgstr ""
"`简介 <introyt1_tutorial.html>`_ || `张量 <tensors_deeper_tutorial.html>`_ || "
"`Autograd <autogradyt_tutorial.html>`_ || **构建模型** || `TensorBoard支持 "
"<tensorboardyt_tutorial.html>`_ || `训练模型 <trainingyt.html>`_ || `模型理解 "
"<captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Building Models with PyTorch"
msgstr "使用PyTorch构建模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=OSqIP-mOWOI>`__."
msgstr "按照以下视频或在`YouTube <https://www.youtube.com/watch?v=OSqIP-mOWOI>`__上学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``torch.nn.Module`` and ``torch.nn.Parameter``"
msgstr "``torch.nn.Module`` 和 ``torch.nn.Parameter``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this video, we’ll be discussing some of the tools PyTorch makes available"
" for building deep learning networks."
msgstr "在这段视频中，我们将讨论PyTorch提供的一些用于构建深度学习网络的工具。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Except for ``Parameter``, the classes we discuss in this video are all "
"subclasses of ``torch.nn.Module``. This is the PyTorch base class meant to "
"encapsulate behaviors specific to PyTorch Models and their components."
msgstr ""
"除了``Parameter``外，在视频中讨论的类都是``torch.nn.Module``的子类。这是一个PyTorch基类，用于封装PyTorch模型及其组件的特定行为。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One important behavior of ``torch.nn.Module`` is registering parameters. If "
"a particular ``Module`` subclass has learning weights, these weights are "
"expressed as instances of ``torch.nn.Parameter``. The ``Parameter`` class is"
" a subclass of ``torch.Tensor``, with the special behavior that when they "
"are assigned as attributes of a ``Module``, they are added to the list of "
"that modules parameters. These parameters may be accessed through the "
"``parameters()`` method on the ``Module`` class."
msgstr ""
"``torch.nn.Module``的重要行为之一是注册参数。如果一个特定``Module``子类有学习权重，这些权重将以``torch.nn.Parameter``实例的形式表达。``Parameter``类是``torch.Tensor``的子类，具有特殊行为：当它们被分配为``Module``的属性时，它们会被添加到该模块参数的列表中。这些参数可以通过``parameters()``方法访问。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As a simple example, here’s a very simple model with two linear layers and "
"an activation function. We’ll create an instance of it and ask it to report "
"on its parameters:"
msgstr "这里是一个简单的示例，一个非常简单的模型，它有两个线性层和一个激活函数。我们将创建其实例并要求它报告其参数："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This shows the fundamental structure of a PyTorch model: there is an "
"``__init__()`` method that defines the layers and other components of a "
"model, and a ``forward()`` method where the computation gets done. Note that"
" we can print the model, or any of its submodules, to learn about its "
"structure."
msgstr ""
"这展示了PyTorch模型的基本结构：模型有一个``__init__()``方法，用于定义模型的层和其他组件，以及一个``forward()``方法用于完成计算。注意，我们可以打印模型或任何其子模块以了解它的结构。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Common Layer Types"
msgstr "常见层类型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Linear Layers"
msgstr "线性层"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The most basic type of neural network layer is a *linear* or *fully "
"connected* layer. This is a layer where every input influences every output "
"of the layer to a degree specified by the layer’s weights. If a model has "
"*m* inputs and *n* outputs, the weights will be an *m* x *n* matrix. For "
"example:"
msgstr ""
"最基本的神经网络层是*线性层*或*全连接层*。这是一个层，其中每个输入都会影响层中的每个输出，影响程度由该层的权重决定。如果一个模型有*m*个输入和*n*个输出，则权重将是一个*m*"
" x *n*的矩阵。例如："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you do the matrix multiplication of ``x`` by the linear layer’s weights, "
"and add the biases, you’ll find that you get the output vector ``y``."
msgstr "如果你对``x``与线性层的权重执行矩阵乘法，并加上偏置，你会发现得到的是输出向量``y``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One other important feature to note: When we checked the weights of our "
"layer with ``lin.weight``, it reported itself as a ``Parameter`` (which is a"
" subclass of ``Tensor``), and let us know that it’s tracking gradients with "
"autograd. This is a default behavior for ``Parameter`` that differs from "
"``Tensor``."
msgstr ""
"另一个需要注意的重要特性是：当我们通过``lin.weight``检查层的权重时，它将其自身报告为一个``Parameter``（这是``Tensor``的子类），并告诉我们它正在使用自动微分跟踪梯度。这是``Parameter``的默认行为，和``Tensor``不同。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Linear layers are used widely in deep learning models. One of the most "
"common places you’ll see them is in classifier models, which will usually "
"have one or more linear layers at the end, where the last layer will have "
"*n* outputs, where *n* is the number of classes the classifier addresses."
msgstr ""
"线性层在深度学习模型中被广泛使用。最常见的地方之一是分类器模型，其中通常在最后会有一个或多个线性层，最后一层会有*n*个输出，其中*n*是分类器处理的类别数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Convolutional Layers"
msgstr "卷积层"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"*Convolutional* layers are built to handle data with a high degree of "
"spatial correlation. They are very commonly used in computer vision, where "
"they detect close groupings of features which the compose into higher-level "
"features. They pop up in other contexts too - for example, in NLP "
"applications, where a word’s immediate context (that is, the other words "
"nearby in the sequence) can affect the meaning of a sentence."
msgstr ""
"*卷积层*专为处理具有高级空间相关性的数据而设计。它们非常常见于计算机视觉领域，用于检测特征的局部分组并将其组合为更高级的特征。它们也会出现在其他领域——例如，自然语言处理（NLP）应用中，其中一个单词的上下文（即在序列中附近的其他单词）会影响句子的含义。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We saw convolutional layers in action in LeNet5 in an earlier video:"
msgstr "我们在之前的视频中看到LeNet5中的卷积层实例："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s break down what’s happening in the convolutional layers of this model."
" Starting with ``conv1``:"
msgstr "让我们分解此模型卷积层中的处理过程。先从``conv1``开始："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"LeNet5 is meant to take in a 1x32x32 black & white image. **The first "
"argument to a convolutional layer’s constructor is the number of input "
"channels.** Here, it is 1. If we were building this model to look at 3-color"
" channels, it would be 3."
msgstr ""
"LeNet5旨在处理1x32x32的黑白图像。**卷积层构造函数的第一个参数是输入通道的数量。**这里为1。如果我们将这个模型用于处理包含3个颜色通道的图像，它将为3。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A convolutional layer is like a window that scans over the image, looking "
"for a pattern it recognizes. These patterns are called *features,* and one "
"of the parameters of a convolutional layer is the number of features we "
"would like it to learn. **This is the second argument to the constructor is "
"the number of output features.** Here, we’re asking our layer to learn 6 "
"features."
msgstr ""
"卷积层就像一个窗口，它会扫描图像以查找它识别的模式。这些模式称为*特征*，卷积层参数之一是我们希望它学习的特征数量。**这是构造函数的第二个参数，表示输出特征的数量。**这里我们要求卷积层学习6个特征。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Just above, I likened the convolutional layer to a window - but how big is "
"the window? **The third argument is the window or kernel size.** Here, the "
"“5” means we’ve chosen a 5x5 kernel. (If you want a kernel with height "
"different from width, you can specify a tuple for this argument - e.g., "
"``(3, 5)`` to get a 3x5 convolution kernel.)"
msgstr ""
"刚才我把卷积层比作一个窗口——但窗口到底有多大呢？**第三个参数是窗口或内核大小。**这里的“5”表示我们选择了一个5x5的内核。（如果你希望内核的高度和宽度不同，可以为此参数指定一个元组，例如``(3,"
" 5)``来获得3x5的卷积内核。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The output of a convolutional layer is an *activation map* - a spatial "
"representation of the presence of features in the input tensor. ``conv1`` "
"will give us an output tensor of 6x28x28; 6 is the number of features, and "
"28 is the height and width of our map. (The 28 comes from the fact that when"
" scanning a 5-pixel window over a 32-pixel row, there are only 28 valid "
"positions.)"
msgstr ""
"卷积层的输出是一个*激活地图*——输入张量中特征出现的空间表示。``conv1``将输出一个6x28x28的张量；其中6是特征数量，而28是我们地图的高度和宽度。（28来源于在32像素的列上扫描一个5像素的窗口时只有28个有效位置。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We then pass the output of the convolution through a ReLU activation "
"function (more on activation functions later), then through a max pooling "
"layer. The max pooling layer takes features near each other in the "
"activation map and groups them together. It does this by reducing the "
"tensor, merging every 2x2 group of cells in the output into a single cell, "
"and assigning that cell the maximum value of the 4 cells that went into it. "
"This gives us a lower-resolution version of the activation map, with "
"dimensions 6x14x14."
msgstr ""
"我们接着将卷积的输出传递到ReLU激活函数（后面会详细讲解激活函数），再经过一个最大池化层。最大池化层会将激活地图中的邻近特征聚合在一起。它会通过减少张量，将输出中的每个2x2单元组合成一个单元，并赋予该单元由输入的4个单元最大值。这样，我们得到一个较低分辨率的激活地图，其维度为6x14x14。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our next convolutional layer, ``conv2``, expects 6 input channels "
"(corresponding to the 6 features sought by the first layer), has 16 output "
"channels, and a 3x3 kernel. It puts out a 16x12x12 activation map, which is "
"again reduced by a max pooling layer to 16x6x6. Prior to passing this output"
" to the linear layers, it is reshaped to a 16 \\* 6 \\* 6 = 576-element "
"vector for consumption by the next layer."
msgstr ""
"我们的下一卷积层``conv2``要求输入通道数为6（对应第一层所寻找的6个特征），输出通道数为16，并使用3x3的内核。它输出一个16x12x12的激活地图，再次经过一个最大池化层，该结果被减少为16x6x6。在将此输出传递至线性层之前，它会被重塑为一个16"
" \\* 6 \\* 6 = 576元素向量以供下一个层使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are convolutional layers for addressing 1D, 2D, and 3D tensors. There "
"are also many more optional arguments for a conv layer constructor, "
"including stride length(e.g., only scanning every second or every third "
"position) in the input, padding (so you can scan out to the edges of the "
"input), and more. See the `documentation "
"<https://pytorch.org/docs/stable/nn.html#convolution-layers>`__ for more "
"information."
msgstr ""
"针对1D、2D和3D张量都有卷积层可用。构造函数还有许多可选参数，包括步幅（例如，仅每隔第二个或每第三个位置扫描输入）、填充（使你可以扫描到输入的边缘），等等。有关更多信息，请参阅`文档"
" <https://pytorch.org/docs/stable/nn.html#convolution-layers>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Recurrent Layers"
msgstr "循环层"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"*Recurrent neural networks* (or *RNNs)* are used for sequential data - "
"anything from time-series measurements from a scientific instrument to "
"natural language sentences to DNA nucleotides. An RNN does this by "
"maintaining a *hidden state* that acts as a sort of memory for what it has "
"seen in the sequence so far."
msgstr ""
"*循环神经网络（RNN）*是一种用于处理序列数据的网络——从科学仪器的时间序列测量到自然语言句子的文本，再到DNA核苷酸。RNN通过维护一个*隐藏状态*来处理序列，它充当了序列中到目前为止看到内容的记忆。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The internal structure of an RNN layer - or its variants, the LSTM (long "
"short-term memory) and GRU (gated recurrent unit) - is moderately complex "
"and beyond the scope of this video, but we’ll show you what one looks like "
"in action with an LSTM-based part-of-speech tagger (a type of classifier "
"that tells you if a word is a noun, verb, etc.):"
msgstr ""
"RNN层的内部结构——或其变种，如LSTM（长短期记忆）和GRU（门控循环单元）——相对复杂，超出了本视频的范围，但我们用一个基于LSTM的词性标注器（即一种可以告诉你一个词是名词、动词等的分类器）示例展示其运行方式："

#: ../../beginner/vt_tutorial.rst:783
msgid "The constructor has four arguments:"
msgstr "构造函数有四个参数："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``vocab_size`` is the number of words in the input vocabulary. Each word is "
"a one-hot vector (or unit vector) in a ``vocab_size``-dimensional space."
msgstr "``vocab_size``是输入词汇表中的单词数量。每个单词是一个``vocab_size``维空间中的一个独热向量（或单位向量）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``tagset_size`` is the number of tags in the output set."
msgstr "``tagset_size``是输出集合中的标签数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``embedding_dim`` is the size of the *embedding* space for the vocabulary. "
"An embedding maps a vocabulary onto a low-dimensional space, where words "
"with similar meanings are close together in the space."
msgstr "``embedding_dim``是词汇表映射到的*嵌入空间*的大小。嵌入将词汇映射到一个低维空间，其中具有类似意义的词在空间中彼此接近。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``hidden_dim`` is the size of the LSTM’s memory."
msgstr "``hidden_dim``是LSTM内存的大小。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The input will be a sentence with the words represented as indices of one-"
"hot vectors. The embedding layer will then map these down to an "
"``embedding_dim``-dimensional space. The LSTM takes this sequence of "
"embeddings and iterates over it, fielding an output vector of length "
"``hidden_dim``. The final linear layer acts as a classifier; applying "
"``log_softmax()`` to the output of the final layer converts the output into "
"a normalized set of estimated probabilities that a given word maps to a "
"given tag."
msgstr ""
"输入将是一个句子，其中单词以独热向量的索引表示。嵌入层将这些索引映射到一个``embedding_dim``维空间。LSTM会对这些嵌入序列进行迭代，输出长度为``hidden_dim``的向量。最终的线性层作为分类器使用；对最终层的输出应用``log_softmax()``会将输出转换为归一化的估算概率集，使得给定单词映射到给定标签的概率能够被预测。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you’d like to see this network in action, check out the `Sequence Models "
"and LSTM Networks "
"<https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html>`__"
" tutorial on pytorch.org."
msgstr ""
"如果您想了解此网络如何运作，可以查看pytorch.org上的`序列模型和LSTM网络教程 "
"<https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Transformers"
msgstr "Transformers（变换器）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"*Transformers* are multi-purpose networks that have taken over the state of "
"the art in NLP with models like BERT. A discussion of transformer "
"architecture is beyond the scope of this video, but PyTorch has a "
"``Transformer`` class that allows you to define the overall parameters of a "
"transformer model - the number of attention heads, the number of encoder & "
"decoder layers, dropout and activation functions, etc. (You can even build "
"the BERT model from this single class, with the right parameters!) The "
"``torch.nn.Transformer`` class also has classes to encapsulate the "
"individual components (``TransformerEncoder``, ``TransformerDecoder``) and "
"subcomponents (``TransformerEncoderLayer``, ``TransformerDecoderLayer``). "
"For details, check out the `documentation "
"<https://pytorch.org/docs/stable/nn.html#transformer-layers>`__ on "
"transformer classes."
msgstr ""
"*Transformers*是多用途网络，随着像BERT这样的模型，它们已成为NLP领域最新的技术标准。关于变换器架构的讨论超出了此视频范围，但PyTorch有一个``Transformer``类，可以定义变换器模型的总体参数——包括注意力头的数量、编码器和解码器层数、dropout和激活函数等。（使用正确的参数，甚至可以从此单分类构建BERT模型！）``torch.nn.Transformer``类还封装了子组件（如``TransformerEncoder``、``TransformerDecoder``以及``TransformerEncoderLayer``、``TransformerDecoderLayer``）。详情请参阅变换器类的`文档"
" <https://pytorch.org/docs/stable/nn.html#transformer-layers>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Other Layers and Functions"
msgstr "其他层和函数"

#: ../../beginner/vt_tutorial.rst:783
msgid "Data Manipulation Layers"
msgstr "数据操作层"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are other layer types that perform important functions in models, but "
"don’t participate in the learning process themselves."
msgstr "还有一些层类型执行模型中的重要功能，但它们本身不参与学习过程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Max pooling** (and its twin, min pooling) reduce a tensor by combining "
"cells, and assigning the maximum value of the input cells to the output cell"
" (we saw this). For example:"
msgstr ""
"**最大池化**（以及它的孪生兄弟，最小池化）通过组合输入单元格来减少张量，并将输入单元格的最大值赋给输出单元格（我们已经看到过这一点）。例如："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you look closely at the values above, you’ll see that each of the values "
"in the maxpooled output is the maximum value of each quadrant of the 6x6 "
"input."
msgstr "仔细观察上述的数值，你会发现max pooled结果中的每个值都是6x6输入中每个象限的最大值。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Normalization layers** re-center and normalize the output of one layer "
"before feeding it to another. Centering and scaling the intermediate tensors"
" has a number of beneficial effects, such as letting you use higher learning"
" rates without exploding/vanishing gradients."
msgstr ""
"**归一化层**在将一个层的输出传递给另一个层之前重新中心化并归一化操作。中心化和缩放中间张量具有一些有益效果，例如允许使用更高的学习率而不会出现梯度爆炸或梯度消失问题。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Running the cell above, we’ve added a large scaling factor and offset to an "
"input tensor; you should see the input tensor’s ``mean()`` somewhere in the "
"neighborhood of 15. After running it through the normalization layer, you "
"can see that the values are smaller, and grouped around zero - in fact, the "
"mean should be very small (> 1e-8)."
msgstr ""
"运行上述单元格时，我们已经向输入张量添加了一个大的缩放因子和偏移量；你应该看到输入张量的``mean()``大约接近15。通过归一化层后，可以看到值变小并集中在零附近——实际上，平均值应该非常小（>"
" 1e-8）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is beneficial because many activation functions (discussed below) have "
"their strongest gradients near 0, but sometimes suffer from vanishing or "
"exploding gradients for inputs that drive them far away from zero. Keeping "
"the data centered around the area of steepest gradient will tend to mean "
"faster, better learning and higher feasible learning rates."
msgstr ""
"这是有益的，因为许多激活函数（将在下方讨论）在0附近拥有最强的梯度，但有时在输入值使它们远离零时会出现梯度消失或梯度爆炸问题。让数据保持在梯度最陡区附近通常意味着更快速、更好的学习以及更高的可行学习率。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Dropout layers** are a tool for encouraging *sparse representations* in "
"your model - that is, pushing it to do inference with less data."
msgstr "**Dropout层**是一种鼓励模型中*稀疏表示*的工具——即推动它使用更少的数据进行推理。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Dropout layers work by randomly setting parts of the input tensor *during "
"training* - dropout layers are always turned off for inference. This forces "
"the model to learn against this masked or reduced dataset. For example:"
msgstr ""
"Dropout层通过在训练期间随机设置部分输入张量实现——训练时使用dropout层，在推理时总是关闭它。这迫使模型基于被屏蔽或减少的数据集学习。例如："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Above, you can see the effect of dropout on a sample tensor. You can use the"
" optional ``p`` argument to set the probability of an individual weight "
"dropping out; if you don’t it defaults to 0.5."
msgstr "上面展示了dropout对样本张量的影响。你可以使用可选的``p``参数设置单个权重丢弃的概率；如果不设置，它默认为0.5。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Activation Functions"
msgstr "激活函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Activation functions make deep learning possible. A neural network is really"
" a program - with many parameters - that *simulates a mathematical "
"function*. If all we did was multiple tensors by layer weights repeatedly, "
"we could only simulate *linear functions;* further, there would be no point "
"to having many layers, as the whole network would reduce could be reduced to"
" a single matrix multiplication. Inserting *non-linear* activation functions"
" between layers is what allows a deep learning model to simulate any "
"function, rather than just linear ones."
msgstr ""
"激活函数使深度学习成为可能。神经网络实际上是一个程序——含有许多参数——其目的是*模拟数学函数*。如果我们只是重复通过层权重乘以张量，我们只能模拟*线性函数*；此外，拥有多个层也没有意义，因为整个网络可以简化为单个矩阵乘法。通过在层之间插入*非线性*激活函数，这使得深度学习模型可以模拟任何函数，而不仅仅是线性函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.nn.Module`` has objects encapsulating all of the major activation "
"functions including ReLU and its many variants, Tanh, Hardtanh, sigmoid, and"
" more. It also includes other functions, such as Softmax, that are most "
"useful at the output stage of a model."
msgstr ""
"``torch.nn.Module``提供了封装所有主要激活函数的对象，包括ReLU及其许多变体、Tanh、Hardtanh、sigmoid以及更多函数。它还包括其他在模型输出阶段最有用的函数，比如Softmax。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loss Functions"
msgstr "损失函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Loss functions tell us how far a model’s prediction is from the correct "
"answer. PyTorch contains a variety of loss functions, including common MSE "
"(mean squared error = L2 norm), Cross Entropy Loss and Negative Likelihood "
"Loss (useful for classifiers), and others."
msgstr ""
"损失函数表明模型的预测与正确答案之间的偏差程度。PyTorch提供了多种损失函数，包括常用的MSE（均方误差 = "
"L2范数）、交叉熵损失和负似然损失（对于分类器非常有用）以及其他函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.024 seconds)"
msgstr "**脚本总运行时间：**（0分钟 0.024秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: modelsyt_tutorial.py "
"<modelsyt_tutorial.py>`"
msgstr ":download:`下载Python源代码: modelsyt_tutorial.py <modelsyt_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: modelsyt_tutorial.ipynb "
"<modelsyt_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: modelsyt_tutorial.ipynb "
"<modelsyt_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**42:24.026** total execution time for **beginner_introyt** files:"
msgstr "**42:24.026** 总执行时间（适用于初学者介绍文件）："

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_trainingyt.py` (``trainingyt.py``)"
msgstr ":ref:`sphx_glr_beginner_introyt_trainingyt.py`（``trainingyt.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "27:16.034"
msgstr "27:16.034"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_introyt_tensorboardyt_tutorial.py` "
"(``tensorboardyt_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_introyt_tensorboardyt_tutorial.py`（``tensorboardyt_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "09:24.977"
msgstr "09:24.977"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_introyt_introyt1_tutorial.py` "
"(``introyt1_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_introyt_introyt1_tutorial.py`（``introyt1_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "05:40.759"
msgstr "05:40.759"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_introyt_autogradyt_tutorial.py` "
"(``autogradyt_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_introyt_autogradyt_tutorial.py`（``autogradyt_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:01.311"
msgstr "00:01.311"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_introyt_tensors_deeper_tutorial.py` "
"(``tensors_deeper_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_introyt_tensors_deeper_tutorial.py`（``tensors_deeper_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.922"
msgstr "00:00.922"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_introyt_modelsyt_tutorial.py` "
"(``modelsyt_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_introyt_modelsyt_tutorial.py`（``modelsyt_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.024"
msgstr "00:00.024"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_introyt_captumyt.py` (``captumyt.py``)"
msgstr ":ref:`sphx_glr_beginner_introyt_captumyt.py`（``captumyt.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_introyt_introyt_index.py` (``introyt_index.py``)"
msgstr ""
":ref:`sphx_glr_beginner_introyt_introyt_index.py`（``introyt_index.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_introyt_tensorboardyt_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_introyt_tensorboardyt_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || `Tensors "
"<tensors_deeper_tutorial.html>`_ || `Autograd <autogradyt_tutorial.html>`_ "
"|| `Building Models <modelsyt_tutorial.html>`_ || **TensorBoard Support** ||"
" `Training Models <trainingyt.html>`_ || `Model Understanding "
"<captumyt.html>`_"
msgstr ""
"`介绍 <introyt1_tutorial.html>`_ || `张量 <tensors_deeper_tutorial.html>`_ || "
"`自动求导 <autogradyt_tutorial.html>`_ || `构建模型 <modelsyt_tutorial.html>`_ || "
"**TensorBoard支持** || `训练模型 <trainingyt.html>`_ || `模型理解<captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch TensorBoard Support"
msgstr "PyTorch TensorBoard支持"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=6CEld3hZgqc>`__."
msgstr ""
"请跟随下面的视频或在`youtube <https://www.youtube.com/watch?v=6CEld3hZgqc>`__观看。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Before You Start"
msgstr "开始之前"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To run this tutorial, you’ll need to install PyTorch, TorchVision, "
"Matplotlib, and TensorBoard."
msgstr "要运行此教程，您需要安装PyTorch、TorchVision、Matplotlib和TensorBoard。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once the dependencies are installed, restart this notebook in the Python "
"environment where you installed them."
msgstr "安装依赖项后，请重新启动安装了这些依赖项的Python环境中的notebook。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this notebook, we’ll be training a variant of LeNet-5 against the "
"Fashion-MNIST dataset. Fashion-MNIST is a set of image tiles depicting "
"various garments, with ten class labels indicating the type of garment "
"depicted."
msgstr ""
"在此notebook中，我们将训练一种变体的LeNet-5与Fashion-MNIST数据集进行对抗。Fashion-"
"MNIST是一组图片瓦片，显示各种服饰类型，并提供十个类别标签以表明对应的服饰种类。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Showing Images in TensorBoard"
msgstr "在TensorBoard中展示图片"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s start by adding sample images from our dataset to TensorBoard:"
msgstr "让我们开始将来自数据集的样本图片添加到TensorBoard:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Above, we used TorchVision and Matplotlib to create a visual grid of a "
"minibatch of our input data. Below, we use the ``add_image()`` call on "
"``SummaryWriter`` to log the image for consumption by TensorBoard, and we "
"also call ``flush()`` to make sure it’s written to disk right away."
msgstr ""
"上述过程中，我们使用TorchVision和Matplotlib创建了一个输入数据小批量的可视化网格。下来，我们使用``SummaryWriter``的``add_image()``调用将图像记录下来以供TensorBoard使用，同时使用``flush()``使其立即写入磁盘。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you start TensorBoard at the command line and open it in a new browser "
"tab (usually at `localhost:6006 <localhost:6006>`__), you should see the "
"image grid under the IMAGES tab."
msgstr ""
"如果您在命令行启动TensorBoard并在新的浏览器标签页中打开它（通常在`localhost:6006 "
"<localhost:6006>`__），您应该能在IMAGES标签下看到图片网格。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Graphing Scalars to Visualize Training"
msgstr "绘制标量以可视化训练过程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"TensorBoard is useful for tracking the progress and efficacy of your "
"training. Below, we’ll run a training loop, track some metrics, and save the"
" data for TensorBoard’s consumption."
msgstr ""
"TensorBoard对于跟踪训练的进展和效率非常有用。以下我们将运行一个训练循环，跟踪一些指标，并保存这些数据以供TensorBoard使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s define a model to categorize our image tiles, and an optimizer and "
"loss function for training:"
msgstr "让我们定义一个模型用于对图片瓦片进行分类，并定义优化器和损失函数以便训练："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now let’s train a single epoch, and evaluate the training vs. validation set"
" losses every 1000 batches:"
msgstr "现在让我们训练一个单独的epoch，并在每1000个批次评估训练与验证集的损失："

#: ../../beginner/vt_tutorial.rst:783
msgid "Switch to your open TensorBoard and have a look at the SCALARS tab."
msgstr "切换到您的TensorBoard页面并查看SCALARS标签。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Visualizing Your Model"
msgstr "可视化您的模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"TensorBoard can also be used to examine the data flow within your model. To "
"do this, call the ``add_graph()`` method with a model and sample input:"
msgstr "TensorBoard还能用于检查模型内的数据流。为此，请使用模型和样本输入调用``add_graph()``方法："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When you switch over to TensorBoard, you should see a GRAPHS tab. Double-"
"click the “NET” node to see the layers and data flow within your model."
msgstr "当您切换到TensorBoard时，应能看到GRAPHS标签。双击“NET”节点查看模型内的层和数据流。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Visualizing Your Dataset with Embeddings"
msgstr "使用嵌入可视化您的数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The 28-by-28 image tiles we’re using can be modeled as 784-dimensional "
"vectors (28 \\* 28 = 784). It can be instructive to project this to a lower-"
"dimensional representation. The ``add_embedding()`` method will project a "
"set of data onto the three dimensions with highest variance, and display "
"them as an interactive 3D chart. The ``add_embedding()`` method does this "
"automatically by projecting to the three dimensions with highest variance."
msgstr ""
"我们使用的28x28图片瓦片可以建模为784维向量（28 \\* 28 = "
"784）。将数据投影到低维表示可能会有启发性。``add_embedding()``方法会将一组数据投影到具有最大方差的三个维度，并以交互式三维图形式显示。``add_embedding()``方法自动完成投影到方差最大的三个维度的操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below, we’ll take a sample of our data, and generate such an embedding:"
msgstr "以下，我们将取样数据并生成如此的嵌入："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now if you switch to TensorBoard and select the PROJECTOR tab, you should "
"see a 3D representation of the projection. You can rotate and zoom the "
"model. Examine it at large and small scales, and see whether you can spot "
"patterns in the projected data and the clustering of labels."
msgstr ""
"现在如果你切换到TensorBoard并选择PROJECTOR标签，应该能看到投影的三维表示。您可以旋转和缩放模型，从大和小的规模检查它，并观察是否能在投影数据和标签聚类中发现模式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For better visibility, it’s recommended to:"
msgstr "为了更好地可视化，建议："

#: ../../beginner/vt_tutorial.rst:783
msgid "Select “label” from the “Color by” drop-down on the left."
msgstr "从左侧的“Color by”下拉菜单中选择“label”。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Toggle the Night Mode icon along the top to place the light-colored images "
"on a dark background."
msgstr "切换顶部的夜间模式图标，以将浅色图片放置到深色背景上。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Other Resources"
msgstr "其他资源"

#: ../../beginner/vt_tutorial.rst:783
msgid "For more information, have a look at:"
msgstr "有关更多信息，请查看："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch documentation on `torch.utils.tensorboard.SummaryWriter "
"<https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter>`__"
msgstr ""
"有关`torch.utils.tensorboard.SummaryWriter "
"<https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter>`__的PyTorch文档"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensorboard tutorial content in the `PyTorch.org Tutorials "
"<https://pytorch.org/tutorials/>`__"
msgstr "`PyTorch.org教程中的Tensorboard教程内容 <https://pytorch.org/tutorials/>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information about TensorBoard, see the `TensorBoard documentation "
"<https://www.tensorflow.org/tensorboard>`__"
msgstr ""
"有关TensorBoard的更多信息，请参阅`TensorBoard文档 "
"<https://www.tensorflow.org/tensorboard>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 9 minutes  24.977 seconds)"
msgstr "**脚本总运行时间：** （9分钟 24.977秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: tensorboardyt_tutorial.py "
"<tensorboardyt_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: tensorboardyt_tutorial.py "
"<tensorboardyt_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: tensorboardyt_tutorial.ipynb "
"<tensorboardyt_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: tensorboardyt_tutorial.ipynb "
"<tensorboardyt_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_introyt_tensors_deeper_tutorial.py>` to download"
" the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_introyt_tensors_deeper_tutorial.py>`"
" 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || **Tensors** || `Autograd "
"<autogradyt_tutorial.html>`_ || `Building Models <modelsyt_tutorial.html>`_ "
"|| `TensorBoard Support <tensorboardyt_tutorial.html>`_ || `Training Models "
"<trainingyt.html>`_ || `Model Understanding <captumyt.html>`_"
msgstr ""
"`介绍 <introyt1_tutorial.html>`_ || **张量** || `自动求导 "
"<autogradyt_tutorial.html>`_ || `构建模型 <modelsyt_tutorial.html>`_ || "
"`TensorBoard支持 <tensorboardyt_tutorial.html>`_ || `训练模型 <trainingyt.html>`_ "
"|| `模型理解 <captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to PyTorch Tensors"
msgstr "PyTorch张量介绍"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=r7QDUPb2dCM>`__."
msgstr "请跟随以下视频或在`youtube <https://www.youtube.com/watch?v=r7QDUPb2dCM>`__观看。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors are the central data abstraction in PyTorch. This interactive "
"notebook provides an in-depth introduction to the ``torch.Tensor`` class."
msgstr "张量是PyTorch中的核心数据抽象。此交互式notebook提供对``torch.Tensor``类的深入介绍。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First things first, let’s import the PyTorch module. We’ll also add Python’s"
" math module to facilitate some of the examples."
msgstr "首先，让我们导入PyTorch模块。我们还会添加Python的数学模块以支持一些操作示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Creating Tensors"
msgstr "创建张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The simplest way to create a tensor is with the ``torch.empty()`` call:"
msgstr "创建张量最简单的方法是使用 ``torch.empty()`` 调用："

#: ../../beginner/vt_tutorial.rst:783
msgid "Let’s upack what we just did:"
msgstr "让我们解读一下刚才完成的操作："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We created a tensor using one of the numerous factory methods attached to "
"the ``torch`` module."
msgstr "我们使用绑定到 ``torch`` 模块的众多工厂方法之一创建了一个张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The tensor itself is 2-dimensional, having 3 rows and 4 columns."
msgstr "张量本身是二维的，有3行4列。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The type of the object returned is ``torch.Tensor``, which is an alias for "
"``torch.FloatTensor``; by default, PyTorch tensors are populated with 32-bit"
" floating point numbers. (More on data types below.)"
msgstr ""
"返回对象的类型是 ``torch.Tensor``，这是 ``torch.FloatTensor`` "
"的别名；默认情况下，PyTorch张量会用32位浮点数填充。（稍后会讨论更多关于数据类型的内容。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You will probably see some random-looking values when printing your tensor. "
"The ``torch.empty()`` call allocates memory for the tensor, but does not "
"initialize it with any values - so what you’re seeing is whatever was in "
"memory at the time of allocation."
msgstr ""
"当打印你的张量时，你可能会看到一些看似随机的值。``torch.empty()`` "
"调用为张量分配了内存，但并没有用任何值进行初始化——所以你看到的是分配时内存中的原始内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A brief note about tensors and their number of dimensions, and terminology:"
msgstr "关于张量及其维度和术语的简短说明："

#: ../../beginner/vt_tutorial.rst:783
msgid "You will sometimes see a 1-dimensional tensor called a *vector.*"
msgstr "有时，你会看到一维张量被称为*向量（vector）*。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Likewise, a 2-dimensional tensor is often referred to as a *matrix.*"
msgstr "同样，二维张量通常被称为*矩阵（matrix）*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Anything with more than two dimensions is generally just called a tensor."
msgstr "具有超过二维的张量通常只是简单地称为张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"More often than not, you’ll want to initialize your tensor with some value. "
"Common cases are all zeros, all ones, or random values, and the ``torch`` "
"module provides factory methods for all of these:"
msgstr "通常情况下，你可能需要用一些值来初始化张量。常见的情况是全零、全一或随机值，``torch`` 模块为这些情况提供了一系列工厂方法："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The fctory methods all do just what you’d expect - we have a tensor full of "
"zeros, another full of ones, and another with random values between 0 and 1."
msgstr "这些工厂方法的行为正如你所期望的那样——我们创建了一个零填充的张量，一个一填充的张量，还有一个值在0到1之间的随机张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Random Tensors and Seeding"
msgstr "随机张量与设置种子"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Speaking of the random tensor, did you notice the call to "
"``torch.manual_seed()`` immediately preceding it? Initializing tensors, such"
" as a model’s learning weights, with random values is common but there are "
"times - especially in research settings - where you’ll want some assurance "
"of the reproducibility of your results. Manually setting your random number "
"generator’s seed is the way to do this. Let’s look more closely:"
msgstr ""
"说到随机张量，你注意到在它之前调用了 ``torch.manual_seed()`` "
"吗？用随机值初始化张量（如模型的学习权重）很常见，但有时——特别是在研究环境中——你会希望你的结果具有可重复性。手动设置随机数生成器的种子是实现这一点的方法。让我们仔细看看："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What you should see above is that ``random1`` and ``random3`` carry "
"identical values, as do ``random2`` and ``random4``. Manually setting the "
"RNG’s seed resets it, so that identical computations depending on random "
"number should, in most settings, provide identical results."
msgstr ""
"你应该会看到 ``random1`` 和 ``random3`` 拥有相同的值，``random2`` 和 ``random4`` "
"一样。手动设置随机数生成器的种子会重置它，因此依赖随机数的相同计算在大多数情况下会产生相同的结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information, see the `PyTorch documentation on reproducibility "
"<https://pytorch.org/docs/stable/notes/randomness.html>`__."
msgstr ""
"更多详细信息，请参阅 `PyTorch 关于可重复性的文档 "
"<https://pytorch.org/docs/stable/notes/randomness.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensor Shapes"
msgstr "张量形状"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Often, when you’re performing operations on two or more tensors, they will "
"need to be of the same *shape* - that is, having the same number of "
"dimensions and the same number of cells in each dimension. For that, we have"
" the ``torch.*_like()`` methods:"
msgstr ""
"通常，当你对两个或多个张量执行操作时，它们需要具有相同的*形状*——也就是说，它们需要具有相同的维数以及每个维度中单元格的数量。为此，我们使用 "
"``torch.*_like()`` 方法："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The first new thing in the code cell above is the use of the ``.shape`` "
"property on a tensor. This property contains a list of the extent of each "
"dimension of a tensor - in our case, ``x`` is a three-dimensional tensor "
"with shape 2 x 2 x 3."
msgstr ""
"上面的代码单元中的新内容是使用了张量的 ``.shape`` 属性。此属性包含每个维度的范围列表——在我们的例子中，``x`` 是一个形状为2 x 2 "
"x 3的三维张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below that, we call the ``.empty_like()``, ``.zeros_like()``, "
"``.ones_like()``, and ``.rand_like()`` methods. Using the ``.shape`` "
"property, we can verify that each of these methods returns a tensor of "
"identical dimensionality and extent."
msgstr ""
"在此下方，我们调用了 ``.empty_like()``、``.zeros_like()``、``.ones_like()`` 和 "
"``.rand_like()`` 方法。使用 ``.shape`` 属性，我们可以验证这些方法每次都返回一个具有相同维度和范围的张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The last way to create a tensor that will cover is to specify its data "
"directly from a PyTorch collection:"
msgstr "创建张量的最后一种方式是直接从 PyTorch 集合中指定其数据："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Using ``torch.tensor()`` is the most straightforward way to create a tensor "
"if you already have data in a Python tuple or list. As shown above, nesting "
"the collections will result in a multi-dimensional tensor."
msgstr ""
"如果你已经在一个Python元组或列表中有数据，使用 ``torch.tensor()`` "
"是创建张量最直接的方法。如上所示，嵌套集合将生成一个多维张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``torch.tensor()`` creates a copy of the data."
msgstr "``torch.tensor()`` 会创建数据的副本。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tensor Data Types"
msgstr "张量数据类型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Setting the datatype of a tensor is possible a couple of ways:"
msgstr "设置张量的数据类型有几种方式："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The simplest way to set the underlying data type of a tensor is with an "
"optional argument at creation time. In the first line of the cell above, we "
"set ``dtype=torch.int16`` for the tensor ``a``. When we print ``a``, we can "
"see that it’s full of ``1`` rather than ``1.`` - Python’s subtle cue that "
"this is an integer type rather than floating point."
msgstr ""
"设置张量底层数据类型的最简单方法是在创建时使用一个可选参数。在上面代码单元的第一行中，我们为张量 ``a`` 设置了 "
"``dtype=torch.int16``。当我们打印 ``a`` 时，我们可以看到它充满了 ``1`` 而不是 ``1.`` "
"这表明它是整数类型而不是浮点数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Another thing to notice about printing ``a`` is that, unlike when we left "
"``dtype`` as the default (32-bit floating point), printing the tensor also "
"specifies its ``dtype``."
msgstr "另一个在打印 ``a`` 时值得注意的点是，与默认的32位浮点类型不同，打印张量时还会显示它的 ``dtype``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You may have also spotted that we went from specifying the tensor’s shape as"
" a series of integer arguments, to grouping those arguments in a tuple. This"
" is not strictly necessary - PyTorch will take a series of initial, "
"unlabeled integer arguments as a tensor shape - but when adding the optional"
" arguments, it can make your intent more readable."
msgstr ""
"你还可能注意到我们从通过一系列整数参数指定张量的形状转变为将这些参数组合在一个元组中。这并非严格必要——PyTorch可以接受一系列初始、未标记的整数参数作为张量形状——但当我们添加可选参数时，这种方式可以使意图更易读。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The other way to set the datatype is with the ``.to()`` method. In the cell "
"above, we create a random floating point tensor ``b`` in the usual way. "
"Following that, we create ``c`` by converting ``b`` to a 32-bit integer with"
" the ``.to()`` method. Note that ``c`` contains all the same values as "
"``b``, but truncated to integers."
msgstr ""
"设置数据类型的另一种方法是使用 ``.to()`` 方法。在上面的代码单元中，我们以常规方式创建了一个浮点随机张量 ``b``。然后，我们使用 "
"``.to()`` 方法将 ``b`` 转换为32位整数，生成张量 ``c``。请注意，``c`` 包含了所有与 ``b`` "
"相同的值，但它们被截断为整数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information, see the `data types documentation "
"<https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype>`__."
msgstr ""
"更多信息，请参见 `数据类型文档 "
"<https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Math & Logic with PyTorch Tensors"
msgstr "使用PyTorch张量进行数学与逻辑操作"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that you know some of the ways to create a tensor… what can you do with "
"them?"
msgstr "现在你知道了一些创建张量的方法……那么你可以用它们做什么呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s look at basic arithmetic first, and how tensors interact with simple "
"scalars:"
msgstr "让我们首先看看基本的算术，以及张量与简单标量的交互方式："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As you can see above, arithmetic operations between tensors and scalars, "
"such as addition, subtraction, multiplication, division, and exponentiation "
"are distributed over every element of the tensor. Because the output of such"
" an operation will be a tensor, you can chain them together with the usual "
"operator precedence rules, as in the line where we create ``threes``."
msgstr ""
"如上所示，张量和标量之间的算术运算，比如加、减、乘、除和幂运算，都会分布在张量的每个元素上。由于这些运算的输出也是一个张量，因此可以按照通常的运算符优先级规则将它们链接在一起，例如我们创建"
" ``threes`` 的那一行。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Similar operations between two tensors also behave like you’d intuitively "
"expect:"
msgstr "两个张量之间的类似操作也会按照直觉行为："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It’s important to note here that all of the tensors in the previous code "
"cell were of identical shape. What happens when we try to perform a binary "
"operation on tensors if dissimilar shape?"
msgstr "这里重要的是注意，前面代码单元中的所有张量都有相同的形状。如果我们尝试对形状不同的张量进行二元操作会发生什么？"

#: ../../beginner/vt_tutorial.rst:783
msgid "The following cell throws a run-time error. This is intentional."
msgstr "以下代码单元会引发一个运行时错误。这是有意的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the general case, you cannot operate on tensors of different shape this "
"way, even in a case like the cell above, where the tensors have an identical"
" number of elements."
msgstr "在通常情况下，你不能以这种方式对形状不同的张量进行操作，即使如上单元中那样，两个张量具有相同数量的元素。"

#: ../../beginner/vt_tutorial.rst:783
msgid "In Brief: Tensor Broadcasting"
msgstr "简要介绍：张量广播"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you are familiar with broadcasting semantics in NumPy ndarrays, you’ll "
"find the same rules apply here."
msgstr "如果你熟悉NumPy ndarrays中的广播语义，你会发现这里的规则相同。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The exception to the same-shapes rule is *tensor broadcasting.* Here’s an "
"example:"
msgstr "同形规则的例外是*张量广播（tensor broadcasting）*。以下是一个例子："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What’s the trick here? How is it we got to multiply a 2x4 tensor by a 1x4 "
"tensor?"
msgstr "这里的诀窍是什么？我们如何将一个2x4的张量乘以一个1x4的张量？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Broadcasting is a way to perform an operation between tensors that have "
"similarities in their shapes. In the example above, the one-row, four-column"
" tensor is multiplied by *both rows* of the two-row, four-column tensor."
msgstr "广播是一种处理具有形状相似性的张量之间操作的方法。上面的例子中，1行4列的张量与两行4列张量的*每一行*相乘。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is an important operation in Deep Learning. The common example is "
"multiplying a tensor of learning weights by a *batch* of input tensors, "
"applying the operation to each instance in the batch separately, and "
"returning a tensor of identical shape - just like our (2, 4) \\* (1, 4) "
"example above returned a tensor of shape (2, 4)."
msgstr ""
"这是深度学习中的一个重要操作。常见的例子是将一个学习权重的张量与一个输入张量*批量（batch）*相乘，对批中的每个实例分别应用操作，并返回一个形状相同的张量——就像我们上面的"
" (2, 4) \\* (1, 4) 示例那样，返回了一个形状为 (2, 4) 的张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The rules for broadcasting are:"
msgstr "广播的规则是："

#: ../../beginner/vt_tutorial.rst:783
msgid "Each tensor must have at least one dimension - no empty tensors."
msgstr "每个张量必须至少有一个维度——没有空张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Comparing the dimension sizes of the two tensors, *going from last to "
"first:*"
msgstr "比较两个张量的维度大小，*从最后一个维度往前比较*："

#: ../../beginner/vt_tutorial.rst:783
msgid "Each dimension must be equal, *or*"
msgstr "每个维度必须相等，*或者*"

#: ../../beginner/vt_tutorial.rst:783
msgid "One of the dimensions must be of size 1, *or*"
msgstr "其中一个维度的大小必须为1，*或者*"

#: ../../beginner/vt_tutorial.rst:783
msgid "The dimension does not exist in one of the tensors"
msgstr "其中一个张量中不存在该维度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors of identical shape, of course, are trivially “broadcastable”, as you"
" saw earlier."
msgstr "形状相同的张量当然是平凡的“可广播”，正如你之前看到的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here are some examples of situations that honor the above rules and allow "
"broadcasting:"
msgstr "以下是一些符合上述规则并允许广播的示例："

#: ../../beginner/vt_tutorial.rst:783
msgid "Look closely at the values of each tensor above:"
msgstr "仔细观察上面每个张量的值："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The multiplication operation that created ``b`` was broadcast over every "
"“layer” of ``a``."
msgstr "创建 ``b`` 的乘法操作是对 ``a`` 的每个“层”进行了广播。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For ``c``, the operation was broadcast over every layer and row of ``a`` - "
"every 3-element column is identical."
msgstr "对于 ``c``，该操作被广播到 ``a`` 的每一层和每一行——每个3个元素的列是相同的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For ``d``, we switched it around - now every *row* is identical, across "
"layers and columns."
msgstr "对于 ``d``，我们调换了方式——现在每*一行*是相同的，跨越层和列。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information on broadcasting, see the `PyTorch documentation "
"<https://pytorch.org/docs/stable/notes/broadcasting.html>`__ on the topic."
msgstr ""
"有关广播的更多信息，请参阅关于该主题的 `PyTorch 文档 "
"<https://pytorch.org/docs/stable/notes/broadcasting.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Here are some examples of attempts at broadcasting that will fail:"
msgstr "以下是一些尝试进行广播但会失败的示例："

#: ../../beginner/vt_tutorial.rst:783
msgid "More Math with Tensors"
msgstr "更多关于张量的数学运算"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch tensors have over three hundred operations that can be performed on "
"them."
msgstr "PyTorch 张量拥有超过三百种可以在其上执行的运算。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here is a small sample from some of the major categories of operations:"
msgstr "以下是主要运算类别中的一些小示例："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is a small sample of operations. For more details and the full "
"inventory of math functions, have a look at the `documentation "
"<https://pytorch.org/docs/stable/torch.html#math-operations>`__. For more "
"details and the full inventory of linear algebra operations, have a look at "
"this `documentation <https://pytorch.org/docs/stable/linalg.html>`__."
msgstr ""
"这只是运算的一个小样本。有关更多详细信息和完整的数学函数清单，请查看 `文档 "
"<https://pytorch.org/docs/stable/torch.html#math-operations>`__。 "
"有关更多详情和完整的线性代数运算，请查看 `文档 <https://pytorch.org/docs/stable/linalg.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Altering Tensors in Place"
msgstr "就地修改张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Most binary operations on tensors will return a third, new tensor. When we "
"say ``c = a * b`` (where ``a`` and ``b`` are tensors), the new tensor ``c`` "
"will occupy a region of memory distinct from the other tensors."
msgstr ""
"大多数针对张量的二元运算会返回第三个、新的张量。当我们说 ``c = a * b``（其中 ``a`` 和 ``b`` 是张量）时，新张量 ``c`` "
"将占用不同于其他张量的内存区域。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are times, though, that you may wish to alter a tensor in place - for "
"example, if you’re doing an element-wise computation where you can discard "
"intermediate values. For this, most of the math functions have a version "
"with an appended underscore (``_``) that will alter a tensor in place."
msgstr ""
"然而有时，你可能希望就地修改张量——例如，如果你正在进行逐元素计算并可以舍弃中间值。为此，大多数数学函数都有一个带有下划线``_``的版本，这些版本会就地修改张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For arithmetic operations, there are functions that behave similarly:"
msgstr "对于算术运算，也有类似的函数："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that these in-place arithmetic functions are methods on the "
"``torch.Tensor`` object, not attached to the ``torch`` module like many "
"other functions (e.g., ``torch.sin()``). As you can see from ``a.add_(b)``, "
"*the calling tensor is the one that gets changed in place.*"
msgstr ""
"请注意，这些就地算术函数是``torch.Tensor``对象上的方法，而不像许多其他函数那样附属于``torch``模块（例如，``torch.sin()``）。正如你从``a.add_(b)``中看到的，*调用的张量是那个被就地更改的张量。*"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There is another option for placing the result of a computation in an "
"existing, allocated tensor. Many of the methods and functions we’ve seen so "
"far - including creation methods! - have an ``out`` argument that lets you "
"specify a tensor to receive the output. If the ``out`` tensor is the correct"
" shape and ``dtype``, this can happen without a new memory allocation:"
msgstr ""
"还有一个选项可以将计算结果放置在一个现有的、已分配的张量中。我们迄今为止见过的许多方法和函数——包括创建方法！——都拥有一个``out``参数，允许你指定一个张量来接收输出。如果``out``张量具有正确的形状和``dtype``，这可以无需重新分配内存完成："

#: ../../beginner/vt_tutorial.rst:783
msgid "Copying Tensors"
msgstr "复制张量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As with any object in Python, assigning a tensor to a variable makes the "
"variable a *label* of the tensor, and does not copy it. For example:"
msgstr "与Python中的任何对象一样，将张量分配给一个变量会使该变量成为张量的*标签*，而不是其副本。例如："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"But what if you want a separate copy of the data to work on? The ``clone()``"
" method is there for you:"
msgstr "但如果你想要一个单独的副本来操作数据怎么办？``clone()``方法就是为你设计的："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**There is an important thing to be aware of when using ``clone()``.** If "
"your source tensor has autograd, enabled then so will the clone. **This will"
" be covered more deeply in the video on autograd,** but if you want the "
"light version of the details, continue on."
msgstr ""
"**使用``clone()``时需要注意一个重要事项。** "
"如果你的源张量启用了自动求导，克隆的张量也会启用。**这将在自动求导的视频中更深入地介绍，**但如果你想要了解简单版的细节，请继续阅读。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"*In many cases, this will be what you want.* For example, if your model has "
"multiple computation paths in its ``forward()`` method, and *both* the "
"original tensor and its clone contribute to the model’s output, then to "
"enable model learning you want autograd turned on for both tensors. If your "
"source tensor has autograd enabled (which it generally will if it’s a set of"
" learning weights or derived from a computation involving the weights), then"
" you’ll get the result you want."
msgstr ""
"在很多情况下，这将是您想要的。比如，如果您的模型在其``forward()``方法中有多个计算路径，并且原始张量及其克隆都对模型的输出有贡献，那么为了使模型学习生效，您希望两个张量都启用自动梯度计算。如果源张量启用了自动梯度计算（通常情况下，如果它是学习权重的一部分或者是由涉及这些权重的计算派生出来的，那么它就会启用自动梯度计算），那么您将获得您想要的结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"On the other hand, if you’re doing a computation where *neither* the "
"original tensor nor its clone need to track gradients, then as long as the "
"source tensor has autograd turned off, you’re good to go."
msgstr "另一方面，如果您进行的是一种计算，其中既不需要原始张量也不需要克隆张量跟踪梯度，那么只要源张量关闭了自动梯度计算，您就没问题了。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"*There is a third case,* though: Imagine you’re performing a computation in "
"your model’s ``forward()`` function, where gradients are turned on for "
"everything by default, but you want to pull out some values mid-stream to "
"generate some metrics. In this case, you *don’t* want the cloned copy of "
"your source tensor to track gradients - performance is improved with "
"autograd’s history tracking turned off. For this, you can use the "
"``.detach()`` method on the source tensor:"
msgstr ""
"不过，还有一种第三种情况：假设您正在模型的``forward()``函数中执行计算，其中默认情况下所有内容都启用了梯度跟踪，但您希望中途提取一些值以生成一些指标。在这种情况下，您不希望源张量的克隆副本跟踪梯度——关闭自动梯度的历史跟踪会提高性能。为此，您可以对源张量使用``.detach()``方法："

#: ../../beginner/vt_tutorial.rst:783
msgid "What’s happening here?"
msgstr "这里发生了什么？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We create ``a`` with ``requires_grad=True`` turned on. **We haven’t covered "
"this optional argument yet, but will during the unit on autograd.**"
msgstr ""
"我们创建了``a``，并开启了``requires_grad=True``。**我们尚未介绍该可选参数，但会在有关自动梯度计算的章节中详细讨论。**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When we print ``a``, it informs us that the property ``requires_grad=True`` "
"- this means that autograd and computation history tracking are turned on."
msgstr "当我们打印``a``时，它告诉我们属性``requires_grad=True``——这意味着自动梯度计算和计算历史跟踪已开启。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We clone ``a`` and label it ``b``. When we print ``b``, we can see that it’s"
" tracking its computation history - it has inherited ``a``\\ ’s autograd "
"settings, and added to the computation history."
msgstr ""
"我们克隆了``a``并将其标记为``b``。当我们打印``b``时，我们可以看到它正在跟踪其计算历史——它继承了``a``的自动梯度设置，并添加了计算历史。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We clone ``a`` into ``c``, but we call ``detach()`` first."
msgstr "我们将``a``克隆为``c``，但首先调用了``detach()``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Printing ``c``, we see no computation history, and no "
"``requires_grad=True``."
msgstr "打印``c``时，我们发现没有计算历史，也没有``requires_grad=True``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``detach()`` method *detaches the tensor from its computation history.* "
"It says, “do whatever comes next as if autograd was off.” It does this "
"*without* changing ``a`` - you can see that when we print ``a`` again at the"
" end, it retains its ``requires_grad=True`` property."
msgstr ""
"``detach()``方法将张量从其计算历史中分离出来。它表示，“之后执行的任何操作都像关闭了自动梯度一样。”它在不改变``a``的情况下完成此操作——您可以看到，当我们最后再次打印``a``时，它仍然具有其``requires_grad=True``属性。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Moving to `Accelerator "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__"
msgstr "转到`加速器 <https://pytorch.org/docs/stable/torch.html#accelerators>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One of the major advantages of PyTorch is its robust acceleration on an "
"`accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__ "
"such as CUDA, MPS, MTIA, or XPU. So far, everything we’ve done has been on "
"CPU. How do we move to the faster hardware?"
msgstr ""
"PyTorch 的一项主要优势是它在 `加速器 "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__ 上的强大加速能力，例如 "
"CUDA、MPS、MTIA 或 XPU。到目前为止，我们做的所有操作都在 CPU 上。如何转移到更快的硬件？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, we should check whether an accelerator is available, with the "
"``is_available()`` method."
msgstr "首先，我们应该使用``is_available()``方法检查是否有加速器可用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you do not have an accelerator, the executable cells in this section will"
" not execute any accelerator-related code."
msgstr "如果您没有加速器，本节中的可执行单元不会执行任何与加速器相关的代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once we’ve determined that one or more accelerators is available, we need to"
" put our data someplace where the accelerator can see it. Your CPU does "
"computation on data in your computer’s RAM. Your accelerator has dedicated "
"memory attached to it. Whenever you want to perform a computation on a "
"device, you must move *all* the data needed for that computation to memory "
"accessible by that device. (Colloquially, “moving the data to memory "
"accessible by the GPU” is shorted to, “moving the data to the GPU”.)"
msgstr ""
"一旦确定有一个或多个加速器可用，我们需要将数据放置到加速器可访问的某处。您的 CPU 在计算机的 RAM "
"中对数据进行计算。而加速器有专用内存与其连接。每当您想在设备上执行计算时，您必须将所有计算所需的数据移动到设备可访问的内存中。（通常情况下，“将数据移动到"
" GPU 的内存”被简化为“将数据移动到 GPU”。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are multiple ways to get your data onto your target device. You may do"
" it at creation time:"
msgstr "有多种方法可以将数据加载到目标设备上。您可以在创建时完成此操作："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By default, new tensors are created on the CPU, so we have to specify when "
"we want to create our tensor on the accelerator with the optional ``device``"
" argument. You can see when we print the new tensor, PyTorch informs us "
"which device it’s on (if it’s not on CPU)."
msgstr ""
"默认情况下，新张量是在 CPU 上创建的，因此当我们希望在加速器上创建张量时需要使用可选的``device``参数进行指定。您可以在打印新张量时看到 "
"PyTorch 告诉我们张量在哪个设备上（如果不是在 CPU 上）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can query the number of accelerators with "
"``torch.accelerator.device_count()``. If you have more than one accelerator,"
" you can specify them by index, take CUDA for example: ``device='cuda:0'``, "
"``device='cuda:1'``, etc."
msgstr ""
"您可以用``torch.accelerator.device_count()``查询加速器数量。如果您有多个加速器，可以通过索引指定它们，以 CUDA "
"为例：``device='cuda:0'``、``device='cuda:1'``等等。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As a coding practice, specifying our devices everywhere with string "
"constants is pretty fragile. In an ideal world, your code would perform "
"robustly whether you’re on CPU or accelerator hardware. You can do this by "
"creating a device handle that can be passed to your tensors instead of a "
"string:"
msgstr ""
"作为一种编码实践，使用字符串常量到处指定设备是非常脆弱的。在理想情况下，无论您是在 CPU "
"还是加速器硬件上，您的代码都应能稳健地运行。您可以通过创建一个能传递给张量的设备句柄来实现这一点，而不是直接使用字符串："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you have an existing tensor living on one device, you can move it to "
"another with the ``to()`` method. The following line of code creates a "
"tensor on CPU, and moves it to whichever device handle you acquired in the "
"previous cell."
msgstr ""
"如果您有一个现有的张量在某一设备上，可以使用``to()``方法将其移动到另一设备。以下代码创建了一个 CPU "
"上的张量，并将其移动到前一个单元中获取的设备句柄上。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It is important to know that in order to do computation involving two or "
"more tensors, *all of the tensors must be on the same device*. The following"
" code will throw a runtime error, regardless of whether you have an "
"accelerator device available, take CUDA for example:"
msgstr ""
"需要知道的是，为了进行涉及两个或更多张量的计算，所有张量必须在同一设备上。如果您有加速器设备可用，无论如何以下代码都会抛出运行时错误，以 CUDA "
"为例："

#: ../../beginner/vt_tutorial.rst:783
msgid "Manipulating Tensor Shapes"
msgstr "张量形状操作"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sometimes, you’ll need to change the shape of your tensor. Below, we’ll look"
" at a few common cases, and how to handle them."
msgstr "有时，您需要更改张量的形状。下面我们看看几个常见案例，以及如何处理它们。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Changing the Number of Dimensions"
msgstr "改变维度的数量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One case where you might need to change the number of dimensions is passing "
"a single instance of input to your model. PyTorch models generally expect "
"*batches* of input."
msgstr "一个可能需要改变维度数量的情况是向您的模型传递单个输入实例。PyTorch 模型通常期待批输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For example, imagine having a model that works on 3 x 226 x 226 images - a "
"226-pixel square with 3 color channels. When you load and transform it, "
"you’ll get a tensor of shape ``(3, 226, 226)``. Your model, though, is "
"expecting input of shape ``(N, 3, 226, 226)``, where ``N`` is the number of "
"images in the batch. So how do you make a batch of one?"
msgstr ""
"例如，假设有一个模型处理3 x 226 x 226的图像——一个226像素的方形，具有3个颜色通道。当您加载并转换图像时，您将得到一个形状为``(3, "
"226, 226)``的张量。但是，您的模型期待的输入形状是``(N, 3, 226, "
"226)``，其中``N``是批中的图像数量。那么如何创建一个单图像的批呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``unsqueeze()`` method adds a dimension of extent 1. ``unsqueeze(0)`` "
"adds it as a new zeroth dimension - now you have a batch of one!"
msgstr ""
"``unsqueeze()``方法添加一个维度范围为1。``unsqueeze(0)``将其作为新的第零维度添加——现在您有一个单输入的批了！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So if that’s *un*\\ squeezing? What do we mean by squeezing? We’re taking "
"advantage of the fact that any dimension of extent 1 *does not* change the "
"number of elements in the tensor."
msgstr "如果这就是*扩展*，我们说的压缩是什么？我们利用了这样一个事实，即任何维度范围为1都*不会*改变张量中的元素数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Continuing the example above, let’s say the model’s output is a 20-element "
"vector for each input. You would then expect the output to have shape ``(N, "
"20)``, where ``N`` is the number of instances in the input batch. That means"
" that for our single-input batch, we’ll get an output of shape ``(1, 20)``."
msgstr ""
"继续上面的例子，假设模型的输出是一个20元素的向量，针对每个输入。然后您会期待输出形状为``(N, "
"20)``，其中``N``是输入批中的实例数量。这意味着对于我们的单输入批，我们将得到形状为``(1, 20)``的输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What if you want to do some *non-batched* computation with that output - "
"something that’s just expecting a 20-element vector?"
msgstr "如果您希望用该输出进行一些*非批处理*计算——一些指望只使用一个20元素向量的计算呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can see from the shapes that our 2-dimensional tensor is now "
"1-dimensional, and if you look closely at the output of the cell above "
"you’ll see that printing ``a`` shows an “extra” set of square brackets "
"``[]`` due to having an extra dimension."
msgstr ""
"从形状可以看到二维张量现在变成了一维张量，如果您仔细观察上面单元输出，您会发现打印``a``显示了一个“额外”的方括号``[]``，这是因为有一个额外维度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You may only ``squeeze()`` dimensions of extent 1. See above where we try to"
" squeeze a dimension of size 2 in ``c``, and get back the same shape we "
"started with. Calls to ``squeeze()`` and ``unsqueeze()`` can only act on "
"dimensions of extent 1 because to do otherwise would change the number of "
"elements in the tensor."
msgstr ""
"您只能对范围为1的维度使用``squeeze()``。查看上面的地方，我们尝试对``c``中的大小为2的维度进行压缩，并得到了与开始时相同的形状。``squeeze()``和``unsqueeze()``的调用只能作用于范围为1的维度，因为否则会改变张量中的元素数量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Another place you might use ``unsqueeze()`` is to ease broadcasting. Recall "
"the example above where we had the following code:"
msgstr "另一个您可能会使用``unsqueeze()``的位置是为了方便广播。回想上面的例子，其中我们有以下代码："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The net effect of that was to broadcast the operation over dimensions 0 and "
"2, causing the random, 3 x 1 tensor to be multiplied element-wise by every "
"3-element column in ``a``."
msgstr "这样做的净效果是对维度0和2进行广播，使得随机3 x 1张量被逐元素地乘以``a``中的每个三元素列。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What if the random vector had just been 3-element vector? We’d lose the "
"ability to do the broadcast, because the final dimensions would not match up"
" according to the broadcasting rules. ``unsqueeze()`` comes to the rescue:"
msgstr "如果随机向量仅是一个三元素向量呢？由于广播规则的最终维度不匹配，我们将失去广播能力。``unsqueeze()``会帮忙解决这个问题："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``squeeze()`` and ``unsqueeze()`` methods also have in-place versions, "
"``squeeze_()`` and ``unsqueeze_()``:"
msgstr ""
"``squeeze()``和``unsqueeze()``方法还有就地版本，分别为``squeeze_()``和``unsqueeze_()``："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sometimes you’ll want to change the shape of a tensor more radically, while "
"still preserving the number of elements and their contents. One case where "
"this happens is at the interface between a convolutional layer of a model "
"and a linear layer of the model - this is common in image classification "
"models. A convolution kernel will yield an output tensor of shape *features "
"x width x height,* but the following linear layer expects a 1-dimensional "
"input. ``reshape()`` will do this for you, provided that the dimensions you "
"request yield the same number of elements as the input tensor has:"
msgstr ""
"有时您可能希望更加激烈地改变张量的形状，同时保持元素数量及其内容不变。这种情况常见于模型的卷积层与线性层接口之间——这在图像分类模型中非常常见。卷积核将产生形状为*特征"
" x 宽度 x 高度*的输出张量，而后续线性层则期望一维输入。只要您请求的维度与输入张量的元素数量相等，``reshape()``就可以完成此操作："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``(6 * 20 * 20,)`` argument in the final line of the cell above is "
"because PyTorch expects a **tuple** when specifying a tensor shape - but "
"when the shape is the first argument of a method, it lets us cheat and just "
"use a series of integers. Here, we had to add the parentheses and comma to "
"convince the method that this is really a one-element tuple."
msgstr ""
"以上单元中最后一行中的``(6 * 20 * 20,)``参数是因为 PyTorch "
"在指定张量形状时期待一个**元组**——但是当形状是方法的第一个参数时，它允许我们仅使用数字序列。这时我们不得不添加括号和逗号，以表明这是一个单元素元组。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When it can, ``reshape()`` will return a *view* on the tensor to be changed "
"- that is, a separate tensor object looking at the same underlying region of"
" memory. *This is important:* That means any change made to the source "
"tensor will be reflected in the view on that tensor, unless you ``clone()`` "
"it."
msgstr ""
"当可能时，``reshape()``会返回一个张管视图——即一个单独的张量对象，查看同一底层的内存区域。*这一点很重要：*这意味着对源张量进行的任何修改都会反映在该张管视图上，除非您将其``clone()``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There *are* conditions, beyond the scope of this introduction, where "
"``reshape()`` has to return a tensor carrying a copy of the data. For more "
"information, see the `docs "
"<https://pytorch.org/docs/stable/torch.html#torch.reshape>`__."
msgstr ""
"有一些超出本介绍范围的情况，其中``reshape()``不得不返回一个包含数据副本的张量。有关更多信息，请参阅`文档 "
"<https://pytorch.org/docs/stable/torch.html#torch.reshape>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "NumPy Bridge"
msgstr "NumPy 桥接"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the section above on broadcasting, it was mentioned that PyTorch’s "
"broadcast semantics are compatible with NumPy’s - but the kinship between "
"PyTorch and NumPy goes even deeper than that."
msgstr "在以上关于广播的部分中提到，PyTorch 的广播语义与 NumPy 的兼容性——但 PyTorch 与 NumPy 的关系实远不止此。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you have existing ML or scientific code with data stored in NumPy "
"ndarrays, you may wish to express that same data as PyTorch tensors, whether"
" to take advantage of PyTorch’s GPU acceleration, or its efficient "
"abstractions for building ML models. It’s easy to switch between ndarrays "
"and PyTorch tensors:"
msgstr ""
"如果您有现有的机器学习或科学代码，其数据存储在 NumPy 的 ndarrays 中，您可能希望将这些数据表示为 PyTorch 张量，无论是为了利用 "
"PyTorch 的 GPU 加速，还是因其构建机器学习模型的高效抽象。切换 ndarrays 和 PyTorch 张量很容易："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch creates a tensor of the same shape and containing the same data as "
"the NumPy array, going so far as to keep NumPy’s default 64-bit float data "
"type."
msgstr "PyTorch 创建了一个与 NumPy 数组形状相同且包含相同数据的张量，甚至保留了 NumPy 默认的 64 位浮点数据类型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The conversion can just as easily go the other way:"
msgstr "转换也可以很容易反过来完成："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It is important to know that these converted objects are using *the same "
"underlying memory* as their source objects, meaning that changes to one are "
"reflected in the other:"
msgstr "需要知道的是，这些转换生成的对象使用*与其源对象相同的底层内存*，这意味着对其中一个的修改会反映到另一个上："

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.922 seconds)"
msgstr "**脚本总运行时间：**（0分钟 0.922秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: tensors_deeper_tutorial.py "
"<tensors_deeper_tutorial.py>`"
msgstr ""
":download:`下载Python源代码：tensors_deeper_tutorial.py "
"<tensors_deeper_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: tensors_deeper_tutorial.ipynb "
"<tensors_deeper_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：tensors_deeper_tutorial.ipynb "
"<tensors_deeper_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_introyt_trainingyt.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_introyt_trainingyt.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction <introyt1_tutorial.html>`_ || `Tensors "
"<tensors_deeper_tutorial.html>`_ || `Autograd <autogradyt_tutorial.html>`_ "
"|| `Building Models <modelsyt_tutorial.html>`_ || `TensorBoard Support "
"<tensorboardyt_tutorial.html>`_ || **Training Models** || `Model "
"Understanding <captumyt.html>`_"
msgstr ""
"`简介 <introyt1_tutorial.html>`_ || `张量 <tensors_deeper_tutorial.html>`_ || "
"`自动梯度 <autogradyt_tutorial.html>`_ || `构建模型 <modelsyt_tutorial.html>`_ || "
"`TensorBoard 支持 <tensorboardyt_tutorial.html>`_ || **训练模型** || `模型理解 "
"<captumyt.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training with PyTorch"
msgstr "使用 PyTorch 进行训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow along with the video below or on `youtube "
"<https://www.youtube.com/watch?v=jF43_wj_DCQ>`__."
msgstr ""
"可观看下面的视频，或者在`YouTube <https://www.youtube.com/watch?v=jF43_wj_DCQ>`__上观看。"

#: ../../beginner/vt_tutorial.rst:783
msgid "In past videos, we’ve discussed and demonstrated:"
msgstr "在以往的视频中，我们讨论并演示了："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Building models with the neural network layers and functions of the torch.nn"
" module"
msgstr "使用torch.nn模块的神经网络层和函数来构建模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The mechanics of automated gradient computation, which is central to "
"gradient-based model training"
msgstr "自动梯度计算的机制，这是基于梯度的模型训练的核心"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using TensorBoard to visualize training progress and other activities"
msgstr "使用TensorBoard可视化训练进展和其他活动"

#: ../../beginner/vt_tutorial.rst:783
msgid "In this video, we’ll be adding some new tools to your inventory:"
msgstr "在本视频中，我们将为您添加一些新的工具。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ll get familiar with the dataset and dataloader abstractions, and how "
"they ease the process of feeding data to your model during a training loop"
msgstr "我们将熟悉数据集和数据加载器抽象，以及它们如何简化在训练循环中为模型提供数据的过程"

#: ../../beginner/vt_tutorial.rst:783
msgid "We’ll discuss specific loss functions and when to use them"
msgstr "我们将讨论特定的损失函数以及何时使用它们"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We’ll look at PyTorch optimizers, which implement algorithms to adjust model"
" weights based on the outcome of a loss function"
msgstr "我们将了解PyTorch优化器，它们实现了根据损失函数结果调整模型权重的算法"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we’ll pull all of these together and see a full PyTorch training "
"loop in action."
msgstr "最后，我们将把这些整合在一起，实际展示完整的PyTorch训练循环。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dataset and DataLoader"
msgstr "数据集和数据加载器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``Dataset`` and ``DataLoader`` classes encapsulate the process of "
"pulling your data from storage and exposing it to your training loop in "
"batches."
msgstr "“Dataset”和“DataLoader”类封装了从存储中提取数据并以批量形式将其暴露给训练循环的过程。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``Dataset`` is responsible for accessing and processing single instances"
" of data."
msgstr "“Dataset”负责访问和处理单个数据实例。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``DataLoader`` pulls instances of data from the ``Dataset`` (either "
"automatically or with a sampler that you define), collects them in batches, "
"and returns them for consumption by your training loop. The ``DataLoader`` "
"works with all kinds of datasets, regardless of the type of data they "
"contain."
msgstr ""
"“DataLoader”从“Dataset”中提取数据实例（可以自动或使用您定义的采样器），将它们收集成批量，并返回给训练循环使用。“DataLoader”可以处理各种类型的数据集，无论数据的类型如何。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this tutorial, we’ll be using the Fashion-MNIST dataset provided by "
"TorchVision. We use ``torchvision.transforms.Normalize()`` to zero-center "
"and normalize the distribution of the image tile content, and download both "
"training and validation data splits."
msgstr ""
"在本教程中，我们将使用TorchVision提供的Fashion-"
"MNIST数据集。我们使用“torchvision.transforms.Normalize()”对图像的内容进行零中心化和归一化，并下载训练和验证数据集。"

#: ../../beginner/vt_tutorial.rst:783
msgid "As always, let’s visualize the data as a sanity check:"
msgstr "像往常一样，让我们将数据可视化以进行基本检查："

#: ../../beginner/vt_tutorial.rst:783
msgid "The Model"
msgstr "模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The model we’ll use in this example is a variant of LeNet-5 - it should be "
"familiar if you’ve watched the previous videos in this series."
msgstr "我们在本示例中使用的模型是LeNet-5的一个变体——如果您观看过系列之前的视频，它应该会很熟悉。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this example, we’ll be using a cross-entropy loss. For demonstration "
"purposes, we’ll create batches of dummy output and label values, run them "
"through the loss function, and examine the result."
msgstr "在本示例中，我们将使用交叉熵损失。为了演示，我们将创建一些虚拟的输出和标签批量值，将它们输入到损失函数中，并观察结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this example, we’ll be using simple `stochastic gradient descent "
"<https://pytorch.org/docs/stable/optim.html>`__ with momentum."
msgstr ""
"本示例中我们将使用简单的`随机梯度下降法 <https://pytorch.org/docs/stable/optim.html>`__配合动量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It can be instructive to try some variations on this optimization scheme:"
msgstr "尝试一些该优化方案的变种可能是很有启发性的："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Learning rate determines the size of the steps the optimizer takes. What "
"does a different learning rate do to the your training results, in terms of "
"accuracy and convergence time?"
msgstr "学习率决定了优化器每次迈出的步伐有多大。不同的学习率会对训练结果的准确性和收敛时间带来什么样的影响？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Momentum nudges the optimizer in the direction of strongest gradient over "
"multiple steps. What does changing this value do to your results?"
msgstr "动量使优化器在多步中朝着最强梯度的方向调整。当调整该值时，结果又会如何变化？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Try some different optimization algorithms, such as averaged SGD, Adagrad, "
"or Adam. How do your results differ?"
msgstr "尝试一些不同的优化算法，例如平均化的SGD、Adagrad或Adam。结果有何不同？"

#: ../../beginner/vt_tutorial.rst:783
msgid "The Training Loop"
msgstr "训练循环"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Below, we have a function that performs one training epoch. It enumerates "
"data from the DataLoader, and on each pass of the loop does the following:"
msgstr "如下所示，我们有一个执行单个训练轮次的函数。它从DataLoader枚举数据，在每次循环中执行以下操作："

#: ../../beginner/vt_tutorial.rst:783
msgid "Gets a batch of training data from the DataLoader"
msgstr "从DataLoader中获取一批训练数据"

#: ../../beginner/vt_tutorial.rst:783
msgid "Zeros the optimizer’s gradients"
msgstr "将优化器的梯度归零"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Performs an inference - that is, gets predictions from the model for an "
"input batch"
msgstr "执行一次推理——即从模型中获取一个输入批量的预测"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Calculates the loss for that set of predictions vs. the labels on the "
"dataset"
msgstr "计算该批预测和数据集标签之间的损失"

#: ../../beginner/vt_tutorial.rst:783
msgid "Calculates the backward gradients over the learning weights"
msgstr "计算学习权重的后向梯度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tells the optimizer to perform one learning step - that is, adjust the "
"model’s learning weights based on the observed gradients for this batch, "
"according to the optimization algorithm we chose"
msgstr "告诉优化器根据我们选择的优化算法对这个批次的观察梯度执行一次学习步骤——即调整模型的学习权重"

#: ../../beginner/vt_tutorial.rst:783
msgid "It reports on the loss for every 1000 batches."
msgstr "它会每1000个批次报告一次损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, it reports the average per-batch loss for the last 1000 batches, "
"for comparison with a validation run"
msgstr "最后，它会报告最后1000个批次的平均每批损失，以便与验证运行进行比较"

#: ../../beginner/vt_tutorial.rst:783
msgid "Per-Epoch Activity"
msgstr "每个轮次的活动"

#: ../../beginner/vt_tutorial.rst:783
msgid "There are a couple of things we’ll want to do once per epoch:"
msgstr "每个轮次，我们需要完成以下几件事情："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Perform validation by checking our relative loss on a set of data that was "
"not used for training, and report this"
msgstr "通过检查未用于训练的数据集上的相对损失来进行验证，并报告结果"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save a copy of the model"
msgstr "保存模型的一个副本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here, we’ll do our reporting in TensorBoard. This will require going to the "
"command line to start TensorBoard, and opening it in another browser tab."
msgstr "在这里，我们将在TensorBoard中进行报告。这需要打开命令行启动TensorBoard，并在另一个浏览器标签页中打开它。"

#: ../../beginner/vt_tutorial.rst:783
msgid "To load a saved version of the model:"
msgstr "加载已保存的模型版本："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once you’ve loaded the model, it’s ready for whatever you need it for - more"
" training, inference, or analysis."
msgstr "一旦加载了模型，它就可以根据需要用于更多训练、推理或分析。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that if your model has constructor parameters that affect model "
"structure, you’ll need to provide them and configure the model identically "
"to the state in which it was saved."
msgstr "请注意，如果模型具有影响模型结构的构造参数，则需要提供这些参数并将模型配置为与保存时的状态一致。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Docs on the `data utilities <https://pytorch.org/docs/stable/data.html>`__, "
"including Dataset and DataLoader, at pytorch.org"
msgstr ""
"在pytorch.org上的`数据工具文档 <https://pytorch.org/docs/stable/data.html>`__, "
"包括Dataset和DataLoader"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A `note on the use of pinned memory "
"<https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning>`__ for"
" GPU training"
msgstr ""
"关于为GPU训练使用`固定内存 <https://pytorch.org/docs/stable/notes/cuda.html#cuda-"
"memory-pinning>`__的说明"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Documentation on the datasets available in `TorchVision "
"<https://pytorch.org/vision/stable/datasets.html>`__, `TorchText "
"<https://pytorch.org/text/stable/datasets.html>`__, and `TorchAudio "
"<https://pytorch.org/audio/stable/datasets.html>`__"
msgstr ""
"TorchVision、`TorchText <https://pytorch.org/text/stable/datasets.html>`__ 和 "
"`TorchAudio <https://pytorch.org/audio/stable/datasets.html>`__中可用数据集的文档"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Documentation on the `loss functions "
"<https://pytorch.org/docs/stable/nn.html#loss-functions>`__ available in "
"PyTorch"
msgstr ""
"PyTorch中可用的`损失函数文档 <https://pytorch.org/docs/stable/nn.html#loss-"
"functions>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Documentation on the `torch.optim package "
"<https://pytorch.org/docs/stable/optim.html>`__, which includes optimizers "
"and related tools, such as learning rate scheduling"
msgstr ""
"`torch.optim包 "
"<https://pytorch.org/docs/stable/optim.html>`__的文档，包括优化器和相关工具，例如学习率调度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A detailed `tutorial on saving and loading models "
"<https://pytorch.org/tutorials/beginner/saving_loading_models.html>`__"
msgstr ""
"关于`保存和加载模型的详细教程 "
"<https://pytorch.org/tutorials/beginner/saving_loading_models.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The `Tutorials section of pytorch.org <https://pytorch.org/tutorials/>`__ "
"contains tutorials on a broad variety of training tasks, including "
"classification in different domains, generative adversarial networks, "
"reinforcement learning, and more"
msgstr ""
"`pytorch.org教程部分 <https://pytorch.org/tutorials/>`__ "
"包括各种训练任务的教程，例如不同领域的分类、生成对抗网络、强化学习等"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 27 minutes  16.034 seconds)"
msgstr "**脚本的总运行时间：**（27分钟16.034秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Python source code: trainingyt.py <trainingyt.py>`"
msgstr ":download:`下载Python源代码: trainingyt.py <trainingyt.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: trainingyt.ipynb <trainingyt.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: trainingyt.ipynb <trainingyt.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_knowledge_distillation_tutorial.py>` to download"
" the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_knowledge_distillation_tutorial.py>`"
" 下载完整的示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Knowledge Distillation Tutorial"
msgstr "知识蒸馏教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Author**: `Alexandros Chariton <https://github.com/AlexandrosChrtn>`_"
msgstr "**作者**: `Alexandros Chariton <https://github.com/AlexandrosChrtn>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Knowledge distillation is a technique that enables knowledge transfer from "
"large, computationally expensive models to smaller ones without losing "
"validity. This allows for deployment on less powerful hardware, making "
"evaluation faster and more efficient."
msgstr ""
"知识蒸馏是一种技术，可以将大型、计算开销高的模型的知识传递到更小的模型中，同时不失去模型的有效性。这允许在性能较低的硬件上部署，从而使评估变得更快速、更高效。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we will run a number of experiments focused at improving "
"the accuracy of a lightweight neural network, using a more powerful network "
"as a teacher. The computational cost and the speed of the lightweight "
"network will remain unaffected, our intervention only focuses on its "
"weights, not on its forward pass. Applications of this technology can be "
"found in devices such as drones or mobile phones. In this tutorial, we do "
"not use any external packages as everything we need is available in "
"``torch`` and ``torchvision``."
msgstr ""
"在本教程中，我们将通过一系列实验，重点提高轻量级神经网络的准确性，使用更强大的网络作为教师。轻量级网络的计算成本和速度将保持不变，我们的干预仅聚焦于其权重，而不是前向传递。这项技术的应用可以在诸如无人机或手机等设备中找到。在本教程中，我们没有使用任何外部包，因为我们所需的一切都可以在“torch”和“torchvision”中找到。"

#: ../../beginner/vt_tutorial.rst:783
msgid "In this tutorial, you will learn:"
msgstr "在本教程中，您将学习："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How to modify model classes to extract hidden representations and use them "
"for further calculations"
msgstr "如何修改模型类以提取隐藏表示并将它们用于进一步计算"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How to modify regular train loops in PyTorch to include additional losses on"
" top of, for example, cross-entropy for classification"
msgstr "如何修改PyTorch中的常规训练循环以包括其他损失，例如用于分类的交叉熵损失之上"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How to improve the performance of lightweight models by using more complex "
"models as teachers"
msgstr "如何使用更复杂的模型作为教师来提高轻量级模型的性能"

#: ../../beginner/vt_tutorial.rst:783
msgid "Prerequisites"
msgstr "前提条件"

#: ../../beginner/vt_tutorial.rst:783
msgid "1 GPU, 4GB of memory"
msgstr "1个GPU，4GB内存"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch v2.0 or later"
msgstr "PyTorch v2.0或更高版本"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"CIFAR-10 dataset (downloaded by the script and saved in a directory called "
"``/data``)"
msgstr "CIFAR-10数据集（由脚本下载并保存到名为“/data”的目录中）"

#: ../../beginner/vt_tutorial.rst:783
msgid "Loading CIFAR-10"
msgstr "加载CIFAR-10"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"CIFAR-10 is a popular image dataset with ten classes. Our objective is to "
"predict one of the following classes for each input image."
msgstr "CIFAR-10是一个流行的包含10类图像的数据集。我们的目标是为每个输入图像预测以下类别之一。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Example of CIFAR-10 images"
msgstr "CIFAR-10图像示例"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The input images are RGB, so they have 3 channels and are 32x32 pixels. "
"Basically, each image is described by 3 x 32 x 32 = 3072 numbers ranging "
"from 0 to 255. A common practice in neural networks is to normalize the "
"input, which is done for multiple reasons, including avoiding saturation in "
"commonly used activation functions and increasing numerical stability. Our "
"normalization process consists of subtracting the mean and dividing by the "
"standard deviation along each channel. The tensors \"mean=[0.485, 0.456, "
"0.406]\" and \"std=[0.229, 0.224, 0.225]\" were already computed, and they "
"represent the mean and standard deviation of each channel in the predefined "
"subset of CIFAR-10 intended to be the training set. Notice how we use these "
"values for the test set as well, without recomputing the mean and standard "
"deviation from scratch. This is because the network was trained on features "
"produced by subtracting and dividing the numbers above, and we want to "
"maintain consistency. Furthermore, in real life, we would not be able to "
"compute the mean and standard deviation of the test set since, under our "
"assumptions, this data would not be accessible at that point."
msgstr ""
"输入图像是RGB图像，因此它们有3个通道，并为32x32像素。基本上，每个图像由3 x 32 x 32 = "
"3072个0到255范围内的数字描述。神经网络中的一个常见做法是对输入进行归一化，这是出于多种原因，包括避免常用激活函数中的饱和现象以及提高数值稳定性。我们的归一化过程包括减去每个通道的平均值并除以标准差。张量\"mean=[0.485,"
" 0.456, 0.406]\"和\"std=[0.229, 0.224, "
"0.225]\"已经计算好，它们分别表示了CIFAR-10训练集预先定义子集的每个通道的平均值和标准差。注意，我们对测试集也使用这些值，而没有从头重新计算平均值和标准差。这是因为网络是在通过减去和除以上述数值后生成的特征上训练的，我们希望保持一致性。此外，在实际中，由于我们的假设，测试集数据在那个阶段往往不可访问，因此我们无法计算其平均值和标准差。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As a closing point, we often refer to this held-out set as the validation "
"set, and we use a separate set, called the test set, after optimizing a "
"model's performance on the validation set. This is done to avoid selecting a"
" model based on the greedy and biased optimization of a single metric."
msgstr ""
"作为一个总结的一点，我们通常将此保持出的数据集称为验证集，并在优化模型的性能时选用单独的数据集，称为测试集。这是为了避免在单一指标上的贪婪和带偏优化下选择模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This section is for CPU users only who are interested in quick results. Use "
"this option only if you're interested in a small scale experiment. Keep in "
"mind the code should run fairly quickly using any GPU. Select only the first"
" ``num_images_to_keep`` images from the train/test dataset"
msgstr ""
"此部分仅适用于对快速结果感兴趣的CPU用户。如果您仅对小规模实验感兴趣，可以使用此选项。请注意，使用任何GPU代码运行速度应该会相当快。只需从训练/测试数据集选择前“num_images_to_keep”图像"

#: ../../beginner/vt_tutorial.rst:783
msgid "Defining model classes and utility functions"
msgstr "定义模型类和实用函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we need to define our model classes. Several user-defined parameters "
"need to be set here. We use two different architectures, keeping the number "
"of filters fixed across our experiments to ensure fair comparisons. Both "
"architectures are Convolutional Neural Networks (CNNs) with a different "
"number of convolutional layers that serve as feature extractors, followed by"
" a classifier with 10 classes. The number of filters and neurons is smaller "
"for the students."
msgstr ""
"接下来，我们需要定义模型类。这里需要设置几个用户定义的参数。我们使用了两种不同的架构，在整个实验中保持滤波器数量固定以确保公平比较。两种架构都是卷积神经网络（CNN），有不同数量的卷积层作为特征提取器，随后是一个具有10个类别的分类器。学生模型的滤波器和神经元数量较少。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We employ 2 functions to help us produce and evaluate the results on our "
"original classification task. One function is called ``train`` and takes the"
" following arguments:"
msgstr "我们使用2个函数来帮助我们产生和评估原始分类任务的结果。一个函数名为“train”，其参数包括："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``model``: A model instance to train (update its weights) via this function."
msgstr "“model”：模型实例，通过此函数对其进行训练（更新其权重）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``train_loader``: We defined our ``train_loader`` above, and its job is to "
"feed the data into the model."
msgstr "“train_loader”：我们在上面定义了“train_loader”，它的工作是将数据输入模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``epochs``: How many times we loop over the dataset."
msgstr "“epochs”：循环遍历数据集的次数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``learning_rate``: The learning rate determines how large our steps towards "
"convergence should be. Too large or too small steps can be detrimental."
msgstr "“learning_rate”：学习率决定了我们收敛的步伐大小。步伐太大或太小都会产生负面影响。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``device``: Determines the device to run the workload on. Can be either CPU "
"or GPU depending on availability."
msgstr "“device”：决定运行任务的设备。根据可用性，既可以是CPU也可以是GPU。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our test function is similar, but it will be invoked with ``test_loader`` to"
" load images from the test set."
msgstr "我们的测试函数与之类似，但会通过“test_loader”加载测试数据集的图像。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Train both networks with Cross-Entropy. The student will be used as a "
"baseline:"
msgstr "用交叉熵训练两个网络。学生网络将用作基准："

#: ../../beginner/vt_tutorial.rst:783
msgid "Cross-entropy runs"
msgstr "交叉熵运行"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For reproducibility, we need to set the torch manual seed. We train networks"
" using different methods, so to compare them fairly, it makes sense to "
"initialize the networks with the same weights. Start by training the teacher"
" network using cross-entropy:"
msgstr ""
"为了重现结果，我们需要设置 torch "
"的手动随机种子。我们使用不同的方法训练网络，因此为了公平比较，需要使用相同的权重初始化网络。首先用交叉熵训练教师网络："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We instantiate one more lightweight network model to compare their "
"performances. Back propagation is sensitive to weight initialization, so we "
"need to make sure these two networks have the exact same initialization."
msgstr "我们再实例化一个轻量级网络模型以比较其性能。反向传播对权重初始化非常敏感，因此需要确保这两个网络具有完全相同的初始化。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To ensure we have created a copy of the first network, we inspect the norm "
"of its first layer. If it matches, then we are safe to conclude that the "
"networks are indeed the same."
msgstr "为了确保我们已复制了第一个网络，我们检查其第一层的范数。如果匹配，则可以安全地得出结论，这两个网络确实相同。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Print the total number of parameters in each model:"
msgstr "打印每个模型的总参数数量："

#: ../../beginner/vt_tutorial.rst:783
msgid "Train and test the lightweight network with cross entropy loss:"
msgstr "用交叉熵损失训练和测试轻量级网络："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As we can see, based on test accuracy, we can now compare the deeper network"
" that is to be used as a teacher with the lightweight network that is our "
"supposed student. So far, our student has not intervened with the teacher, "
"therefore this performance is achieved by the student itself. The metrics so"
" far can be seen with the following lines:"
msgstr ""
"如我们所见，根据测试准确率，现在可以将作为教师的深层网络与作为假定学生的轻量级网络进行比较。目前为止，学生网络还没有干预教师，因此此性能是由学生本身实现的。到目前为止的度量可以通过以下行看到："

#: ../../beginner/vt_tutorial.rst:783
msgid "Knowledge distillation run"
msgstr "知识蒸馏运行"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now let's try to improve the test accuracy of the student network by "
"incorporating the teacher. Knowledge distillation is a straightforward "
"technique to achieve this, based on the fact that both networks output a "
"probability distribution over our classes. Therefore, the two networks share"
" the same number of output neurons. The method works by incorporating an "
"additional loss into the traditional cross entropy loss, which is based on "
"the softmax output of the teacher network. The assumption is that the output"
" activations of a properly trained teacher network carry additional "
"information that can be leveraged by a student network during training. The "
"original work suggests that utilizing ratios of smaller probabilities in the"
" soft targets can help achieve the underlying objective of deep neural "
"networks, which is to create a similarity structure over the data where "
"similar objects are mapped closer together. For example, in CIFAR-10, a "
"truck could be mistaken for an automobile or airplane, if its wheels are "
"present, but it is less likely to be mistaken for a dog. Therefore, it makes"
" sense to assume that valuable information resides not only in the top "
"prediction of a properly trained model but in the entire output "
"distribution. However, cross entropy alone does not sufficiently exploit "
"this information as the activations for non-predicted classes tend to be so "
"small that propagated gradients do not meaningfully change the weights to "
"construct this desirable vector space."
msgstr ""
"现在让我们尝试通过引入教师网络来提高学生网络的测试准确率。知识蒸馏是一种基于两种网络对类别输出概率分布的简单技术。因此，这两个网络共享相同数量的输出神经元。该方法通过在传统交叉熵损失中加入一个附加损失来工作，该附加损失基于教师网络的"
" softmax "
"输出。假设一个经过适当训练的教师网络的输出激活包含额外的信息，学生网络在训练过程中可以利用这些信息。原始研究表明，利用软目标中较小概率的比例可以帮助实现深度神经网络的基本目标，即在数据上创建一个相似性结构，其中相似的对象被映射得更接近。例如，在"
" CIFAR-10 "
"数据集中，如果卡车带有轮子，可能会被误认为是汽车或飞机，但很少会被误认为是狗。因此，假设一个经过适当训练的模型的所有输出分布中都存在有价值的信息是有意义的，而不仅仅是其最高预测。但交叉熵并不能充分利用这些信息，因为非预测类别的激活往往过小，反向传播的梯度不能有效地改变权重以构建所期望的向量空间。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As we continue defining our first helper function that introduces a teacher-"
"student dynamic, we need to include a few extra parameters:"
msgstr "在定义第一个引入教师-学生动态的辅助函数时，我们需要包括一些额外的参数："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``T``: Temperature controls the smoothness of the output distributions. "
"Larger ``T`` leads to smoother distributions, thus smaller probabilities get"
" a larger boost."
msgstr "``T``：温度控制输出分布的平滑程度。较大的 ``T`` 会导致分布更平滑，因此小概率会获得更大的提升。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``soft_target_loss_weight``: A weight assigned to the extra objective we're "
"about to include."
msgstr "``soft_target_loss_weight``：分配给我们将要包含的额外目标的权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ce_loss_weight``: A weight assigned to cross-entropy. Tuning these weights"
" pushes the network towards optimizing for either objective."
msgstr "``ce_loss_weight``：分配给交叉熵的权重。调整这些权重可引导网络优化目标。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Distillation loss is calculated from the logits of the networks. It only "
"returns gradients to the student:"
msgstr "蒸馏损失根据网络的 logits 计算，仅对学生返回梯度："

#: ../../beginner/vt_tutorial.rst:783
msgid "Cosine loss minimization run"
msgstr "余弦损失最小化运行"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Feel free to play around with the temperature parameter that controls the "
"softness of the softmax function and the loss coefficients. In neural "
"networks, it is easy to include additional loss functions to the main "
"objectives to achieve goals like better generalization. Let's try including "
"an objective for the student, but now let's focus on their hidden states "
"rather than their output layers. Our goal is to convey information from the "
"teacher's representation to the student by including a naive loss function, "
"whose minimization implies that the flattened vectors that are subsequently "
"passed to the classifiers have become more *similar* as the loss decreases. "
"Of course, the teacher does not update its weights, so the minimization "
"depends only on the student's weights. The rationale behind this method is "
"that we are operating under the assumption that the teacher model has a "
"better internal representation that is unlikely to be achieved by the "
"student without external intervention, therefore we artificially push the "
"student to mimic the internal representation of the teacher. Whether or not "
"this will end up helping the student is not straightforward, though, because"
" pushing the lightweight network to reach this point could be a good thing, "
"assuming that we have found an internal representation that leads to better "
"test accuracy, but it could also be harmful because the networks have "
"different architectures and the student does not have the same learning "
"capacity as the teacher. In other words, there is no reason for these two "
"vectors, the student's and the teacher's to match per component. The student"
" could reach an internal representation that is a permutation of the "
"teacher's and it would be just as efficient. Nonetheless, we can still run a"
" quick experiment to figure out the impact of this method. We will be using "
"the ``CosineEmbeddingLoss`` which is given by the following formula:"
msgstr ""
"可以自由调整控制 softmax "
"函数柔和程度的温度参数和损失系数。在神经网络中，通过在主要目标中加入额外的损失函数，可以实现更好的泛化效果。让我们尝试为学生包含一个目标，但这次聚焦于隐藏层状态而不是输出层。我们的目标是通过包括一个简单的损失函数，使得被分类器随后传递的展平向量随着损失的减少变得更加"
" "
"*相似*。当然，教师不会更新其权重，因此最小化仅取决于学生的权重。此方法的基本原理是我们假设教师模型具有更好的内部表示，学生在没有外部干预的情况下不可能达到，因此我们人为地推动学生模仿教师的内部表示。然而，这是否有助于学生并不直观，因为推动轻量级网络到达此状态可能是好事，假设我们确实找到了一个导致更优测试准确率的内部表示，但也可能有害，因为网络具有不同的架构，学生的学习能力不同于教师。换句话说，没有理由要求这两个向量，学生的和教师的，在每个分量上都匹配。学生可以达到一个是教师表示的置换的内部表示，这同样高效。不过，我们仍然可以运行一个快速实验来了解此方法的影响。我们将使用``CosineEmbeddingLoss``，其公式如下："

#: ../../beginner/vt_tutorial.rst:783
msgid "Formula for CosineEmbeddingLoss"
msgstr "CosineEmbeddingLoss公式"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Obviously, there is one thing that we need to resolve first. When we applied"
" distillation to the output layer we mentioned that both networks have the "
"same number of neurons, equal to the number of classes. However, this is not"
" the case for the layer following our convolutional layers. Here, the "
"teacher has more neurons than the student after the flattening of the final "
"convolutional layer. Our loss function accepts two vectors of equal "
"dimensionality as inputs, therefore we need to somehow match them. We will "
"solve this by including an average pooling layer after the teacher's "
"convolutional layer to reduce its dimensionality to match that of the "
"student."
msgstr ""
"显然，我们需要先解决一个问题。当我们在输出层应用蒸馏时提到，两种网络拥有相同数量的神经元，等于类别数。然而，这并不适用于卷积层之后的层次。这里，教师在最终卷积层展平后的神经元数多于学生。我们的损失函数接受两个具有相同维数的向量作为输入，因此我们需要以某种方式匹配它们。我们将通过在教师的卷积层之后包括一个平均池化层来解决这个问题，该层将其维数缩小，以与学生匹配。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To proceed, we will modify our model classes, or create new ones. Now, the "
"forward function returns not only the logits of the network but also the "
"flattened hidden representation after the convolutional layer. We include "
"the aforementioned pooling for the modified teacher."
msgstr ""
"接下来，我们将修改模型类，或者创建新类。现在，前向函数不仅返回网络的 "
"logits，还返回卷积层后的展平隐藏表示。对于修改后的教师网络，我们包括上述池化操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Naturally, we need to change the train loop because now the model returns a "
"tuple ``(logits, hidden_representation)``. Using a sample input tensor we "
"can print their shapes."
msgstr ""
"自然地，我们需要更改训练循环，因为现在模型返回一个元组``(logits, "
"hidden_representation)``。使用一个示例输入张量，我们可以打印其形状。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In our case, ``hidden_representation_size`` is ``1024``. This is the "
"flattened feature map of the final convolutional layer of the student and as"
" you can see, it is the input for its classifier. It is ``1024`` for the "
"teacher too, because we made it so with ``avg_pool1d`` from ``2048``. The "
"loss applied here only affects the weights of the student prior to the loss "
"calculation. In other words, it does not affect the classifier of the "
"student. The modified training loop is the following:"
msgstr ""
"在我们的案例中，``hidden_representation_size`` 是 "
"``1024``。这是学生最终卷积层展平的特征图，如您所见，它是其分类器的输入。对于教师也是``1024``，因为我们通过``avg_pool1d``从``2048``调整而来。此处应用的损失仅影响学生的权重，而不影响分类器。以下是修改后的训练循环："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In Cosine Loss minimization, we want to maximize the cosine similarity of "
"the two representations by returning gradients to the student:"
msgstr "在余弦损失最小化中，我们希望通过将梯度返回给学生来最大化两个表示的余弦相似度："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We need to modify our test function for the same reason. Here we ignore the "
"hidden representation returned by the model."
msgstr "出于相同的原因，我们需要修改测试函数。这里我们忽略模型返回的隐藏表示。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this case, we could easily include both knowledge distillation and cosine"
" loss minimization in the same function. It is common to combine methods to "
"achieve better performance in teacher-student paradigms. For now, we can run"
" a simple train-test session."
msgstr ""
"在这种情况下，我们可以轻松地将知识蒸馏和余弦损失最小化包含在同一个函数中。在教师-"
"学生范式中，组合方法以获得更好的性能是很常见的。目前，我们可以运行一个简单的训练-测试会话。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Intermediate regressor run"
msgstr "中间回归器运行"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our naive minimization does not guarantee better results for several "
"reasons, one being the dimensionality of the vectors. Cosine similarity "
"generally works better than Euclidean distance for vectors of higher "
"dimensionality, but we were dealing with vectors with 1024 components each, "
"so it is much harder to extract meaningful similarities. Furthermore, as we "
"mentioned, pushing towards a match of the hidden representation of the "
"teacher and the student is not supported by theory. There are no good "
"reasons why we should be aiming for a 1:1 match of these vectors. We will "
"provide a final example of training intervention by including an extra "
"network called regressor. The objective is to first extract the feature map "
"of the teacher after a convolutional layer, then extract a feature map of "
"the student after a convolutional layer, and finally try to match these "
"maps. However, this time, we will introduce a regressor between the networks"
" to facilitate the matching process. The regressor will be trainable and "
"ideally will do a better job than our naive cosine loss minimization scheme."
" Its main job is to match the dimensionality of these feature maps so that "
"we can properly define a loss function between the teacher and the student. "
"Defining such a loss function provides a teaching \"path,\" which is "
"basically a flow to back-propagate gradients that will change the student's "
"weights. Focusing on the output of the convolutional layers right before "
"each classifier for our original networks, we have the following shapes:"
msgstr ""
"我们的简单最小化不能保证更好的结果，其中一个原因是向量的维数。余弦相似性通常比欧几里得距离在高维向量上更有效，但我们处理的是每个拥有 1024 "
"个组成部分的向量，因此难以提取有意义的相似性。此外，如我们提到的，推动教师和学生的隐藏表示匹配在理论上并不支持。没有充分的理由认为我们应该追求这些向量的"
" 1:1 "
"匹配。我们将通过包含一个称为回归器的额外网络来提供一个最终的训练干预示例。目标是首先提取教师在卷积层后的特征图，然后提取学生在卷积层后的特征图，最后尝试匹配这些特征图。然而，这一次，我们将在网络之间引入一个回归器来促进匹配过程。回归器是可训练的，理想情况下会比我们简单的余弦损失最小化方案表现更好。其主要任务是匹配这些特征图的维数，以便我们可以在教师和学生之间正确定义损失函数。定义这样的损失函数提供了一条“教学路径”，即反向传播梯度的流，以更改学生的权重。聚焦于分类器之前每个网络的卷积层输出，我们有以下形状："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have 32 filters for the teacher and 16 filters for the student. We will "
"include a trainable layer that converts the feature map of the student to "
"the shape of the feature map of the teacher. In practice, we modify the "
"lightweight class to return the hidden state after an intermediate regressor"
" that matches the sizes of the convolutional feature maps and the teacher "
"class to return the output of the final convolutional layer without pooling "
"or flattening."
msgstr ""
"教师网络有 32 个滤波器，学生网络有 16 "
"个滤波器。我们将在学生特征图和教师特征图之间加入一个可训练的层，转换学生的特征图形状以匹配教师的特征图形状。在实践中，我们修改轻量级类别，使其返回经过中间回归器后的隐藏状态，这些回归器匹配卷积特征图的尺寸，并修改教师类别以返回最终卷积层的输出（不进行池化或展平）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The trainable layer matches the shapes of the intermediate tensors and Mean "
"Squared Error (MSE) is properly defined:"
msgstr "该可训练层匹配中间张量的形状，均方误差 (MSE) 可正确定义："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"After that, we have to update our train loop again. This time, we extract "
"the regressor output of the student, the feature map of the teacher, we "
"calculate the ``MSE`` on these tensors (they have the exact same shape so "
"it's properly defined) and we back propagate gradients based on that loss, "
"in addition to the regular cross entropy loss of the classification task."
msgstr ""
"之后，我们需要再次更新训练循环。这次，我们提取学生的回归器输出和教师的特征图，在这些具有相同形状的张量上计算 "
"``MSE``，然后基于该损失以及分类任务的常规交叉熵损失进行反向传播梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It is expected that the final method will work better than ``CosineLoss`` "
"because now we have allowed a trainable layer between the teacher and the "
"student, which gives the student some wiggle room when it comes to learning,"
" rather than pushing the student to copy the teacher's representation. "
"Including the extra network is the idea behind hint-based distillation."
msgstr ""
"我们预计最终的方法会比 ``CosineLoss`` "
"更好，因为现在我们允许在教师和学生之间有一个可训练层，这为学生提供了在学习时的灵活性，而不是单纯地让学生复制教师的表示。引入额外的网络层是基于提示蒸馏的理念。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"None of the methods above increases the number of parameters for the network"
" or inference time, so the performance increase comes at the little cost of "
"calculating gradients during training. In ML applications, we mostly care "
"about inference time because training happens before the model deployment. "
"If our lightweight model is still too heavy for deployment, we can apply "
"different ideas, such as post-training quantization. Additional losses can "
"be applied in many tasks, not just classification, and you can experiment "
"with quantities like coefficients, temperature, or number of neurons. Feel "
"free to tune any numbers in the tutorial above, but keep in mind, if you "
"change the number of neurons / filters chances are a shape mismatch might "
"occur."
msgstr ""
"以上方法均没有增加网络参数数量或推理时间，因此性能的提升只需要在训练期间计算梯度的少量开销。在机器学习应用中，我们主要关注的是推理时间，因为训练是在模型部署之前完成的。如果我们的轻量化模型在部署时仍然太重，我们可以采用诸如训练后量化的其他方案。额外的损失函数可以应用于许多任务，而不仅限于分类问题，您可以尝试不同的值，比如系数、温度或神经元数量。随意调整上述教程中的任何数字，但请记住，如果更改神经元/过滤器的数量，很可能会出现形状不匹配的问题。"

#: ../../beginner/vt_tutorial.rst:783
msgid "For more information, see:"
msgstr "有关更多信息，请参见："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural "
"network. In: Neural Information Processing System Deep Learning Workshop "
"(2015) <https://arxiv.org/abs/1503.02531>`_"
msgstr ""
"`Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural "
"network. In: Neural Information Processing System Deep Learning Workshop "
"(2015) <https://arxiv.org/abs/1503.02531>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: "
"Fitnets: Hints for thin deep nets. In: Proceedings of the International "
"Conference on Learning Representations (2015) "
"<https://arxiv.org/abs/1412.6550>`_"
msgstr ""
"`Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: "
"Fitnets: Hints for thin deep nets. In: Proceedings of the International "
"Conference on Learning Representations (2015) "
"<https://arxiv.org/abs/1412.6550>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 3 minutes  0.153 seconds)"
msgstr "**脚本总运行时间:** ( 3 分钟 0.153 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: knowledge_distillation_tutorial.py "
"<knowledge_distillation_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: knowledge_distillation_tutorial.py "
"<knowledge_distillation_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: knowledge_distillation_tutorial.ipynb "
"<knowledge_distillation_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: knowledge_distillation_tutorial.ipynb "
"<knowledge_distillation_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_nlp_advanced_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_nlp_advanced_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Advanced: Making Dynamic Decisions and the Bi-LSTM CRF"
msgstr "高级: 动态决策和双向 LSTM-CRF"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dynamic versus Static Deep Learning Toolkits"
msgstr "动态与静态深度学习工具包"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Pytorch is a *dynamic* neural network kit. Another example of a dynamic kit "
"is `Dynet <https://github.com/clab/dynet>`__ (I mention this because working"
" with Pytorch and Dynet is similar. If you see an example in Dynet, it will "
"probably help you implement it in Pytorch). The opposite is the *static* "
"tool kit, which includes Theano, Keras, TensorFlow, etc. The core difference"
" is the following:"
msgstr ""
"Pytorch 是一个 *动态* 神经网络工具包。另一个动态工具包的例子是 `Dynet "
"<https://github.com/clab/dynet>`__（我提到这个是因为使用 Pytorch 和 Dynet 的方式类似。如果您在 "
"Dynet 中看到一个示例，它可能有助于您在 Pytorch 中实现）。相反的是 *静态* 工具包，包括 Theano、Keras、TensorFlow"
" 等。主要差异如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In a static toolkit, you define a computation graph once, compile it, and "
"then stream instances to it."
msgstr "在静态工具包中，您定义一次计算图，编译它，然后向它传输实例。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In a dynamic toolkit, you define a computation graph *for each instance*. It"
" is never compiled and is executed on-the-fly"
msgstr "在动态工具包中，您为 *每个实例* 定义计算图。它永远不会被编译，而是即时执行。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Without a lot of experience, it is difficult to appreciate the difference. "
"One example is to suppose we want to build a deep constituent parser. "
"Suppose our model involves roughly the following steps:"
msgstr "没有丰富的经验，很难理解这种差异。一种例子是假设我们想构建一个深层组件解析器。假设我们的模型大致包括以下步骤："

#: ../../beginner/vt_tutorial.rst:783
msgid "We build the tree bottom up"
msgstr "我们自下而上构建树"

#: ../../beginner/vt_tutorial.rst:783
msgid "Tag the root nodes (the words of the sentence)"
msgstr "标记根节点（句子的单词）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"From there, use a neural network and the embeddings of the words to find "
"combinations that form constituents. Whenever you form a new constituent, "
"use some sort of technique to get an embedding of the constituent. In this "
"case, our network architecture will depend completely on the input sentence."
" In the sentence \"The green cat scratched the wall\", at some point in the "
"model, we will want to combine the span :math:`(i,j,r) = (1, 3, \\text{NP})`"
" (that is, an NP constituent spans word 1 to word 3, in this case \"The "
"green cat\")."
msgstr ""
"然后，使用神经网络和单词嵌入找到构成成分的组合。每当您形成一个新的成分时，使用某种技术获取成分的嵌入。在这种情况下，网络架构将完全依赖于输入句子。在句子"
" \"The green cat scratched the wall\" 中，模型的某些点，我们将需要组合范围 :math:`(i,j,r) = "
"(1, 3, \\text{NP})`（即，一个 NP 成分覆盖单词1到单词3，这里是 \"The green cat\"）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"However, another sentence might be \"Somewhere, the big fat cat scratched "
"the wall\". In this sentence, we will want to form the constituent "
":math:`(2, 4, NP)` at some point. The constituents we will want to form will"
" depend on the instance. If we just compile the computation graph once, as "
"in a static toolkit, it will be exceptionally difficult or impossible to "
"program this logic. In a dynamic toolkit though, there isn't just 1 pre-"
"defined computation graph. There can be a new computation graph for each "
"instance, so this problem goes away."
msgstr ""
"然而，另一个句子可能是 \"Somewhere, the big fat cat scratched the "
"wall\"。在这个句子中，我们将需要在某些点形成成分 :math:`(2, 4, "
"NP)`。您需要形成的成分将取决于实例。如果我们仅编译一次计算图，就像在静态工具包中，会非常困难甚至不可能编程实现这种逻辑。然而，在动态工具包中，不仅只有一个预定义的计算图。每个实例都可以有一个新的计算图，所以这个问题就迎刃而解了。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Dynamic toolkits also have the advantage of being easier to debug and the "
"code more closely resembling the host language (by that I mean that Pytorch "
"and Dynet look more like actual Python code than Keras or Theano)."
msgstr ""
"动态工具包还具有调试更轻松和代码更类似于主语言的优势（我指的是 Pytorch 和 Dynet 看起来更像实际的 Python 代码，而 Keras 或"
" Theano 则不是）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Bi-LSTM Conditional Random Field Discussion"
msgstr "双向 LSTM 条件随机场讨论"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this section, we will see a full, complicated example of a Bi-LSTM "
"Conditional Random Field for named-entity recognition. The LSTM tagger above"
" is typically sufficient for part-of-speech tagging, but a sequence model "
"like the CRF is really essential for strong performance on NER. Familiarity "
"with CRF's is assumed. Although this name sounds scary, all the model is a "
"CRF but where an LSTM provides the features. This is an advanced model "
"though, far more complicated than any earlier model in this tutorial. If you"
" want to skip it, that is fine. To see if you're ready, see if you can:"
msgstr ""
"在这一节中，我们将看到一个用于命名实体识别的双向 LSTM 条件随机场的完整且复杂的示例。上述的 LSTM 标记器通常足以完成词性标注任务，但像 CRF"
" 这样的序列模型对命名实体识别的强性能至关重要。假设您熟悉 CRF 的原理。虽然这个名字听起来有些吓人，但实际上这个模型只是一个 CRF，其中使用了 "
"LSTM 提供特征。这是一个高级模型，比本教程中的任何早期模型都更复杂。如果您想跳过，也可以。要查看您是否准备好，可以检查是否能够："

#: ../../beginner/vt_tutorial.rst:783
msgid "Write the recurrence for the viterbi variable at step i for tag k."
msgstr "写出第 i 步的 viterbi 变量递归对于标签 k。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Modify the above recurrence to compute the forward variables instead."
msgstr "修改上述递归以计算前向变量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Modify again the above recurrence to compute the forward variables in log-"
"space (hint: log-sum-exp)"
msgstr "再次修改上述递归以在对数空间中计算前向变量（提示: log-sum-exp）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you can do those three things, you should be able to understand the code "
"below. Recall that the CRF computes a conditional probability. Let :math:`y`"
" be a tag sequence and :math:`x` an input sequence of words. Then we compute"
msgstr ""
"如果您能完成上述三件事，您应该能够理解以下代码。回忆一下，CRF 计算条件概率。设 :math:`y` 为标签序列，:math:`x` "
"为单词输入序列。那么我们计算"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} "
"\\exp{(\\text{Score}(x, y')})}"
msgstr ""
"P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y&apos;} \\exp{(\\text{Score}(x, y&apos;)})}\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Where the score is determined by defining some log potentials :math:`\\log "
"\\psi_i(x,y)` such that"
msgstr "分数是通过定义一些对数势 :math:`\\log \\psi_i(x,y)` 来确定的，如下："

#: ../../beginner/vt_tutorial.rst:783
msgid "\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)"
msgstr ""
"\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To make the partition function tractable, the potentials must look only at "
"local features."
msgstr "为了使分区函数可处理，势函数必须仅关注局部特征。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the Bi-LSTM CRF, we define two kinds of potentials: emission and "
"transition. The emission potential for the word at index :math:`i` comes "
"from the hidden state of the Bi-LSTM at timestep :math:`i`. The transition "
"scores are stored in a :math:`|T|x|T|` matrix :math:`\\textbf{P}`, where "
":math:`T` is the tag set. In my implementation, :math:`\\textbf{P}_{j,k}` is"
" the score of transitioning to tag :math:`j` from tag :math:`k`. So:"
msgstr ""
"在双向 LSTM-CRF 中，我们定义了两种势函数：发射势和转移势。索引为 :math:`i` 的单词的发射势来自双向 LSTM 在时间步 "
":math:`i` 的隐藏状态。转移分数存储在一个 :math:`|T|x|T|` 矩阵 :math:`\\textbf{P}` 中，其中 "
":math:`T` 是标签集。在我的实现中，:math:`\\textbf{P}_{j,k}` 是从标签 :math:`k` 转移到标签 "
":math:`j` 的分数。因此："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) "
"+ \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)"
msgstr ""
"\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}"
msgstr ""
"= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"where in this second expression, we think of the tags as being assigned "
"unique non-negative indices."
msgstr "在这个第二表达式中，我们认为标签被分配了唯一的非负索引。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If the above discussion was too brief, you can check out `this "
"<http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__ write up from Michael "
"Collins on CRFs."
msgstr ""
"如果上述讨论过于简略，可以查看 Michael Collins 关于 CRF 的 `这篇文章 "
"<http://www.cs.columbia.edu/%7Emcollins/crf.pdf>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Implementation Notes"
msgstr "实现笔记"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The example below implements the forward algorithm in log space to compute "
"the partition function, and the viterbi algorithm to decode. Backpropagation"
" will compute the gradients automatically for us. We don't have to do "
"anything by hand."
msgstr "以下示例在对数空间中实现了正向算法以计算分区函数，以及 viterbi 算法以解码。反向传播将自动为我们计算梯度。我们不需要手动完成。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The implementation is not optimized. If you understand what is going on, "
"you'll probably quickly see that iterating over the next tag in the forward "
"algorithm could probably be done in one big operation. I wanted to code to "
"be more readable. If you want to make the relevant change, you could "
"probably use this tagger for real tasks."
msgstr ""
"该实现并未优化。如果您理解发生了什么，您可能很快会发现，在正向算法中迭代下一个标签可能可以通过一个大的操作完成。我希望代码更加易读。如果您想进行相关更改，您可能可以将此标记器用于实际任务。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Helper functions to make the code more readable."
msgstr "辅助函数使代码更易读。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Create model"
msgstr "创建模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Run training"
msgstr "运行训练"

#: ../../beginner/vt_tutorial.rst:783
msgid "Exercise: A new loss function for discriminative tagging"
msgstr "练习：一种新的用于区分标记的损失函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It wasn't really necessary for us to create a computation graph when doing "
"decoding, since we do not backpropagate from the viterbi path score. Since "
"we have it anyway, try training the tagger where the loss function is the "
"difference between the Viterbi path score and the score of the gold-standard"
" path. It should be clear that this function is non-negative and 0 when the "
"predicted tag sequence is the correct tag sequence. This is essentially "
"*structured perceptron*."
msgstr ""
"在解码时实际上不需要创建计算图，因为我们不会从 viterbi 路径结果反向传播。既然我们已经创建了它，尝试一种训练标记器的新方法，其中损失函数是 "
"viterbi 路径分数与黄金标准路径分数之间的差值。显然该函数是非负的，当预测标签序列是正确标签序列时，该值为0。这本质上是 *结构化感知机*。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This modification should be short, since Viterbi and score\\_sentence are "
"already implemented. This is an example of the shape of the computation "
"graph *depending on the training instance*. Although I haven't tried "
"implementing this in a static toolkit, I imagine that it is possible but "
"much less straightforward."
msgstr ""
"该修改应很短，因为 viterbi 和 score\\_sentence "
"已实现。这是计算图形状*取决于训练实例*的一个例子。虽然我尚未尝试在静态工具包中实现它，但我想它可能是可行的，但很不直观。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Pick up some real data and do a comparison!"
msgstr "选择一些真实数据并进行比较！"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  5.567 seconds)"
msgstr "**脚本总运行时间:** ( 0 分钟 5.567 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: advanced_tutorial.py "
"<advanced_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: advanced_tutorial.py <advanced_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: advanced_tutorial.ipynb "
"<advanced_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: advanced_tutorial.ipynb "
"<advanced_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_nlp_deep_learning_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_nlp_deep_learning_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Deep Learning with PyTorch"
msgstr "使用 PyTorch 进行深度学习"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Deep Learning Building Blocks: Affine maps, non-linearities and objectives"
msgstr "深度学习构建块：仿射映射、非线性和目标"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Deep learning consists of composing linearities with non-linearities in "
"clever ways. The introduction of non-linearities allows for powerful models."
" In this section, we will play with these core components, make up an "
"objective function, and see how the model is trained."
msgstr ""
"深度学习通过巧妙地组合线性变换和非线性变换组成。引入非线性变换使得模型更加强大。在这一节中，我们将运用这些核心组件，构造一个目标函数，并了解模型的训练过程。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Affine Maps"
msgstr "仿射映射"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One of the core workhorses of deep learning is the affine map, which is a "
"function :math:`f(x)` where"
msgstr "深度学习的核心之一是仿射映射，它是一个函数 :math:`f(x)` 其中"

#: ../../beginner/vt_tutorial.rst:783
msgid "f(x) = Ax + b"
msgstr ""
"f(x) = Ax + b\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"for a matrix :math:`A` and vectors :math:`x, b`. The parameters to be "
"learned here are :math:`A` and :math:`b`. Often, :math:`b` is refered to as "
"the *bias* term."
msgstr "其中矩阵 :math:`A` 和向量 :math:`x, b` 为需要学习的参数。通常，:math:`b` 被称为 *偏置* 项。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch and most other deep learning frameworks do things a little "
"differently than traditional linear algebra. It maps the rows of the input "
"instead of the columns. That is, the :math:`i`'th row of the output below is"
" the mapping of the :math:`i`'th row of the input under :math:`A`, plus the "
"bias term. Look at the example below."
msgstr ""
"PyTorch 和大多数其他深度学习框架的运作方式与传统线性代数稍有不同。它映射输入的行而不是列。也就是说，下面输出的第 :math:`i` "
"行为输入的第 :math:`i` 行经过 :math:`A` 的映射，加上偏置项。请看下面的示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Non-Linearities"
msgstr "非线性变换"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, note the following fact, which will explain why we need non-"
"linearities in the first place. Suppose we have two affine maps :math:`f(x) "
"= Ax + b` and :math:`g(x) = Cx + d`. What is :math:`f(g(x))`?"
msgstr ""
"首先，注意以下事实，它将解释为什么我们首先需要非线性变换。假设我们有两个仿射映射 :math:`f(x) = Ax + b` 和 :math:`g(x)"
" = Cx + d`。那么 :math:`f(g(x))` 是什么？"

#: ../../beginner/vt_tutorial.rst:783
msgid "f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)"
msgstr ""
"f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":math:`AC` is a matrix and :math:`Ad + b` is a vector, so we see that "
"composing affine maps gives you an affine map."
msgstr ":math:`AC` 是一个矩阵，而 :math:`Ad + b` 是一个向量，所以我们看到仿射映射的组合仍然是一个仿射映射。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"From this, you can see that if you wanted your neural network to be long "
"chains of affine compositions, that this adds no new power to your model "
"than just doing a single affine map."
msgstr "从中可以看出，如果您希望神经网络由多个仿射组合构成长链，这不会为模型带来任何新功能，相比只进行一次仿射映射。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If we introduce non-linearities in between the affine layers, this is no "
"longer the case, and we can build much more powerful models."
msgstr "如果我们在仿射层之间引入非线性变换，情况就不再如此，我们可以构建更强大的模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are a few core non-linearities. :math:`\\tanh(x), \\sigma(x), "
"\\text{ReLU}(x)` are the most common. You are probably wondering: \"why "
"these functions? I can think of plenty of other non-linearities.\" The "
"reason for this is that they have gradients that are easy to compute, and "
"computing gradients is essential for learning. For example"
msgstr ""
"有几个核心非线性函数：:math:`\\tanh(x), \\sigma(x), "
"\\text{ReLU}(x)`是最常见的。你可能会好奇：“为什么是这些函数？我还能想到其他许多非线性函数呢。”原因在于它们的梯度易于计算，而计算梯度是学习的关键。例如"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))"
msgstr ""
"\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A quick note: although you may have learned some neural networks in your "
"intro to AI class where :math:`\\sigma(x)` was the default non-linearity, "
"typically people shy away from it in practice. This is because the gradient "
"*vanishes* very quickly as the absolute value of the argument grows. Small "
"gradients means it is hard to learn. Most people default to tanh or ReLU."
msgstr ""
"一个快速提示：尽管你可能在人工智能入门课程中学习了一些使用:math:`\\sigma(x)`作为默认非线性的神经网络，但通常人们在实际应用中会避开它。这是因为当参数的绝对值增大时，梯度会迅速“消失”。梯度变小意味着学习变得困难。大多数人倾向于使用tanh或ReLU。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Softmax and Probabilities"
msgstr "Softmax和概率"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The function :math:`\\text{Softmax}(x)` is also just a non-linearity, but it"
" is special in that it usually is the last operation done in a network. This"
" is because it takes in a vector of real numbers and returns a probability "
"distribution. Its definition is as follows. Let :math:`x` be a vector of "
"real numbers (positive, negative, whatever, there are no constraints). Then "
"the i'th component of :math:`\\text{Softmax}(x)` is"
msgstr ""
"函数:math:`\\text{Softmax}(x)`也是一个非线性，但它通常是网络中最后一个操作。这是因为它接受一个实数向量并返回一个概率分布。定义如下。设:math:`x`是一个实数向量（正数、负数都可以，没有任何限制）。那么:math:`\\text{Softmax}(x)`的第i个分量是"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
msgstr ""
"\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It should be clear that the output is a probability distribution: each "
"element is non-negative and the sum over all components is 1."
msgstr "显然其输出是一个概率分布：每个元素都是非负的，并且所有分量的总和为1。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You could also think of it as just applying an element-wise exponentiation "
"operator to the input to make everything non-negative and then dividing by "
"the normalization constant."
msgstr "你也可以将其理解为对输入逐元素执行指数操作以使所有值非负，然后除以归一化常数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Objective Functions"
msgstr "目标函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The objective function is the function that your network is being trained to"
" minimize (in which case it is often called a *loss function* or *cost "
"function*). This proceeds by first choosing a training instance, running it "
"through your neural network, and then computing the loss of the output. The "
"parameters of the model are then updated by taking the derivative of the "
"loss function. Intuitively, if your model is completely confident in its "
"answer, and its answer is wrong, your loss will be high. If it is very "
"confident in its answer, and its answer is correct, the loss will be low."
msgstr ""
"目标函数是你的网络被训练来最小化的函数（在这种情况下通常称为*损失函数*或*成本函数*）。其过程如下：首先选择一个训练实例，将其通过你的神经网络运行，然后计算输出的损失。然后通过对损失函数求导对模型的参数进行更新。直观来说，如果你的模型对答案完全自信，但答案是错误的，则损失会很高。如果它对答案非常自信且答案是正确的，则损失会很低。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The idea behind minimizing the loss function on your training examples is "
"that your network will hopefully generalize well and have small loss on "
"unseen examples in your dev set, test set, or in production. An example loss"
" function is the *negative log likelihood loss*, which is a very common "
"objective for multi-class classification. For supervised multi-class "
"classification, this means training the network to minimize the negative log"
" probability of the correct output (or equivalently, maximize the log "
"probability of the correct output)."
msgstr ""
"在训练样本上最小化损失函数的目标是希望你的网络能够很好地推广，并对开发集、测试集或生产中的未见样本产生较小的损失。一个例子是*负对数似然损失*，这是一个非常常见的多类分类目标。在监督的多类分类中，这意味着将网络训练为最小化正确输出的负对数概率（或者等价地，最大化正确输出的对数概率）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimization and Training"
msgstr "优化和训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So what we can compute a loss function for an instance? What do we do with "
"that? We saw earlier that Tensors know how to compute gradients with respect"
" to the things that were used to compute it. Well, since our loss is an "
"Tensor, we can compute gradients with respect to all of the parameters used "
"to compute it! Then we can perform standard gradient updates. Let "
":math:`\\theta` be our parameters, :math:`L(\\theta)` the loss function, and"
" :math:`\\eta` a positive learning rate. Then:"
msgstr ""
"所以我们可以计算一个样本的损失函数？那接下来该怎么办？我们之前看到，张量知道如何计算其涉及的参数的梯度。由于我们的损失是一个张量，我们可以计算出所有计算损失所使用参数的梯度！然后我们可以执行标准梯度更新。设:math:`\\theta`为我们的参数，:math:`L(\\theta)`为损失函数，:math:`\\eta`为正的学习率。那么："

#: ../../beginner/vt_tutorial.rst:783
msgid "\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)"
msgstr ""
"\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are a huge collection of algorithms and active research in attempting "
"to do something more than just this vanilla gradient update. Many attempt to"
" vary the learning rate based on what is happening at train time. You don't "
"need to worry about what specifically these algorithms are doing unless you "
"are really interested. Torch provides many in the torch.optim package, and "
"they are all completely transparent. Using the simplest gradient update is "
"the same as the more complicated algorithms. Trying different update "
"algorithms and different parameters for the update algorithms (like "
"different initial learning rates) is important in optimizing your network's "
"performance. Often, just replacing vanilla SGD with an optimizer like Adam "
"or RMSProp will boost performance noticably."
msgstr ""
"存在大量的算法和活跃研究试图在基本梯度更新之上做更多的事情。许多算法试图根据训练时间发生的情况改变学习率。除非你确实对这些算法感兴趣，否则不必担心它们具体在做什么。Torch在torch.optim包中提供了许多算法，它们都完全透明。使用最简单的梯度更新与使用更复杂的算法是一样的。尝试不同的更新算法以及这些算法的不同参数（比如不同的初始学习率）对优化网络性能很重要。通常，仅仅将标准SGD替换为诸如Adam或RMSProp等优化器就能显著提升性能。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Creating Network Components in PyTorch"
msgstr "在PyTorch中创建网络组件"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before we move on to our focus on NLP, lets do an annotated example of "
"building a network in PyTorch using only affine maps and non-linearities. We"
" will also see how to compute a loss function, using PyTorch's built in "
"negative log likelihood, and update parameters by backpropagation."
msgstr ""
"在深入探讨NLP之前，让我们通过一个实例展示如何仅使用仿射映射和非线性构建PyTorch网络的注释示例。我们还将看到如何使用PyTorch内置的负对数似然来计算损失函数，并通过反向传播更新参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"All network components should inherit from nn.Module and override the "
"forward() method. That is about it, as far as the boilerplate is concerned. "
"Inheriting from nn.Module provides functionality to your component. For "
"example, it makes it keep track of its trainable parameters, you can swap it"
" between CPU and GPU with the ``.to(device)`` method, where device can be a "
"CPU device ``torch.device(\"cpu\")`` or CUDA device "
"``torch.device(\"cuda:0\")``."
msgstr ""
"所有网络组件都应该从nn.Module继承并重写forward()方法。这就是样板代码的主要内容。从nn.Module继承会为你的组件提供功能。例如，它使组件能够跟踪其可训练参数，你可以通过``.to(device)``方法在CPU和GPU之间切换，device可以是CPU设备``torch.device(\"cpu\")``或CUDA设备``torch.device(\"cuda:0\")``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's write an annotated example of a network that takes in a sparse bag-of-"
"words representation and outputs a probability distribution over two labels:"
" \"English\" and \"Spanish\". This model is just logistic regression."
msgstr "让我们编写一个注释示例，展示一个网络如何将稀疏的词袋表示输入并输出两个标签：“英语”和“西班牙语”的概率分布。这个模型只是逻辑回归。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Example: Logistic Regression Bag-of-Words classifier"
msgstr "示例：逻辑回归词袋分类器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our model will map a sparse BoW representation to log probabilities over "
"labels. We assign each word in the vocab an index. For example, say our "
"entire vocab is two words \"hello\" and \"world\", with indices 0 and 1 "
"respectively. The BoW vector for the sentence \"hello hello hello hello\" is"
msgstr ""
"我们的模型将稀疏词袋表示映射到标签上的对数概率。我们为词汇中的每个单词分配一个索引。例如，假设我们的整个词汇表只有两个单词“hello”和“world”，其索引分别是0和1。那么句子“hello"
" hello hello hello”的词袋向量是"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\left[ 4, 0 \\right]"
msgstr ""
"\\left[ 4, 0 \\right]\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "For \"hello world world hello\", it is"
msgstr "对于“hello world world hello”，对应的是"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\left[ 2, 2 \\right]"
msgstr ""
"\\left[ 2, 2 \\right]\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "etc. In general, it is"
msgstr "以此类推。一般来说，它是"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]"
msgstr ""
"\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "Denote this BOW vector as :math:`x`. The output of our network is:"
msgstr "我们将这个词袋向量表示为:math:`x`。网络的输出是："

#: ../../beginner/vt_tutorial.rst:783
msgid "\\log \\text{Softmax}(Ax + b)"
msgstr ""
"\\log \\text{Softmax}(Ax + b)\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That is, we pass the input through an affine map and then do log softmax."
msgstr "也就是说，我们将输入通过一个仿射映射，然后进行对数softmax操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Which of the above values corresponds to the log probability of ENGLISH, and"
" which to SPANISH? We never defined it, but we need to if we want to train "
"the thing."
msgstr "以上哪个值对应英语的对数概率，哪个对应西班牙语的对数概率？我们从未定义它，但如果我们想要训练这个模型，就需要定义。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So lets train! To do this, we pass instances through to get log "
"probabilities, compute a loss function, compute the gradient of the loss "
"function, and then update the parameters with a gradient step. Loss "
"functions are provided by Torch in the nn package. nn.NLLLoss() is the "
"negative log likelihood loss we want. It also defines optimization functions"
" in torch.optim. Here, we will just use SGD."
msgstr ""
"好了，现在开始训练吧！为此，我们将实例输入网络以获得对数概率，计算损失函数，对损失函数求梯度，然后使用梯度步骤更新参数。Torch在nn包中提供了损失函数。我们需要的负对数似然损失是nn.NLLLoss()。它还在torch.optim中定义了优化函数。这里，我们将使用SGD。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that the *input* to NLLLoss is a vector of log probabilities, and a "
"target label. It doesn't compute the log probabilities for us. This is why "
"the last layer of our network is log softmax. The loss function "
"nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log "
"softmax for you."
msgstr ""
"请注意，NLLLoss的*输入*是一个对数概率向量和一个目标标签。它不会为我们计算对数概率。这就是为什么我们网络的最后一层是对数softmax。损失函数nn.CrossEntropyLoss()与NLLLoss()相同，只不过它会为你计算对数softmax。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We got the right answer! You can see that the log probability for Spanish is"
" much higher in the first example, and the log probability for English is "
"much higher in the second for the test data, as it should be."
msgstr "我们得到了正确答案！你可以看到在第一个示例中，西班牙语的对数概率比英语高得多；而在第二个测试数据中，英语的对数概率更高，这正是预期的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now you see how to make a PyTorch component, pass some data through it and "
"do gradient updates. We are ready to dig deeper into what deep NLP has to "
"offer."
msgstr "现在你已经了解如何创建一个PyTorch组件，将一些数据输入其中并进行梯度更新。我们准备深入挖掘深度NLP的潜力。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.439 seconds)"
msgstr "**脚本的总运行时间:** （0分钟 0.439秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: deep_learning_tutorial.py "
"<deep_learning_tutorial.py>`"
msgstr ""
":download:`下载Python源码: deep_learning_tutorial.py "
"<deep_learning_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: deep_learning_tutorial.ipynb "
"<deep_learning_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: deep_learning_tutorial.ipynb "
"<deep_learning_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Deep Learning for NLP with Pytorch"
msgstr "使用Pytorch进行深度学习应用于NLP"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"These tutorials will walk you through the key ideas of deep learning "
"programming using Pytorch. Many of the concepts (such as the computation "
"graph abstraction and autograd) are not unique to Pytorch and are relevant "
"to any deep learning toolkit out there."
msgstr ""
"这些教程将引导你使用Pytorch编程深入学习的关键理念。许多概念（例如计算图抽象和自动梯度计算）不是Pytorch独有的，它们也适用于任何深度学习工具包。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"They are focused specifically on NLP for people who have never written code "
"in any deep learning framework (e.g, TensorFlow,Theano, Keras, DyNet). The "
"tutorials assumes working knowledge of core NLP problems: part-of-speech "
"tagging, language modeling, etc. It also assumes familiarity with neural "
"networks at the level of an intro AI class (such as one from the Russel and "
"Norvig book). Usually, these courses cover the basic backpropagation "
"algorithm on feed-forward neural networks, and make the point that they are "
"chains of compositions of linearities and non-linearities. This tutorial "
"aims to get you started writing deep learning code, given you have this "
"prerequisite knowledge."
msgstr ""
"它们专注于NLP，面向从未在任何深度学习框架中编写代码的人（例如TensorFlow, Theano, Keras, "
"DyNet）。教程假设你对核心NLP问题有一定的工作知识，例如词性标注、语言模型等。同时假设你对神经网络具有入门级AI课程的了解（例如Russel 和 "
"Norvig书中的课程）。通常，这些课程涵盖了基本的反馈神经网络上的反向传播算法，并指出它们是线性和非线性组合的链。这个教程旨在帮助你基于这些预备知识开始编写深度学习代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note these tutorials are about *models*, not data. For all of the models, a "
"few test examples are created with small dimensionality so you can see how "
"the weights change as it trains. If you have some real data you want to try,"
" you should be able to rip out any of the models from this notebook and use "
"them on it."
msgstr ""
"请注意，这些教程涉及的是*模型*，而不是数据。对于所有模型，都创建了一些维度较小的测试实例，你可以看到它们在训练过程中权重如何变化。如果你有一些真实数据想尝试，可以从这个笔记本中摘取任何模型并直接使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "pytorch_tutorial.py"
msgstr "pytorch_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Introduction to PyTorch "
"https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html"
msgstr ""
"PyTorch入门教程 https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "deep_learning_tutorial.py"
msgstr "deep_learning_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Deep Learning with PyTorch "
"https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html"
msgstr ""
"使用PyTorch进行深度学习 "
"https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "word_embeddings_tutorial.py"
msgstr "word_embeddings_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Word Embeddings: Encoding Lexical Semantics "
"https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
msgstr ""
"词嵌入: 编码词汇语义 "
"https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "sequence_models_tutorial.py"
msgstr "sequence_models_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sequence Models and Long Short-Term Memory Networks "
"https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
msgstr ""
"序列模型与长短时记忆网络 "
"https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "advanced_tutorial.py"
msgstr "advanced_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Advanced: Making Dynamic Decisions and the Bi-LSTM CRF "
"https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"
msgstr ""
"高级教程: 动态决策与Bi-LSTM CRF "
"https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_nlp_pytorch_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_nlp_pytorch_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Sequence Models and Long Short-Term Memory Networks"
msgstr "序列模型与长短时记忆网络"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_nlp_sequence_models_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_nlp_sequence_models_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Word Embeddings: Encoding Lexical Semantics"
msgstr "词嵌入: 编码词汇语义"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_nlp_word_embeddings_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_nlp_word_embeddings_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_nlp_deep_learning_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_nlp_deep_learning_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_nlp_advanced_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_nlp_advanced_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_nlp_pytorch_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_nlp_pytorch_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to Torch's tensor library"
msgstr "Torch张量库简介"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"All of deep learning is computations on tensors, which are generalizations "
"of a matrix that can be indexed in more than 2 dimensions. We will see "
"exactly what this means in-depth later. First, let's look what we can do "
"with tensors."
msgstr ""
"所有深度学习都是在张量上的计算。张量是矩阵的广义化，可以在多个维度上进行索引。我们稍后会深入探讨其具体含义。首先，让我们看看张量可以做什么。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Tensors can be created from Python lists with the torch.tensor() function."
msgstr "张量可以使用torch.tensor()函数从Python列表创建。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"What is a 3D tensor anyway? Think about it like this. If you have a vector, "
"indexing into the vector gives you a scalar. If you have a matrix, indexing "
"into the matrix gives you a vector. If you have a 3D tensor, then indexing "
"into the tensor gives you a matrix!"
msgstr ""
"什么是三维张量呢？可以这样想。如果你有一个向量，索引向量会得到一个标量。如果你有一个矩阵，索引矩阵会得到一个向量。如果你有一个三维张量，那么索引张量会得到一个矩阵！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A note on terminology: when I say \"tensor\" in this tutorial, it refers to "
"any torch.Tensor object. Matrices and vectors are special cases of "
"torch.Tensors, where their dimension is 2 and 1 respectively. When I am "
"talking about 3D tensors, I will explicitly use the term \"3D tensor\"."
msgstr ""
"关于术语说明：在本教程中，当我说“张量”时，它指的是任何 torch.Tensor 对象。矩阵和向量是 torch.Tensor 的特殊情况，分别具有 "
"2 和 1 的维度。当我讨论三维张量时，我会明确使用术语“三维张量”。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can also create tensors of other data types. To create a tensor of "
"integer types, try torch.tensor([[1, 2], [3, 4]]) (where all elements in the"
" list are integers). You can also specify a data type by passing in "
"``dtype=torch.data_type``. Check the documentation for more data types, but "
"Float and Long will be the most common."
msgstr ""
"你还可以创建其他数据类型的张量。要创建整数类型的张量，可以尝试 torch.tensor([[1, 2], [3, "
"4]])（列表中的所有元素均为整数）。你也可以通过传递参数 ``dtype=torch.data_type`` "
"指定数据类型。请查看文档了解更多数据类型，但 Float 和 Long 是最常用的数据类型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can create a tensor with random data and the supplied dimensionality "
"with torch.randn()"
msgstr "使用 torch.randn() 方法可以创建具有随机数据和指定维度的张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Operations with Tensors"
msgstr "张量操作"

#: ../../beginner/vt_tutorial.rst:783
msgid "You can operate on tensors in the ways you would expect."
msgstr "你可以按照预期的方式对张量进行操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `the documentation <https://pytorch.org/docs/torch.html>`__ for a "
"complete list of the massive number of operations available to you. They "
"expand beyond just mathematical operations."
msgstr ""
"完整的操作列表可以查看 `文档 <https://pytorch.org/docs/torch.html>`__。这些操作不仅仅涵盖数学运算。"

#: ../../beginner/vt_tutorial.rst:783
msgid "One helpful operation that we will make use of later is concatenation."
msgstr "一个我们将来会经常使用的操作是拼接。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Reshaping Tensors"
msgstr "张量的重塑"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use the .view() method to reshape a tensor. This method receives heavy use, "
"because many neural network components expect their inputs to have a certain"
" shape. Often you will need to reshape before passing your data to the "
"component."
msgstr ""
"使用 .view() 方法可以重塑张量。此方法被频繁使用，因为许多神经网络组件要求输入具有特定的形状。通常在将数据传递给组件之前需要进行重塑。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Computation Graphs and Automatic Differentiation"
msgstr "计算图和自动微分"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The concept of a computation graph is essential to efficient deep learning "
"programming, because it allows you to not have to write the back propagation"
" gradients yourself. A computation graph is simply a specification of how "
"your data is combined to give you the output. Since the graph totally "
"specifies what parameters were involved with which operations, it contains "
"enough information to compute derivatives. This probably sounds vague, so "
"let's see what is going on using the fundamental flag ``requires_grad``."
msgstr ""
"计算图的概念对于高效的深度学习编程至关重要，因为它使你无需自己编写反向传播的梯度。计算图只是一个数据如何组合以产生输出的规范。由于计算图完全指定了哪些参数参与了哪些操作，它包含足够的信息来计算导数。这可能听起来很模糊，因此我们将使用基础标志"
" ``requires_grad`` 来查看具体情况。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"First, think from a programmers perspective. What is stored in the "
"torch.Tensor objects we were creating above? Obviously the data and the "
"shape, and maybe a few other things. But when we added two tensors together,"
" we got an output tensor. All this output tensor knows is its data and "
"shape. It has no idea that it was the sum of two other tensors (it could "
"have been read in from a file, it could be the result of some other "
"operation, etc.)"
msgstr ""
"首先，从程序员的角度思考。我们上面创建的 torch.Tensor "
"对象中存储了什么？显然是数据和形状，以及一些其他信息。但当我们将两个张量相加后，我们得到了一个输出张量。此输出张量只知道其数据和形状。它并不知道自己是两个其他张量的和（它可能是从文件读取的，可能是某些其他操作的结果，等等）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If ``requires_grad=True``, the Tensor object keeps track of how it was "
"created. Let's see it in action."
msgstr "如果 ``requires_grad=True``，该 Tensor 对象会记录它是如何创建的。让我们来看一下实际操作。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So Tensors know what created them. z knows that it wasn't read in from a "
"file, it wasn't the result of a multiplication or exponential or whatever. "
"And if you keep following z.grad_fn, you will find yourself at x and y."
msgstr ""
"因此张量知道是什么创建了它。z 知道它不是从文件读取的，不是乘法或指数等操作的结果。如果继续跟踪 z.grad_fn，你会发现其源头是 x 和 y。"

#: ../../beginner/vt_tutorial.rst:783
msgid "But how does that help us compute a gradient?"
msgstr "但这如何帮助我们计算梯度呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"So now, what is the derivative of this sum with respect to the first "
"component of x? In math, we want"
msgstr "那么，这个和对 x 的第一个分量的导数是什么？在数学上，我们想要"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\frac{\\partial s}{\\partial x_0}"
msgstr "\\frac{\\partial s}{\\partial x_0}"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Well, s knows that it was created as a sum of the tensor z. z knows that it "
"was the sum x + y. So"
msgstr "好吧，s 知道它是通过张量 z 的加和创建的。z 知道它是 x + y 的加和。所以"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + "
"y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}"
msgstr ""
"s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"And so s contains enough information to determine that the derivative we "
"want is 1!"
msgstr "因此 s 包含了足够的信息来确定我们想要的导数是 1！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Of course this glosses over the challenge of how to actually compute that "
"derivative. The point here is that s is carrying along enough information "
"that it is possible to compute it. In reality, the developers of Pytorch "
"program the sum() and + operations to know how to compute their gradients, "
"and run the back propagation algorithm. An in-depth discussion of that "
"algorithm is beyond the scope of this tutorial."
msgstr ""
"当然，这忽略了如何实际计算这个导数的挑战。关键在于 s 携带了足够的信息，能够使计算成为可能。实际上，Pytorch 的开发者对 sum() 和 + "
"操作进行了编程，使其知道如何计算梯度，并运行了反向传播算法。详细讨论该算法超出了本教程的范围。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's have Pytorch compute the gradient, and see that we were right: (note "
"if you run this block multiple times, the gradient will increment. That is "
"because Pytorch *accumulates* the gradient into the .grad property, since "
"for many models this is very convenient.)"
msgstr ""
"让我们让 Pytorch 计算梯度，并验证我们的推测是正确的：（请注意，如果多次运行此代码块，梯度会累加。这是因为 Pytorch *累积* 梯度到 "
".grad 属性中，因为对于许多模型来说，这种方式非常方便。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Understanding what is going on in the block below is crucial for being a "
"successful programmer in deep learning."
msgstr "理解下面区块的内容是成为深度学习成功程序员的关键。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can also stop autograd from tracking history on Tensors with "
"``.requires_grad=True`` by wrapping the code block in ``with "
"torch.no_grad():``"
msgstr ""
"你还可以通过将代码块包装在 ``with torch.no_grad():`` 中，停止 autograd 对 "
"``.requires_grad=True`` 的张量的历史跟踪。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.347 seconds)"
msgstr "**脚本的总运行时间：**（0 分钟 0.347 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: pytorch_tutorial.py "
"<pytorch_tutorial.py>`"
msgstr ":download:`下载 Python 源代码: pytorch_tutorial.py <pytorch_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: pytorch_tutorial.ipynb "
"<pytorch_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: pytorch_tutorial.ipynb "
"<pytorch_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_nlp_sequence_models_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_nlp_sequence_models_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"At this point, we have seen various feed-forward networks. That is, there is"
" no state maintained by the network at all. This might not be the behavior "
"we want. Sequence models are central to NLP: they are models where there is "
"some sort of dependence through time between your inputs. The classical "
"example of a sequence model is the Hidden Markov Model for part-of-speech "
"tagging. Another example is the conditional random field."
msgstr ""
"到目前为止，我们已经看到了各种前馈网络。也就是说，网络根本没有维护任何状态。这可能不是我们想要的行为。序列模型是自然语言处理的核心：它们是一些输入之间通过时间存在某种依赖关系的模型。序列模型的经典例子是用于词性标注的隐马尔可夫模型。另一个例子是条件随机场。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A recurrent neural network is a network that maintains some kind of state. "
"For example, its output could be used as part of the next input, so that "
"information can propagate along as the network passes over the sequence. In "
"the case of an LSTM, for each element in the sequence, there is a "
"corresponding *hidden state* :math:`h_t`, which in principle can contain "
"information from arbitrary points earlier in the sequence. We can use the "
"hidden state to predict words in a language model, part-of-speech tags, and "
"a myriad of other things."
msgstr ""
"循环神经网络是一种维护某种状态的网络。例如，它的输出可以作为下一次输入的一部分，使信息可以在网络遍历序列时传播。在 LSTM "
"的情况下，对于序列中的每个元素，都有一个对应的 *隐藏状态* "
":math:`h_t`，它原则上可以包含序列中任意点早期的信息。我们可以使用隐藏状态预测语言模型中的词、词性标注以及一系列其他内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid "LSTMs in Pytorch"
msgstr "Pytorch 中的 LSTM"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before getting to the example, note a few things. Pytorch's LSTM expects all"
" of its inputs to be 3D tensors. The semantics of the axes of these tensors "
"is important. The first axis is the sequence itself, the second indexes "
"instances in the mini-batch, and the third indexes elements of the input. We"
" haven't discussed mini-batching, so let's just ignore that and assume we "
"will always have just 1 dimension on the second axis. If we want to run the "
"sequence model over the sentence \"The cow jumped\", our input should look "
"like"
msgstr ""
"在开始示例之前，注意几点。Pytorch 的 LSTM "
"要求所有的输入都是三维张量。这些张量的轴含义非常重要。第一个轴是序列本身，第二个轴索引小批量中的实例，第三个轴索引输入的元素。我们尚未讨论小批量处理，所以不妨忽略这些，假设我们在第二个轴总是只有一个维度。如果我们想要在句子"
" \"The cow jumped\" 上运行序列模型，我们的输入应该看起来像"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\begin{bmatrix}\n"
"\\overbrace{q_\\text{The}}^\\text{row vector} \\\\\n"
"q_\\text{cow} \\\\\n"
"q_\\text{jumped}\n"
"\\end{bmatrix}"
msgstr ""
"\\begin{bmatrix}\n"
"\\overbrace{q_\\text{The}}^\\text{行向量} \\\\\n"
"q_\\text{cow} \\\\\n"
"q_\\text{jumped}\n"
"\\end{bmatrix}"

#: ../../beginner/vt_tutorial.rst:783
msgid "Except remember there is an additional 2nd dimension with size 1."
msgstr "但请记住还有一个额外的第二维度，大小为 1。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In addition, you could go through the sequence one at a time, in which case "
"the 1st axis will have size 1 also."
msgstr "此外，你也可以一次通过序列中的一个元素，在这种情况下，第一个轴的大小也为 1。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's see a quick example."
msgstr "让我们看一个快速示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Example: An LSTM for Part-of-Speech Tagging"
msgstr "示例：用于词性标注的 LSTM"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this section, we will use an LSTM to get part of speech tags. We will not"
" use Viterbi or Forward-Backward or anything like that, but as a "
"(challenging) exercise to the reader, think about how Viterbi could be used "
"after you have seen what is going on. In this example, we also refer to "
"embeddings. If you are unfamiliar with embeddings, you can read up about "
"them `here "
"<https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html>`__."
msgstr ""
"在本节中，我们将使用 LSTM 获取词性标注。我们不会使用 Viterbi "
"或前向后向算法等，但作为一个（具有挑战性的）读者练习，请在了解示例内容后思考如何可以结合 Viterbi "
"来使用。在本示例中，我们还提到嵌入。如果你不熟悉嵌入，可以在 `此处 "
"<https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html>`__"
" 查阅相关信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The model is as follows: let our input sentence be :math:`w_1, \\dots, w_M`,"
" where :math:`w_i \\in V`, our vocab. Also, let :math:`T` be our tag set, "
"and :math:`y_i` the tag of word :math:`w_i`. Denote our prediction of the "
"tag of word :math:`w_i` by :math:`\\hat{y}_i`."
msgstr ""
"模型如下：假设我们的输入句子为 :math:`w_1, \\dots, w_M`，其中 :math:`w_i \\in "
"V`，即是我们的词汇表。此外，假设 :math:`T` 是我们的标签集，:math:`y_i` 是单词 :math:`w_i` 的标签。记我们对单词 "
":math:`w_i` 的标签预测为 :math:`\\hat{y}_i`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is a structure prediction, model, where our output is a sequence "
":math:`\\hat{y}_1, \\dots, \\hat{y}_M`, where :math:`\\hat{y}_i \\in T`."
msgstr ""
"这是一种结构预测模型，其输出是一个序列 :math:`\\hat{y}_1, \\dots, \\hat{y}_M`，其中 "
":math:`\\hat{y}_i \\in T`。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To do the prediction, pass an LSTM over the sentence. Denote the hidden "
"state at timestep :math:`i` as :math:`h_i`. Also, assign each tag a unique "
"index (like how we had word\\_to\\_ix in the word embeddings section). Then "
"our prediction rule for :math:`\\hat{y}_i` is"
msgstr ""
"为了进行预测，将 LSTM 应用于句子，记第 i 时间步的隐藏状态为 "
":math:`h_i`。此外，将每个标签赋予一个唯一的索引（类似于我们在单词嵌入部分中处理 word\\_to\\_ix 的方式）。然后我们对 "
":math:`\\hat{y}_i` 的预测规则是"

#: ../../beginner/vt_tutorial.rst:783
msgid "\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j"
msgstr ""
"\\hat{y}_i = \\text{argmax}_j \\  (\\log \\text{Softmax}(Ah_i + b))_j\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That is, take the log softmax of the affine map of the hidden state, and the"
" predicted tag is the tag that has the maximum value in this vector. Note "
"this implies immediately that the dimensionality of the target space of "
":math:`A` is :math:`|T|`."
msgstr ""
"也就是说，取隐藏状态线性变换的 log softmax，然后预测的标签是此向量中最大值对应的标签。注意，这立即表明 :math:`A` "
"的目标空间的维度是 :math:`|T|`。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Prepare data:"
msgstr "准备数据："

#: ../../beginner/vt_tutorial.rst:783
msgid "Create the model:"
msgstr "创建模型："

#: ../../beginner/vt_tutorial.rst:783
msgid "Train the model:"
msgstr "训练模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Exercise: Augmenting the LSTM part-of-speech tagger with character-level "
"features"
msgstr "练习：通过字符级特征扩展 LSTM 词性标注器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the example above, each word had an embedding, which served as the inputs"
" to our sequence model. Let's augment the word embeddings with a "
"representation derived from the characters of the word. We expect that this "
"should help significantly, since character-level information like affixes "
"have a large bearing on part-of-speech. For example, words with the affix "
"*-ly* are almost always tagged as adverbs in English."
msgstr ""
"在上面的示例中，每个单词都有一个嵌入，这作为我们序列模型的输入。我们将通过单词的字符派生表示扩展单词嵌入。我们预计这将显著改善效果，因为字符级信息如词缀对词性具有很大影响。例如，带有词缀"
" *-ly* 的单词几乎总是被标记为英语中的副词。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To do this, let :math:`c_w` be the character-level representation of word "
":math:`w`. Let :math:`x_w` be the word embedding as before. Then the input "
"to our sequence model is the concatenation of :math:`x_w` and :math:`c_w`. "
"So if :math:`x_w` has dimension 5, and :math:`c_w` dimension 3, then our "
"LSTM should accept an input of dimension 8."
msgstr ""
"为此，令 :math:`c_w` 是单词 :math:`w` 的字符级表示。令 :math:`x_w` 是单词嵌入，与之前一致。然后我们序列模型的输入是"
" :math:`x_w` 和 :math:`c_w` 的拼接。因此，如果 :math:`x_w` 的维度是 5，而 :math:`c_w` 的维度是 "
"3，那么我们的 LSTM 应该接受一个维度为 8 的输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To get the character level representation, do an LSTM over the characters of"
" a word, and let :math:`c_w` be the final hidden state of this LSTM. Hints:"
msgstr "为了获得字符级表示，对单词的字符执行一个 LSTM，并令 :math:`c_w` 为此 LSTM 的最后一个隐藏状态。提示："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There are going to be two LSTM's in your new model. The original one that "
"outputs POS tag scores, and the new one that outputs a character-level "
"representation of each word."
msgstr "你的新模型中将有两个 LSTM。一个是输出词性标注分数的原始 LSTM，另一个是输出每个单词字符级表示的新 LSTM。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To do a sequence model over characters, you will have to embed characters. "
"The character embeddings will be the input to the character LSTM."
msgstr "为了设置字符级序列模型，你需要嵌入字符。字符嵌入将是字符 LSTM 的输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  1.014 seconds)"
msgstr "**脚本的总运行时间：**（0 分钟 1.014 秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: sequence_models_tutorial.py "
"<sequence_models_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: sequence_models_tutorial.py "
"<sequence_models_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: sequence_models_tutorial.ipynb "
"<sequence_models_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: sequence_models_tutorial.ipynb "
"<sequence_models_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**00:08.019** total execution time for **beginner_nlp** files:"
msgstr "**00:08.019** 初学者 NLP 文件总执行时间："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_nlp_advanced_tutorial.py` (``advanced_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_nlp_advanced_tutorial.py` (``advanced_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:05.567"
msgstr "00:05.567"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_nlp_sequence_models_tutorial.py` "
"(``sequence_models_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_nlp_sequence_models_tutorial.py` "
"(``sequence_models_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:01.014"
msgstr "00:01.014"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_nlp_word_embeddings_tutorial.py` "
"(``word_embeddings_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_nlp_word_embeddings_tutorial.py` "
"(``word_embeddings_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.652"
msgstr "00:00.652"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_nlp_deep_learning_tutorial.py` "
"(``deep_learning_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_nlp_deep_learning_tutorial.py` "
"(``deep_learning_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.439"
msgstr "00:00.439"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_nlp_pytorch_tutorial.py` (``pytorch_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_nlp_pytorch_tutorial.py` (``pytorch_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.347"
msgstr "00:00.347"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_nlp_word_embeddings_tutorial.py>` to download "
"the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_nlp_word_embeddings_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Word embeddings are dense vectors of real numbers, one per word in your "
"vocabulary. In NLP, it is almost always the case that your features are "
"words! But how should you represent a word in a computer? You could store "
"its ascii character representation, but that only tells you what the word "
"*is*, it doesn't say much about what it *means* (you might be able to derive"
" its part of speech from its affixes, or properties from its capitalization,"
" but not much). Even more, in what sense could you combine these "
"representations? We often want dense outputs from our neural networks, where"
" the inputs are :math:`|V|` dimensional, where :math:`V` is our vocabulary, "
"but often the outputs are only a few dimensional (if we are only predicting "
"a handful of labels, for instance). How do we get from a massive dimensional"
" space to a smaller dimensional space?"
msgstr ""
"词嵌入是实数的密集向量，每个词汇表中的词都有一个对应的向量。在自然语言处理（NLP）中，特征几乎总是词！但如何在计算机中表示一个词呢？你可以存储它的ASCII字符表示，但这仅仅能告诉你这个词的*是什么*，却不能告诉它*意味着什么*（你可能通过词缀推导出它的词性，或通过大小写推导出它的属性，但信息有限）。此外，这些表示如何结合在一起呢？我们通常希望从神经网络得到一些低维度的输出，而输入却是维度为"
" :math:`|V|` 的空间，其中 :math:`V` "
"是词汇表，但输出通常只有几个维度（例如我们只预测一小部分标签时）。如何从如此高维的空间映射到低维空间呢？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How about instead of ascii representations, we use a one-hot encoding? That "
"is, we represent the word :math:`w` by"
msgstr "如果不用ASCII表示，我们可以用独热编码来表示词吗？即我们用以下形式表示词 :math:`w`："

#: ../../beginner/vt_tutorial.rst:783
msgid "\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}"
msgstr ""
"\\overbrace{\\left[ 0, 0, \\dots, 1, \\dots, 0, 0 \\right]}^\\text{|V| elements}\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"where the 1 is in a location unique to :math:`w`. Any other word will have a"
" 1 in some other location, and a 0 everywhere else."
msgstr "其中1的位置对于 :math:`w` 是独一无二的。其他任何词都将在其他位置是1，而其他位置皆为0。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"There is an enormous drawback to this representation, besides just how huge "
"it is. It basically treats all words as independent entities with no "
"relation to each other. What we really want is some notion of *similarity* "
"between words. Why? Let's see an example."
msgstr ""
"这种表示方式有一个巨大缺点，除了它非常大以外，它基本上将所有词视为独立的个体，没有相互关系。我们真正想要的是词之间某种*相似性*的概念。为什么？我们来看一个例子。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Suppose we are building a language model. Suppose we have seen the sentences"
msgstr "假设我们正在构建一个语言模型。假设我们在训练数据中看到如下句子："

#: ../../beginner/vt_tutorial.rst:783
msgid "The mathematician ran to the store."
msgstr "数学家跑向商店。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The physicist ran to the store."
msgstr "物理学家跑向商店。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The mathematician solved the open problem."
msgstr "数学家解决了这个未决问题。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"in our training data. Now suppose we get a new sentence never before seen in"
" our training data:"
msgstr "现在假设我们遇到一个从未在训练数据中出现的新句子："

#: ../../beginner/vt_tutorial.rst:783
msgid "The physicist solved the open problem."
msgstr "物理学家解决了这个未决问题。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Our language model might do OK on this sentence, but wouldn't it be much "
"better if we could use the following two facts:"
msgstr "我们的语言模型可能对这个句子效果还可以，但如果我们能够利用以下两点岂不是更好："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have seen  mathematician and physicist in the same role in a sentence. "
"Somehow they have a semantic relation."
msgstr "我们已经看到数学家和物理学家在句子中的相同角色。某种意义上它们有语义关系。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have seen mathematician in the same role  in this new unseen sentence as "
"we are now seeing physicist."
msgstr "我们已经看到数学家在这个新未见句子中的相同角色，而现在我们正在看到物理学家。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"and then infer that physicist is actually a good fit in the new unseen "
"sentence? This is what we mean by a notion of similarity: we mean *semantic "
"similarity*, not simply having similar orthographic representations. It is a"
" technique to combat the sparsity of linguistic data, by connecting the dots"
" between what we have seen and what we haven't. This example of course "
"relies on a fundamental linguistic assumption: that words appearing in "
"similar contexts are related to each other semantically. This is called the "
"`distributional hypothesis "
"<https://en.wikipedia.org/wiki/Distributional_semantics>`__."
msgstr ""
"然后推断出物理学家实际上是这个新未见句子的一个很好的拟合吗？这就是我们所说的相似性的概念：我们指的是*语义相似性*，而不仅仅是具有类似的拼写表示。这是一种解决语言数据稀疏性的技术，通过连接我们已见的和未见的点。当然这个例子依赖于一个基本的语言学假设：即在相似上下文中出现的词语义上相关。这被称为`分布假设"
" <https://en.wikipedia.org/wiki/Distributional_semantics>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Getting Dense Word Embeddings"
msgstr "获取密集词嵌入"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"How can we solve this problem? That is, how could we actually encode "
"semantic similarity in words? Maybe we think up some semantic attributes. "
"For example, we see that both mathematicians and physicists can run, so "
"maybe we give these words a high score for the \"is able to run\" semantic "
"attribute. Think of some other attributes, and imagine what you might score "
"some common words on those attributes."
msgstr ""
"我们如何解决这个问题？也就是说，我们如何实际在词中编码语义相似性？或许我们可以想到一些语义属性。例如，我们看到数学家和物理学家都可以跑步，所以我们给这些词一个“能够跑步”语义属性的高分。想想其他属性，并想象你可能会给一些常见词在这些属性上的得分。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If each attribute is a dimension, then we might give each word a vector, "
"like this:"
msgstr "如果每个属性是一个维度，那么我们可能会给每个词一个向量，如下所示："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n"
"\\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]"
msgstr ""
" q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n"
"\\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n"
"\\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]"
msgstr ""
" q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n"
"\\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]"

#: ../../beginner/vt_tutorial.rst:783
msgid "Then we can get a measure of similarity between these words by doing:"
msgstr "然后我们可以通过以下操作获得这些词之间的相似性度量："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = "
"q_\\text{physicist} \\cdot q_\\text{mathematician}"
msgstr ""
"\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = q_\\text{physicist} \\cdot q_\\text{mathematician}\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "Although it is more common to normalize by the lengths:"
msgstr "虽然更常见的是通过长度归一化："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"\\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n"
"{\\| q_\\text{physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)"
msgstr ""
" \\text{Similarity}(\\text{physicist}, \\text{mathematician}) = \\frac{q_\\text{physicist} \\cdot q_\\text{mathematician}}\n"
"{\\| q_\\text{physicist} \\| \\| q_\\text{mathematician} \\|} = \\cos (\\phi)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Where :math:`\\phi` is the angle between the two vectors. That way, "
"extremely similar words (words whose embeddings point in the same direction)"
" will have similarity 1. Extremely dissimilar words should have similarity "
"-1."
msgstr "其中 :math:`\\phi` 是两个向量之间的角度。这样，极其相似的词（嵌入指向相同方向的词）相似度为1。极其不相似的词应该是-1。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can think of the sparse one-hot vectors from the beginning of this "
"section as a special case of these new vectors we have defined, where each "
"word basically has similarity 0, and we gave each word some unique semantic "
"attribute. These new vectors are *dense*, which is to say their entries are "
"(typically) non-zero."
msgstr ""
"你可以将本节开始的稀疏独热向量视为我们定义的新向量的一个特殊情况，其中每个词基本上有相似度0，我们给每个词一些独特的语义属性。新的向量是*密集的*，也就是说它们的条目（通常）非零。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"But these new vectors are a big pain: you could think of thousands of "
"different semantic attributes that might be relevant to determining "
"similarity, and how on earth would you set the values of the different "
"attributes? Central to the idea of deep learning is that the neural network "
"learns representations of the features, rather than requiring the programmer"
" to design them herself. So why not just let the word embeddings be "
"parameters in our model, and then be updated during training? This is "
"exactly what we will do. We will have some *latent semantic attributes* that"
" the network can, in principle, learn. Note that the word embeddings will "
"probably not be interpretable. That is, although with our hand-crafted "
"vectors above we can see that mathematicians and physicists are similar in "
"that they both like coffee, if we allow a neural network to learn the "
"embeddings and see that both mathematicians and physicists have a large "
"value in the second dimension, it is not clear what that means. They are "
"similar in some latent semantic dimension, but this probably has no "
"interpretation to us."
msgstr ""
"但这些新向量非常麻烦：你可能会想到成千上万个不同的语义属性，这些属性对于确定相似性可能是相关的，但你如何确定不同属性的值呢？深度学习的核心概念是神经网络学习特征的表示，而不是要求程序员自己设计特征。那么为什么不让词嵌入成为模型的参数，并在训练过程中进行更新呢？这正是我们将要做的。我们将拥有一些*潜在语义属性*，神经网络原则上可以学习。注意词嵌入可能无法解释。也就是说，虽然在上面手工设计的向量中我们看到了数学家和物理学家在“喜欢喝咖啡”方面相似，但如果我们让神经网络学习嵌入并看到数学家和物理学家在第二个维度上有一个大值，这不一定清楚其含义。它们在某种潜在语义维度上是相似的，但这对我们来说可能没有解释。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In summary, **word embeddings are a representation of the *semantics* of a "
"word, efficiently encoding semantic information that might be relevant to "
"the task at hand**. You can embed other things too: part of speech tags, "
"parse trees, anything! The idea of feature embeddings is central to the "
"field."
msgstr ""
"总之，**词嵌入是一种表示词语义的方式，有效地编码可能与当前任务相关的语义信息**。你也可以嵌入其他东西：词性标签、解析树、任何东西！特征嵌入的概念是该领域的核心。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Word Embeddings in Pytorch"
msgstr "在Pytorch中使用词嵌入"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before we get to a worked example and an exercise, a few quick notes about "
"how to use embeddings in Pytorch and in deep learning programming in "
"general. Similar to how we defined a unique index for each word when making "
"one-hot vectors, we also need to define an index for each word when using "
"embeddings. These will be keys into a lookup table. That is, embeddings are "
"stored as a :math:`|V| \\times D` matrix, where :math:`D` is the "
"dimensionality of the embeddings, such that the word assigned index "
":math:`i` has its embedding stored in the :math:`i`'th row of the matrix. In"
" all of my code, the mapping from words to indices is a dictionary named "
"word\\_to\\_ix."
msgstr ""
"在我们进入一个实际例子和练习之前，先快速了解如何在Pytorch中以及深度学习编程中使用嵌入。类似于我们在制作独热向量时为每个词定义唯一索引，我们在使用嵌入时也需要为每个词定义一个索引。这些将是查找表的键。即嵌入存储为一个"
" :math:`|V| \\times D` 矩阵，其中 :math:`D` 是嵌入的维度，这样被分配索引 :math:`i` 的词的嵌入存储在矩阵的第"
" :math:`i` 行。在我的代码中，从词到索引的映射是一个名为 word\\_to\\_ix 的词典。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The module that allows you to use embeddings is torch.nn.Embedding, which "
"takes two arguments: the vocabulary size, and the dimensionality of the "
"embeddings."
msgstr "允许您使用嵌入的模块是 torch.nn.Embedding，它需要两个参数：词汇表的大小和嵌入的维度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To index into this table, you must use torch.LongTensor (since the indices "
"are integers, not floats)."
msgstr "要索引到这个表中，您必须使用 torch.LongTensor（因为索引是整数而不是浮点数）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "An Example: N-Gram Language Modeling"
msgstr "一个例子：N-Gram语言建模"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Recall that in an n-gram language model, given a sequence of words "
":math:`w`, we want to compute"
msgstr "回忆一下，在一个n-gram语言模型中，给定一个词序列 :math:`w`，我们希望计算"

#: ../../beginner/vt_tutorial.rst:783
msgid "P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )"
msgstr ""
"P(w_i | w_{i-1}, w_{i-2}, \\dots, w_{i-n+1} )\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "Where :math:`w_i` is the ith word of the sequence."
msgstr "其中 :math:`w_i` 是序列的第i个词。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example, we will compute the loss function on some training examples"
" and update the parameters with backpropagation."
msgstr "在这个例子中，我们将在一些训练样本上计算损失函数，并通过反向传播更新参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Exercise: Computing Word Embeddings: Continuous Bag-of-Words"
msgstr "练习：计算词嵌入：连续词袋模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep "
"learning. It is a model that tries to predict words given the context of a "
"few words before and a few words after the target word. This is distinct "
"from language modeling, since CBOW is not sequential and does not have to be"
" probabilistic. Typically, CBOW is used to quickly train word embeddings, "
"and these embeddings are used to initialize the embeddings of some more "
"complicated model. Usually, this is referred to as *pretraining embeddings*."
" It almost always helps performance a couple of percent."
msgstr ""
"连续词袋模型（CBOW）在NLP深度学习中经常使用。这是一个尝试根据目标词之前和之后的几个词的上下文预测目标词的模型。这与语言建模不同，因为CBOW不是连续的，也不一定需要是概率的。通常，CBOW用于快速训练词嵌入，而这些嵌入被用来初始化一些更复杂模型的嵌入。这通常被称为*嵌入预训练*。它几乎总是能提高性能几个百分点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The CBOW model is as follows. Given a target word :math:`w_i` and an "
":math:`N` context window on each side, :math:`w_{i-1}, \\dots, w_{i-N}` and "
":math:`w_{i+1}, \\dots, w_{i+N}`, referring to all context words "
"collectively as :math:`C`, CBOW tries to minimize"
msgstr ""
"CBOW模型如下。给定目标词 :math:`w_i` 和一个在每侧的 :math:`N` 的上下文窗口 :math:`w_{i-1}, \\dots, "
"w_{i-N}` 和 :math:`w_{i+1}, \\dots, w_{i+N}`，统称所有上下文词为 :math:`C`，CBOW试图最小化"

#: ../../beginner/vt_tutorial.rst:783
msgid "-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right)"
msgstr ""
"-\\log p(w_i | C) = -\\log \\text{Softmax}\\left(A(\\sum_{w \\in C} q_w) + b\\right)\n"
"\n"

#: ../../beginner/vt_tutorial.rst:783
msgid "where :math:`q_w` is the embedding for word :math:`w`."
msgstr "其中 :math:`q_w` 是词 :math:`w` 的嵌入。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Implement this model in Pytorch by filling in the class below. Some tips:"
msgstr "通过填空以下类在Pytorch中实现此模型。一些提示："

#: ../../beginner/vt_tutorial.rst:783
msgid "Think about which parameters you need to define."
msgstr "想一想需要定义哪些参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Make sure you know what shape each operation expects. Use .view() if you "
"need to reshape."
msgstr "确保您知道每个操作期望的形状。如果需要，使用.view()重塑。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.652 seconds)"
msgstr "**脚本的总运行时间:** ( 0 分钟 0.652 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: word_embeddings_tutorial.py "
"<word_embeddings_tutorial.py>`"
msgstr ""
":download:`下载Python源码: word_embeddings_tutorial.py "
"<word_embeddings_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: word_embeddings_tutorial.ipynb "
"<word_embeddings_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: word_embeddings_tutorial.ipynb "
"<word_embeddings_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_nn_tutorial.py>` to download "
"the full example code"
msgstr "点击 :ref:`此处 <sphx_glr_download_beginner_nn_tutorial.py>` 下载完整的示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is `torch.nn` *really*?"
msgstr "什么是`torch.nn` *真正是*？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Authors:** Jeremy Howard, `fast.ai <https://www.fast.ai>`_. Thanks to "
"Rachel Thomas and Francisco Ingham."
msgstr ""
"**作者：** Jeremy Howard, `fast.ai <https://www.fast.ai>`_。感谢Rachel "
"Thomas和Francisco Ingham。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We recommend running this tutorial as a notebook, not a script. To download "
"the notebook (``.ipynb``) file, click the link at the top of the page."
msgstr "我们推荐以笔记本而不是脚本方式运行此教程。要下载笔记本（``.ipynb``）文件，请点击页面顶部的链接。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch provides the elegantly designed modules and classes `torch.nn "
"<https://pytorch.org/docs/stable/nn.html>`_ , `torch.optim "
"<https://pytorch.org/docs/stable/optim.html>`_ , `Dataset "
"<https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`_"
" , and `DataLoader "
"<https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`_"
" to help you create and train neural networks. In order to fully utilize "
"their power and customize them for your problem, you need to really "
"understand exactly what they're doing. To develop this understanding, we "
"will first train basic neural net on the MNIST data set without using any "
"features from these models; we will initially only use the most basic "
"PyTorch tensor functionality. Then, we will incrementally add one feature "
"from ``torch.nn``, ``torch.optim``, ``Dataset``, or ``DataLoader`` at a "
"time, showing exactly what each piece does, and how it works to make the "
"code either more concise, or more flexible."
msgstr ""
"PyTorch提供了设计优雅的模块和类：`torch.nn <https://pytorch.org/docs/stable/nn.html>`_ "
"、`torch.optim <https://pytorch.org/docs/stable/optim.html>`_ 、`Dataset "
"<https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`_"
" 和 `DataLoader "
"<https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`_"
" "
"来帮助您创建和训练神经网络。为了充分利用它们的能力并为您的问题定制它们，您需要真正了解它们的具体用途。为了发展这种理解，我们首先将训练一个基本的神经网络来处理MNIST数据集，且不使用这些模块的任何特性；我们最初仅使用PyTorch张量的最基本功能。然后我们会逐一添加``torch.nn``、``torch.optim``、``Dataset``或``DataLoader``中的一个特性，展示每个部分确切的功能，以及如何使代码更简洁或更灵活。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**This tutorial assumes you already have PyTorch installed, and are familiar"
" with the basics of tensor operations.** (If you're familiar with Numpy "
"array operations, you'll find the PyTorch tensor operations used here nearly"
" identical)."
msgstr ""
"**本教程假设您已经安装了PyTorch，并熟悉张量操作的基本内容。**（如果您熟悉Numpy数组操作，您会发现这里使用的PyTorch张量操作几乎完全相同）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "MNIST data setup"
msgstr "MNIST数据设置"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will use the classic `MNIST "
"<https://yann.lecun.com/exdb/mnist/index.html>`_ dataset, which consists of "
"black-and-white images of hand-drawn digits (between 0 and 9)."
msgstr ""
"我们将使用经典的 `MNIST <https://yann.lecun.com/exdb/mnist/index.html>`_ "
"数据集，该数据集由手绘数字（0到9）的黑白图片组成。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will use `pathlib <https://docs.python.org/3/library/pathlib.html>`_ for "
"dealing with paths (part of the Python 3 standard library), and will "
"download the dataset using `requests <http://docs.python-"
"requests.org/en/master/>`_. We will only import modules when we use them, so"
" you can see exactly what's being used at each point."
msgstr ""
"我们将使用 `pathlib <https://docs.python.org/3/library/pathlib.html>`_ "
"来处理路径（Python 3标准库的一部分），并使用 `requests <http://docs.python-"
"requests.org/en/master/>`_ 下载数据集。我们只在使用模块时导入它们，这样您可以确切地看到每个点使用了什么。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This dataset is in numpy array format, and has been stored using pickle, a "
"python-specific format for serializing data."
msgstr "这个数据集是以NumPy数组格式存在，并使用Python特定的序列化格式Pickle进行存储。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each image is 28 x 28, and is being stored as a flattened row of length 784 "
"(=28x28). Let's take a look at one; we need to reshape it to 2d first."
msgstr "每张图片是28 x 28，因此存储为长度为784 (=28x28)的一维数组形式。我们先来看一下其中一个吧，需要先将其重新变形为二维。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to "
"convert our data."
msgstr "PyTorch使用``torch.tensor``而不是NumPy数组，所以我们需要转换数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Neural net from scratch (without ``torch.nn``)"
msgstr "从零开始实现神经网络（不使用``torch.nn``）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's first create a model using nothing but PyTorch tensor operations. "
"We're assuming you're already familiar with the basics of neural networks. "
"(If you're not, you can learn them at `course.fast.ai "
"<https://course.fast.ai>`_)."
msgstr ""
"首先，我们只使用PyTorch的张量操作来创建一个模型。假定你已经熟悉神经网络的基础知识。（如果不熟悉，可以在`course.fast.ai "
"<https://course.fast.ai>`_进行学习）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch provides methods to create random or zero-filled tensors, which we "
"will use to create our weights and bias for a simple linear model. These are"
" just regular tensors, with one very special addition: we tell PyTorch that "
"they require a gradient. This causes PyTorch to record all of the operations"
" done on the tensor, so that it can calculate the gradient during back-"
"propagation *automatically*!"
msgstr ""
"PyTorch提供了创建随机或零填充张量的方法，我们将使用它来创建简单线性模型的权重和偏置。这些只是普通张量，但有一个特别的功能：我们告诉PyTorch它们需要梯度。这使得PyTorch能够记录张量上所做的所有操作，从而在反向传播时*自动*计算梯度！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For the weights, we set ``requires_grad`` **after** the initialization, "
"since we don't want that step included in the gradient. (Note that a "
"trailing ``_`` in PyTorch signifies that the operation is performed in-"
"place.)"
msgstr ""
"对于权重，我们在初始化之后设置``requires_grad``，因为我们不希望这个步骤也被包含在梯度计算中。（注意PyTorch中带有后缀``_``的操作表示在原地修改）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We are initializing the weights here with `Xavier initialisation "
"<http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>`_ (by multiplying "
"with ``1/sqrt(n)``)."
msgstr ""
"我们在这里使用`Xavier初始化 "
"<http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>`_（通过乘以``1/sqrt(n)``）来初始化权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Thanks to PyTorch's ability to calculate gradients automatically, we can use"
" any standard Python function (or callable object) as a model! So let's just"
" write a plain matrix multiplication and broadcasted addition to create a "
"simple linear model. We also need an activation function, so we'll write "
"`log_softmax` and use it. Remember: although PyTorch provides lots of "
"prewritten loss functions, activation functions, and so forth, you can "
"easily write your own using plain python. PyTorch will even create fast "
"accelerator or vectorized CPU code for your function automatically."
msgstr ""
"得益于PyTorch的自动梯度计算能力，我们可以使用任何标准Python函数（或可调用对象）作为模型！所以我们仅通过普通的矩阵乘法和广播加法来创建一个简单的线性模型。同时我们还需要一个激活函数，因此我们编写`log_softmax`并使用它。请记住：虽然PyTorch提供了许多预定义的损失函数、激活函数等，你可以轻松使用纯Python编写自己的函数。PyTorch甚至会自动为你的函数创建快速加速代码或矢量化CPU代码。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the above, the ``@`` stands for the matrix multiplication operation. We "
"will call our function on one batch of data (in this case, 64 images).  This"
" is one *forward pass*.  Note that our predictions won't be any better than "
"random at this stage, since we start with random weights."
msgstr ""
"在上面，``@``表示矩阵乘法操作。我们将在一个数据批次（本例中为64张图片）上调用我们的函数。这是一次*前向传播*。注意，由于我们从随机权重开始，我们的预测目前不会比随机猜测更好。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As you see, the ``preds`` tensor contains not only the tensor values, but "
"also a gradient function. We'll use this later to do backprop."
msgstr "如你所见，``preds``张量不仅包含张量值，还包含梯度函数。稍后我们会用它进行反向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's implement negative log-likelihood to use as the loss function (again, "
"we can just use standard Python):"
msgstr "我们来实现负对数似然作为损失函数（同样我们可以使用标准Python）："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's check our loss with our random model, so we can see if we improve "
"after a backprop pass later."
msgstr "我们用随机模型检查一下损失，以便稍后通过一次反向传播看看是否有所改进。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's also implement a function to calculate the accuracy of our model. For "
"each prediction, if the index with the largest value matches the target "
"value, then the prediction was correct."
msgstr "我们还可以实现一个函数来计算模型的准确率。对于每个预测，如果最大值的索引与目标值匹配，那么预测就是正确的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's check the accuracy of our random model, so we can see if our accuracy "
"improves as our loss improves."
msgstr "我们检查一下随机模型的准确率，以便在损失改善时看看准确率是否有提高。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We can now run a training loop.  For each iteration, we will:"
msgstr "我们现在可以运行一个训练循环。对于每次迭代，我们将："

#: ../../beginner/vt_tutorial.rst:783
msgid "select a mini-batch of data (of size ``bs``)"
msgstr "选择一个数据小批量（大小``bs``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "use the model to make predictions"
msgstr "使用模型进行预测"

#: ../../beginner/vt_tutorial.rst:783
msgid "calculate the loss"
msgstr "计算损失"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``loss.backward()`` updates the gradients of the model, in this case, "
"``weights`` and ``bias``."
msgstr "``loss.backward()``会更新模型的梯度，在本例中是``weights``和``bias``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We now use these gradients to update the weights and bias.  We do this "
"within the ``torch.no_grad()`` context manager, because we do not want these"
" actions to be recorded for our next calculation of the gradient.  You can "
"read more about how PyTorch's Autograd records operations `here "
"<https://pytorch.org/docs/stable/notes/autograd.html>`_."
msgstr ""
"我们现在利用这些梯度更新权重和偏置。我们在``torch.no_grad()``上下文管理器中完成此操作，因为我们不希望这些动作被记录到下一次梯度计算中。可以在这里了解更多有关PyTorch自动梯度记录的信息：《`PyTorch"
" Autograd笔记 <https://pytorch.org/docs/stable/notes/autograd.html>`_》。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We then set the gradients to zero, so that we are ready for the next loop. "
"Otherwise, our gradients would record a running tally of all the operations "
"that had happened (i.e. ``loss.backward()`` *adds* the gradients to whatever"
" is already stored, rather than replacing them)."
msgstr ""
"然后将梯度设置为零，以便为下一个循环做好准备。否则，梯度将记录所有之前发生的操作（即``loss.backward()``会将梯度*添加*到已经存储的值，而不是替换它们）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can use the standard python debugger to step through PyTorch code, "
"allowing you to check the various variable values at each step. Uncomment "
"``set_trace()`` below to try it out."
msgstr "你可以使用标准Python调试器逐步检查PyTorch代码，查看每个步骤中的变量值。取消注释下面的``set_trace()``以试一下。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That's it: we've created and trained a minimal neural network (in this case,"
" a logistic regression, since we have no hidden layers) entirely from "
"scratch!"
msgstr "就这样：我们完全从零创建并训练了一个最简化的神经网络（在本例中是逻辑回归，因为我们没有隐藏层）！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's check the loss and accuracy and compare those to what we got earlier. "
"We expect that the loss will have decreased and accuracy to have increased, "
"and they have."
msgstr "我们检查一下损失和准确率，并将其与之前的结果进行比较。我们预计损失会减少，准确率会提高，这的确发生了。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using ``torch.nn.functional``"
msgstr "使用``torch.nn.functional``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will now refactor our code, so that it does the same thing as before, "
"only we'll start taking advantage of PyTorch's ``nn`` classes to make it "
"more concise and flexible. At each step from here, we should be making our "
"code one or more of: shorter, more understandable, and/or more flexible."
msgstr ""
"我们现在将重构代码，使其做与之前相同的事情，只是开始利用PyTorch的``nn``类使代码更简洁和灵活。从现在开始的每一步，我们的代码应该变得更简洁、易理解和/或更灵活。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The first and easiest step is to make our code shorter by replacing our "
"hand-written activation and loss functions with those from "
"``torch.nn.functional`` (which is generally imported into the namespace "
"``F`` by convention). This module contains all the functions in the "
"``torch.nn`` library (whereas other parts of the library contain classes). "
"As well as a wide range of loss and activation functions, you'll also find "
"here some convenient functions for creating neural nets, such as pooling "
"functions. (There are also functions for doing convolutions, linear layers, "
"etc, but as we'll see, these are usually better handled using other parts of"
" the library.)"
msgstr ""
"第一步也是最简单的一步，通过用``torch.nn.functional``（通常按约定导入为``F``）中的预定义的激活和损失函数替换自己编写的函数，使代码更简洁。此模块包含了``torch.nn``库中的所有函数（而库的其他部分包含类）。除了大量损失和激活函数，你还可以在这里找到一些创建神经网络的便利函数，例如池化函数。（虽然也有用于卷积、线性层等的函数，但正如我们将看到的，这些通常更适合使用库的其他部分处理。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you're using negative log likelihood loss and log softmax activation, "
"then Pytorch provides a single function ``F.cross_entropy`` that combines "
"the two. So we can even remove the activation function from our model."
msgstr ""
"如果使用负对数似然损失和对数softmax激活，那么Pytorch提供一个组合这两者的单一函数``F.cross_entropy``。所以我们甚至可以从模型中移除激活函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that we no longer call ``log_softmax`` in the ``model`` function. Let's"
" confirm that our loss and accuracy are the same as before:"
msgstr "注意我们不再在``model``函数中调用``log_softmax``。确认一下我们的损失和准确率与之前相同："

#: ../../beginner/vt_tutorial.rst:783
msgid "Refactor using ``nn.Module``"
msgstr "使用``nn.Module``进行重构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and "
"more concise training loop. We subclass ``nn.Module`` (which itself is a "
"class and able to keep track of state).  In this case, we want to create a "
"class that holds our weights, bias, and method for the forward step.  "
"``nn.Module`` has a number of attributes and methods (such as "
"``.parameters()`` and ``.zero_grad()``) which we will be using."
msgstr ""
"接下来，我们使用``nn.Module``和``nn.Parameter``实现更清晰、更简洁的训练循环。我们继承``nn.Module``（它本身是一个类，能够跟踪状态）。在本例中，我们想创建一个包含权重、偏置和前向步骤方法的类。``nn.Module``具有许多属性和方法（例如``.parameters()``和``.zero_grad()``），我们稍后会使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``nn.Module`` (uppercase M) is a PyTorch specific concept, and is a class "
"we'll be using a lot. ``nn.Module`` is not to be confused with the Python "
"concept of a (lowercase ``m``) `module "
"<https://docs.python.org/3/tutorial/modules.html>`_, which is a file of "
"Python code that can be imported."
msgstr ""
"``nn.Module``（大写的M）是PyTorch特定的概念，是我们将经常使用的一个类。需要注意的是，它与Python中的`模块 "
"<https://docs.python.org/3/tutorial/modules.html>`_（小写的m，表示可以导入的Python代码文件）不同。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Since we're now using an object instead of just using a function, we first "
"have to instantiate our model:"
msgstr "现在我们使用对象而不是函数，所以需要首先实例化模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we can calculate the loss in the same way as before. Note that "
"``nn.Module`` objects are used as if they are functions (i.e they are "
"*callable*), but behind the scenes Pytorch will call our ``forward`` method "
"automatically."
msgstr ""
"我们可以像之前一样计算损失。注意``nn.Module``对象可以当函数使用（即它们是*可调用的*），但实际上PyTorch会自动调用我们的``forward``方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Previously for our training loop we had to update the values for each "
"parameter by name, and manually zero out the grads for each parameter "
"separately, like this:"
msgstr "之前对于训练循环，我们不得不按照名字逐个更新每个参数的值，并单独将每个参数的梯度清零，比如这样："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we can take advantage of model.parameters() and model.zero_grad() (which"
" are both defined by PyTorch for ``nn.Module``) to make those steps more "
"concise and less prone to the error of forgetting some of our parameters, "
"particularly if we had a more complicated model:"
msgstr ""
"现在我们可以利用``model.parameters()``和``model.zero_grad()``（这两个方法由PyTorch为``nn.Module``定义）更加简洁地完成这些步骤，而且不会因为忘记某些参数而引入错误，特别是在模型更复杂时："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We'll wrap our little training loop in a ``fit`` function so we can run it "
"again later."
msgstr "我们将把小型训练循环封装到一个``fit``函数中，以便稍后可以再次运行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's double-check that our loss has gone down:"
msgstr "确认一下我们的损失已经减少："

#: ../../beginner/vt_tutorial.rst:783
msgid "Refactor using ``nn.Linear``"
msgstr "使用``nn.Linear``进行重构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We continue to refactor our code.  Instead of manually defining and "
"initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @ "
"self.weights + self.bias``, we will instead use the Pytorch class `nn.Linear"
" <https://pytorch.org/docs/stable/nn.html#linear-layers>`_ for a linear "
"layer, which does all that for us. Pytorch has many types of predefined "
"layers that can greatly simplify our code, and often makes it faster too."
msgstr ""
"我们继续重构代码。我们不再手动定义和初始化``self.weights``和``self.bias``，并计算``xb  @ self.weights "
"+ self.bias``。而是使用PyTorch类`nn.Linear "
"<https://pytorch.org/docs/stable/nn.html#linear-"
"layers>`_来创建线性层，这将为我们完成所有这些工作。PyTorch有许多预定义的层，可以显著简化代码，而且通常还能提高速度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We instantiate our model and calculate the loss in the same way as before:"
msgstr "我们以与以前相同的方式实例化模型并计算损失："

#: ../../beginner/vt_tutorial.rst:783
msgid "We are still able to use our same ``fit`` method as before."
msgstr "我们仍然可以使用之前相同的``fit``方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Refactor using ``torch.optim``"
msgstr "使用``torch.optim``进行重构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Pytorch also has a package with various optimization algorithms, "
"``torch.optim``. We can use the ``step`` method from our optimizer to take a"
" forward step, instead of manually updating each parameter."
msgstr ""
"PyTorch还有一个包含多种优化算法的包``torch.optim``。我们可以使用优化器的``step``方法来进行一步更新，而不需要单独更新每个参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This will let us replace our previous manually coded optimization step:"
msgstr "这将让我们可以用以下代码替换之前手动编写的优化步骤："

#: ../../beginner/vt_tutorial.rst:783
msgid "and instead use just:"
msgstr "而仅需要以下代码："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"(``optim.zero_grad()`` resets the gradient to 0 and we need to call it "
"before computing the gradient for the next minibatch.)"
msgstr "（``optim.zero_grad()``将梯度重置为0，在计算下一个小批量的梯度之前需要调用它。）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We'll define a little function to create our model and optimizer so we can "
"reuse it in the future."
msgstr "我们将定义一个小函数来创建模型和优化器，以便在将来重复使用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Refactor using Dataset"
msgstr "使用Dataset进行重构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch has an abstract Dataset class.  A Dataset can be anything that has a"
" ``__len__`` function (called by Python's standard ``len`` function) and a "
"``__getitem__`` function as a way of indexing into it. `This tutorial "
"<https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_ walks "
"through a nice example of creating a custom ``FacialLandmarkDataset`` class "
"as a subclass of ``Dataset``."
msgstr ""
"PyTorch有一个抽象的`Dataset`类。任何具有``__len__``函数（由Python标准函数``len``调用）和``__getitem__``函数（作为索引方式）的东西都可以是Dataset类。`这个教程"
" "
"<https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`_展示了一个通过创建一个定制的``FacialLandmarkDataset``类作为``Dataset``子类的例子。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch's `TensorDataset "
"<https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset>`_"
" is a Dataset wrapping tensors. By defining a length and way of indexing, "
"this also gives us a way to iterate, index, and slice along the first "
"dimension of a tensor. This will make it easier to access both the "
"independent and dependent variables in the same line as we train."
msgstr ""
"PyTorch的`TensorDataset "
"<https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset>`_是一个封装张量的Dataset类。通过定义长度和索引方式，这也为我们提供了一种迭代、索引和切片张量第一个维度的方式。这将使在训练中同时访问自变量和因变量变得更容易。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Both ``x_train`` and ``y_train`` can be combined in a single "
"``TensorDataset``, which will be easier to iterate over and slice."
msgstr "``x_train``和``y_train``可以组合到一个``TensorDataset``中，这会让迭代和切片更简便。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Previously, we had to iterate through minibatches of ``x`` and ``y`` values "
"separately:"
msgstr "之前，我们必须分别迭代``x``和``y``值的小批量："

#: ../../beginner/vt_tutorial.rst:783
msgid "Now, we can do these two steps together:"
msgstr "现在我们可以将这两个步骤一起完成："

#: ../../beginner/vt_tutorial.rst:783
msgid "Refactor using ``DataLoader``"
msgstr "使用``DataLoader``进行重构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch's ``DataLoader`` is responsible for managing batches. You can create"
" a ``DataLoader`` from any ``Dataset``. ``DataLoader`` makes it easier to "
"iterate over batches. Rather than having to use ``train_ds[i*bs : "
"i*bs+bs]``, the ``DataLoader`` gives us each minibatch automatically."
msgstr ""
"PyTorch的``DataLoader``负责管理批次。你可以从任何Dataset中创建一个``DataLoader``。``DataLoader``使迭代批次更加方便，而不用像之前使用``train_ds[i*bs"
" : i*bs+bs]``的方式，它会自动给我们每个小批量。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Previously, our loop iterated over batches ``(xb, yb)`` like this:"
msgstr "之前，我们的循环针对批次``(xb, yb)``进行了如下迭代："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, our loop is much cleaner, as ``(xb, yb)`` are loaded automatically from"
" the data loader:"
msgstr "现在，我们的循环更加整洁，因为``(xb, yb)``由数据加载器自动加载："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Thanks to PyTorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and "
"``DataLoader``, our training loop is now dramatically smaller and easier to "
"understand. Let's now try to add the basic features necessary to create "
"effective models in practice."
msgstr ""
"得益于 PyTorch 的 ``nn.Module``、``nn.Parameter``、``Dataset`` 和 "
"``DataLoader``，我们的训练循环现在大大缩短且更易于理解。现在，让我们尝试添加一些实际中创建高效模型所需的基本功能。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Add validation"
msgstr "添加验证"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In section 1, we were just trying to get a reasonable training loop set up "
"for use on our training data.  In reality, you **always** should also have a"
" `validation set <https://www.fast.ai/2017/11/13/validation-sets/>`_, in "
"order to identify if you are overfitting."
msgstr ""
"在第一部分中，我们仅仅是在为训练数据设置一个合理的训练循环。实际上，你始终应该有一个`验证集 "
"<https://www.fast.ai/2017/11/13/validation-sets/>`_，以识别是否发生了过拟合。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Shuffling the training data is `important <https://www.quora.com/Does-the-"
"order-of-training-data-matter-when-training-neural-networks>`_ to prevent "
"correlation between batches and overfitting. On the other hand, the "
"validation loss will be identical whether we shuffle the validation set or "
"not. Since shuffling takes extra time, it makes no sense to shuffle the "
"validation data."
msgstr ""
"对训练数据进行`随机化 <https://www.quora.com/Does-the-order-of-training-data-matter-"
"when-training-neural-networks>`_ "
"非常重要，可以防止批量间的相关性并避免过拟合。另一方面，无论是否对验证集进行随机化，验证损失都会是相同的。由于随机化需要额外的时间，因此随机化验证数据没有意义。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We'll use a batch size for the validation set that is twice as large as that"
" for the training set. This is because the validation set does not need "
"backpropagation and thus takes less memory (it doesn't need to store the "
"gradients). We take advantage of this to use a larger batch size and compute"
" the loss more quickly."
msgstr ""
"我们将使用的验证集的批量大小是训练集的两倍。这是因为验证集不需要反向传播，因此占用的内存更少（它不需要存储梯度）。我们利用这一点来使用更大的批量大小并更快地计算损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will calculate and print the validation loss at the end of each epoch."
msgstr "我们将在每个周期结束时计算并打印验证损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"(Note that we always call ``model.train()`` before training, and "
"``model.eval()`` before inference, because these are used by layers such as "
"``nn.BatchNorm2d`` and ``nn.Dropout`` to ensure appropriate behavior for "
"these different phases.)"
msgstr ""
"（注意，我们在训练前始终调用 ``model.train()``，在推理前调用 ``model.eval()``，因为像 "
"``nn.BatchNorm2d`` 和 ``nn.Dropout`` 这样的层会根据这些不同阶段确保其适当的行为。）"

#: ../../beginner/vt_tutorial.rst:783
msgid "Create fit() and get_data()"
msgstr "创建 fit() 和 get_data()"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We'll now do a little refactoring of our own. Since we go through a similar "
"process twice of calculating the loss for both the training set and the "
"validation set, let's make that into its own function, ``loss_batch``, which"
" computes the loss for one batch."
msgstr ""
"现在我们来进行一些重构。由于我们在计算训练集和验证集的损失时进行了类似的过程，因此我们把它封装成自己的函数 "
"``loss_batch``，它计算单个批量的损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We pass an optimizer in for the training set, and use it to perform "
"backprop.  For the validation set, we don't pass an optimizer, so the method"
" doesn't perform backprop."
msgstr "我们在训练集上传入一个优化器，然后用它进行反向传播。而在验证集上，我们不传入优化器，因此该方法不会进行反向传播。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``fit`` runs the necessary operations to train our model and compute the "
"training and validation losses for each epoch."
msgstr "``fit`` 执行必要的操作来训练我们的模型，并为每个周期计算训练和验证损失。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``get_data`` returns dataloaders for the training and validation sets."
msgstr "``get_data`` 返回用于训练和验证集的数据加载器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, our whole process of obtaining the data loaders and fitting the model "
"can be run in 3 lines of code:"
msgstr "现在，我们获取数据加载器和拟合模型的过程可以用三行代码完成："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can use these basic 3 lines of code to train a wide variety of models. "
"Let's see if we can use them to train a convolutional neural network (CNN)!"
msgstr "你可以使用这三行代码来训练各种类型的模型。让我们看看是否可以用它们来训练卷积神经网络 (CNN)！"

#: ../../beginner/vt_tutorial.rst:783
msgid "Switch to CNN"
msgstr "切换到 CNN"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We are now going to build our neural network with three convolutional "
"layers. Because none of the functions in the previous section assume "
"anything about the model form, we'll be able to use them to train a CNN "
"without any modification."
msgstr "我们现在将用三层卷积层构建我们的神经网络。由于上一部分中的函数没有假设任何模型形式，我们可以在不作任何修改的情况下用它们来训练 CNN。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will use PyTorch's predefined `Conv2d "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ class as our "
"convolutional layer. We define a CNN with 3 convolutional layers. Each "
"convolution is followed by a ReLU.  At the end, we perform an average "
"pooling.  (Note that ``view`` is PyTorch's version of Numpy's ``reshape``)"
msgstr ""
"我们将使用 PyTorch 预定义的 `Conv2d "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ "
"类作为我们的卷积层。我们定义一个具有 3 个卷积层的 CNN。每个卷积后接一个 ReLU。最后，我们执行一个平均池化操作。（注意，``view`` 是 "
"PyTorch 的 Numpy ``reshape`` 版本）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_ is a variation"
" on stochastic gradient descent that takes previous updates into account as "
"well and generally leads to faster training."
msgstr ""
"`动量法 <https://cs231n.github.io/neural-networks-3/#sgd>`_ "
"是一种基于随机梯度下降的变体，它考虑了以前的更新，并通常能加速训练。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using ``nn.Sequential``"
msgstr "使用 ``nn.Sequential``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.nn`` has another handy class we can use to simplify our code: "
"`Sequential <https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential>`_ "
". A ``Sequential`` object runs each of the modules contained within it, in a"
" sequential manner. This is a simpler way of writing our neural network."
msgstr ""
"``torch.nn`` 提供了另一个可以简化代码的便捷类：`Sequential "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential>`_。一个 "
"``Sequential`` 对象按顺序运行其中包含的各个模块。这是编写神经网络的一种更简单的方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To take advantage of this, we need to be able to easily define a **custom "
"layer** from a given function.  For instance, PyTorch doesn't have a `view` "
"layer, and we need to create one for our network. ``Lambda`` will create a "
"layer that we can then use when defining a network with ``Sequential``."
msgstr ""
"为了利用这一点，我们需要能够从给定的函数轻松定义一个**自定义层**。例如，PyTorch 没有 `view` "
"层，因此我们需要为我们的网络创建一个这样的层。``Lambda`` 可以创建一个可以在使用 ``Sequential`` 定义网络时使用的层。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The model created with ``Sequential`` is simple:"
msgstr "用 ``Sequential`` 创建的模型非常简单："

#: ../../beginner/vt_tutorial.rst:783
msgid "Wrapping ``DataLoader``"
msgstr "封装 ``DataLoader``"

#: ../../beginner/vt_tutorial.rst:783
msgid "Our CNN is fairly concise, but it only works with MNIST, because:"
msgstr "我们的 CNN 代码相当简洁，但它只能在 MNIST 数据集上运行，因为："

#: ../../beginner/vt_tutorial.rst:783
msgid "It assumes the input is a 28\\*28 long vector"
msgstr "它假设输入是一个 28\\*28 的长向量"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It assumes that the final CNN grid size is 4\\*4 (since that's the average "
"pooling kernel size we used)"
msgstr "它假设最终的 CNN 网格尺寸是 4\\*4（因为我们使用了 4\\*4 的平均池化核大小）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's get rid of these two assumptions, so our model works with any 2d "
"single channel image. First, we can remove the initial Lambda layer by "
"moving the data preprocessing into a generator:"
msgstr ""
"我们来去除这两个假设，使我们的模型能够处理任意 2D 单通道图像。首先，我们可以通过将数据预处理移到生成器中来去掉最初的 Lambda 层："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which "
"allows us to define the size of the *output* tensor we want, rather than the"
" *input* tensor we have. As a result, our model will work with any size "
"input."
msgstr ""
"接下来，我们可以用 ``nn.AdaptiveAvgPool2d`` 替代 "
"``nn.AvgPool2d``，这样我们可以定义我们想要的*输出*张量的大小，而不是我们拥有的*输入*张量的大小。结果就是我们的模型可以处理任意大小的输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's try it out:"
msgstr "我们来试试吧："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Using your `Accelerator "
"<https://pytorch.org/docs/stable/torch.html#accelerators>`__"
msgstr ""
"使用你的 `加速器 <https://pytorch.org/docs/stable/torch.html#accelerators>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you're lucky enough to have access to an accelerator such as CUDA (you "
"can rent one for about $0.50/hour from most cloud providers) you can use it "
"to speed up your code. First check that your accelerator is working in "
"Pytorch:"
msgstr ""
"如果你足够幸运拥有类似 CUDA 的加速器（你可以从大多数云服务提供商那里以大约 "
"$0.50/小时的价格租用），你可以用它来加速代码。首先，检查你的加速器是否在 PyTorch 中正常工作："

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's update ``preprocess`` to move batches to the accelerator:"
msgstr "让我们更新 ``preprocess`` 以将批量移动到加速器上："

#: ../../beginner/vt_tutorial.rst:783
msgid "Finally, we can move our model to the accelerator."
msgstr "最后，我们可以将模型移动到加速器上。"

#: ../../beginner/vt_tutorial.rst:783
msgid "You should find it runs faster now:"
msgstr "你应该会发现代码运行速度更快了："

#: ../../beginner/vt_tutorial.rst:783
msgid "Closing thoughts"
msgstr "结束语"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We now have a general data pipeline and training loop which you can use for "
"training many types of models using Pytorch. To see how simple training a "
"model can now be, take a look at the `mnist_sample notebook "
"<https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb>`__."
msgstr ""
"现在，我们有了一个通用的数据管道和训练循环，可以用来训练使用 PyTorch 的多种类型的模型。想看看现在训练一个模型有多简单，请查看 "
"`mnist_sample 笔记本 "
"<https://github.com/fastai/fastai_dev/blob/master/dev_nb/mnist_sample.ipynb>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Of course, there are many things you'll want to add, such as data "
"augmentation, hyperparameter tuning, monitoring training, transfer learning,"
" and so forth. These features are available in the fastai library, which has"
" been developed using the same design approach shown in this tutorial, "
"providing a natural next step for practitioners looking to take their models"
" further."
msgstr ""
"当然，仍有许多你可能希望添加的功能，例如数据增强、超参数调优、训练监控、迁移学习等。这些功能在 fastai "
"库中提供，该库使用与本教程中展示的设计方法相同的方式开发，对于希望进一步提升模型的实践者来说是自然的下一步。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We promised at the start of this tutorial we'd explain through example each "
"of ``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's "
"summarize what we've seen:"
msgstr ""
"我们在本教程的开头承诺将通过示例讲解 ``torch.nn``、``torch.optim``、``Dataset`` 和 "
"``DataLoader``。现在，让我们总结一下我们所看到的内容："

#: ../../beginner/vt_tutorial.rst:783
msgid "``torch.nn``:"
msgstr "``torch.nn``:"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``Module``: creates a callable which behaves like a function, but can also "
"contain state(such as neural net layer weights). It knows what ``Parameter``"
" (s) it contains and can zero all their gradients, loop through them for "
"weight updates, etc."
msgstr ""
"``Module``: 创建一个既可以像函数一样调用又可以包含状态（如神经网络层权重）的实例。它知道它包含哪些 "
"``Parameter``（参数），并可以清零它们的梯度、循环访问它们以进行权重更新等。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has "
"weights that need updating during backprop. Only tensors with the "
"`requires_grad` attribute set are updated"
msgstr ""
"``Parameter``: Tensor 的一个封装，告知 ``Module`` 它拥有需要在反向传播期间更新的权重。只有设置了 "
"`requires_grad` 属性的张量才会被更新。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``functional``: a module(usually imported into the ``F`` namespace by "
"convention) which contains activation functions, loss functions, etc, as "
"well as non-stateful versions of layers such as convolutional and linear "
"layers."
msgstr ""
"``functional``: 一个模块（通常根据惯例导入到 ``F`` "
"命名空间中），包含激活函数、损失函数等内容，还包括无状态版本的层，如卷积层和线性层。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.optim``: Contains optimizers such as ``SGD``, which update the "
"weights of ``Parameter`` during the backward step"
msgstr "``torch.optim``: 包含诸如 ``SGD`` 的优化器，用于在反向步骤中更新 ``Parameter`` 的权重。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``Dataset``: An abstract interface of objects with a ``__len__`` and a "
"``__getitem__``, including classes provided with Pytorch such as "
"``TensorDataset``"
msgstr ""
"``Dataset``: 一个抽象接口，具有 ``__len__`` 和 ``__getitem__`` 的对象，包括 PyTorch 提供的 "
"``TensorDataset`` 等类。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns "
"batches of data."
msgstr "``DataLoader``: 接受任何 ``Dataset`` 并创建一个迭代器，该迭代器返回数据批次。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 1 minutes  7.140 seconds)"
msgstr "**脚本总运行时间：** (1 分钟 7.140 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: nn_tutorial.py <nn_tutorial.py>`"
msgstr ":download:`下载 Python 源代码：nn_tutorial.py <nn_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: nn_tutorial.ipynb <nn_tutorial.ipynb>`"
msgstr ":download:`下载 Jupyter 笔记本：nn_tutorial.ipynb <nn_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_onnx_export_control_flow_model_to_onnx_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_beginner_onnx_export_control_flow_model_to_onnx_tutorial.py>`"
" 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction to ONNX <intro_onnx.html>`_ || `Exporting a PyTorch model to "
"ONNX <export_simple_model_to_onnx_tutorial.html>`_ || `Extending the ONNX "
"exporter operator support <onnx_registry_tutorial.html>`_ || **`Export a "
"model with control flow to ONNX**"
msgstr ""
"`ONNX 简介 <intro_onnx.html>`_ || `将 PyTorch 模型导出到 ONNX "
"<export_simple_model_to_onnx_tutorial.html>`_ || `扩展 ONNX 导出器运算符支持 "
"<onnx_registry_tutorial.html>`_ || **`将带有控制流的模型导出到 ONNX**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Export a model with control flow to ONNX"
msgstr "将带有控制流的模型导出到 ONNX"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author**: `Xavier Dupré <https://github.com/xadupre>`_"
msgstr "**作者**: `Xavier Dupré <https://github.com/xadupre>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Overview"
msgstr "概述"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial demonstrates how to handle control flow logic while exporting "
"a PyTorch model to ONNX. It highlights the challenges of exporting "
"conditional statements directly and provides solutions to circumvent them."
msgstr "本教程演示了如何在将 PyTorch 模型导出到 ONNX 时处理控制流逻辑。它强调了直接导出条件语句的挑战，并提供了解决方法来规避它们。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Conditional logic cannot be exported into ONNX unless they refactored to use"
" :func:`torch.cond`. Let's start with a simple model implementing a test."
msgstr "除非使用 :func:`torch.cond` 重构控制流逻辑，否则无法将其导出到 ONNX。让我们从一个实现测试的简单模型开始吧。"

#: ../../beginner/vt_tutorial.rst:783
msgid "What you will learn:"
msgstr "你将学习到的内容："

#: ../../beginner/vt_tutorial.rst:783
msgid "How to refactor the model to use :func:`torch.cond` for exporting."
msgstr "如何重构模型以使用 :func:`torch.cond` 进行导出。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to export a model with control flow logic to ONNX."
msgstr "如何将带有控制流逻辑的模型导出到 ONNX。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to optimize the exported model using the ONNX optimizer."
msgstr "如何使用 ONNX 优化器优化导出的模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "``torch >= 2.6``"
msgstr "``torch >= 2.6``"

#: ../../beginner/vt_tutorial.rst:783
msgid "Define the Models"
msgstr "定义模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Two models are defined:"
msgstr "定义了两个模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ForwardWithControlFlowTest``: A model with a forward method containing an "
"if-else conditional."
msgstr "``ForwardWithControlFlowTest``: 一个包含 if-else 条件的 forward 方法的模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``ModelWithControlFlowTest``: A model that incorporates "
"``ForwardWithControlFlowTest`` as part of a simple MLP. The models are "
"tested with a random input tensor to confirm they execute as expected."
msgstr ""
"``ModelWithControlFlowTest``: 一个包含 ``ForwardWithControlFlowTest`` 的简单 MLP "
"模型的一部分。这些模型使用随机输入张量进行测试以确认其按预期执行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Exporting the Model: First Attempt"
msgstr "导出模型：第一次尝试"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Exporting this model using torch.export.export fails because the control "
"flow logic in the forward pass creates a graph break that the exporter "
"cannot handle. This behavior is expected, as conditional logic not written "
"using :func:`torch.cond` is unsupported."
msgstr ""
"由于前向传递中的控制流逻辑会创建一个导出器无法处理的图拆分，所以使用 torch.export.export "
"导出该模型会失败。这种行为是预期的，因为不使用 :func:`torch.cond` 编写的条件逻辑是不被支持的。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A try-except block is used to capture the expected failure during the export"
" process. If the export unexpectedly succeeds, an ``AssertionError`` is "
"raised."
msgstr "使用 try-except 块捕获导出过程中的预期失败。如果导出意外成功，则会引发 ``AssertionError``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using :func:`torch.onnx.export` with JIT Tracing"
msgstr "使用带有 JIT 跟踪的 :func:`torch.onnx.export`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When exporting the model using :func:`torch.onnx.export` with the "
"dynamo=True argument, the exporter defaults to using JIT tracing. This "
"fallback allows the model to export, but the resulting ONNX graph may not "
"faithfully represent the original model logic due to the limitations of "
"tracing."
msgstr ""
"当导出模型时，使用 :func:`torch.onnx.export` 并添加 dynamo=True 参数。此时，导出器会默认为使用 JIT "
"跟踪。这种回退允许模型成功导出，但由于跟踪的限制，生成的 ONNX 图可能不能充分代表原始模型逻辑。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Suggested Patch: Refactoring with :func:`torch.cond`"
msgstr "建议的修补方法：使用 :func:`torch.cond` 进行重构"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To make the control flow exportable, the tutorial demonstrates replacing the"
" forward method in ``ForwardWithControlFlowTest`` with a refactored version "
"that uses :func:`torch.cond``."
msgstr ""
"为了使控制流可导出，本教程演示了如何将 ``ForwardWithControlFlowTest`` 中的 forward 方法替换为使用 "
":func:`torch.cond` 的重构版本。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Details of the Refactoring:"
msgstr "重构的详情："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Two helper functions (identity2 and neg) represent the branches of the "
"conditional logic: * :func:`torch.cond`` is used to specify the condition "
"and the two branches along with the input arguments. * The updated forward "
"method is then dynamically assigned to the ``ForwardWithControlFlowTest`` "
"instance within the model. A list of submodules is printed to confirm the "
"replacement."
msgstr ""
"两个辅助函数（identity2 和 neg）表示条件逻辑的分支： * :func:`torch.cond` 用于指定条件和两个分支以及输入参数。 * "
"更新后的 forward 方法随后被动态分配给模型内 ``ForwardWithControlFlowTest`` "
"实例。打印出子模块列表以确认已完成替换。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's see what the FX graph looks like."
msgstr "让我们看看 FX 图是什么样子。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Let's export again."
msgstr "让我们再试着导出一次。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can optimize the model and get rid of the model local functions created "
"to capture the control flow branches."
msgstr "我们可以优化模型并去除为捕获控制流分支而创建的模型局部函数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial demonstrates the challenges of exporting models with "
"conditional logic to ONNX and presents a practical solution using "
":func:`torch.cond`. While the default exporters may fail or produce "
"imperfect graphs, refactoring the model's logic ensures compatibility and "
"generates a faithful ONNX representation."
msgstr ""
"本教程展示了将具有条件逻辑的模型导出到ONNX的挑战，并使用函数:func:`torch.cond`提供了实际解决方案。虽然默认导出器可能失败或生成不完美的图，但通过重构模型逻辑可以确保兼容性并生成忠实的ONNX表示。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"By understanding these techniques, we can overcome common pitfalls when "
"working with control flow in PyTorch models and ensure smooth integration "
"with ONNX workflows."
msgstr "通过理解这些技术，我们可以克服在处理PyTorch模型控制流时的常见陷阱，并确保与ONNX工作流程的顺畅集成。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Further reading"
msgstr "进一步阅读"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The list below refers to tutorials that ranges from basic examples to "
"advanced scenarios, not necessarily in the order they are listed. Feel free "
"to jump directly to specific topics of your interest or sit tight and have "
"fun going through all of them to learn all there is about the ONNX exporter."
msgstr ""
"以下列表包含从基础示例到高级场景的教程，不一定按列出的顺序排列。可以直接跳到您感兴趣的特定主题，或者坐稳了逐步浏览所有内容，了解关于ONNX导出器的一切。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"1. `Exporting a PyTorch model to ONNX "
"<export_simple_model_to_onnx_tutorial.html>`_"
msgstr "1. `将PyTorch模型导出为ONNX <export_simple_model_to_onnx_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"2. `Extending the ONNX exporter operator support "
"<onnx_registry_tutorial.html>`_"
msgstr "2. `扩展ONNX导出器操作符支持 <onnx_registry_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"3. `Export a model with control flow to ONNX "
"<export_control_flow_model_to_onnx_tutorial.html>`_"
msgstr ""
"3. `将带控制流的模型导出为ONNX <export_control_flow_model_to_onnx_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  2.749 seconds)"
msgstr "**脚本总运行时间：** (0 分钟 2.749 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: "
"export_control_flow_model_to_onnx_tutorial.py "
"<export_control_flow_model_to_onnx_tutorial.py>`"
msgstr ""
":download:`下载Python源码：export_control_flow_model_to_onnx_tutorial.py "
"<export_control_flow_model_to_onnx_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: "
"export_control_flow_model_to_onnx_tutorial.ipynb "
"<export_control_flow_model_to_onnx_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：export_control_flow_model_to_onnx_tutorial.ipynb "
"<export_control_flow_model_to_onnx_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_onnx_export_simple_model_to_onnx_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_beginner_onnx_export_simple_model_to_onnx_tutorial.py>` "
"下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction to ONNX <intro_onnx.html>`_ || **Exporting a PyTorch model to "
"ONNX** || `Extending the ONNX exporter operator support "
"<onnx_registry_tutorial.html>`_ || `Export a model with control flow to ONNX"
" <export_control_flow_model_to_onnx_tutorial.html>`_"
msgstr ""
"`ONNX简介 <intro_onnx.html>`_ || **将PyTorch模型导出为ONNX** || `扩展ONNX导出器操作符支持 "
"<onnx_registry_tutorial.html>`_ || `将带控制流的模型导出为ONNX "
"<export_control_flow_model_to_onnx_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Export a PyTorch model to ONNX"
msgstr "导出PyTorch模型到ONNX"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Author**: `Ti-Tai Wang <https://github.com/titaiwangms>`_, `Justin Chu "
"<justinchu@microsoft.com>`_, `Thiago Crepaldi "
"<https://github.com/thiagocrepaldi>`_."
msgstr ""
"**作者**: `Ti-Tai Wang <https://github.com/titaiwangms>`_, `Justin Chu "
"<justinchu@microsoft.com>`_, `Thiago Crepaldi "
"<https://github.com/thiagocrepaldi>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "As of PyTorch 2.5, there are two versions of ONNX Exporter."
msgstr "截至PyTorch 2.5，有两个版本的ONNX导出器。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.onnx.export(..., dynamo=True)`` is the newest (still in beta) "
"exporter using ``torch.export`` and Torch FX to capture the graph. It was "
"released with PyTorch 2.5"
msgstr ""
"``torch.onnx.export(..., dynamo=True)`` "
"是最新的（仍处于测试阶段）导出器，使用``torch.export``和Torch FX捕获图形。它随PyTorch 2.5一起发布。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.onnx.export`` uses TorchScript and has been available since PyTorch "
"1.2.0"
msgstr "``torch.onnx.export`` 使用TorchScript并自PyTorch 1.2.0以来一直可用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the `60 Minute Blitz "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`_, "
"we had the opportunity to learn about PyTorch at a high level and train a "
"small neural network to classify images. In this tutorial, we are going to "
"expand this to describe how to convert a model defined in PyTorch into the "
"ONNX format using the ``torch.onnx.export(..., dynamo=True)`` ONNX exporter."
msgstr ""
"在 `60分钟速成教程 "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`_ "
"中，我们有机会从高层次了解PyTorch并训练一个小型神经网络以分类图像。在本教程中，我们将扩展此内容，描述如何使用``torch.onnx.export(...,"
" dynamo=True)`` ONNX导出器将PyTorch模型转换为ONNX格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"While PyTorch is great for iterating on the development of models, the model"
" can be deployed to production using different formats, including `ONNX "
"<https://onnx.ai/>`_ (Open Neural Network Exchange)!"
msgstr ""
"虽然PyTorch非常适合模型开发迭代，但模型可以使用包括`ONNX "
"<https://onnx.ai/>`_（开放神经网络交换）在内的不同格式部署到生产环境！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"ONNX is a flexible open standard format for representing machine learning "
"models which standardized representations of machine learning allow them to "
"be executed across a gamut of hardware platforms and runtime environments "
"from large-scale cloud-based supercomputers to resource-constrained edge "
"devices, such as your web browser and phone."
msgstr ""
"ONNX是一种表示机器学习模型的灵活开放标准格式，通过对机器学习的标准化表示，允许模型在从大型基于云的超级计算机到资源受限的边缘设备（如您的网络浏览器和手机）的各种硬件平台和运行环境中执行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "In this tutorial, we’ll learn how to:"
msgstr "在本教程中，我们将学习如何："

#: ../../beginner/vt_tutorial.rst:783
msgid "Install the required dependencies."
msgstr "安装所需的依赖项。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Author a simple image classifier model."
msgstr "编写一个简单的图像分类器模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Export the model to ONNX format."
msgstr "将模型导出为ONNX格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save the ONNX model in a file."
msgstr "将ONNX模型保存到文件中。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Visualize the ONNX model graph using `Netron "
"<https://github.com/lutzroeder/netron>`_."
msgstr "使用`Netron <https://github.com/lutzroeder/netron>`_可视化ONNX模型图。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Execute the ONNX model with `ONNX Runtime`"
msgstr "使用`ONNX Runtime`执行ONNX模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Compare the PyTorch results with the ones from the ONNX Runtime."
msgstr "比较PyTorch结果和ONNX Runtime的结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid "1. Install the required dependencies"
msgstr "1. 安装所需的依赖项"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Because the ONNX exporter uses ``onnx`` and ``onnxscript`` to translate "
"PyTorch operators into ONNX operators, we will need to install them."
msgstr "因为ONNX导出器使用``onnx``和``onnxscript``将PyTorch操作符翻译为ONNX操作符，所以我们需要安装它们。"

#: ../../beginner/vt_tutorial.rst:783
msgid "2. Author a simple image classifier model"
msgstr "2. 编写一个简单的图像分类器模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once your environment is set up, let’s start modeling our image classifier "
"with PyTorch, exactly like we did in the `60 Minute Blitz "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`_."
msgstr ""
"一旦环境设置完成后，让我们开始用PyTorch建模我们的图像分类器，就像我们在`60分钟速成教程 "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`_中所做的一样。"

#: ../../beginner/vt_tutorial.rst:783
msgid "3. Export the model to ONNX format"
msgstr "3. 将模型导出为ONNX格式"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have our model defined, we need to instantiate it and create a "
"random 32x32 input. Next, we can export the model to ONNX format."
msgstr "现在我们已经定义了模型，需要将其实例化并创建一个随机32x32输入。然后，我们可以将模型导出为ONNX格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "3.5. (Optional) Optimize the ONNX model"
msgstr "3.5.（可选）优化ONNX模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ONNX model can be optimized with constant folding, and elimination of "
"redundant nodes. The optimization is done in-place, so the original ONNX "
"model is modified."
msgstr "可以对ONNX模型进行优化，例如常量折叠和消除多余节点。优化是在原位完成的，因此原始ONNX模型会被修改。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As we can see, we didn't need any code change to the model. The resulting "
"ONNX model is stored within ``torch.onnx.ONNXProgram`` as a binary protobuf "
"file."
msgstr ""
"正如我们所看到的，无需对模型进行任何代码更改。生成的ONNX模型以二进制protobuf文件的形式存储在``torch.onnx.ONNXProgram``中。"

#: ../../beginner/vt_tutorial.rst:783
msgid "4. Save the ONNX model in a file"
msgstr "4. 将ONNX模型保存到文件"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Although having the exported model loaded in memory is useful in many "
"applications, we can save it to disk with the following code:"
msgstr "虽然将导出的模型加载到内存中在许多应用中非常有用，但我们可以使用以下代码将其保存到磁盘："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can load the ONNX file back into memory and check if it is well formed "
"with the following code:"
msgstr "您可以将ONNX文件加载回内存并使用以下代码检查它是否格式正确："

#: ../../beginner/vt_tutorial.rst:783
msgid "5. Visualize the ONNX model graph using Netron"
msgstr "5. 使用Netron可视化ONNX模型图"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have our model saved in a file, we can visualize it with `Netron"
" <https://github.com/lutzroeder/netron>`_. Netron can either be installed on"
" macos, Linux or Windows computers, or run directly from the browser. Let's "
"try the web version by opening the following link: https://netron.app/."
msgstr ""
"现在我们已将模型保存到文件中，可以使用`Netron "
"<https://github.com/lutzroeder/netron>`_进行可视化。Netron可以安装在macOS、Linux或Windows计算机上，也可以直接从浏览器运行。让我们通过打开以下链接尝试Web版本：https://netron.app/。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Once Netron is open, we can drag and drop our "
"``image_classifier_model.onnx`` file into the browser or select it after "
"clicking the **Open model** button."
msgstr ""
"Netron打开后，我们可以将``image_classifier_model.onnx``文件拖放到浏览器内或在点击**打开模型**按钮后选择它。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"And that is it! We have successfully exported our PyTorch model to ONNX "
"format and visualized it with Netron."
msgstr "到此为止！我们已成功将PyTorch模型导出为ONNX格式并使用Netron对其进行了可视化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "6. Execute the ONNX model with ONNX Runtime"
msgstr "6. 使用ONNX Runtime执行ONNX模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The last step is executing the ONNX model with `ONNX Runtime`, but before we"
" do that, let's install ONNX Runtime."
msgstr "最后一步是使用`ONNX Runtime`执行ONNX模型，但在此之前，让我们安装ONNX Runtime。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ONNX standard does not support all the data structure and types that "
"PyTorch does, so we need to adapt PyTorch input's to ONNX format before "
"feeding it to ONNX Runtime. In our example, the input happens to be the "
"same, but it might have more inputs than the original PyTorch model in more "
"complex models."
msgstr ""
"ONNX标准不支持PyTorch的所有数据结构和类型，因此我们需要在将输入提供给ONNX "
"Runtime之前将PyTorch的输入适配成ONNX格式。在我们的示例中，输入恰好是相同的，但在更复杂的模型中可能有比原始PyTorch模型更多的输入。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"ONNX Runtime requires an additional step that involves converting all "
"PyTorch tensors to Numpy (in CPU) and wrap them on a dictionary with keys "
"being a string with the input name as key and the numpy tensor as the value."
msgstr ""
"ONNX "
"Runtime需要额外的步骤，包括将所有PyTorch张量转换为Numpy（在CPU中），并将它们包裹在一个字典中，其键为输入名称的字符串，值为Numpy张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now we can create an *ONNX Runtime Inference Session*, execute the ONNX "
"model with the processed input and get the output. In this tutorial, ONNX "
"Runtime is executed on CPU, but it could be executed on GPU as well."
msgstr ""
"现在我们可以创建一个*ONNX Runtime推理会话*，使用处理后的输入执行ONNX模型并获取输出。在本教程中，ONNX "
"Runtime在CPU上执行，但也可以在GPU上执行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "7. Compare the PyTorch results with the ones from the ONNX Runtime"
msgstr "7. 比较PyTorch结果与ONNX Runtime的结果"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The best way to determine whether the exported model is looking good is "
"through numerical evaluation against PyTorch, which is our source of truth."
msgstr "确定导出的模型是否表现良好的最佳方法是与PyTorch进行数值评估，PyTorch是我们的真值来源。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For that, we need to execute the PyTorch model with the same input and "
"compare the results with ONNX Runtime's. Before comparing the results, we "
"need to convert the PyTorch's output to match ONNX's format."
msgstr ""
"为此，我们需要使用相同的输入执行PyTorch模型，并将结果与ONNX "
"Runtime的结果进行比较。在比较结果之前，我们需要将PyTorch的输出转换为匹配ONNX的格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"That is about it! We have successfully exported our PyTorch model to ONNX "
"format, saved the model to disk, viewed it using Netron, executed it with "
"ONNX Runtime and finally compared its numerical results with PyTorch's."
msgstr ""
"就是这样！我们成功将PyTorch模型导出为ONNX格式，将模型保存到磁盘，使用Netron查看它，使用ONNX "
"Runtime执行它并最终比较其数值结果与PyTorch的结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  1.574 seconds)"
msgstr "**脚本总运行时间：** (0 分钟 1.574 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: "
"export_simple_model_to_onnx_tutorial.py "
"<export_simple_model_to_onnx_tutorial.py>`"
msgstr ""
":download:`下载Python源码：export_simple_model_to_onnx_tutorial.py "
"<export_simple_model_to_onnx_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: "
"export_simple_model_to_onnx_tutorial.ipynb "
"<export_simple_model_to_onnx_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本：export_simple_model_to_onnx_tutorial.ipynb "
"<export_simple_model_to_onnx_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "ONNX"
msgstr "ONNX"

#: ../../beginner/vt_tutorial.rst:783
msgid "intro_onnx.py"
msgstr "intro_onnx.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Introduction to ONNX "
"https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html"
msgstr "ONNX简介 https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "export_simple_model_to_onnx_tutorial.py"
msgstr "export_simple_model_to_onnx_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Exporting a PyTorch model to ONNX "
"https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html"
msgstr ""
"导出PyTorch模型到ONNX "
"https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "onnx_registry_tutorial.py"
msgstr "onnx_registry_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Extending the ONNX exporter operator support "
"https://pytorch.org/tutorials/beginner/onnx/onnx_registry_tutorial.html"
msgstr ""
"扩展ONNX导出器操作符支持 "
"https://pytorch.org/tutorials/beginner/onnx/onnx_registry_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "export_control_flow_model_to_onnx_tutorial.py"
msgstr "export_control_flow_model_to_onnx_tutorial.py"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Export a model with control flow to ONNX "
"https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html"
msgstr ""
"将带控制流的模型导出为ONNX "
"https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Introduction to ONNX** ||"
msgstr "**ONNX简介** ||"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_onnx_intro_onnx.py`"
msgstr ":ref:`sphx_glr_beginner_onnx_intro_onnx.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid "`Introduction to ONNX <intro_onnx.html>`_ ||"
msgstr "`ONNX简介 <intro_onnx.html>`_ ||"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_onnx_export_control_flow_model_to_onnx_tutorial.py`"
msgstr ""
":ref:`sphx_glr_beginner_onnx_export_control_flow_model_to_onnx_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_onnx_export_simple_model_to_onnx_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_onnx_export_simple_model_to_onnx_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_onnx_onnx_registry_tutorial.py`"
msgstr ":ref:`sphx_glr_beginner_onnx_onnx_registry_tutorial.py`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_onnx_intro_onnx.py>` to "
"download the full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_beginner_onnx_intro_onnx.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Introduction to ONNX** || `Exporting a PyTorch model to ONNX "
"<export_simple_model_to_onnx_tutorial.html>`_ || `Extending the ONNX "
"exporter operator support <onnx_registry_tutorial.html>`_ || `Export a model"
" with control flow to ONNX "
"<export_control_flow_model_to_onnx_tutorial.html>`_"
msgstr ""
"**ONNX简介** || `将PyTorch模型导出为ONNX "
"<export_simple_model_to_onnx_tutorial.html>`_ || `扩展ONNX导出器操作符支持 "
"<onnx_registry_tutorial.html>`_ || `将带控制流的模型导出为ONNX "
"<export_control_flow_model_to_onnx_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Introduction to ONNX"
msgstr "ONNX简介"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Authors: `Ti-Tai Wang <https://github.com/titaiwangms>`_, `Thiago Crepaldi "
"<https://github.com/thiagocrepaldi>`_."
msgstr ""
"作者: `Ti-Tai Wang <https://github.com/titaiwangms>`_, `Thiago Crepaldi "
"<https://github.com/thiagocrepaldi>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Open Neural Network eXchange (ONNX) <https://onnx.ai/>`_ is an open "
"standard format for representing machine learning models. The ``torch.onnx``"
" module provides APIs to capture the computation graph from a native PyTorch"
" :class:`torch.nn.Module` model and convert it into an `ONNX graph "
"<https://github.com/onnx/onnx/blob/main/docs/IR.md>`_."
msgstr ""
"`开放神经网络交换（ONNX） <https://onnx.ai/>`_ 是用于表示机器学习模型的开放标准格式。``torch.onnx`` "
"模块提供了从原生PyTorch :class:`torch.nn.Module` 模型捕获计算图并将其转换为 `ONNX图形 "
"<https://github.com/onnx/onnx/blob/main/docs/IR.md>`_ 的API。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The exported model can be consumed by any of the many `runtimes that support"
" ONNX <https://onnx.ai/supported-tools.html#deployModel>`_, including "
"Microsoft's `ONNX Runtime <https://www.onnxruntime.ai>`_."
msgstr ""
"导出的模型可以被任何支持ONNX的`运行时 <https://onnx.ai/supported-tools.html#deployModel>`_ "
"消耗，包括微软的`ONNX Runtime <https://www.onnxruntime.ai>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Currently, you can choose either through `TorchScript "
"https://pytorch.org/docs/stable/jit.html`_ or `ExportedProgram "
"https://pytorch.org/docs/stable/export.html`_ to export the model to ONNX by"
" the boolean parameter dynamo in `torch.onnx.export "
"<https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export>`_."
" In this tutorial, we will focus on the ``ExportedProgram`` approach."
msgstr ""
"目前，您可以通过`TorchScript https://pytorch.org/docs/stable/jit.html`_ 或 "
"`ExportedProgram https://pytorch.org/docs/stable/export.html`_ "
"之一根据`torch.onnx.export "
"<https://pytorch.org/docs/stable/onnx_torchscript.html#torch.onnx.export>`_的布尔参数dynamo将模型导出为ONNX。在本教程中，我们将重点介绍``ExportedProgram``方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When setting ``dynamo=True``, the exporter will use `torch.export "
"<https://pytorch.org/docs/stable/export.html>`_ to capture an "
"``ExportedProgram``, before translating the graph into ONNX representations."
" This approach is the new and recommended way to export models to ONNX. It "
"works with PyTorch 2.0 features more robustly, has better support for newer "
"ONNX operator sets, and consumes less resources to make exporting larger "
"models possible."
msgstr ""
"设置``dynamo=True``时，导出器将使用`torch.export "
"<https://pytorch.org/docs/stable/export.html>`_ "
"捕获``ExportedProgram``，然后将图形转换为ONNX表示。这种方法是导出模型到ONNX的新推荐方式。它更稳健地支持PyTorch "
"2.0功能，对更新的ONNX操作符集支持更好，并使用更少的资源使得导出更大的模型成为可能。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dependencies"
msgstr "依赖项"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch 2.5.0 or newer is required."
msgstr "需要PyTorch 2.5.0或更新版本。"

#: ../../beginner/vt_tutorial.rst:783
msgid "The ONNX exporter depends on extra Python packages:"
msgstr "ONNX导出器依赖于额外的Python包："

#: ../../beginner/vt_tutorial.rst:783
msgid "`ONNX <https://onnx.ai>`_ standard library"
msgstr "`ONNX <https://onnx.ai>`_ 标准库"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`ONNX Script <https://onnxscript.ai>`_ library that enables developers to "
"author ONNX operators, functions and models using a subset of Python in an "
"expressive, and yet simple fashion"
msgstr ""
"`ONNX Script <https://onnxscript.ai>`_ 库让开发者能够使用 Python 的一个子集以表达性且简单的方式编写 "
"ONNX 操作符、函数和模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`ONNX Runtime <https://onnxruntime.ai>`_ accelerated machine learning "
"library."
msgstr "`ONNX Runtime <https://onnxruntime.ai>`_ 是一个加速的机器学习库。"

#: ../../beginner/vt_tutorial.rst:783
msgid "They can be installed through `pip <https://pypi.org/project/pip/>`_:"
msgstr "它们可以通过 `pip <https://pypi.org/project/pip/>`_ 安装："

#: ../../beginner/vt_tutorial.rst:783
msgid "To validate the installation, run the following commands:"
msgstr "要验证安装，运行以下命令："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Each `import` must succeed without any errors and the library versions must "
"be printed out."
msgstr "每个 `import` 必须没有任何错误地成功，并且库的版本号需要被打印出来。"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Python source code: intro_onnx.py <intro_onnx.py>`"
msgstr ":download:`下载 Python 源代码: intro_onnx.py <intro_onnx.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: intro_onnx.ipynb <intro_onnx.ipynb>`"
msgstr ":download:`下载 Jupyter notebook: intro_onnx.ipynb <intro_onnx.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here "
"<sphx_glr_download_beginner_onnx_onnx_registry_tutorial.py>` to download the"
" full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_onnx_onnx_registry_tutorial.py>` "
"下载完整的示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Introduction to ONNX <intro_onnx.html>`_ || `Exporting a PyTorch model to "
"ONNX <export_simple_model_to_onnx_tutorial.html>`_ || **Extending the ONNX "
"exporter operator support** || `Export a model with control flow to ONNX "
"<export_control_flow_model_to_onnx_tutorial.html>`_"
msgstr ""
"`ONNX 简介 <intro_onnx.html>`_ || `将 PyTorch 模型导出到 ONNX "
"<export_simple_model_to_onnx_tutorial.html>`_ || **扩展 ONNX 导出器的操作符支持** || "
"`将带有控制流的模型导出到 ONNX <export_control_flow_model_to_onnx_tutorial.html>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Extending the ONNX Exporter Operator Support"
msgstr "扩展 ONNX 导出器的操作符支持"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Authors:** `Ti-Tai Wang <titaiwang@microsoft.com>`_, `Justin Chu "
"<justinchu@microsoft.com>`_"
msgstr ""
"**作者：** `Ti-Tai Wang <titaiwang@microsoft.com>`_, `Justin Chu "
"<justinchu@microsoft.com>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial describes how you can create ONNX implementation for "
"unsupported PyTorch operators or replace existing implementation with your "
"own."
msgstr "本教程描述了如何为不受支持的 PyTorch 操作符创建 ONNX 实现或用您自己的实现替换现有实现。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will cover three scenarios that require extending the ONNX exporter's "
"operator support:"
msgstr "我们将介绍三种需要扩展 ONNX 导出器操作符支持的场景："

#: ../../beginner/vt_tutorial.rst:783
msgid "Overriding the implementation of an existing PyTorch operator"
msgstr "重写现有 PyTorch 操作符的实现"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using custom ONNX operators"
msgstr "使用自定义的 ONNX 操作符"

#: ../../beginner/vt_tutorial.rst:783
msgid "Supporting a custom PyTorch operator"
msgstr "支持自定义 PyTorch 操作符"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to override or add support for PyTorch operators in ONNX."
msgstr "如何在 ONNX 中重写或新增对 PyTorch 操作符的支持。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to integrate custom ONNX operators for specialized runtimes."
msgstr "如何为特殊运行时集成自定义 ONNX 操作符。"

#: ../../beginner/vt_tutorial.rst:783
msgid "How to implement and translate custom PyTorch operators to ONNX."
msgstr "如何实现和转换自定义的 PyTorch 操作符为 ONNX。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before starting this tutorial, make sure you have completed the following "
"prerequisites:"
msgstr "在开始本教程之前，请确保您已完成以下先决条件："

#: ../../beginner/vt_tutorial.rst:783
msgid "The target PyTorch operator"
msgstr "目标 PyTorch 操作符"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Completed the `ONNX Script tutorial "
"<https://github.com/microsoft/onnxscript/blob/main/docs/tutorial/index.md>`_"
" before proceeding"
msgstr ""
"完成 `ONNX Script 教程 "
"<https://github.com/microsoft/onnxscript/blob/main/docs/tutorial/index.md>`_"
" 之后再继续"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The implementation of the operator using `ONNX Script "
"<https://github.com/microsoft/onnxscript>`__"
msgstr "使用 `ONNX Script <https://github.com/microsoft/onnxscript>`__ 实现操作符"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Although the ONNX exporter team does their best efforts to support all "
"PyTorch operators, some of them might not be supported yet. In this section,"
" we will demonstrate how you can add unsupported PyTorch operators to the "
"ONNX Registry."
msgstr ""
"虽然 ONNX 导出器团队尽力支持所有 PyTorch 操作符，但某些操作符可能仍未被支持。在本节中，我们将演示如何将不支持的 PyTorch "
"操作符添加到 ONNX 注册表。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The steps to implement unsupported PyTorch operators are the same as those "
"for replacing the implementation of an existing PyTorch operator with a "
"custom one. Because we don't actually have an unsupported PyTorch operator "
"to use in this tutorial, we are going to leverage this and replace the "
"implementation of ``torch.ops.aten.add.Tensor`` with a custom implementation"
" the same way we would if the operator was not implemented by the ONNX "
"exporter."
msgstr ""
"实现不支持的 PyTorch 操作符的步骤与用自定义实现替换现有 PyTorch 操作符的步骤相同。由于在本教程中我们实际上没有一个不支持的 "
"PyTorch 操作符可以使用，我们将用这种方式替换 ``torch.ops.aten.add.Tensor`` 的实现，就像如果该操作符未被 ONNX"
" 导出器实现一样。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When a model cannot be exported to ONNX due to an unsupported operator, the "
"ONNX exporter will show an error message similar to:"
msgstr "当由于不支持的操作符导致模型无法导出到 ONNX 时，ONNX 导出器将显示类似以下的错误消息："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The error message indicates that the unsupported PyTorch operator is "
"``torch.ops.aten.add.Tensor``. The operator is of type ``<class "
"'torch._ops.OpOverload'>``, and this operator is what we will use as the "
"target to register our custom implementation."
msgstr ""
"错误消息表明不支持的 PyTorch 操作符是 ``torch.ops.aten.add.Tensor``。该操作符是类型 ``<class "
"&apos;torch._ops.OpOverload&apos;>``, 这是我们将用作目标以注册自定义实现的操作符。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now let's inspect the model and verify the model is using the custom "
"implementation."
msgstr "现在让我们检查模型并验证模型正在使用自定义实现。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The translation is using our custom implementation: In node ``node_Add_0``, "
"``input_y`` now comes first, and ``input_x`` comes second."
msgstr "转换是使用我们的自定义实现: 在节点 ``node_Add_0`` 中, ``input_y`` 现在在前，``input_x`` 在后。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We can use ONNX Runtime to run the model and verify the results by calling "
"the :class:`torch.onnx.ONNXProgram` directly on the input tensors."
msgstr ""
"我们可以使用 ONNX Runtime 运行模型并通过直接对输入张量调用 :class:`torch.onnx.ONNXProgram` 验证结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this case, we create a model with standard PyTorch operators, but the "
"runtime (such as Microsoft's ONNX Runtime) can provide a custom "
"implementation for that kernel, effectively replacing the existing "
"implementation."
msgstr ""
"在这种情况下，我们使用标准的 PyTorch 操作符创建一个模型，但运行时（例如 Microsoft's ONNX "
"Runtime）可以为该内核提供一个自定义实现，从而有效地替换现有的实现。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the following example, we use the ``com.microsoft.Gelu`` operator "
"provided by ONNX Runtime, which is not the same ``Gelu`` from ONNX spec."
msgstr ""
"在以下示例中，我们使用由 ONNX Runtime 提供的 ``com.microsoft.Gelu`` 操作符，它不同于来自 ONNX 规范的 "
"``Gelu``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's inspect the model and verify the model uses op_type ``Gelu`` from "
"namespace ``com.microsoft``."
msgstr "让我们检查模型并验证模型使用的是来自命名空间 ``com.microsoft`` 的 op_type ``Gelu``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Similar to the previous example, we can use ONNX Runtime to run the model "
"and verify the results."
msgstr "类似于前一个示例，我们可以使用 ONNX Runtime 运行模型并验证结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this case, the operator is an operator that is user implemented and "
"registered to PyTorch."
msgstr "在此情况下，该操作符是用户实现并注册到 PyTorch 的操作符。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the following example, we would like to use a custom operator that takes "
"one tensor input, and returns one output. The operator adds the input to "
"itself, and returns the rounded result."
msgstr "在以下示例中，我们希望使用一个自定义操作符，该操作符接受一个张量输入，并返回一个输出。该操作符将输入加上它自己，并返回已舍入的结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Firstly, we assume the custom operator is implemented and registered with "
"``torch.library.custom_op()``. You can refer to `Creating new custom ops in "
"Python "
"<https://pytorch.org/docs/stable/library.html#torch.library.custom_op>`_ for"
" a detailed guide on how to create custom operators."
msgstr ""
"首先，我们假设自定义操作符已通过 ``torch.library.custom_op()`` 实现并注册。您可以参考 `在 Python "
"中创建新的自定义操作符 "
"<https://pytorch.org/docs/stable/library.html#torch.library.custom_op>`_ "
"以获取关于如何创建自定义操作符的详细指南。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The translation is using our custom implementation to translate the "
"``torch.ops.mylibrary.add_and_round_op.default`` operator in the "
":class:`torch.export.ExportedProgram`` to the ONNX operator ``Add`` and "
"``Round``."
msgstr ""
"转换是使用我们的自定义实现将 :class:`torch.export.ExportedProgram`` 中的 "
"``torch.ops.mylibrary.add_and_round_op.default`` 操作符转换为 ONNX 操作符 ``Add`` 和 "
"``Round``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Finally we verify the results."
msgstr "最后，我们验证结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Congratulations! In this tutorial, we explored the "
"``custom_translation_table`` option and discovered how to create custom "
"implementations for unsupported or existing PyTorch operators using ONNX "
"Script."
msgstr ""
"恭喜！在本教程中，我们探索了 ``custom_translation_table`` 选项，并学习了如何使用 ONNX Script 为不支持或现有的"
" PyTorch 操作符创建自定义实现。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we leveraged ONNX Runtime to execute the model and compare the "
"results with PyTorch, providing us with a comprehensive understanding of "
"handling unsupported operators in the ONNX ecosystem."
msgstr ""
"最后，我们利用 ONNX Runtime 执行模型并将结果与 PyTorch 进行比较，为我们提供了对在 ONNX "
"生态系统中处理不支持的操作符的全面理解。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  2.710 seconds)"
msgstr "**脚本的总运行时间：** ( 0 分钟  2.710 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: onnx_registry_tutorial.py "
"<onnx_registry_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: onnx_registry_tutorial.py "
"<onnx_registry_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: onnx_registry_tutorial.ipynb "
"<onnx_registry_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: onnx_registry_tutorial.ipynb "
"<onnx_registry_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**00:07.033** total execution time for **beginner_onnx** files:"
msgstr "**00:07.033** 初学者_onnx 文件的总执行时间："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_onnx_export_control_flow_model_to_onnx_tutorial.py` "
"(``export_control_flow_model_to_onnx_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_onnx_export_control_flow_model_to_onnx_tutorial.py`（``export_control_flow_model_to_onnx_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:02.749"
msgstr "00:02.749"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_onnx_onnx_registry_tutorial.py` "
"(``onnx_registry_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_onnx_onnx_registry_tutorial.py`（``onnx_registry_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:02.710"
msgstr "00:02.710"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_onnx_export_simple_model_to_onnx_tutorial.py` "
"(``export_simple_model_to_onnx_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_onnx_export_simple_model_to_onnx_tutorial.py`（``export_simple_model_to_onnx_tutorial.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:01.574"
msgstr "00:01.574"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_onnx_intro_onnx.py` (``intro_onnx.py``)"
msgstr ":ref:`sphx_glr_beginner_onnx_intro_onnx.py`（``intro_onnx.py``）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_profiler.py>` to download the "
"full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_beginner_profiler.py>` 下载完整的示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Profiling your PyTorch Module"
msgstr "分析您的 PyTorch 模块"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author:** `Suraj Subramanian <https://github.com/suraj813>`_"
msgstr "**作者：** `Suraj Subramanian <https://github.com/suraj813>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch includes a profiler API that is useful to identify the time and "
"memory costs of various PyTorch operations in your code. Profiler can be "
"easily integrated in your code, and the results can be printed as a table or"
" returned in a JSON trace file."
msgstr ""
"PyTorch 包含一个分析器 API，可用于识别代码中各种 PyTorch "
"操作的时间和内存成本。分析器可以轻松集成到您的代码中，结果可以作为表格打印，或者以 JSON 跟踪文件返回。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Profiler supports multithreaded models. Profiler runs in the same thread as "
"the operation but it will also profile child operators that might run in "
"another thread. Concurrently-running profilers will be scoped to their own "
"thread to prevent mixing of results."
msgstr ""
"分析器支持多线程模型。分析器在与操作相同的线程中运行，但它还会分析可能运行在其他线程中的子操作符。并发运行的分析器将限制在它们自己的线程中以防止结果混杂。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"PyTorch 1.8 introduces the new API that will replace the older profiler API "
"in the future releases. Check the new API at `this page "
"<https://pytorch.org/docs/master/profiler.html>`__."
msgstr ""
"PyTorch 1.8 引入了新的 API，未来版本中将替代较旧的分析器 API。查看新 API 请参考 `此页面 "
"<https://pytorch.org/docs/master/profiler.html>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Head on over to `this recipe "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`__ for "
"a quicker walkthrough of Profiler API usage."
msgstr ""
"前往 `这个教程 "
"<https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html>`__ "
"快速了解分析器 API 的用法。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Performance debugging using Profiler"
msgstr "使用分析器进行性能调试"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Profiler can be useful to identify performance bottlenecks in your models. "
"In this example, we build a custom module that performs two sub-tasks:"
msgstr "分析器可用于识别模型中的性能瓶颈。在此示例中，我们构建了一个执行两个子任务的自定义模块："

#: ../../beginner/vt_tutorial.rst:783
msgid "a linear transformation on the input, and"
msgstr "对输入进行线性变换，以及"

#: ../../beginner/vt_tutorial.rst:783
msgid "use the transformation result to get indices on a mask tensor."
msgstr "使用变换结果获取掩码张量上的索引。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We wrap the code for each sub-task in separate labelled context managers "
"using ``profiler.record_function(\"label\")``. In the profiler output, the "
"aggregate performance metrics of all operations in the sub-task will show up"
" under its corresponding label."
msgstr ""
"我们使用 ``profiler.record_function(\"label\")`` "
"将每个子任务的代码包裹在单独的标记上下文管理器中。在分析器输出中，子任务中所有操作的整体性能指标将显示在其对应的标签下。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that using Profiler incurs some overhead, and is best used only for "
"investigating code. Remember to remove it if you are benchmarking runtimes."
msgstr "请注意，使用分析器会带来一些开销，最好仅在调查代码时使用。若进行运行时基准测试，请记得将其移除。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Profile the forward pass"
msgstr "分析前向传播过程"

#: ../../beginner/vt_tutorial.rst:783
msgid "We initialize random input and mask tensors, and the model."
msgstr "我们初始化随机输入和掩码张量，以及模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before we run the profiler, we warm-up CUDA to ensure accurate performance "
"benchmarking. We wrap the forward pass of our module in the "
"``profiler.profile`` context manager. The ``with_stack=True`` parameter "
"appends the file and line number of the operation in the trace."
msgstr ""
"在运行分析器之前，我们先热启动 CUDA，以确保性能基准测试的准确性。我们将模块的前向传播封装在 ``profiler.profile`` "
"上下文管理器中。参数 ``with_stack=True`` 会在跟踪中附加操作的文件和行号。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``with_stack=True`` incurs an additional overhead, and is better suited for "
"investigating code. Remember to remove it if you are benchmarking "
"performance."
msgstr "``with_stack=True`` 会带来额外的开销，因此更适用于调查代码。若进行性能基准测试，请记得将其移除。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Print profiler results"
msgstr "打印分析器结果"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Finally, we print the profiler results. ``profiler.key_averages`` aggregates"
" the results by operator name, and optionally by input shapes and/or stack "
"trace events. Grouping by input shapes is useful to identify which tensor "
"shapes are utilized by the model."
msgstr ""
"最后，我们打印分析器结果。``profiler.key_averages`` "
"按操作符名称汇总结果，并可选择按输入形状和/或堆栈跟踪事件分组。按输入形状分组有助于识别模型使用的张量形状。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here, we use ``group_by_stack_n=5`` which aggregates runtimes by the "
"operation and its traceback (truncated to the most recent 5 events), and "
"display the events in the order they are registered. The table can also be "
"sorted by passing a ``sort_by`` argument (refer to the `docs "
"<https://pytorch.org/docs/stable/autograd.html#profiler>`__ for valid "
"sorting keys)."
msgstr ""
"在这里，我们使用 ``group_by_stack_n=5``，按操作及其堆栈回溯（截取最近 5 "
"个事件）汇总运行时间，并按注册顺序显示事件。表格还可以通过传递 ``sort_by`` 参数进行排序（有关有效的排序键，请参考 `文档 "
"<https://pytorch.org/docs/stable/autograd.html#profiler>`__）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When running profiler in a notebook, you might see entries like ``<ipython-"
"input-18-193a910735e8>(13): forward`` instead of filenames in the "
"stacktrace. These correspond to ``<notebook-cell>(line number): calling-"
"function``."
msgstr ""
"在 notebook 中运行分析器时，您可能会在堆栈跟踪中看到类似 ``<ipython-input-18-193a910735e8>(13): "
"forward`` 这样的条目，而不是文件名。这些对应于 ``<notebook-cell>(行号): 调用函数``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Improve memory performance"
msgstr "改进内存性能"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Note that the most expensive operations - in terms of memory and time - are "
"at ``forward (10)`` representing the operations within MASK INDICES. Let’s "
"try to tackle the memory consumption first. We can see that the ``.to()`` "
"operation at line 12 consumes 953.67 Mb. This operation copies ``mask`` to "
"the CPU. ``mask`` is initialized with a ``torch.double`` datatype. Can we "
"reduce the memory footprint by casting it to ``torch.float`` instead?"
msgstr ""
"注意，在内存和时间方面最耗资源的操作是 ``forward (10)``，表示掩码索引操作中的操作。让我们先尝试解决内存消耗问题。我们可以看到，第 12"
" 行的 ``.to()`` 操作消耗了 953.67 MB。此操作将 ``mask`` 复制到 CPU 上。``mask`` 的初始化数据类型是 "
"``torch.double``。我们能通过将其转换为 ``torch.float`` 来减少内存占用吗？"

#: ../../beginner/vt_tutorial.rst:783
msgid "The CPU memory footprint for this operation has halved."
msgstr "此操作的 CPU 内存占用减少了一半。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Improve time performance"
msgstr "改进时间性能"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"While the time consumed has also reduced a bit, it’s still too high. Turns "
"out copying a matrix from CUDA to CPU is pretty expensive! The "
"``aten::copy_`` operator in ``forward (12)`` copies ``mask`` to CPU so that "
"it can use the NumPy ``argwhere`` function. ``aten::copy_`` at "
"``forward(13)`` copies the array back to CUDA as a tensor. We could "
"eliminate both of these if we use a ``torch`` function ``nonzero()`` here "
"instead."
msgstr ""
"虽然时间消耗也有所减少，但仍然过高。事实证明，将矩阵从 CUDA 复制到 CPU 非常昂贵！``forward (12)`` 处的 "
"``aten::copy_`` 操作符将 ``mask`` 复制到 CPU，以便使用 NumPy 的 ``argwhere`` 函数。``forward"
" (13)`` 处的 ``aten::copy_`` 将数组作为张量复制回 CUDA。如果我们在此处使用 ``torch`` 函数 "
"``nonzero()``，可以消除这两种情况。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We have seen how Profiler can be used to investigate time and memory "
"bottlenecks in PyTorch models. Read more about Profiler here:"
msgstr "我们已经了解了如何利用分析器调查 PyTorch 模型中的时间和内存瓶颈。阅读更多关于分析器的信息："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Profiler Usage Recipe "
"<https://pytorch.org/tutorials/recipes/recipes/profiler.html>`__"
msgstr ""
"`使用Profiler的教程 "
"<https://pytorch.org/tutorials/recipes/recipes/profiler.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Profiling RPC-Based Workloads "
"<https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html>`__"
msgstr ""
"`分析基于RPC的工作负载 "
"<https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Profiler API Docs "
"<https://pytorch.org/docs/stable/autograd.html?highlight=profiler#profiler>`__"
msgstr ""
"`Profiler API文档 "
"<https://pytorch.org/docs/stable/autograd.html?highlight=profiler#profiler>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Python source code: profiler.py <profiler.py>`"
msgstr ":download:`下载Python源代码: profiler.py <profiler.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ":download:`Download Jupyter notebook: profiler.ipynb <profiler.ipynb>`"
msgstr ":download:`下载Jupyter笔记本: profiler.ipynb <profiler.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch Cheat Sheet"
msgstr "PyTorch速查表"

#: ../../beginner/vt_tutorial.rst:783
msgid "General"
msgstr "通用功能"

#: ../../beginner/vt_tutorial.rst:783
msgid "Neural Network API"
msgstr "神经网络API"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `autograd <https://pytorch.org/docs/stable/autograd.html>`__, `nn "
"<https://pytorch.org/docs/stable/nn.html>`__, `functional "
"<https://pytorch.org/docs/stable/nn.html#torch-nn-functional>`__ and `optim "
"<https://pytorch.org/docs/stable/optim.html>`__"
msgstr ""
"请参阅`autograd <https://pytorch.org/docs/stable/autograd.html>`__，`nn "
"<https://pytorch.org/docs/stable/nn.html>`__，`functional "
"<https://pytorch.org/docs/stable/nn.html#torch-nn-functional>`__和`optim "
"<https://pytorch.org/docs/stable/optim.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "See `onnx <https://pytorch.org/docs/stable/onnx.html>`__"
msgstr "请参阅`onnx <https://pytorch.org/docs/stable/onnx.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Vision"
msgstr "视觉"

#: ../../beginner/vt_tutorial.rst:783
msgid "See `torchvision <https://pytorch.org/vision/stable/index.html>`__"
msgstr "请参阅`torchvision <https://pytorch.org/vision/stable/index.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Distributed Training"
msgstr "分布式训练"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `distributed <https://pytorch.org/docs/stable/distributed.html>`__ and "
"`multiprocessing <https://pytorch.org/docs/stable/multiprocessing.html>`__"
msgstr ""
"请参阅`distributed "
"<https://pytorch.org/docs/stable/distributed.html>`__和`multiprocessing "
"<https://pytorch.org/docs/stable/multiprocessing.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Creation"
msgstr "创建"

#: ../../beginner/vt_tutorial.rst:783
msgid "See `tensor <https://pytorch.org/docs/stable/tensors.html>`__"
msgstr "请参阅`tensor <https://pytorch.org/docs/stable/tensors.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dimensionality"
msgstr "维度操作"

#: ../../beginner/vt_tutorial.rst:783
msgid "Algebra"
msgstr "代数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `math operations "
"<https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations>`__"
msgstr ""
"请参阅`数学操作 <https://pytorch.org/docs/stable/torch.html?highlight=mm#math-"
"operations>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "GPU Usage"
msgstr "GPU使用"

#: ../../beginner/vt_tutorial.rst:783
msgid "See `cuda <https://pytorch.org/docs/stable/cuda.html>`__"
msgstr "请参阅`cuda <https://pytorch.org/docs/stable/cuda.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Deep Learning"
msgstr "深度学习"

#: ../../beginner/vt_tutorial.rst:783
msgid "See `nn <https://pytorch.org/docs/stable/nn.html>`__"
msgstr "请参阅`nn <https://pytorch.org/docs/stable/nn.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `loss functions <https://pytorch.org/docs/stable/nn.html#loss-"
"functions>`__"
msgstr "请参阅`损失函数 <https://pytorch.org/docs/stable/nn.html#loss-functions>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `activation functions <https://pytorch.org/docs/stable/nn.html#non-"
"linear-activations-weighted-sum-nonlinearity>`__"
msgstr ""
"请参阅`激活函数 <https://pytorch.org/docs/stable/nn.html#non-linear-activations-"
"weighted-sum-nonlinearity>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimizers"
msgstr "优化器"

#: ../../beginner/vt_tutorial.rst:783
msgid "See `optimizers <https://pytorch.org/docs/stable/optim.html>`__"
msgstr "请参阅`优化器 <https://pytorch.org/docs/stable/optim.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Learning rate scheduling"
msgstr "学习率调度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `learning rate scheduler "
"<https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate>`__"
msgstr ""
"请参阅`学习率调度器 <https://pytorch.org/docs/stable/optim.html#how-to-adjust-"
"learning-rate>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Data Utilities"
msgstr "数据工具"

#: ../../beginner/vt_tutorial.rst:783
msgid "Datasets"
msgstr "数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `datasets "
"<https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`__"
msgstr ""
"请参阅`数据集 "
"<https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Dataloaders and ``DataSamplers``"
msgstr "数据加载器和``DataSamplers``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"See `dataloader "
"<https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__"
msgstr ""
"请参阅`数据加载器 "
"<https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Also see"
msgstr "也请参阅"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Deep Learning with PyTorch: A 60 Minute Blitz "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`__"
msgstr ""
"`使用PyTorch进行深度学习: 60分钟速成 "
"<https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "`PyTorch Forums <https://discuss.pytorch.org/>`__"
msgstr "`PyTorch论坛 <https://discuss.pytorch.org/>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`PyTorch for Numpy users <https://github.com/wkentaro/pytorch-for-numpy-"
"users>`__"
msgstr ""
"`为Numpy用户准备的PyTorch <https://github.com/wkentaro/pytorch-for-numpy-users>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "Learning PyTorch with Examples"
msgstr "通过示例学习PyTorch"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Author**: `Justin Johnson <https://github.com/jcjohnson/pytorch-"
"examples>`_"
msgstr ""
"**作者**: `Justin Johnson <https://github.com/jcjohnson/pytorch-examples>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This is one of our older PyTorch tutorials. You can view our latest beginner"
" content in `Learn the Basics "
"<https://pytorch.org/tutorials/beginner/basics/intro.html>`_."
msgstr ""
"这是我们较早的PyTorch教程之一。您可以在`学习基础知识 "
"<https://pytorch.org/tutorials/beginner/basics/intro.html>`_中查看最新的初学者内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This tutorial introduces the fundamental concepts of `PyTorch "
"<https://github.com/pytorch/pytorch>`__ through self-contained examples."
msgstr "本教程通过独立的示例介绍了`PyTorch <https://github.com/pytorch/pytorch>`__的基本概念。"

#: ../../beginner/vt_tutorial.rst:783
msgid "At its core, PyTorch provides two main features:"
msgstr "PyTorch核心提供两大主要功能:"

#: ../../beginner/vt_tutorial.rst:783
msgid "An n-dimensional Tensor, similar to numpy but can run on GPUs"
msgstr "一个n维张量，类似于numpy，但可以在GPU上运行"

#: ../../beginner/vt_tutorial.rst:783
msgid "Automatic differentiation for building and training neural networks"
msgstr "用于构建和训练神经网络的自动微分功能"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will use a problem of fitting :math:`y=\\sin(x)` with a third order "
"polynomial as our running example. The network will have four parameters, "
"and will be trained with gradient descent to fit random data by minimizing "
"the Euclidean distance between the network output and the true output."
msgstr ""
"我们将以拟合 :math:`y=\\sin(x)` "
"的三阶多项式问题作为实例。网络将有四个参数，并通过梯度下降训练以最小化网络输出与真实输出之间的欧几里得距离来拟合随机数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can browse the individual examples at the :ref:`end of this page "
"<examples-download>`."
msgstr "您可以在:ref:`本页末尾 <examples-download>`浏览各个示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Table of Contents"
msgstr "目录"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Before introducing PyTorch, we will first implement the network using numpy."
msgstr "在介绍PyTorch之前，我们将首先使用numpy来实现网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Numpy provides an n-dimensional array object, and many functions for "
"manipulating these arrays. Numpy is a generic framework for scientific "
"computing; it does not know anything about computation graphs, or deep "
"learning, or gradients. However we can easily use numpy to fit a third order"
" polynomial to sine function by manually implementing the forward and "
"backward passes through the network using numpy operations:"
msgstr ""
"Numpy提供了一个n维数组对象，以及许多操作这些数组的函数。Numpy是一个通用的科学计算框架；它不涉及计算图、深度学习或梯度。但我们可以轻松使用numpy通过手动实现网络的前向和后向传播来拟合正弦函数的三阶多项式："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Numpy is a great framework, but it cannot utilize GPUs to accelerate its "
"numerical computations. For modern deep neural networks, GPUs often provide "
"speedups of `50x or greater <https://github.com/jcjohnson/cnn-"
"benchmarks>`__, so unfortunately numpy won't be enough for modern deep "
"learning."
msgstr ""
"Numpy是一个很棒的框架，但它无法利用GPU加速其数值计算。对于现代深度神经网络，GPU通常提供`50倍及以上的加速 "
"<https://github.com/jcjohnson/cnn-benchmarks>`__，因此遗憾的是numpy不足以支持现代深度学习。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here we introduce the most fundamental PyTorch concept: the **Tensor**. A "
"PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an "
"n-dimensional array, and PyTorch provides many functions for operating on "
"these Tensors. Behind the scenes, Tensors can keep track of a computational "
"graph and gradients, but they're also useful as a generic tool for "
"scientific computing."
msgstr ""
"在这里，我们介绍PyTorch的最基本概念：**张量（Tensor）**。PyTorch张量在概念上与numpy数组完全相同：张量是一个n维数组，而PyTorch提供了许多操作这些张量的函数。在后台，张量可以跟踪计算图和梯度，但它们也是科学计算的通用工具。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their "
"numeric computations. To run a PyTorch Tensor on GPU, you simply need to "
"specify the correct device."
msgstr "与numpy不同的是，PyTorch张量可以利用GPU来加速其数值计算。要在GPU上运行PyTorch张量，只需要指定正确的设备即可。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here we use PyTorch Tensors to fit a third order polynomial to sine "
"function. Like the numpy example above we need to manually implement the "
"forward and backward passes through the network:"
msgstr "在这里我们使用PyTorch张量来拟合正弦函数的三阶多项式。与上述numpy示例类似，我们需要手动实现网络的前向和后向传播："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the above examples, we had to manually implement both the forward and "
"backward passes of our neural network. Manually implementing the backward "
"pass is not a big deal for a small two-layer network, but can quickly get "
"very hairy for large complex networks."
msgstr ""
"在上述示例中，我们不得不手动实现我们神经网络的前向和后向传播。对于一个小型的两层网络来说，手动实现后向传播并不难，但对于大型复杂网络来说可能会变得非常棘手。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Thankfully, we can use `automatic differentiation "
"<https://en.wikipedia.org/wiki/Automatic_differentiation>`__ to automate the"
" computation of backward passes in neural networks. The **autograd** package"
" in PyTorch provides exactly this functionality. When using autograd, the "
"forward pass of your network will define a **computational graph**; nodes in"
" the graph will be Tensors, and edges will be functions that produce output "
"Tensors from input Tensors. Backpropagating through this graph then allows "
"you to easily compute gradients."
msgstr ""
"幸运的是，我们可以使用`自动微分 "
"<https://en.wikipedia.org/wiki/Automatic_differentiation>`__来自动化计算神经网络中的后向传播。PyTorch中的**autograd**包正是提供了这种功能。当使用autograd时，网络的前向传播将定义一个**计算图**；图中的节点是张量，边是从输入张量生成输出张量的函数。通过这个图进行反向传播可以轻松计算梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This sounds complicated, it's pretty simple to use in practice. Each Tensor "
"represents a node in a computational graph. If ``x`` is a Tensor that has "
"``x.requires_grad=True`` then ``x.grad`` is another Tensor holding the "
"gradient of ``x`` with respect to some scalar value."
msgstr ""
"这听起来很复杂，但实际上使用起来非常简单。每个张量代表计算图中的一个节点。如果``x``是一个张量，且``x.requires_grad=True``，那么``x.grad``是另一个包含``x``相对于某个标量值的梯度的张量。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here we use PyTorch Tensors and autograd to implement our fitting sine wave "
"with third order polynomial example; now we no longer need to manually "
"implement the backward pass through the network:"
msgstr "在这里我们使用PyTorch张量和autograd来实现我们拟合正弦波的三阶多项式示例；现在我们不再需要手动实现网络的后向传播："

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Defining new autograd functions"
msgstr "PyTorch: 定义新的autograd函数"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Under the hood, each primitive autograd operator is really two functions "
"that operate on Tensors. The **forward** function computes output Tensors "
"from input Tensors. The **backward** function receives the gradient of the "
"output Tensors with respect to some scalar value, and computes the gradient "
"of the input Tensors with respect to that same scalar value."
msgstr ""
"在底层，每个原始的autograd操作符实际上有两个处理张量的函数。**前向传播**函数从输入张量计算输出张量。**后向传播**函数接收输出张量相对于某个标量值的梯度，并计算输入张量相对于同一标量值的梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In PyTorch we can easily define our own autograd operator by defining a "
"subclass of ``torch.autograd.Function`` and implementing the ``forward`` and"
" ``backward`` functions. We can then use our new autograd operator by "
"constructing an instance and calling it like a function, passing Tensors "
"containing input data."
msgstr ""
"在PyTorch中，我们可以通过定义``torch.autograd.Function``的子类并实现``forward``和``backward``函数来轻松定义自己的autograd操作符。然后我们可以通过创建实例并将输入数据的张量传递给它，如同调用函数一样来使用新的autograd操作符。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example we define our model as :math:`y=a+b P_3(c+dx)` instead of "
":math:`y=a+bx+cx^2+dx^3`, where "
":math:`P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)` is the `Legendre "
"polynomial`_ of degree three. We write our own custom autograd function for "
"computing forward and backward of :math:`P_3`, and use it to implement our "
"model:"
msgstr ""
"在此示例中，我们将模型定义为 :math:`y=a+b P_3(c+dx)` 而不是 :math:`y=a+bx+cx^2+dx^3`，其中 "
":math:`P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)` "
"是三阶`勒让德多项式`_。我们编写自己的自定义autograd函数来计算 :math:`P_3` 的前向和后向传播，并使用它来实现我们的模型："

#: ../../beginner/vt_tutorial.rst:783
msgid "``nn`` module"
msgstr "``nn``模块"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: ``nn``"
msgstr "PyTorch: ``nn``"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Computational graphs and autograd are a very powerful paradigm for defining "
"complex operators and automatically taking derivatives; however for large "
"neural networks raw autograd can be a bit too low-level."
msgstr "计算图和autograd是定义复杂操作并自动生成导数的非常强大的范式；然而对于大型神经网络，原始的autograd可能有点过于低级。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When building neural networks we frequently think of arranging the "
"computation into **layers**, some of which have **learnable parameters** "
"which will be optimized during learning."
msgstr "在构建神经网络时，我们通常将计算组织为带有一些**可学习参数**的**层**，这些参数将在学习期间进行优化。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In TensorFlow, packages like `Keras <https://github.com/fchollet/keras>`__, "
"`TensorFlow-Slim <https://github.com/google-research/tf-slim>`__, and "
"`TFLearn <http://tflearn.org/>`__ provide higher-level abstractions over raw"
" computational graphs that are useful for building neural networks."
msgstr ""
"在TensorFlow中，像`Keras <https://github.com/fchollet/keras>`__，`TensorFlow-Slim"
" <https://github.com/google-research/tf-slim>`__，以及`TFLearn "
"<http://tflearn.org/>`__等软件包提供了在原始计算图上的较高级抽象，非常适用于构建神经网络。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In PyTorch, the ``nn`` package serves this same purpose. The ``nn`` package "
"defines a set of **Modules**, which are roughly equivalent to neural network"
" layers. A Module receives input Tensors and computes output Tensors, but "
"may also hold internal state such as Tensors containing learnable "
"parameters. The ``nn`` package also defines a set of useful loss functions "
"that are commonly used when training neural networks."
msgstr ""
"在PyTorch中，``nn``包也具有相同的用途。``nn``包定义了一组**模块**，它们大致相当于神经网络层。一个模块接收输入张量并计算输出张量，但也可能包含内部状态，例如包含可学习参数的张量。``nn``包还定义了一组有用的损失函数，这些函数在训练神经网络时常用。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example we use the ``nn`` package to implement our polynomial model "
"network:"
msgstr "在这个示例中，我们使用``nn``包来实现我们的多项式模型网络："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Up to this point we have updated the weights of our models by manually "
"mutating the Tensors holding learnable parameters with ``torch.no_grad()``. "
"This is not a huge burden for simple optimization algorithms like stochastic"
" gradient descent, but in practice we often train neural networks using more"
" sophisticated optimizers like ``AdaGrad``, ``RMSProp``, ``Adam``, and "
"other."
msgstr ""
"到目前为止，我们通过使用``torch.no_grad()``手动修改包含可学习参数的张量来更新我们的模型的权重。这对于像随机梯度下降这样的简单优化算法来说并不麻烦，但在实践中我们通常使用更复杂的优化器来训练神经网络，如``AdaGrad``，``RMSProp``，``Adam``等。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The ``optim`` package in PyTorch abstracts the idea of an optimization "
"algorithm and provides implementations of commonly used optimization "
"algorithms."
msgstr "PyTorch中的``optim``包抽象了优化算法的概念，并提供了常用优化算法的实现。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example we will use the ``nn`` package to define our model as "
"before, but we will optimize the model using the ``RMSprop`` algorithm "
"provided by the ``optim`` package:"
msgstr ""
"在这个示例中，我们将像以前一样使用``nn``包定义我们的模型，但我们将使用``optim``包提供的``RMSprop``算法来优化该模型："

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch: Custom ``nn`` Modules"
msgstr "PyTorch: 自定义``nn``模块"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Sometimes you will want to specify models that are more complex than a "
"sequence of existing Modules; for these cases you can define your own "
"Modules by subclassing ``nn.Module`` and defining a ``forward`` which "
"receives input Tensors and produces output Tensors using other modules or "
"other autograd operations on Tensors."
msgstr ""
"有时你可能希望指定比现有模块序列更复杂的模型；在这些情况下，你可以通过子类化``nn.Module``并定义一个接收输入张量并使用其他模块或张量上的autograd操作生成输出张量的``forward``方法来定义自己的模块。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this example we implement our third order polynomial as a custom Module "
"subclass:"
msgstr "在这个示例中，我们将我们的三阶多项式实现为一个自定义模块子类："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"As an example of dynamic graphs and weight sharing, we implement a very "
"strange model: a third-fifth order polynomial that on each forward pass "
"chooses a random number between 3 and 5 and uses that many orders, reusing "
"the same weights multiple times to compute the fourth and fifth order."
msgstr ""
"作为动态图和权重共享的示例，我们实现了一个非常奇怪的模型：一个三到五阶多项式，它在每次前向传播时选择一个随机数（范围从3到5），并使用那么多阶数，重复使用相同的权重多次来计算第四和第五阶。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For this model we can use normal Python flow control to implement the loop, "
"and we can implement weight sharing by simply reusing the same parameter "
"multiple times when defining the forward pass."
msgstr "对于这个模型，我们可以使用普通的Python流程控制来实现循环，并通过简单地在定义前向传播时多次使用相同的参数来实现权重共享。"

#: ../../beginner/vt_tutorial.rst:783
msgid "We can easily implement this model as a Module subclass:"
msgstr "我们可以轻松将此模型实现为模块子类："

#: ../../beginner/vt_tutorial.rst:783
msgid "Examples"
msgstr "示例"

#: ../../beginner/vt_tutorial.rst:783
msgid "You can browse the above examples here."
msgstr "您可以在这里浏览上述示例。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_saving_loading_models.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_beginner_saving_loading_models.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving and Loading Models"
msgstr "保存和加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This document provides solutions to a variety of use cases regarding the "
"saving and loading of PyTorch models. Feel free to read the whole document, "
"or just skip to the code you need for a desired use case."
msgstr "本文档提供了有关保存和加载 PyTorch 模型的各种用例的解决方案。可以阅读完整文档，也可直接跳到需要的代码段以满足特定用例。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When it comes to saving and loading models, there are three core functions "
"to be familiar with:"
msgstr "在保存和加载模型时，有三个核心功能需要熟悉："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`torch.save "
"<https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save>`__: "
"Saves a serialized object to disk. This function uses Python’s `pickle "
"<https://docs.python.org/3/library/pickle.html>`__ utility for "
"serialization. Models, tensors, and dictionaries of all kinds of objects can"
" be saved using this function."
msgstr ""
"`torch.save "
"<https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save>`__: "
"将序列化对象保存到磁盘。此函数使用 Python 的 `pickle "
"<https://docs.python.org/3/library/pickle.html>`__ "
"工具进行序列化。可以使用此函数保存模型、张量和各种对象字典。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`torch.load "
"<https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load>`__:"
" Uses `pickle <https://docs.python.org/3/library/pickle.html>`__\\ ’s "
"unpickling facilities to deserialize pickled object files to memory. This "
"function also facilitates the device to load the data into (see `Saving & "
"Loading Model Across Devices <#saving-loading-model-across-devices>`__)."
msgstr ""
"`torch.load "
"<https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load>`__:"
" 使用 `pickle <https://docs.python.org/3/library/pickle.html>`__ "
"的反序列化功能将对象文件从磁盘加载到内存中。此函数还支持将数据加载到指定设备（参见 `跨设备保存和加载模型 <#saving-loading-"
"model-across-devices>`）。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`torch.nn.Module.load_state_dict "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict>`__:"
" Loads a model’s parameter dictionary using a deserialized *state_dict*. For"
" more information on *state_dict*, see `What is a state_dict? <#what-is-a-"
"state-dict>`__."
msgstr ""
"`torch.nn.Module.load_state_dict "
"<https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=load_state_dict#torch.nn.Module.load_state_dict>`__:"
" 使用反序列化的 *state_dict* 加载模型的参数字典。有关 *state_dict* 的更多信息，请参见 `什么是 state_dict? "
"<#what-is-a-state-dict>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Contents:**"
msgstr "**目录：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "`What is a state_dict? <#what-is-a-state-dict>`__"
msgstr "`什么是 state_dict? <#what-is-a-state-dict>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Saving & Loading Model for Inference <#saving-loading-model-for-"
"inference>`__"
msgstr "`用于推断的模型保存和加载 <#saving-loading-model-for-inference>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Saving & Loading a General Checkpoint <#saving-loading-a-general-"
"checkpoint-for-inference-and-or-resuming-training>`__"
msgstr ""
"`保存和加载通用检查点 <#saving-loading-a-general-checkpoint-for-inference-and-or-"
"resuming-training>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Saving Multiple Models in One File <#saving-multiple-models-in-one-file>`__"
msgstr "`在一个文件中保存多个模型 <#saving-multiple-models-in-one-file>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Warmstarting Model Using Parameters from a Different Model <#warmstarting-"
"model-using-parameters-from-a-different-model>`__"
msgstr ""
"`使用不同模型的参数进行热启动 <#warmstarting-model-using-parameters-from-a-different-"
"model>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Saving & Loading Model Across Devices <#saving-loading-model-across-"
"devices>`__"
msgstr "`跨设备保存和加载模型 <#saving-loading-model-across-devices>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is a ``state_dict``?"
msgstr "什么是 ``state_dict``？"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In PyTorch, the learnable parameters (i.e. weights and biases) of an "
"``torch.nn.Module`` model are contained in the model’s *parameters* "
"(accessed with ``model.parameters()``). A *state_dict* is simply a Python "
"dictionary object that maps each layer to its parameter tensor. Note that "
"only layers with learnable parameters (convolutional layers, linear layers, "
"etc.) and registered buffers (batchnorm's running_mean) have entries in the "
"model’s *state_dict*. Optimizer objects (``torch.optim``) also have a "
"*state_dict*, which contains information about the optimizer's state, as "
"well as the hyperparameters used."
msgstr ""
"在 PyTorch 中，可学习参数（即权重和偏置）存储在 ``torch.nn.Module`` 模型的 *parameters* 中（通过 "
"``model.parameters()`` 访问）。*state_dict* 是一个简单的 Python "
"字典对象，它将每一层映射到其参数张量。需要注意的是，仅有具有可学习参数（如卷积层、线性层等）的层以及注册的缓冲区（例如 batchnorm 的 "
"running_mean）会进入模型的 *state_dict* 中。优化器对象（``torch.optim``）也拥有 "
"*state_dict*，其中包含优化器的状态信息及使用的超参数。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Because *state_dict* objects are Python dictionaries, they can be easily "
"saved, updated, altered, and restored, adding a great deal of modularity to "
"PyTorch models and optimizers."
msgstr ""
"由于 *state_dict* 对象是 Python 字典，因此它可以轻松保存、更新、更改和恢复，从而为 PyTorch "
"模型和优化器添加了很大的模块化灵活性。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let’s take a look at the *state_dict* from the simple model used in the "
"`Training a classifier "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-"
"glr-beginner-blitz-cifar10-tutorial-py>`__ tutorial."
msgstr ""
"让我们看看 `训练分类器 "
"<https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-"
"glr-beginner-blitz-cifar10-tutorial-py>`__ 教程中所使用的简单模型的 *state_dict*。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Output:**"
msgstr "**输出：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving & Loading Model for Inference"
msgstr "用于推断的模型保存和加载"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save/Load ``state_dict`` (Recommended)"
msgstr "保存/加载 ``state_dict``（推荐）"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Save:**"
msgstr "**保存：**"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Load:**"
msgstr "**加载：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The 1.6 release of PyTorch switched ``torch.save`` to use a new zip file-"
"based format. ``torch.load`` still retains the ability to load files in the "
"old format. If for any reason you want ``torch.save`` to use the old format,"
" pass the ``kwarg`` parameter ``_use_new_zipfile_serialization=False``."
msgstr ""
"PyTorch 1.6 版本切换了 ``torch.save`` 的实现，改为使用新的基于 zip 文件的格式。而 ``torch.load`` "
"仍然保留了加载旧格式文件的功能。如果由于任何原因需要 ``torch.save`` 使用旧格式，可以将参数 "
"``_use_new_zipfile_serialization=False`` 传递给 ``kwarg``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When saving a model for inference, it is only necessary to save the trained "
"model’s learned parameters. Saving the model’s *state_dict* with the "
"``torch.save()`` function will give you the most flexibility for restoring "
"the model later, which is why it is the recommended method for saving "
"models."
msgstr ""
"在为推断保存模型时，只需要保存训练模型的已学到的参数即可。使用 ``torch.save()`` 函数保存模型的 *state_dict* "
"可以为后续恢复模型提供最大灵活性，因此是推荐的方法。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A common PyTorch convention is to save models using either a ``.pt`` or "
"``.pth`` file extension."
msgstr "PyTorch 的一个常见约定是使用 ``.pt`` 或 ``.pth`` 文件扩展名保存模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Remember that you must call ``model.eval()`` to set dropout and batch "
"normalization layers to evaluation mode before running inference. Failing to"
" do this will yield inconsistent inference results."
msgstr ""
"记住，在运行推断之前，必须通过调用 ``model.eval()`` 将 dropout "
"和批规范化层设置为评估模式。如果没有这样做，将会产生不一致的推断结果。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Notice that the ``load_state_dict()`` function takes a dictionary object, "
"NOT a path to a saved object. This means that you must deserialize the saved"
" *state_dict* before you pass it to the ``load_state_dict()`` function. For "
"example, you CANNOT load using ``model.load_state_dict(PATH)``."
msgstr ""
"注意，``load_state_dict()`` 函数接受的是字典对象，而不是保存的对象路径。这意味着在将保存的 *state_dict* 传递给 "
"``load_state_dict()`` 函数之前，必须先对其进行反序列化。例如，不能通过 "
"``model.load_state_dict(PATH)`` 直接加载。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you only plan to keep the best performing model (according to the "
"acquired validation loss), don't forget that ``best_model_state = "
"model.state_dict()`` returns a reference to the state and not its copy! You "
"must serialize ``best_model_state`` or use ``best_model_state = "
"deepcopy(model.state_dict())`` otherwise your best ``best_model_state`` will"
" keep getting updated by the subsequent training iterations. As a result, "
"the final model state will be the state of the overfitted model."
msgstr ""
"如果计划只保留性能最佳的模型（根据获得的验证损失），不要忘记 ``best_model_state = model.state_dict()`` "
"返回的是状态的引用而不是其副本！必须序列化 ``best_model_state`` 或使用 ``best_model_state = "
"deepcopy(model.state_dict())``，否则后续训练迭代会持续更新 "
"`best_model_state`，最终模型状态会是过拟合的状态。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save/Load Entire Model"
msgstr "保存/加载整个模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This save/load process uses the most intuitive syntax and involves the least"
" amount of code. Saving a model in this way will save the entire module "
"using Python’s `pickle <https://docs.python.org/3/library/pickle.html>`__ "
"module. The disadvantage of this approach is that the serialized data is "
"bound to the specific classes and the exact directory structure used when "
"the model is saved. The reason for this is because pickle does not save the "
"model class itself. Rather, it saves a path to the file containing the "
"class, which is used during load time. Because of this, your code can break "
"in various ways when used in other projects or after refactors."
msgstr ""
"这种保存/加载过程使用最直观的语法并且代码量最少。以这种方式保存模型将使用 Python 的 `pickle "
"<https://docs.python.org/3/library/pickle.html>`__ "
"模块保存整个模块。此方法的缺点是序列化的数据会绑定到保存模型时使用的特定类和确切的目录结构。原因在于 pickle "
"并不保存模型类本身，而是保存一个指向包含类的文件路径，并在加载时使用。因此，您的代码在用于其他项目或重构后可能会以多种方式中断。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Export/Load Model in TorchScript Format"
msgstr "以 TorchScript 格式导出/加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"One common way to do inference with a trained model is to use `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`__, an intermediate "
"representation of a PyTorch model that can be run in Python as well as in a "
"high performance environment like C++. TorchScript is actually the "
"recommended model format for scaled inference and deployment."
msgstr ""
"使用训练模型进行推断的一种常见方法是使用 `TorchScript "
"<https://pytorch.org/docs/stable/jit.html>`__，它是 PyTorch 模型的一种中间表示，可以在 "
"Python 中运行，也可以在高性能环境（如 C++）中运行。TorchScript 实际上是用于扩展推断和部署的推荐模型格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Using the TorchScript format, you will be able to load the exported model "
"and run inference without defining the model class."
msgstr "使用 TorchScript 格式，您可以在不定义模型类的情况下加载导出的模型并进行推断。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Export:**"
msgstr "**导出：**"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"For more information on TorchScript, feel free to visit the dedicated "
"`tutorials "
"<https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>`__."
" You will get familiar with the tracing conversion and learn how to run a "
"TorchScript module in a `C++ environment "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`__."
msgstr ""
"有关 TorchScript 的更多信息，请随时访问专门的 `教程 "
"<https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>`__。您将熟悉追踪转换并学习如何在"
" `C++ 环境 <https://pytorch.org/tutorials/advanced/cpp_export.html>`__ 中运行 "
"TorchScript 模块。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Saving & Loading a General Checkpoint for Inference and/or Resuming Training"
msgstr "用于推断和/或恢复训练的通用检查点保存和加载"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save:"
msgstr "保存："

#: ../../beginner/vt_tutorial.rst:783
msgid "Load:"
msgstr "加载："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When saving a general checkpoint, to be used for either inference or "
"resuming training, you must save more than just the model’s *state_dict*. It"
" is important to also save the optimizer's *state_dict*, as this contains "
"buffers and parameters that are updated as the model trains. Other items "
"that you may want to save are the epoch you left off on, the latest recorded"
" training loss, external ``torch.nn.Embedding`` layers, etc. As a result, "
"such a checkpoint is often 2~3 times larger than the model alone."
msgstr ""
"保存通用检查点时，要用于推断或恢复训练，不仅需要保存模型的 *state_dict*，还需要保存优化器的 "
"*state_dict*，因为它包含在模型训练时更新的缓冲区和参数。其他可能需要保存的项包括您中断的 epoch、最新记录的训练损失、外部 "
"``torch.nn.Embedding`` 层等。因此，这样的检查点通常比仅保存模型的文件大 2~3 倍。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To save multiple components, organize them in a dictionary and use "
"``torch.save()`` to serialize the dictionary. A common PyTorch convention is"
" to save these checkpoints using the ``.tar`` file extension."
msgstr ""
"为了保存多个组件，可以将它们组织为字典并使用 ``torch.save()`` 将字典序列化。PyTorch 的一个常见约定是使用 ``.tar`` "
"文件扩展名保存这些检查点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To load the items, first initialize the model and optimizer, then load the "
"dictionary locally using ``torch.load()``. From here, you can easily access "
"the saved items by simply querying the dictionary as you would expect."
msgstr "加载这些项时，首先初始化模型和优化器，然后使用 ``torch.load()`` 在本地加载字典。从此处可以轻松通过查询字典访问保存的项。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Remember that you must call ``model.eval()`` to set dropout and batch "
"normalization layers to evaluation mode before running inference. Failing to"
" do this will yield inconsistent inference results. If you wish to resuming "
"training, call ``model.train()`` to ensure these layers are in training "
"mode."
msgstr ""
"记住，在运行推断之前，必须通过调用 ``model.eval()`` 将 dropout 和批规范化层设置为评估模式。如果计划恢复训练，请调用 "
"``model.train()`` 确保这些层处于训练模式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving Multiple Models in One File"
msgstr "在一个文件中保存多个模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When saving a model comprised of multiple ``torch.nn.Modules``, such as a "
"GAN, a sequence-to-sequence model, or an ensemble of models, you follow the "
"same approach as when you are saving a general checkpoint. In other words, "
"save a dictionary of each model’s *state_dict* and corresponding optimizer. "
"As mentioned before, you can save any other items that may aid you in "
"resuming training by simply appending them to the dictionary."
msgstr ""
"在保存由多个 ``torch.nn.Modules`` 组成的模型时，例如 "
"GAN、序列到序列模型或模型集，可以采用与保存通用检查点相同的方法。换句话说，保存每个模型的 *state_dict* "
"和相应优化器的字典。如前所述，可以通过简单地将其添加到字典中保存任何其他可能在恢复训练时有所帮助的项。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"A common PyTorch convention is to save these checkpoints using the ``.tar`` "
"file extension."
msgstr "PyTorch 的一个常见约定是使用 ``.tar`` 文件扩展名保存这些检查点。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To load the models, first initialize the models and optimizers, then load "
"the dictionary locally using ``torch.load()``. From here, you can easily "
"access the saved items by simply querying the dictionary as you would "
"expect."
msgstr ""
"加载这些模型时，首先初始化模型和优化器，然后使用 ``torch.load()`` 在本地加载字典。从此处可以轻松通过查询字典访问保存的项目。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Remember that you must call ``model.eval()`` to set dropout and batch "
"normalization layers to evaluation mode before running inference. Failing to"
" do this will yield inconsistent inference results. If you wish to resuming "
"training, call ``model.train()`` to set these layers to training mode."
msgstr ""
"记住，在运行推断之前必须通过调用 ``model.eval()`` 将 dropout 和批规范化层设置为评估模式。如果计划恢复训练，请调用 "
"``model.train()`` 将这些层设置为训练模式。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Warmstarting Model Using Parameters from a Different Model"
msgstr "使用不同模型的参数进行热启动"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Partially loading a model or loading a partial model are common scenarios "
"when transfer learning or training a new complex model. Leveraging trained "
"parameters, even if only a few are usable, will help to warmstart the "
"training process and hopefully help your model converge much faster than "
"training from scratch."
msgstr ""
"部分加载模型或加载部分模型是迁移学习或训练新复杂模型时的常见场景。即使只有少量参数可以使用，利用已训练的参数也能帮助热启动训练过程，并有望大幅加快模型收敛速度，比从头开始训练更快。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Whether you are loading from a partial *state_dict*, which is missing some "
"keys, or loading a *state_dict* with more keys than the model that you are "
"loading into, you can set the ``strict`` argument to **False** in the "
"``load_state_dict()`` function to ignore non-matching keys."
msgstr ""
"当从部分 *state_dict* 加载时（缺少某些键），或者从一个具有比目标模型更多键的 *state_dict* 加载时，可以在 "
"``load_state_dict()`` 函数中将 ``strict`` 参数设置为 **False** 以忽略不匹配的键。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you want to load parameters from one layer to another, but some keys do "
"not match, simply change the name of the parameter keys in the *state_dict* "
"that you are loading to match the keys in the model that you are loading "
"into."
msgstr ""
"如果希望将参数从一个层加载到另一个层，但某些键不匹配，可以简单地更改所加载 *state_dict* 中的参数键名称，以匹配目标模型中的键。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving & Loading Model Across Devices"
msgstr "跨设备保存和加载模型"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save on GPU, Load on CPU"
msgstr "在 GPU 上保存，在 CPU 加载"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When loading a model on a CPU that was trained with a GPU, pass "
"``torch.device('cpu')`` to the ``map_location`` argument in the "
"``torch.load()`` function. In this case, the storages underlying the tensors"
" are dynamically remapped to the CPU device using the ``map_location`` "
"argument."
msgstr ""
"在 CPU 上加载使用 GPU 训练的模型时，将 ``torch.device('cpu')`` 传递给 ``torch.load()`` 函数中的 "
"``map_location`` 参数。在这种情况下，张量的底层存储将通过 ``map_location`` 参数动态重新映射到 CPU 设备。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save on GPU, Load on GPU"
msgstr "在 GPU 上保存，在 GPU 加载"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When loading a model on a GPU that was trained and saved on GPU, simply "
"convert the initialized ``model`` to a CUDA optimized model using "
"``model.to(torch.device('cuda'))``. Also, be sure to use the "
"``.to(torch.device('cuda'))`` function on all model inputs to prepare the "
"data for the model. Note that calling ``my_tensor.to(device)`` returns a new"
" copy of ``my_tensor`` on GPU. It does NOT overwrite ``my_tensor``. "
"Therefore, remember to manually overwrite tensors: ``my_tensor = "
"my_tensor.to(torch.device('cuda'))``."
msgstr ""
"当在GPU上加载一个在GPU上训练并保存的模型时，只需使用``model.to(torch.device('cuda'))``将初始化的``model``转换为CUDA优化的模型。同时，请确保对所有模型输入使用``.to(torch.device('cuda'))``函数，以将数据准备好供模型使用。请注意，调用``my_tensor.to(device)``会返回GPU上的``my_tensor``的新副本，而不会覆盖``my_tensor``。因此，请记得手动覆盖张量：``my_tensor"
" = my_tensor.to(torch.device('cuda'))``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Save on CPU, Load on GPU"
msgstr "在CPU上保存，在GPU上加载"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"When loading a model on a GPU that was trained and saved on CPU, set the "
"``map_location`` argument in the ``torch.load()`` function to "
"``cuda:device_id``. This loads the model to a given GPU device. Next, be "
"sure to call ``model.to(torch.device('cuda'))`` to convert the model’s "
"parameter tensors to CUDA tensors. Finally, be sure to use the "
"``.to(torch.device('cuda'))`` function on all model inputs to prepare the "
"data for the CUDA optimized model. Note that calling "
"``my_tensor.to(device)`` returns a new copy of ``my_tensor`` on GPU. It does"
" NOT overwrite ``my_tensor``. Therefore, remember to manually overwrite "
"tensors: ``my_tensor = my_tensor.to(torch.device('cuda'))``."
msgstr ""
"当在GPU上加载一个在CPU上训练并保存的模型时，在``torch.load()``函数中将``map_location``参数设置为``cuda:device_id``。这样可以将模型加载到指定的GPU设备。接着，请确保调用``model.to(torch.device('cuda'))``将模型的参数张量转换为CUDA张量。最后，请确保对所有模型输入使用``.to(torch.device('cuda'))``函数，以将数据准备好供CUDA优化的模型使用。请注意，调用``my_tensor.to(device)``会返回GPU上的``my_tensor``的新副本，而不会覆盖``my_tensor``。因此，请记得手动覆盖张量：``my_tensor"
" = my_tensor.to(torch.device('cuda'))``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving ``torch.nn.DataParallel`` Models"
msgstr "保存``torch.nn.DataParallel``模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"``torch.nn.DataParallel`` is a model wrapper that enables parallel GPU "
"utilization. To save a ``DataParallel`` model generically, save the "
"``model.module.state_dict()``. This way, you have the flexibility to load "
"the model any way you want to any device you want."
msgstr ""
"``torch.nn.DataParallel``是一个模型包装器，可实现并行使用多块GPU。为了通用性地保存``DataParallel``模型，保存``model.module.state_dict()``。这样，您可以根据需要将模型加载到任何设备。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: saving_loading_models.py "
"<saving_loading_models.py>`"
msgstr ""
":download:`下载Python源代码: saving_loading_models.py <saving_loading_models.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: saving_loading_models.ipynb "
"<saving_loading_models.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook: saving_loading_models.ipynb "
"<saving_loading_models.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "**00:01.405** total execution time for **beginner** files:"
msgstr "**00:01.405** 为 **初学者** 文件的总执行时间："

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_fgsm_tutorial.py` (``fgsm_tutorial.py``)"
msgstr ":ref:`sphx_glr_beginner_fgsm_tutorial.py` (``fgsm_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.747"
msgstr "00:00.747"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_chatbot_tutorial.py` (``chatbot_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_chatbot_tutorial.py` (``chatbot_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.326"
msgstr "00:00.326"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_data_loading_tutorial.py` "
"(``data_loading_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_data_loading_tutorial.py` "
"(``data_loading_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.316"
msgstr "00:00.316"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_deploy_seq2seq_hybrid_frontend_tutorial.py` "
"(``deploy_seq2seq_hybrid_frontend_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_deploy_seq2seq_hybrid_frontend_tutorial.py` "
"(``deploy_seq2seq_hybrid_frontend_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.012"
msgstr "00:00.012"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_flava_finetuning_tutorial.py` "
"(``flava_finetuning_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_flava_finetuning_tutorial.py` "
"(``flava_finetuning_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid "00:00.004"
msgstr "00:00.004"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_Intro_to_TorchScript_tutorial.py` "
"(``Intro_to_TorchScript_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_Intro_to_TorchScript_tutorial.py` "
"(``Intro_to_TorchScript_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_dcgan_faces_tutorial.py` "
"(``dcgan_faces_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_dcgan_faces_tutorial.py` "
"(``dcgan_faces_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_hyperparameter_tuning_tutorial.py` "
"(``hyperparameter_tuning_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_hyperparameter_tuning_tutorial.py` "
"(``hyperparameter_tuning_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_knowledge_distillation_tutorial.py` "
"(``knowledge_distillation_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_knowledge_distillation_tutorial.py` "
"(``knowledge_distillation_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_nn_tutorial.py` (``nn_tutorial.py``)"
msgstr ":ref:`sphx_glr_beginner_nn_tutorial.py` (``nn_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_profiler.py` (``profiler.py``)"
msgstr ":ref:`sphx_glr_beginner_profiler.py` (``profiler.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_saving_loading_models.py` "
"(``saving_loading_models.py``)"
msgstr ""
":ref:`sphx_glr_beginner_saving_loading_models.py` "
"(``saving_loading_models.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_template_tutorial.py` (``template_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_template_tutorial.py` (``template_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":ref:`sphx_glr_beginner_transfer_learning_tutorial.py` "
"(``transfer_learning_tutorial.py``)"
msgstr ""
":ref:`sphx_glr_beginner_transfer_learning_tutorial.py` "
"(``transfer_learning_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ":ref:`sphx_glr_beginner_vt_tutorial.py` (``vt_tutorial.py``)"
msgstr ":ref:`sphx_glr_beginner_vt_tutorial.py` (``vt_tutorial.py``)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"T5-Base Model for Summarization, Sentiment Classification, and Translation"
msgstr "用于摘要、情感分类和翻译的T5-Base模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_template_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_beginner_template_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Template Tutorial"
msgstr "模板教程"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Author:** `FirstName LastName <https://github.com/username>`_"
msgstr "**作者：** `名字 姓氏 <https://github.com/username>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid "Item 1"
msgstr "项目1"

#: ../../beginner/vt_tutorial.rst:783
msgid "Item 2"
msgstr "项目2"

#: ../../beginner/vt_tutorial.rst:783
msgid "Item 3"
msgstr "项目3"

#: ../../beginner/vt_tutorial.rst:783
msgid "PyTorch v2.0.0"
msgstr "PyTorch v2.0.0"

#: ../../beginner/vt_tutorial.rst:783
msgid "GPU ???"
msgstr "GPU ???"

#: ../../beginner/vt_tutorial.rst:783
msgid "Other items 3"
msgstr "其他项目3"

#: ../../beginner/vt_tutorial.rst:783
msgid "If you have a video, add it here like this:"
msgstr "如果有视频，请像这样添加："

#: ../../beginner/vt_tutorial.rst:783
msgid "To test your tutorial locally, you can do one of the following:"
msgstr "要在本地测试您的教程，您可以执行以下操作之一："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can control specific files that generate the results by using "
"``GALLERY_PATTERN`` environment variable. The GALLERY_PATTERN variable "
"respects regular expressions. For example to run only "
"``neural_style_transfer_tutorial.py``, use the following command:"
msgstr ""
"您可以通过使用``GALLERY_PATTERN``环境变量控控制生成结果的具体文件。GALLERY_PATTERN变量支持正则表达式。例如，要仅运行``neural_style_transfer_tutorial.py``，请使用以下命令："

#: ../../beginner/vt_tutorial.rst:783
msgid "or"
msgstr "或者"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Make a copy of this repository and add only your tutorial to the "
"`beginner_source` directory removing all other tutorials. Then run ``make "
"html``."
msgstr "复制此仓库，并仅将您的教程添加到`beginner_source`目录中，删除所有其他教程。然后运行``make html``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Verify that all outputs were generated correctly in the created HTML."
msgstr "在生成的HTML中验证所有输出都正确生成。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Describe Why is this topic important? Add Links to relevant research papers."
msgstr "描述为什么此主题重要？添加指向相关研究论文的链接。"

#: ../../beginner/vt_tutorial.rst:783
msgid "This tutorial walks you through the process of...."
msgstr "本教程将带您完成以下过程...."

#: ../../beginner/vt_tutorial.rst:783
msgid "Example code (the output below is generated automatically):"
msgstr "示例代码（以下输出是自动生成的）："

#: ../../beginner/vt_tutorial.rst:783
msgid "(Optional) Additional Exercises"
msgstr "（可选）附加练习"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Add additional practice exercises for users to test their knowledge. "
"Example: `NLP from Scratch "
"<https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#exercises>`__."
msgstr ""
"为用户添加额外的练习以测试他们的知识。例如：`从零开始的NLP "
"<https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#exercises>`__。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Summarize the steps and concepts covered. Highlight key takeaways."
msgstr "总结涉及的步骤和概念。突出关键要点。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Link1"
msgstr "链接1"

#: ../../beginner/vt_tutorial.rst:783
msgid "Link2"
msgstr "链接2"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  0.006 seconds)"
msgstr "**脚本总运行时间：**（0分钟 0.006秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: template_tutorial.py "
"<template_tutorial.py>`"
msgstr ":download:`下载Python源代码: template_tutorial.py <template_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: template_tutorial.ipynb "
"<template_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook: template_tutorial.ipynb "
"<template_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Text classification with the torchtext library"
msgstr "使用torchtext库进行文本分类"

#: ../../beginner/vt_tutorial.rst:783
msgid "Preprocess custom text dataset using torchtext"
msgstr "使用torchtext预处理自定义文本数据集"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_transfer_learning_tutorial.py>`"
" to download the full example code"
msgstr ""
"点击 :ref:`此处 "
"<sphx_glr_download_beginner_transfer_learning_tutorial.py>`下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Transfer Learning for Computer Vision Tutorial"
msgstr "计算机视觉迁移学习教程"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, you will learn how to train a convolutional neural network"
" for image classification using transfer learning. You can read more about "
"the transfer learning at `cs231n notes <https://cs231n.github.io/transfer-"
"learning/>`__"
msgstr ""
"在本教程中，您将学习如何使用迁移学习训练一个用于图像分类的卷积神经网络。您可以在`cs231n notes "
"<https://cs231n.github.io/transfer-learning/>`__中阅读更多关于迁移学习的信息。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Quoting these notes,"
msgstr "引述这些笔记，"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In practice, very few people train an entire Convolutional Network from "
"scratch (with random initialization), because it is relatively rare to have "
"a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on"
" a very large dataset (e.g. ImageNet, which contains 1.2 million images with"
" 1000 categories), and then use the ConvNet either as an initialization or a"
" fixed feature extractor for the task of interest."
msgstr ""
"实际上，很少有人从头开始（随机初始化）训练整个卷积网络，因为很少有大小足够的数据集。相反，通常会在一个非常大的数据集（例如包含1000个类别、120万张图像的ImageNet）上预训练一个卷积网络，然后将其用作初始化或固定的特征提取器来处理感兴趣的任务。"

#: ../../beginner/vt_tutorial.rst:783
msgid "These two major transfer learning scenarios look as follows:"
msgstr "这两种主要的迁移学习场景如下："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**Finetuning the ConvNet**: Instead of random initialization, we initialize "
"the network with a pretrained network, like the one that is trained on "
"imagenet 1000 dataset. Rest of the training looks as usual."
msgstr ""
"**微调卷积网络**：与随机初始化不同，我们使用一个预训练的网络（如在imagenet "
"1000数据集上训练的网络）进行初始化，其余的训练过程与通常一致。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"**ConvNet as fixed feature extractor**: Here, we will freeze the weights for"
" all of the network except that of the final fully connected layer. This "
"last fully connected layer is replaced with a new one with random weights "
"and only this layer is trained."
msgstr ""
"**卷积网络作为固定特征提取器**：在此处，我们将冻结除最后一个全连接层以外的所有网络权重。最后这个全连接层将被替换为一个具有随机权重的新层，仅训练这个层。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Load Data"
msgstr "加载数据"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"We will use torchvision and torch.utils.data packages for loading the data."
msgstr "我们将使用torchvision和torch.utils.data包来加载数据。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The problem we're going to solve today is to train a model to classify "
"**ants** and **bees**. We have about 120 training images each for ants and "
"bees. There are 75 validation images for each class. Usually, this is a very"
" small dataset to generalize upon, if trained from scratch. Since we are "
"using transfer learning, we should be able to generalize reasonably well."
msgstr ""
"今天我们要解决的问题是训练一个模型来分类**蚂蚁**和**蜜蜂**。我们每个类别大约有120张训练图片，每个类别有75张验证图片。通常情况下，从头训练这是一个很小的数据集，不足以进行泛化。由于我们使用迁移学习，我们应该能够做到合理泛化。"

#: ../../beginner/vt_tutorial.rst:783
msgid "This dataset is a very small subset of imagenet."
msgstr "这个数据集是Imagenet的一个小子集。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Download the data from `here "
"<https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_ and extract "
"it to the current directory."
msgstr ""
"从`这里 "
"<https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_下载数据并将其解压到当前目录。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Visualize a few images"
msgstr "可视化一些图片"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Let's visualize a few training images so as to understand the data "
"augmentations."
msgstr "让我们可视化一些训练图片，以了解数据增强。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Training the model"
msgstr "训练模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now, let's write a general function to train a model. Here, we will "
"illustrate:"
msgstr "现在，让我们编写一个通用函数来训练模型。这里，我们将说明："

#: ../../beginner/vt_tutorial.rst:783
msgid "Scheduling the learning rate"
msgstr "调整学习率"

#: ../../beginner/vt_tutorial.rst:783
msgid "Saving the best model"
msgstr "保存最佳模型"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In the following, parameter ``scheduler`` is an LR scheduler object from "
"``torch.optim.lr_scheduler``."
msgstr "在下面的内容中，参数``scheduler``是``torch.optim.lr_scheduler``中的一个LR调度对象。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Visualizing the model predictions"
msgstr "可视化模型预测"

#: ../../beginner/vt_tutorial.rst:783
msgid "Generic function to display predictions for a few images"
msgstr "用于显示一些图片预测结果的通用功能"

#: ../../beginner/vt_tutorial.rst:783
msgid "Finetuning the ConvNet"
msgstr "微调卷积网络"

#: ../../beginner/vt_tutorial.rst:783
msgid "Load a pretrained model and reset final fully connected layer."
msgstr "加载一个预训练模型并重置最终全连接层。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Train and evaluate"
msgstr "训练和评估"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"It should take around 15-25 min on CPU. On GPU though, it takes less than a "
"minute."
msgstr "在CPU上大约需要15-25分钟。在GPU上，运行速度不到一分钟。"

#: ../../beginner/vt_tutorial.rst:783
msgid "ConvNet as fixed feature extractor"
msgstr "卷积网络作为固定特征提取器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Here, we need to freeze all the network except the final layer. We need to "
"set ``requires_grad = False`` to freeze the parameters so that the gradients"
" are not computed in ``backward()``."
msgstr ""
"在这篇教程中，我们需要冻结除最后一层之外的所有网络层。我们需要设置``requires_grad = "
"False``以冻结参数，这样在``backward()``中就不会计算梯度。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can read more about this in the documentation `here "
"<https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-"
"backward>`__."
msgstr ""
"您可以在`这里 <https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-"
"from-backward>`__的文档中了解更多这方面内容。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"On CPU this will take about half the time compared to previous scenario. "
"This is expected as gradients don't need to be computed for most of the "
"network. However, forward does need to be computed."
msgstr "在CPU上该方法与之前比可以节约一半时间。这是预期之中的，因为大部分网络不需要计算梯度。但前向传播仍需要计算。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Inference on custom images"
msgstr "自定义图片推断"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Use the trained model to make predictions on custom images and visualize the"
" predicted class labels along with the images."
msgstr "使用训练好的模型对自定义图片进行预测，并可视化带有预测类别标签的图片。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Further Learning"
msgstr "进一步学习"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"If you would like to learn more about the applications of transfer learning,"
" checkout our `Quantized Transfer Learning for Computer Vision Tutorial "
"<https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_."
msgstr ""
"如果您想了解有关迁移学习应用的更多信息，请查看我们的`量化迁移学习用于计算机视觉教程 "
"<https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 1 minutes  10.558 seconds)"
msgstr "**脚本总运行时间：**（1分钟 10.558秒）"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: transfer_learning_tutorial.py "
"<transfer_learning_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: transfer_learning_tutorial.py "
"<transfer_learning_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: transfer_learning_tutorial.ipynb "
"<transfer_learning_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter Notebook: transfer_learning_tutorial.ipynb "
"<transfer_learning_tutorial.ipynb>`"

#: ../../beginner/vt_tutorial.rst:783
msgid "Language Modeling with ``nn.Transformer`` and torchtext"
msgstr "使用``nn.Transformer``和torchtext进行语言建模"

#: ../../beginner/vt_tutorial.rst:783
msgid "The content is deprecated."
msgstr "内容已经弃用。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Language Translation with ``nn.Transformer`` and torchtext"
msgstr "使用``nn.Transformer``和torchtext进行语言翻译"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Click :ref:`here <sphx_glr_download_beginner_vt_tutorial.py>` to download "
"the full example code"
msgstr "点击 :ref:`此处 <sphx_glr_download_beginner_vt_tutorial.py>` 下载完整示例代码"

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimizing Vision Transformer Model for Deployment"
msgstr "优化视觉Transformer模型以供部署"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Jeff Tang <https://github.com/jeffxtang>`_, `Geeta Chauhan "
"<https://github.com/gchauhan/>`_"
msgstr ""
"`Jeff Tang <https://github.com/jeffxtang>`_，`Geeta Chauhan "
"<https://github.com/gchauhan/>`_"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Vision Transformer models apply the cutting-edge attention-based transformer"
" models, introduced in Natural Language Processing to achieve all kinds of "
"the state of the art (SOTA) results, to Computer Vision tasks. Facebook "
"Data-efficient Image Transformers `DeiT <https://ai.facebook.com/blog/data-"
"efficient-image-transformers-a-promising-new-technique-for-image-"
"classification>`_ is a Vision Transformer model trained on ImageNet for "
"image classification."
msgstr ""
"视觉Transformer模型将最前沿的基于注意力的Transformer模型应用于自然语言处理，以完成所有种类的最先进（SOTA）任务，应用于计算机视觉任务。Facebook的高效数据图像Transformer模型（`DeiT"
" <https://ai.facebook.com/blog/data-efficient-image-transformers-a-"
"promising-new-technique-for-image-"
"classification>`_）是在ImageNet上训练、用于图像分类的视觉Transformer模型。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"In this tutorial, we will first cover what DeiT is and how to use it, then "
"go through the complete steps of scripting, quantizing, optimizing, and "
"using the model in iOS and Android apps. We will also compare the "
"performance of quantized, optimized and non-quantized, non-optimized models,"
" and show the benefits of applying quantization and optimization to the "
"model along the steps."
msgstr ""
"在本教程中，我们首先介绍什么是DeiT以及如何使用，然后全面解析脚本化、量化、优化以及在iOS和安卓应用中的使用步骤。我们还将比较量化优化模型与非量化非优化模型的性能，并展示沿着这些步骤对模型应用量化和优化的好处。"

#: ../../beginner/vt_tutorial.rst:783
msgid "What is DeiT"
msgstr "什么是DeiT"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Convolutional Neural Networks (CNNs) have been the main models for image "
"classification since deep learning took off in 2012, but CNNs typically "
"require hundreds of millions of images for training to achieve the SOTA "
"results. DeiT is a vision transformer model that requires a lot less data "
"and computing resources for training to compete with the leading CNNs in "
"performing image classification, which is made possible by two key "
"components of of DeiT:"
msgstr ""
"自2012年深度学习兴起以来，卷积神经网络（CNNs）一直是图像分类的主力模型，但实现SOTA结果通常需要数亿张图像进行训练。而DeiT是一种视觉Transformer模型，其需要较少的数据和计算资源进行训练，但可以与领先的CNNs竞争图像分类性能，这主要得益于DeiT的两个关键组件："

#: ../../beginner/vt_tutorial.rst:783
msgid "Data augmentation that simulates training on a much larger dataset;"
msgstr "数据增强，它模拟了在更大数据集上的训练；"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Native distillation that allows the transformer network to learn from a "
"CNN’s output."
msgstr "原生蒸馏，它使Transformer网络能够学习CNN的输出。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"DeiT shows that Transformers can be successfully applied to computer vision "
"tasks, with limited access to data and resources. For more details on DeiT, "
"see the `repo <https://github.com/facebookresearch/deit>`_ and `paper "
"<https://arxiv.org/abs/2012.12877>`_."
msgstr ""
"DeiT展示了Transformers可以成功应用于计算机视觉任务，即使访问数据和资源有限。有关DeiT的更多细节，请参阅`代码库 "
"<https://github.com/facebookresearch/deit>`_ 和 `论文 "
"<https://arxiv.org/abs/2012.12877>`_。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Classifying Images with DeiT"
msgstr "使用DeiT进行图像分类"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Follow the ``README.md`` at the DeiT repository for detailed information on "
"how to classify images using DeiT, or for a quick test, first install the "
"required packages:"
msgstr "请按照DeiT代码库中的``README.md``获取有关如何使用DeiT进行图像分类的详细信息，或者为了快速测试，首先安装所需的软件包："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To run in Google Colab, install dependencies by running the following "
"command:"
msgstr "如果在Google Colab中运行，请通过运行以下命令安装依赖项："

#: ../../beginner/vt_tutorial.rst:783
msgid "then run the script below:"
msgstr "然后运行下面的脚本："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The output should be 269, which, according to the ImageNet list of class "
"index to `labels file "
"<https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_, maps to ``timber "
"wolf, grey wolf, gray wolf, Canis lupus``."
msgstr ""
"输出结果应该是269，根据ImageNet的`类别索引到标签文件 "
"<https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a>`_，它对应于``timber wolf, "
"grey wolf, gray wolf, Canis lupus``。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Now that we have verified that we can use the DeiT model to classify images,"
" let’s see how to modify the model so it can run on iOS and Android apps."
msgstr "现在我们已经验证了可以使用DeiT模型对图像进行分类，接下来看看如何修改该模型以便可以在iOS和Android应用程序上运行。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Scripting DeiT"
msgstr "脚本化DeiT"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To use the model on mobile, we first need to script the model. See the "
"`Script and Optimize recipe "
"<https://pytorch.org/tutorials/recipes/script_optimized.html>`_ for a quick "
"overview. Run the code below to convert the DeiT model used in the previous "
"step to the TorchScript format that can run on mobile."
msgstr ""
"为了在移动设备上使用模型，我们首先需要脚本化模型。请参阅`脚本和优化指南 "
"<https://pytorch.org/tutorials/recipes/script_optimized.html>`_进行快速了解。运行下面的代码，将上一步骤中使用的DeiT模型转换为可在移动设备上运行的TorchScript格式。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The scripted model file ``fbdeit_scripted.pt`` of size about 346MB is "
"generated."
msgstr "生成了大小约为346MB的脚本化模型文件``fbdeit_scripted.pt``。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Quantizing DeiT"
msgstr "量化DeiT"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To reduce the trained model size significantly while keeping the inference "
"accuracy about the same, quantization can be applied to the model. Thanks to"
" the transformer model used in DeiT, we can easily apply dynamic-"
"quantization to the model, because dynamic quantization works best for LSTM "
"and transformer models (see `here "
"<https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-"
"quantization>`_ for more details)."
msgstr ""
"为了在保持推理精度基本相同的情况下显著减小训练模型的大小，可以对模型应用量化技术。得益于DeiT使用的Transformer模型，对其应用动态量化很方便，因为动态量化对LSTM和Transformer模型效果最佳（查看`此处"
" "
"<https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-"
"quantization>`_了解更多详情）。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Now run the code below:"
msgstr "现在运行下面的代码："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"This generates the scripted and quantized version of the model "
"``fbdeit_quantized_scripted.pt``, with size about 89MB, a 74% reduction of "
"the non-quantized model size of 346MB!"
msgstr ""
"生成的脚本化和量化版本模型``fbdeit_quantized_scripted.pt``，大小约为89MB，比非量化版本的346MB减少了74%！"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"You can use the ``scripted_quantized_model`` to generate the same inference "
"result:"
msgstr "您可以使用``scripted_quantized_model``生成相同的推理结果："

#: ../../beginner/vt_tutorial.rst:783
msgid "Optimizing DeiT"
msgstr "优化DeiT"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The final step before using the quantized and scripted model on mobile is to"
" optimize it:"
msgstr "在使用量化和脚本化模型于移动设备前的最后一步是优化模型："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The generated ``fbdeit_optimized_scripted_quantized.pt`` file has about the "
"same size as the quantized, scripted, but non-optimized model. The inference"
" result remains the same."
msgstr ""
"生成的``fbdeit_optimized_scripted_quantized.pt``文件大小与量化、脚本化但未优化的模型基本相同。推理结果保持不变。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Using Lite Interpreter"
msgstr "使用精简版解释器"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To see how much model size reduction and inference speed up the Lite "
"Interpreter can result in, let’s create the lite version of the model."
msgstr "为了看看模型大小减少和推理速度提升的效果，使用精简版解释器创建模型的精简版。"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"Although the lite model size is comparable to the non-lite version, when "
"running the lite version on mobile, the inference speed up is expected."
msgstr "尽管精简版模型的大小与非精简版本相当，但在移动设备上运行精简版时，推理速度预计会更快。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Comparing Inference Speed"
msgstr "比较推理速度"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"To see how the inference speed differs for the four models - the original "
"model, the scripted model, the quantized-and-scripted model, the optimized-"
"quantized-and-scripted model - run the code below:"
msgstr "为了比较四种模型（原始模型、脚本化模型、量化并脚本化模型及优化量化脚本化模型）的推理速度，运行以下代码："

#: ../../beginner/vt_tutorial.rst:783
msgid "The results running on a Google Colab are:"
msgstr "在Google Colab上的运行结果为："

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"The following results summarize the inference time taken by each model and "
"the percentage reduction of each model relative to the original model."
msgstr "以下结果总结了每种模型的推理时间以及每种模型相对于原始模型的百分比减少。"

#: ../../beginner/vt_tutorial.rst:783
msgid "Learn More"
msgstr "了解更多"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Facebook Data-efficient Image Transformers "
"<https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-"
"new-technique-for-image-classification>`__"
msgstr ""
"`Facebook高效图像Transformer <https://ai.facebook.com/blog/data-efficient-image-"
"transformers-a-promising-new-technique-for-image-classification>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Vision Transformer with ImageNet and MNIST on iOS "
"<https://github.com/pytorch/ios-demo-app/tree/master/ViT4MNIST>`__"
msgstr ""
"`iOS平台上的ImageNet和MNIST Vision Transformer <https://github.com/pytorch/ios-"
"demo-app/tree/master/ViT4MNIST>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
"`Vision Transformer with ImageNet and MNIST on Android "
"<https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST>`__"
msgstr ""
"`Android平台上的ImageNet和MNIST Vision Transformer "
"<https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST>`__"

#: ../../beginner/vt_tutorial.rst:783
msgid "**Total running time of the script:** ( 0 minutes  48.391 seconds)"
msgstr "**脚本总运行时间：** ( 0 分 48.391 秒)"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Python source code: vt_tutorial.py <vt_tutorial.py>`"
msgstr ":下载:`下载Python源码: vt_tutorial.py <vt_tutorial.py>`"

#: ../../beginner/vt_tutorial.rst:783
msgid ""
":download:`Download Jupyter notebook: vt_tutorial.ipynb <vt_tutorial.ipynb>`"
msgstr ":下载:`下载Jupyter笔记本: vt_tutorial.ipynb <vt_tutorial.ipynb>`"
