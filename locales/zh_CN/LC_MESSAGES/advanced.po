#
msgid ""
msgstr ""

#: ../../advanced/usb_semisup_learn.rst:355
msgid "ONNX Live Tutorial"
msgstr "ONNX 实时教程"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "This tutorial has been deprecated."
msgstr "本教程已被弃用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Redirecting in 3 seconds..."
msgstr "3秒后重定向..."

#: ../../advanced/usb_semisup_learn.rst:355
msgid "edit"
msgstr "编辑"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_coding_ddpg.py>` to download "
"the full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_advanced_coding_ddpg.py>` 下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "TorchRL objectives: Coding a DDPG loss"
msgstr "TorchRL 目标: 编写 DDPG 损失"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `Vincent Moens <https://github.com/vmoens>`_"
msgstr "**作者**: `Vincent Moens <https://github.com/vmoens>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Overview"
msgstr "概述"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL separates the training of RL algorithms in various pieces that will "
"be assembled in your training script: the environment, the data collection "
"and storage, the model and finally the loss function."
msgstr "TorchRL 将RL算法的训练拆分成多个部分，这些部分将在您的训练脚本中组装：环境、数据收集与存储、模型以及损失函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL losses (or \"objectives\") are stateful objects that contain the "
"trainable parameters (policy and value models). This tutorial will guide you"
" through the steps to code a loss from the ground up using TorchRL."
msgstr ""
"TorchRL 损失（或称“目标”）是包含可训练参数（策略和价值模型）的有状态对象。本教程将指导您从头编写一个使用 TorchRL 的损失模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To this aim, we will be focusing on DDPG, which is a relatively "
"straightforward algorithm to code. `Deep Deterministic Policy Gradient "
"<https://arxiv.org/abs/1509.02971>`_ (DDPG) is a simple continuous control "
"algorithm. It consists in learning a parametric value function for an "
"action-observation pair, and then learning a policy that outputs actions "
"that maximize this value function given a certain observation."
msgstr ""
"为此，我们将专注于 DDPG，它是一个相对容易编码的算法。`深度确定性策略梯度 <https://arxiv.org/abs/1509.02971>`_"
" (DDPG) 是一个简单的连续控制算法。它包括学习一个参数化的值函数，用于动作-"
"观测对，然后学习一个策略，使其输出的动作能够在给定特定观测的情况下最大化该值函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "What you will learn:"
msgstr "您将学习到:"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "how to write a loss module and customize its value estimator;"
msgstr "如何编写损失模块并自定义其值估计器；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"how to build an environment in TorchRL, including transforms (for example, "
"data normalization) and parallel execution;"
msgstr "如何使用 TorchRL 构建环境，包括变换（例如数据归一化）和并行执行；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "how to design a policy and value network;"
msgstr "如何设计策略和价值网络；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"how to collect data from your environment efficiently and store them in a "
"replay buffer;"
msgstr "如何高效地从环境中收集数据，并将其存储在回放缓冲区中；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"how to store trajectories (and not transitions) in your replay buffer);"
msgstr "如何在回放缓冲区中存储轨迹（而不是过渡数据）；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "how to evaluate your model."
msgstr "如何评估您的模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Prerequisites"
msgstr "前提条件"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial assumes that you have completed the `PPO tutorial "
"<reinforcement_ppo.html>`_ which gives an overview of the TorchRL components"
" and dependencies, such as :class:`tensordict.TensorDict` and "
":class:`tensordict.nn.TensorDictModules`, although it should be sufficiently"
" transparent to be understood without a deep understanding of these classes."
msgstr ""
"本教程假设您已完成 `PPO教程 <reinforcement_ppo.html>`_ ，该教程概述了 TorchRL 组件和依赖项，例如 "
":class:`tensordict.TensorDict` 和 "
":class:`tensordict.nn.TensorDictModules`，尽管即使对这些类不了解也应该可以理解教程内容。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We do not aim at giving a SOTA implementation of the algorithm, but rather "
"to provide a high-level illustration of TorchRL's loss implementations and "
"the library features that are to be used in the context of this algorithm."
msgstr "我们的目标不是提供该算法的顶尖实现，而是提供 TorchRL 损失实现的高层说明以及在该算法上下文中使用库特性的示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Imports and setup"
msgstr "导入和设置"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We will execute the policy on CUDA if available"
msgstr "我们将在支持 CUDA 的情况下在 CUDA 上执行策略"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "TorchRL :class:`~torchrl.objectives.LossModule`"
msgstr "TorchRL :class:`~torchrl.objectives.LossModule`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL provides a series of losses to use in your training scripts. The aim"
" is to have losses that are easily reusable/swappable and that have a simple"
" signature."
msgstr "TorchRL 提供了一系列可用于训练脚本的损失模块。目标是拥有易于重用/替换且具有简单签名的损失模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The main characteristics of TorchRL losses are:"
msgstr "TorchRL 损失的主要特征是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"They are stateful objects: they contain a copy of the trainable parameters "
"such that ``loss_module.parameters()`` gives whatever is needed to train the"
" algorithm."
msgstr "它们是有状态对象：它们包含可训练参数的副本，因此 ``loss_module.parameters()`` 返回训练算法所需的所有参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"They follow the ``TensorDict`` convention: the "
":meth:`torch.nn.Module.forward` method will receive a TensorDict as input "
"that contains all the necessary information to return a loss value."
msgstr ""
"它们遵循 ``TensorDict`` 约定：:meth:`torch.nn.Module.forward` 方法将接收包含所有必要信息以返回损失值的 "
"TensorDict 作为输入。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"They output a :class:`tensordict.TensorDict` instance with the loss values "
"written under a ``\"loss_<smth>\"`` where ``smth`` is a string describing "
"the loss. Additional keys in the ``TensorDict`` may be useful metrics to log"
" during training time."
msgstr ""
"它们输出一个 :class:`tensordict.TensorDict` 实例，其中损失值以 ``\"loss_<smth>\"`` 为键写入，其中 "
"``smth`` 是描述损失的字符串。在 ``TensorDict`` 中的其他键可能是训练期间记录的有用指标。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The reason we return independent losses is to let the user use a different "
"optimizer for different sets of parameters for instance. Summing the losses "
"can be simply done via"
msgstr "返回单独的损失的原因是，为了让用户可以对不同参数集使用不同的优化器。例如，可以通过简单求和来合并这些损失。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The ``__init__`` method"
msgstr "``__init__``方法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The parent class of all losses is :class:`~torchrl.objectives.LossModule`. "
"As many other components of the library, its "
":meth:`~torchrl.objectives.LossModule.forward` method expects as input a "
":class:`tensordict.TensorDict` instance sampled from an experience replay "
"buffer, or any similar data structure. Using this format makes it possible "
"to re-use the module across modalities, or in complex settings where the "
"model needs to read multiple entries for instance. In other words, it allows"
" us to code a loss module that is oblivious to the data type that is being "
"given to is and that focuses on running the elementary steps of the loss "
"function and only those."
msgstr ""
"所有损失的父类是 :class:`~torchrl.objectives.LossModule`。与库的许多其他组件一样，其 "
":meth:`~torchrl.objectives.LossModule.forward` 方法期望从经验回放缓冲区或任何类似数据结构中采样的 "
":class:`tensordict.TensorDict` "
"实例作为输入。使用这种格式可以让模块跨模态重用，或在复杂设置中（例如模型需要读取多个条目的情况）使用。换句话说，它允许我们编写一个对给定数据类型无感的损失模块，仅专注于运行损失函数的基本步骤。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To keep the tutorial as didactic as we can, we'll be displaying each method "
"of the class independently and we'll be populating the class at a later "
"stage."
msgstr "为了使教程尽可能具有教学性，我们将单独显示类的每个方法，并在后续阶段填充该类。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let us start with the :meth:`~torchrl.objectives.LossModule.__init__` "
"method. DDPG aims at solving a control task with a simple strategy: training"
" a policy to output actions that maximize the value predicted by a value "
"network. Hence, our loss module needs to receive two networks in its "
"constructor: an actor and a value networks. We expect both of these to be "
"TensorDict-compatible objects, such as "
":class:`tensordict.nn.TensorDictModule`. Our loss function will need to "
"compute a target value and fit the value network to this, and generate an "
"action and fit the policy such that its value estimate is maximized."
msgstr ""
"让我们从 :meth:`~torchrl.objectives.LossModule.__init__` 方法开始。DDPG "
"旨在通过一种简单策略解决控制任务：训练一个策略，以输出最大化价值网络预测价值的动作。因此，我们的损失模块需要在其构造函数中接收两个网络：一个为 "
"actor 网络，另一个为价值网络。我们期望这两个网络都是与 TensorDict 兼容的对象，例如 "
":class:`tensordict.nn.TensorDictModule`。我们的损失函数需要计算目标值并使价值网络拟合这个目标值，并生成一个动作并使策略拟合以最大化其价值估计。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The crucial step of the :meth:`LossModule.__init__` method is the call to "
":meth:`~torchrl.LossModule.convert_to_functional`. This method will extract "
"the parameters from the module and convert it to a functional module. "
"Strictly speaking, this is not necessary and one may perfectly code all the "
"losses without it. However, we encourage its usage for the following reason."
msgstr ""
":meth:`LossModule.__init__` 方法的关键步骤是调用 "
":meth:`~torchrl.LossModule.convert_to_functional`。此方法会从模块中提取参数并将其转换为功能模块。从严格意义上讲，这不是必须的，可以完全不使用它来编码所有损失。然而，我们鼓励使用它，原因如下。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The reason TorchRL does this is that RL algorithms often execute the same "
"model with different sets of parameters, called \"trainable\" and \"target\""
" parameters. The \"trainable\" parameters are those that the optimizer needs"
" to fit. The \"target\" parameters are usually a copy of the former's with "
"some time lag (absolute or diluted through a moving average). These target "
"parameters are used to compute the value associated with the next "
"observation. One the advantages of using a set of target parameters for the "
"value model that do not match exactly the current configuration is that they"
" provide a pessimistic bound on the value function being computed. Pay "
"attention to the ``create_target_params`` keyword argument below: this "
"argument tells the "
":meth:`~torchrl.objectives.LossModule.convert_to_functional` method to "
"create a set of target parameters in the loss module to be used for target "
"value computation. If this is set to ``False`` (see the actor network for "
"instance) the ``target_actor_network_params`` attribute will still be "
"accessible but this will just return a **detached** version of the actor "
"parameters."
msgstr ""
"TorchRL 这样做的原因是，RL "
"算法通常使用不同参数集（称为“可训练”和“目标”参数）多次执行相同模型。“可训练”参数是优化器需要拟合的参数。而“目标”参数通常是前者的副本，并有一定时间滞后（绝对或通过移动平均稀释）。这些目标参数用于计算与下一个观测相关的价值。使用一组与当前配置不完全匹配的目标参数的优点之一是，它们为计算的价值函数提供一个悲观的界限。请注意下面的"
" ``create_target_params`` 关键字参数：此参数告诉 "
":meth:`~torchrl.objectives.LossModule.convert_to_functional` "
"方法在损失模块中创建一组目标参数以用于目标值计算。如果设置为 ``False``（例如 actor "
"网络），``target_actor_network_params`` 属性将仍然可访问，但这将仅返回 actor 参数的 **分离** 版本。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Later, we will see how the target parameters should be updated in TorchRL."
msgstr "稍后，我们将讨论如何在 TorchRL 中更新目标参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The value estimator loss method"
msgstr "价值估计器损失方法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In many RL algorithm, the value network (or Q-value network) is trained "
"based on an empirical value estimate. This can be bootstrapped (TD(0), low "
"variance, high bias), meaning that the target value is obtained using the "
"next reward and nothing else, or a Monte-Carlo estimate can be obtained "
"(TD(1)) in which case the whole sequence of upcoming rewards will be used "
"(high variance, low bias). An intermediate estimator (TD(:math:`\\lambda`)) "
"can also be used to compromise bias and variance. TorchRL makes it easy to "
"use one or the other estimator via the "
":class:`~torchrl.objectives.utils.ValueEstimators` Enum class, which "
"contains pointers to all the value estimators implemented. Let us define the"
" default value function here. We will take the simplest version (TD(0)), and"
" show later on how this can be changed."
msgstr ""
"在许多 RL 算法中，价值网络（或 Q "
"值网络）的训练基于经验值估计。它可以是自举的（TD(0)，低方差，高偏差），这意味着目标值是通过下一个奖励获得的，或者可以是蒙特卡罗估计（TD(1)），在这种情况下，要使用即将到来的奖励序列（高方差，低偏差）。也可以使用中间估计器（TD(:math:`\\lambda`)）来权衡偏差和方差。TorchRL"
" 通过 :class:`~torchrl.objectives.utils.ValueEstimators` "
"枚举类方便地实现了这些估计器，它包含所有价值估计器的指针。让我们在这里定义默认值函数。我们将选择最简单的版本（TD(0)），并在后面展示如何更改此设置。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We also need to give some instructions to DDPG on how to build the value "
"estimator, depending on the user query. Depending on the estimator provided,"
" we will build the corresponding module to be used at train time:"
msgstr "我们还需要向 DDPG 提供一些如何构建价值估计器的指令，这取决于用户的需求。根据提供的估计器，我们将构建相应的模块以在训练时使用："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``make_value_estimator`` method can but does not need to be called: if "
"not, the :class:`~torchrl.objectives.LossModule` will query this method with"
" its default estimator."
msgstr ""
"``make_value_estimator`` "
"方法可以调用，也可以不调用：如果不调用，:class:`~torchrl.objectives.LossModule` 将使用其默认估计器自行查询。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The actor loss method"
msgstr "actor 损失方法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The central piece of an RL algorithm is the training loss for the actor. In "
"the case of DDPG, this function is quite simple: we just need to compute the"
" value associated with an action computed using the policy and optimize the "
"actor weights to maximize this value."
msgstr ""
"一个 RL 算法的核心部分是 actor 的训练损失。在 DDPG 的情况下，此功能相当简单：我们只需计算使用策略计算的一个动作相关的值，并优化 "
"actor 权重以最大化该值。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"When computing this value, we must make sure to take the value parameters "
"out of the graph, otherwise the actor and value loss will be mixed up. For "
"this, the :func:`~torchrl.objectives.utils.hold_out_params` function can be "
"used."
msgstr ""
"在计算此值时，我们必须确保将价值参数从计算图中移除，否则 actor 和价值损失会混在一起。为此，可以使用 "
":func:`~torchrl.objectives.utils.hold_out_params` 函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The value loss method"
msgstr "价值损失方法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We now need to optimize our value network parameters. To do this, we will "
"rely on the value estimator of our class:"
msgstr "我们现在需要优化我们的价值网络参数。为此，我们将依赖于我们类的价值估计器："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Putting things together in a forward call"
msgstr "将各部分结合到一个 forward 调用中"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The only missing piece is the forward method, which will glue together the "
"value and actor loss, collect the cost values and write them in a "
"``TensorDict`` delivered to the user."
msgstr ""
"唯一缺少的部分是 forward 方法，它将粘合价值和 actor 损失，收集成本值并将其写入提供给用户的 ``TensorDict`` 中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have our loss, we can use it to train a policy to solve a "
"control task."
msgstr "现在我们有了我们的损失，我们可以用它来训练一个政策以解决控制任务。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Environment"
msgstr "环境"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In most algorithms, the first thing that needs to be taken care of is the "
"construction of the environment as it conditions the remainder of the "
"training script."
msgstr "在大多数算法中，首先需要处理的是环境的构建，因为它影响其余训练脚本。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this example, we will be using the ``\"cheetah\"`` task. The goal is to "
"make a half-cheetah run as fast as possible."
msgstr "在本示例中，我们将使用 ``\"cheetah\"`` 任务。目标是让一个半猎豹尽可能快地奔跑。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In TorchRL, one can create such a task by relying on ``dm_control`` or "
"``gym``:"
msgstr "在 TorchRL 中，可以通过依赖 ``dm_control`` 或 ``gym`` 来创建这样的任务："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "or"
msgstr "或者"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"By default, these environment disable rendering. Training from states is "
"usually easier than training from images. To keep things simple, we focus on"
" learning from states only. To pass the pixels to the ``tensordicts`` that "
"are collected by :func:`env.step()`, simply pass the ``from_pixels=True`` "
"argument to the constructor:"
msgstr ""
"默认情况下，这些环境禁用了渲染。从状态进行训练通常比从图像进行训练更容易。为使事情简单，我们专注于仅从状态进行学习。要将像素传递到 "
":func:`env.step()` 收集的 ``tensordicts`` 中，只需将 ``from_pixels=True`` 参数传递给构造函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We write a :func:`make_env` helper function that will create an environment "
"with either one of the two backends considered above (``dm-control`` or "
"``gym``)."
msgstr ""
"我们编写了一个 :func:`make_env` 帮助函数，它将创建一个使用上述两个后端之一（``dm-control`` 或 ``gym``）的环境。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Transforms"
msgstr "变换"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have a base environment, we may want to modify its "
"representation to make it more policy-friendly. In TorchRL, transforms are "
"appended to the base environment in a specialized "
":class:`torchr.envs.TransformedEnv` class."
msgstr ""
"现在我们有了一个基础环境，我们可能希望修改其表示形式以使其更易于策略使用。在 TorchRL 中，变换被追加到一个专门的 "
":class:`torchr.envs.TransformedEnv` 类的基础环境中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It is common in DDPG to rescale the reward using some heuristic value. We "
"will multiply the reward by 5 in this example."
msgstr "在 DDPG 中，通常会根据某种启发式值重新缩放奖励。在本示例中，我们将奖励乘以 5。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we are using :mod:`dm_control`, it is also important to build an "
"interface between the simulator which works with double precision numbers, "
"and our script which presumably uses single precision ones. This "
"transformation goes both ways: when calling :func:`env.step`, our actions "
"will need to be represented in double precision, and the output will need to"
" be transformed to single precision. The "
":class:`~torchrl.envs.DoubleToFloat` transform does exactly this: the "
"``in_keys`` list refers to the keys that will need to be transformed from "
"double to float, while the ``in_keys_inv`` refers to those that need to be "
"transformed to double before being passed to the environment."
msgstr ""
"如果我们使用 "
":mod:`dm_control`，还需要在模拟器（使用双精度数值）与我们的脚本（假定使用单精度数值）之间构建一个接口。这种转换是双向的：调用 "
":func:`env.step` "
"时，我们的动作需要用双精度表示，而输出需要转换为单精度。:class:`~torchrl.envs.DoubleToFloat` "
"转换正是这么做的：``in_keys`` 列表表示需要从双精度转换为单精度的键，而 ``in_keys_inv`` "
"表示需要在传递给环境之前从单精度转换为双精度的键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We concatenate the state keys together using the "
":class:`~torchrl.envs.CatTensors` transform."
msgstr "我们使用 :class:`~torchrl.envs.CatTensors` 转换将状态键连接在一起。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, we also leave the possibility of normalizing the states: we will "
"take care of computing the normalizing constants later on."
msgstr "最后，我们还保留了归一化状态的可能性：稍后我们将负责计算归一化常数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Parallel execution"
msgstr "并行执行"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The following helper function allows us to run environments in parallel. "
"Running environments in parallel can significantly speed up the collection "
"throughput. When using transformed environment, we need to choose whether we"
" want to execute the transform individually for each environment, or "
"centralize the data and transform it in batch. Both approaches are easy to "
"code:"
msgstr ""
"以下辅助函数允许我们并行运行环境。并行运行环境可以显著提高数据收集的吞吐量。使用转换环境时，我们需要选择是单独为每个环境执行转换，还是将数据集中起来并批量转换。两种方法都容易实现："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To leverage the vectorization capabilities of PyTorch, we adopt the first "
"method:"
msgstr "为了利用 PyTorch 的矢量化能力，我们采用第一种方法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``frame_skip`` batches multiple step together with a single action If > 1, "
"the other frame counts (for example, frames_per_batch, total_frames) need to"
" be adjusted to have a consistent total number of frames collected across "
"experiments. This is important as raising the frame-skip but keeping the "
"total number of frames unchanged may seem like cheating: all things "
"compared, a dataset of 10M elements collected with a frame-skip of 2 and "
"another with a frame-skip of 1 actually have a ratio of interactions with "
"the environment of 2:1! In a nutshell, one should be cautious about the "
"frame-count of a training script when dealing with frame skipping as this "
"may lead to biased comparisons between training strategies."
msgstr ""
"``frame_skip`` 将多个步骤批量处理为一个动作。如果 frame_skip > 1，其他帧计数（如 frames_per_batch 和 "
"total_frames）需要调整，以在实验中收集一致的总帧数。这一点很重要，增加 frame-skip "
"而保持总帧数不变可能看起来像作弊：综合比较一下，使用 frame-skip 为 2 的数据集和使用 frame-skip 为 1 "
"的数据集的环境交互次数实际上为 2:1！简而言之，在处理帧跳过时，应该谨慎处理训练脚本的帧计数，因为这可能导致训练策略之间的比较产生偏差。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Scaling the reward helps us control the signal magnitude for a more "
"efficient learning."
msgstr "调整奖励有助于我们控制信号的幅度，从而实现更高效的学习。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We also define when a trajectory will be truncated. A thousand steps (500 if"
" frame-skip = 2) is a good number to use for the cheetah task:"
msgstr "我们还定义了何时截断一个轨迹。对猎豹任务而言，一千步（frame-skip = 2 时为 500 步）是一个合适的数字："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Normalization of the observations"
msgstr "观察值的归一化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To compute the normalizing statistics, we run an arbitrary number of random "
"steps in the environment and compute the mean and standard deviation of the "
"collected observations. The :func:`ObservationNorm.init_stats()` method can "
"be used for this purpose. To get the summary statistics, we create a dummy "
"environment and run it for a given number of steps, collect data over a "
"given number of steps and compute its summary statistics."
msgstr ""
"为了计算归一化统计数据，我们在环境中运行任意数量的随机步，并计算收集到的观察值的均值和标准差。可以使用 "
":func:`ObservationNorm.init_stats()` "
"方法完成此操作。为了获得汇总统计数据，我们创建一个虚拟环境并运行指定数量的步，收集数据并计算其汇总统计。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Normalization stats"
msgstr "归一化统计"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Number of random steps used as for stats computation using "
"``ObservationNorm``"
msgstr "使用 ``ObservationNorm`` 作为统计计算的随机步数量"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Number of environments in each data collector"
msgstr "每个数据收集器中的环境数量"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We pass the stats computed earlier to normalize the output of our "
"environment:"
msgstr "我们将之前计算的统计数据传递给归一化我们的环境输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building the model"
msgstr "构建模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We now turn to the setup of the model. As we have seen, DDPG requires a "
"value network, trained to estimate the value of a state-action pair, and a "
"parametric actor that learns how to select actions that maximize this value."
msgstr ""
"现在我们开始设置模型。如我们所见，DDPG 需要一个值网络，用于估计状态-动作对的值；还需要一个参数化的actor，它学习如何选择最大化值的动作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Recall that building a TorchRL module requires two steps:"
msgstr "请记住，构建一个 TorchRL 模块需要两个步骤："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "writing the :class:`torch.nn.Module` that will be used as network,"
msgstr "编写将用作网络的 :class:`torch.nn.Module`，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"wrapping the network in a :class:`tensordict.nn.TensorDictModule` where the "
"data flow is handled by specifying the input and output keys."
msgstr "将网络包装在 :class:`tensordict.nn.TensorDictModule` 中，其中通过指定输入和输出键来处理数据流。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In more complex scenarios, :class:`tensordict.nn.TensorDictSequential` can "
"also be used."
msgstr "在更复杂的情况下，还可以使用 :class:`tensordict.nn.TensorDictSequential`。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The Q-Value network is wrapped in a :class:`~torchrl.modules.ValueOperator` "
"that automatically sets the ``out_keys`` to ``\"state_action_value`` for "
"q-value networks and ``state_value`` for other value networks."
msgstr ""
"Q-值网络被包装在 :class:`~torchrl.modules.ValueOperator` 中，该操作会自动将 ``out_keys`` 设置为"
" ``\"state_action_value``（用于 Q 值网络）和 ``state_value``（用于其他值网络）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL provides a built-in version of the DDPG networks as presented in the"
" original paper. These can be found under "
":class:`~torchrl.modules.DdpgMlpActor` and "
":class:`~torchrl.modules.DdpgMlpQNet`."
msgstr ""
"TorchRL 提供了论文中介绍的 DDPG 网络的内置版本。这些可以通过 :class:`~torchrl.modules.DdpgMlpActor`"
" 和 :class:`~torchrl.modules.DdpgMlpQNet` 找到。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Since we use lazy modules, it is necessary to materialize the lazy modules "
"before being able to move the policy from device to device and achieve other"
" operations. Hence, it is good practice to run the modules with a small "
"sample of data. For this purpose, we generate fake data from the environment"
" specs."
msgstr ""
"由于我们使用惰性模块，所以在能够将策略从设备移动到设备并执行其他操作之前，必须实现惰性模块。因此，建议用一小部分数据运行模块。为此，我们从环境规格中生成伪数据。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Exploration"
msgstr "探索"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The policy is passed into a "
":class:`~torchrl.modules.OrnsteinUhlenbeckProcessModule` exploration module,"
" as suggested in the original paper. Let's define the number of frames "
"before OU noise reaches its minimum value"
msgstr ""
"策略被传递到 :class:`~torchrl.modules.OrnsteinUhlenbeckProcessModule` "
"探索模块中，正如原始论文所建议的。接下来定义 OU 噪声达到最小值之前的帧数"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Data collector"
msgstr "数据收集器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL provides specialized classes to help you collect data by executing "
"the policy in the environment. These \"data collectors\" iteratively compute"
" the action to be executed at a given time, then execute a step in the "
"environment and reset it when required. Data collectors are designed to help"
" developers have a tight control on the number of frames per batch of data, "
"on the (a)sync nature of this collection and on the resources allocated to "
"the data collection (for example GPU, number of workers, and so on)."
msgstr ""
"TorchRL "
"提供了专门的类，用于通过在环境中执行策略来帮助您收集数据。这些\"数据收集器\"迭代地计算在特定时间点要执行的动作，然后在环境中执行一步操作并在需要时重置环境。数据收集器设计用于帮助开发人员严格控制每批数据的帧数量、数据收集的同步/异步性质以及分配给数据收集的资源（例如"
" GPU、工作者数量等）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here we will use :class:`~torchrl.collectors.SyncDataCollector`, a simple, "
"single-process data collector. TorchRL offers other collectors, such as "
":class:`~torchrl.collectors.MultiaSyncDataCollector`, which executed the "
"rollouts in an asynchronous manner (for example, data will be collected "
"while the policy is being optimized, thereby decoupling the training and "
"data collection)."
msgstr ""
"这里我们将使用 :class:`~torchrl.collectors.SyncDataCollector`，一个简单的单进程数据收集器。TorchRL"
" 提供了其他收集器，例如 "
":class:`~torchrl.collectors.MultiaSyncDataCollector`，它以异步方式执行数据游走（例如，在优化策略时收集数据，从而解耦训练与数据收集）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The parameters to specify are:"
msgstr "需要指定的参数有："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "an environment factory or an environment,"
msgstr "一个环境工厂或环境，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "the policy,"
msgstr "策略，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "the total number of frames before the collector is considered empty,"
msgstr "在收集器被认为为空之前的总帧数，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"the maximum number of frames per trajectory (useful for non-terminating "
"environments, like ``dm_control`` ones)."
msgstr "每个轨迹的最大帧数（对于非终止环境，如 ``dm_control``）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``max_frames_per_traj`` passed to the collector will have the effect of "
"registering a new :class:`~torchrl.envs.StepCounter` transform with the "
"environment used for inference. We can achieve the same result manually, as "
"we do in this script."
msgstr ""
"传递给收集器的 ``max_frames_per_traj`` 将使推理所使用的环境注册一个新的 "
":class:`~torchrl.envs.StepCounter` 转换。我们可以像本脚本中这样手动实现同样的效果。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "One should also pass:"
msgstr "还应传递："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "the number of frames in each batch collected,"
msgstr "每批收集的帧数，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "the number of random steps executed independently from the policy,"
msgstr "独立于策略执行的随机步数量，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "the devices used for policy execution"
msgstr "用于策略执行的设备，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"the devices used to store data before the data is passed to the main "
"process."
msgstr "在数据传递给主进程之前用于存储数据的设备。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The total frames we will use during training should be around 1M."
msgstr "训练期间我们将使用的总帧数应约为 1 百万。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The number of frames returned by the collector at each iteration of the "
"outer loop is equal to the length of each sub-trajectories times the number "
"of environments run in parallel in each collector."
msgstr "收集器在外部循环的每次迭代中返回的帧数等于每个子轨迹的长度乘以每个收集器中并行运行的环境数量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In other words, we expect batches from the collector to have a shape "
"``[env_per_collector, traj_len]`` where "
"``traj_len=frames_per_batch/env_per_collector``:"
msgstr ""
"换句话说，我们期望来自收集器的批次形状为 ``[env_per_collector, traj_len]``，其中 "
"``traj_len=frames_per_batch/env_per_collector``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Evaluator: building your recorder object"
msgstr "评估器：构建您的记录器对象"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As the training data is obtained using some exploration strategy, the true "
"performance of our algorithm needs to be assessed in deterministic mode. We "
"do this using a dedicated class, ``Recorder``, which executes the policy in "
"the environment at a given frequency and returns some statistics obtained "
"from these simulations."
msgstr ""
"由于训练数据是通过某种探索策略获得的，我们的算法的真实性能需要在确定性模式下进行评估。我们使用一个专门的类 ``Recorder`` "
"来完成这一点，它根据给定频率在环境中执行策略并返回从这些模拟中获得的统计数据。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The following helper function builds this object:"
msgstr "以下辅助函数将构建该对象："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We will be recording the performance every 10 batch collected"
msgstr "我们将每收集 10 批次记录一次性能"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Replay buffer"
msgstr "回放缓冲区"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Replay buffers come in two flavors: prioritized (where some error signal is "
"used to give a higher likelihood of sampling to some items than others) and "
"regular, circular experience replay."
msgstr "回放缓冲区有两种类型：优先级（某些误差信号用于赋予某些条目比其他条目更高的采样概率）和常规循环经验回放。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL replay buffers are composable: one can pick up the storage, sampling"
" and writing strategies. It is also possible to store tensors on physical "
"memory using a memory-mapped array. The following function takes care of "
"creating the replay buffer with the desired hyperparameters:"
msgstr ""
"TorchRL "
"的回放缓冲区是可组合的：可以选择存储、采样和写入策略。还可以使用内存映射数组将张量存储在物理内存中。以下函数负责创建具有所需超参数的回放缓冲区："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We'll store the replay buffer in a temporary directory on disk"
msgstr "我们将在磁盘的临时目录中存储回放缓冲区"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Replay buffer storage and batch size"
msgstr "回放缓冲区存储和批大小"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL replay buffer counts the number of elements along the first "
"dimension. Since we'll be feeding trajectories to our buffer, we need to "
"adapt the buffer size by dividing it by the length of the sub-trajectories "
"yielded by our data collector. Regarding the batch-size, our sampling "
"strategy will consist in sampling trajectories of length ``traj_len=200`` "
"before selecting sub-trajectories or length ``random_crop_len=25`` on which "
"the loss will be computed. This strategy balances the choice of storing "
"whole trajectories of a certain length with the need for providing samples "
"with a sufficient heterogeneity to our loss. The following figure shows the "
"dataflow from a collector that gets 8 frames in each batch with 2 "
"environments run in parallel, feeds them to a replay buffer that contains "
"1000 trajectories and samples sub-trajectories of 2 time steps each."
msgstr ""
"TorchRL "
"回放缓冲区沿第一个维度计数元素数量。由于我们将向缓冲区馈送轨迹，因此我们需要通过将缓冲区大小除以数据收集器生成的子轨迹长度来调整缓冲区大小。关于批大小，我们的采样策略将包括在选择用于计算损失的长度为"
" ``random_crop_len=25`` 的子轨迹之前，采样长度为 ``traj_len=200`` "
"的轨迹。这一策略平衡了存储一定长度的完整轨迹与为损失提供具有足够异质性的样本之间的选择。以下图展示了一个数据流，在并行运行 2 "
"个环境的收集器中，每批收集 8 帧，将这些帧馈送至包含 1000 个轨迹的回放缓冲区，并对每 2 个时间步的子轨迹进行采样。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Storing trajectories in the replay buffer"
msgstr "将轨迹存储到回放缓冲区"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let's start with the number of frames stored in the buffer"
msgstr "从缓冲区中存储的帧数量开始"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Prioritized replay buffer is disabled by default"
msgstr "优先回放缓冲区默认情况下是禁用的"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We also need to define how many updates we'll be doing per batch of data "
"collected. This is known as the update-to-data or ``UTD`` ratio:"
msgstr "我们还需要定义每批收集的数据要进行的更新次数。这称为更新对数据的比率（``UTD`` 比率）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We'll be feeding the loss with trajectories of length 25:"
msgstr "我们将向损失馈送长度为 25 的轨迹："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the original paper, the authors perform one update with a batch of 64 "
"elements for each frame collected. Here, we reproduce the same ratio but "
"while realizing several updates at each batch collection. We adapt our "
"batch-size to achieve the same number of update-per-frame ratio:"
msgstr ""
"在原始论文中，作者使用一批 64 "
"个元素进行一次更新，用于每收集到的帧。在这里，我们再现相同的更新对帧比率，但在每批数据收集时进行多次更新。我们调整批大小以实现相同的更新对帧比率："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Loss module construction"
msgstr "损失模块构建"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We build our loss module with the actor and ``qnet`` we've just created. "
"Because we have target parameters to update, we _must_ create a target "
"network updater."
msgstr "我们使用刚才创建的 actor 和 ``qnet`` 构建我们的损失模块。由于我们有目标参数需要更新，_必须_创建目标网络更新器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "let's use the TD(lambda) estimator!"
msgstr "让我们使用 TD(lambda)估算器！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Off-policy usually dictates a TD(0) estimator. Here, we use a "
"TD(:math:`\\lambda`) estimator, which will introduce some bias as the "
"trajectory that follows a certain state has been collected with an outdated "
"policy. This trick, as the multi-step trick that can be used during data "
"collection, are alternative versions of \"hacks\" that we usually find to "
"work well in practice despite the fact that they introduce some bias in the "
"return estimates."
msgstr ""
"离线策略通常使用 TD(0) 估算器。这里，我们使用 TD(:math:`\\lambda`) "
"估算器，这将引入一些偏差，因为遵循某种状态的轨迹是用过期策略收集的。这一技巧，以及可以在数据收集期间使用的多步技巧，是通常发现尽管会引入偏差但在实际中提供良好效果的\"小窍门\"的替代版本。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Target network updater"
msgstr "目标网络更新器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Target networks are a crucial part of off-policy RL algorithms. Updating the"
" target network parameters is made easy thanks to the "
":class:`~torchrl.objectives.HardUpdate` and "
":class:`~torchrl.objectives.SoftUpdate` classes. They're built with the loss"
" module as argument, and the update is achieved via a call to "
"`updater.step()` at the appropriate location in the training loop."
msgstr ""
"目标网络是离线策略强化学习算法的重要部分。更新目标网络参数在 :class:`~torchrl.objectives.HardUpdate` 和 "
":class:`~torchrl.objectives.SoftUpdate` "
"类的帮助下变得简单。它们以损失模块作为参数构建，通过在训练循环中的适当位置调用 `updater.step()` 方法进行更新。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Optimizer"
msgstr "优化器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, we will use the Adam optimizer for the policy and value network:"
msgstr "最后，我们将使用 Adam 优化器对策略和值网络进行优化："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Time to train the policy"
msgstr "开始训练策略"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The training loop is pretty straightforward now that we have built all the "
"modules we need."
msgstr "现在我们已经构建了所需的所有模块，训练循环非常直接。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Experiment results"
msgstr "实验结果"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We make a simple plot of the average rewards during training. We can observe"
" that our policy learned quite well to solve the task."
msgstr "我们制作了一个训练期间平均奖励的简单图表。可以观察到我们的策略很好地学会了完成任务。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As already mentioned above, to get a more reasonable performance, use a "
"greater value for ``total_frames`` for example, 1M."
msgstr "如上所述，为了获得更合理的性能，请使用较大的``total_frames``值，例如1百万。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Conclusion"
msgstr "总结"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we have learned how to code a loss module in TorchRL given"
" the concrete example of DDPG."
msgstr "在本教程中，我们学习了如何在TorchRL中编码一个损失模块，以DDPG为具体示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The key takeaways are:"
msgstr "关键要点是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How to use the :class:`~torchrl.objectives.LossModule` class to code up a "
"new loss component;"
msgstr "如何使用:class:`~torchrl.objectives.LossModule`类来编写一个新的损失组件；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How to use (or not) a target network, and how to update its parameters;"
msgstr "如何使用（或不使用）目标网络，以及如何更新其参数；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to create an optimizer associated with a loss module."
msgstr "如何创建与损失模块关联的优化器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Next Steps"
msgstr "接下来的步骤"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "To iterate further on this loss module we might consider:"
msgstr "为了进一步完善该损失模块，我们可以考虑："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Using `@dispatch` (see `[Feature] Distpatch IQL loss module "
"<https://github.com/pytorch/rl/pull/1230>`_.)"
msgstr ""
"使用`@dispatch`（参见`[Feature] Dispatch IQL loss module "
"<https://github.com/pytorch/rl/pull/1230>`_）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Allowing flexible TensorDict keys."
msgstr "允许灵活的TensorDict键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Total running time of the script:** ( 0 minutes  0.000 seconds)"
msgstr "**脚本的总运行时间:**（0分钟 0.000秒）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: coding_ddpg.py <coding_ddpg.py>`"
msgstr ":download:`下载Python源代码: coding_ddpg.py <coding_ddpg.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: coding_ddpg.ipynb <coding_ddpg.ipynb>`"
msgstr ":download:`下载Jupyter notebook: coding_ddpg.ipynb <coding_ddpg.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`由Sphinx-Gallery生成的图库 <https://sphinx-gallery.github.io>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Autograd in C++ Frontend"
msgstr "C++前端中的自动梯度"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``autograd`` package is crucial for building highly flexible and dynamic"
" neural networks in PyTorch. Most of the autograd APIs in PyTorch Python "
"frontend are also available in C++ frontend, allowing easy translation of "
"autograd code from Python to C++."
msgstr ""
"``autograd``包对于在PyTorch中构建高度灵活和动态的神经网络至关重要。PyTorch "
"Python前端中的大多数自动梯度API也可以在C++前端中使用，从而简化了将自动梯度代码从Python转换为C++的过程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial explore several examples of doing autograd in PyTorch C++ "
"frontend. Note that this tutorial assumes that you already have a basic "
"understanding of autograd in Python frontend. If that's not the case, please"
" first read `Autograd: Automatic Differentiation "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`_."
msgstr ""
"在本教程中，探索了如何在PyTorch "
"C++前端进行自动梯度计算的一些示例。请注意，本教程假定您已经对Python前端中的自动梯度有了基本的理解。如果没有，请先阅读`自动梯度：自动微分 "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Basic autograd operations"
msgstr "基本自动梯度操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"(Adapted from `this tutorial "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#autograd-"
"automatic-differentiation>`_)"
msgstr ""
"（改编自`本教程 "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#autograd-"
"automatic-differentiation>`_）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Create a tensor and set ``torch::requires_grad()`` to track computation with"
" it"
msgstr "创建一个张量并设置``torch::requires_grad()``以跟踪其计算"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Out:"
msgstr "输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Do a tensor operation:"
msgstr "执行一个张量操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``y`` was created as a result of an operation, so it has a ``grad_fn``."
msgstr "由于``y``是由操作生成的，因此它有一个``grad_fn``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Do more operations on ``y``"
msgstr "对``y``进行更多操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``.requires_grad_( ... )`` changes an existing tensor's ``requires_grad`` "
"flag in-place."
msgstr "``.requires_grad_(...)``就地更改现有张量的``requires_grad``标志。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's backprop now. Because ``out`` contains a single scalar, "
"``out.backward()`` is equivalent to ``out.backward(torch::tensor(1.))``."
msgstr ""
"现在让我们反向传播。由于``out``包含单个标量，``out.backward()``相当于``out.backward(torch::tensor(1.))``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Print gradients d(out)/dx"
msgstr "打印梯度d(out)/dx"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You should have got a matrix of ``4.5``. For explanations on how we arrive "
"at this value, please see `the corresponding section in this tutorial "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients>`_."
msgstr ""
"您应该得到一个``4.5``的矩阵。有关于如何得出此值的解释，请参阅`教程中的相应部分 "
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Now let's take a look at an example of vector-Jacobian product:"
msgstr "现在让我们来看一个向量-雅可比矩阵积的示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we want the vector-Jacobian product, pass the vector to ``backward`` as "
"argument:"
msgstr "如果我们想要向量-雅可比矩阵积，就将向量作为参数传递给``backward``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can also stop autograd from tracking history on tensors that require "
"gradients either by putting ``torch::NoGradGuard`` in a code block"
msgstr "您还可以通过将``torch::NoGradGuard``放置在代码块中来停止自动梯度在需要梯度的张量上跟踪历史记录。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Or by using ``.detach()`` to get a new tensor with the same content but that"
" does not require gradients:"
msgstr "或者通过使用``.detach()``获取一个具有相同内容但不需要梯度的新张量："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For more information on C++ tensor autograd APIs such as ``grad`` / "
"``requires_grad`` / ``is_leaf`` / ``backward`` / ``detach`` / ``detach_`` / "
"``register_hook`` / ``retain_grad``, please see `the corresponding C++ API "
"docs <https://pytorch.org/cppdocs/api/classat_1_1_tensor.html>`_."
msgstr ""
"有关C++张量自动梯度API的更多信息，例如``grad`` / ``requires_grad`` / ``is_leaf`` / "
"``backward`` / ``detach`` / ``detach_`` / ``register_hook`` / "
"``retain_grad``，请参阅`相应的C++ API文档 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Computing higher-order gradients in C++"
msgstr "在C++中计算高阶梯度"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"One of the applications of higher-order gradients is calculating gradient "
"penalty. Let's see an example of it using ``torch::autograd::grad``:"
msgstr "高阶梯度的一个应用是计算梯度惩罚。以下是使用``torch::autograd::grad``的示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Please see the documentation for ``torch::autograd::backward`` (`link "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1403bf65b1f4f8c8506a9e6e5312d030.html>`_)"
" and ``torch::autograd::grad`` (`link "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1ab9fa15dc09a8891c26525fb61d33401a.html>`_)"
" for more information on how to use them."
msgstr ""
"有关``torch::autograd::backward`` (`链接 "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1403bf65b1f4f8c8506a9e6e5312d030.html>`_)"
" 和``torch::autograd::grad`` (`链接 "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1ab9fa15dc09a8891c26525fb61d33401a.html>`_)"
" 的更多信息，请参阅文档。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using custom autograd function in C++"
msgstr "在C++中使用自定义自动梯度函数"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"(Adapted from `this tutorial "
"<https://pytorch.org/docs/stable/notes/extending.html#extending-torch-"
"autograd>`_)"
msgstr ""
"（改编自`本教程 <https://pytorch.org/docs/stable/notes/extending.html#extending-"
"torch-autograd>`_）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Adding a new elementary operation to ``torch::autograd`` requires "
"implementing a new ``torch::autograd::Function`` subclass for each "
"operation. ``torch::autograd::Function`` s are what ``torch::autograd`` uses"
" to compute the results and gradients, and encode the operation history. "
"Every new function requires you to implement 2 methods: ``forward`` and "
"``backward``, and please see `this link "
"<https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html>`_"
" for the detailed requirements."
msgstr ""
"向``torch::autograd``添加新的基本操作需要为每个操作实现一个新的``torch::autograd::Function``子类。``torch::autograd::Function``用于计算结果和梯度并编码操作历史。每个新函数需要实现2个方法：``forward``和``backward``，更多详细要求请参阅`此链接"
" "
"<https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Below you can find code for a ``Linear`` function from ``torch::nn``:"
msgstr "以下是``torch::nn``中``Linear``函数的代码："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Then, we can use the ``LinearFunction`` in the following way:"
msgstr "然后，我们可以按以下方式使用``LinearFunction``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here, we give an additional example of a function that is parametrized by "
"non-tensor arguments:"
msgstr "这里，我们给出了一个按非张量参数化函数的额外示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Then, we can use the ``MulConstant`` in the following way:"
msgstr "然后，我们可以按以下方式使用``MulConstant``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For more information on ``torch::autograd::Function``, please see `its "
"documentation "
"<https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html>`_."
msgstr ""
"有关``torch::autograd::Function``的更多信息，请参阅`其文档 "
"<https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Translating autograd code from Python to C++"
msgstr "将自动梯度代码从Python翻译到C++"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"On a high level, the easiest way to use autograd in C++ is to have working "
"autograd code in Python first, and then translate your autograd code from "
"Python to C++ using the following table:"
msgstr ""
"从高层次看，在C++中使用自动梯度最简单的方法是首先在Python中实现有效的自动梯度代码，然后按照以下表格将您的Python代码翻译为C++："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Python"
msgstr "Python"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "C++"
msgstr "C++"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.autograd.backward``"
msgstr "``torch.autograd.backward``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::autograd::backward`` (`link "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1403bf65b1f4f8c8506a9e6e5312d030.html>`_)"
msgstr ""
"``torch::autograd::backward`` (`链接 "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1403bf65b1f4f8c8506a9e6e5312d030.html>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.autograd.grad``"
msgstr "``torch.autograd.grad``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::autograd::grad`` (`link "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1ab9fa15dc09a8891c26525fb61d33401a.html>`_)"
msgstr ""
"``torch::autograd::grad`` (`链接 "
"<https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1ab9fa15dc09a8891c26525fb61d33401a.html>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.detach``"
msgstr "``torch.Tensor.detach``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::detach`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor6detachEv>`_)"
msgstr ""
"``torch::Tensor::detach`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor6detachEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.detach_``"
msgstr "``torch.Tensor.detach_``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::detach_`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7detach_Ev>`_)"
msgstr ""
"``torch::Tensor::detach_`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7detach_Ev>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.backward``"
msgstr "``torch.Tensor.backward``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::backward`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8backwardERK6Tensorbb>`_)"
msgstr ""
"``torch::Tensor::backward`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8backwardERK6Tensorbb>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.register_hook``"
msgstr "``torch.Tensor.register_hook``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::register_hook`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor13register_hookE18hook_return_void_tI1TERR1T>`_)"
msgstr ""
"``torch::Tensor::register_hook`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4I0ENK2at6Tensor13register_hookE18hook_return_void_tI1TERR1T>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.requires_grad``"
msgstr "``torch.Tensor.requires_grad``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::requires_grad_`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor14requires_grad_Eb>`_)"
msgstr ""
"``torch::Tensor::requires_grad_`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor14requires_grad_Eb>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.retain_grad``"
msgstr "``torch.Tensor.retain_grad``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::retain_grad`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor11retain_gradEv>`_)"
msgstr ""
"``torch::Tensor::retain_grad`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor11retain_gradEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.grad``"
msgstr "``torch.Tensor.grad``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::grad`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4gradEv>`_)"
msgstr ""
"``torch::Tensor::grad`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4gradEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.grad_fn``"
msgstr "``torch.Tensor.grad_fn``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::grad_fn`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7grad_fnEv>`_)"
msgstr ""
"``torch::Tensor::grad_fn`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7grad_fnEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.set_data``"
msgstr "``torch.Tensor.set_data``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::set_data`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8set_dataERK6Tensor>`_)"
msgstr ""
"``torch::Tensor::set_data`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor8set_dataERK6Tensor>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.data``"
msgstr "``torch.Tensor.data``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::data`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4dataEv>`_)"
msgstr ""
"``torch::Tensor::data`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4dataEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.output_nr``"
msgstr "``torch.Tensor.output_nr``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::output_nr`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor9output_nrEv>`_)"
msgstr ""
"``torch::Tensor::output_nr`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor9output_nrEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.Tensor.is_leaf``"
msgstr "``torch.Tensor.is_leaf``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::Tensor::is_leaf`` (`link "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7is_leafEv>`_)"
msgstr ""
"``torch::Tensor::is_leaf`` (`链接 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7is_leafEv>`_)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After translation, most of your Python autograd code should just work in "
"C++. If that's not the case, please file a bug report at `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_ and we will fix it as soon as "
"possible."
msgstr ""
"翻译后，大多数Python自动梯度代码在C++中应当能够正常工作。如果无法正常工作，请通过`GitHub问题 "
"<https://github.com/pytorch/pytorch/issues>`_提交错误报告，我们将尽快修复。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You should now have a good overview of PyTorch's C++ autograd API. You can "
"find the code examples displayed in this note `here "
"<https://github.com/pytorch/examples/tree/master/cpp/autograd>`_. As always,"
" if you run into any problems or have questions, you can use our `forum "
"<https://discuss.pytorch.org/>`_ or `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_ to get in touch."
msgstr ""
"您现在应该对PyTorch C++自动梯度API有了良好的概览。可以通过`这里 "
"<https://github.com/pytorch/examples/tree/master/cpp/autograd>`_找到本笔记中显示的代码示例。如果遇到任何问题或有疑问，您可以使用我们的`论坛"
" <https://discuss.pytorch.org/>`_或`GitHub问题 "
"<https://github.com/pytorch/pytorch/issues>`_与我们联系。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using CUDA Graphs in PyTorch C++ API"
msgstr "在PyTorch C++ API中使用CUDA图"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"|edit| View and edit this tutorial in `GitHub "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst>`__."
" The full source code is available on `GitHub "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs>`__."
msgstr ""
"|编辑| 在`GitHub "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst>`__中查看和编辑本教程。完整的源代码可在`GitHub"
" "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs>`__中找到。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Prerequisites:"
msgstr "先决条件："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`Using the PyTorch C++ Frontend <../advanced_source/cpp_frontend.html>`__"
msgstr "`使用PyTorch C++前端 <../advanced_source/cpp_frontend.html>`__"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "`CUDA semantics <https://pytorch.org/docs/master/notes/cuda.html>`__"
msgstr "`CUDA语义 <https://pytorch.org/docs/master/notes/cuda.html>`__"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Pytorch 2.0 or later"
msgstr "Pytorch 2.0或更高版本"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "CUDA 11 or later"
msgstr "CUDA 11或更高版本"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"NVIDIA’s CUDA Graphs have been a part of CUDA Toolkit library since the "
"release of `version 10 <https://developer.nvidia.com/blog/cuda-graphs/>`_. "
"They are capable of greatly reducing the CPU overhead increasing the "
"performance of applications."
msgstr ""
"自从`版本10 <https://developer.nvidia.com/blog/cuda-"
"graphs/>`_发布以来，NVIDIA的CUDA图已成为CUDA Toolkit库的一部分。它们能够显著减少CPU开销，提高应用程序的性能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we will be focusing on using CUDA Graphs for `C++ frontend"
" of PyTorch <https://pytorch.org/tutorials/advanced/cpp_frontend.html>`_. "
"The C++ frontend is mostly utilized in production and deployment "
"applications which are important parts of PyTorch use cases. Since `the "
"first appearance <https://pytorch.org/blog/accelerating-pytorch-with-cuda-"
"graphs/>`_ the CUDA Graphs won users’ and developer’s hearts for being a "
"very performant and at the same time simple-to-use tool. In fact, CUDA "
"Graphs are used by default in ``torch.compile`` of PyTorch 2.0 to boost the "
"productivity of training and inference."
msgstr ""
"在本教程中，我们将专注于为`PyTorch C++前端 "
"<https://pytorch.org/tutorials/advanced/cpp_frontend.html>`_使用CUDA图。C++前端主要用于生产和部署应用，这些是PyTorch用例的重要组成部分。从`首次亮相"
" <https://pytorch.org/blog/accelerating-pytorch-with-cuda-"
"graphs/>`_以来，CUDA图因其高性能且易于使用而深得用户和开发者青睐。实际上，在PyTorch "
"2.0的``torch.compile``中，默认使用CUDA图来提高训练和推理的效率。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We would like to demonstrate CUDA Graphs usage on PyTorch’s `MNIST example "
"<https://github.com/pytorch/examples/tree/main/cpp/mnist>`_. The usage of "
"CUDA Graphs in LibTorch (C++ Frontend) is very similar to its `Python "
"counterpart <https://pytorch.org/docs/main/notes/cuda.html#cuda-graphs>`_ "
"but with some differences in syntax and functionality."
msgstr ""
"我们希望展示在PyTorch的`MNIST示例 "
"<https://github.com/pytorch/examples/tree/main/cpp/mnist>`_中使用CUDA图的过程。在LibTorch（C++前端）中使用CUDA图与其`Python对应"
" <https://pytorch.org/docs/main/notes/cuda.html#cuda-"
"graphs>`_非常相似，但在语法和功能上有一些不同。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Getting Started"
msgstr "开始"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The main training loop consists of the several steps and depicted in the "
"following code chunk:"
msgstr "主要的训练循环包括以下几个步骤，并显示在以下代码片段中："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The example above includes a forward pass, a backward pass, and weight "
"updates."
msgstr "上面的示例包括前向传播、反向传播和权重更新。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we will be applying CUDA Graph on all the compute steps "
"through the whole-network graph capture. But before doing so, we need to "
"slightly modify the source code. What we need to do is preallocate tensors "
"for reusing them in the main training loop. Here is an example "
"implementation:"
msgstr ""
"在本教程中，我们将在整个网络图捕获中应用CUDA图的所有计算步骤。但在此之前，我们需要略微修改源代码。我们需要做的是预分配张量以便在主训练循环中重用它们。以下是一个示例实现："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Where ``training_step`` simply consists of forward and backward passes with "
"corresponding optimizer calls:"
msgstr "其中``training_step``仅包含前向和反向传播以及相应的优化器调用："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch’s CUDA Graphs API is relying on Stream Capture which in our case "
"would be used like this:"
msgstr "PyTorch 的 CUDA Graphs API 依赖于 Stream Capture，在我们的例子中可以这样使用："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Before the actual graph capture, it is important to run several warm-up "
"iterations on side stream to prepare CUDA cache as well as CUDA libraries "
"(like CUBLAS and CUDNN) that will be used during the training:"
msgstr ""
"在实际捕获图之前，重要的是在侧流上运行几个预热迭代，以准备 CUDA 缓存以及在训练期间将使用的 CUDA 库（如 CUBLAS 和 CUDNN）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After the successful graph capture, we can replace ``training_step(model, "
"optimizer, data, targets, output, loss);`` call via ``graph.replay();`` to "
"do the training step."
msgstr ""
"在成功捕获图后，我们可以通过 ``graph.replay();`` 替代调用 ``training_step(model, optimizer, "
"data, targets, output, loss);`` 来执行训练步骤。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Training Results"
msgstr "训练结果"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Taking the code for a spin we can see the following output from ordinary "
"non-graphed training:"
msgstr "运行代码后，我们可以看到来自普通非图化训练的以下输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "While the training with the CUDA Graph produces the following output:"
msgstr "使用 CUDA Graph 进行训练时产生以下输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As we can see, just by applying a CUDA Graph on the `MNIST example "
"<https://github.com/pytorch/examples/tree/main/cpp/mnist>`_ we were able to "
"gain the performance by more than six times for training. This kind of large"
" performance improvement was achievable due to the small model size. In case"
" of larger models with heavy GPU usage, the CPU overhead is less impactful "
"so the improvement will be smaller. Nevertheless, it is always advantageous "
"to use CUDA Graphs to gain the performance of GPUs."
msgstr ""
"正如我们所看到的，仅仅通过在 `MNIST 示例 "
"<https://github.com/pytorch/examples/tree/main/cpp/mnist>`_ 上应用 CUDA "
"Graph，我们就将训练性能提升了六倍以上。这种大的性能改进是由于模型较小。在使用 GPU 负载较重的大型模型的情况下，CPU "
"的开销较小，因此改进幅度会更小。然而，使用 CUDA Graphs 总是有助于提高 GPU 的性能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Custom C++ and CUDA Operators"
msgstr "自定义 C++ 和 CUDA 算子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author:** `Richard Zou <https://github.com/zou3519>`_"
msgstr "**作者：** `Richard Zou <https://github.com/zou3519>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"What you will learn"
msgstr ""
"您将学到什么"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to integrate custom operators written in C++/CUDA with PyTorch"
msgstr "如何将用 C++/CUDA 编写的自定义算子集成到 PyTorch 中"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to test custom operators using ``torch.library.opcheck``"
msgstr "如何使用 ``torch.library.opcheck`` 测试自定义算子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Prerequisites"
msgstr ""
"前置条件"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PyTorch 2.4 or later"
msgstr "PyTorch 2.4 或更高版本"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Basic understanding of C++ and CUDA programming"
msgstr "对 C++ 和 CUDA 编程的基本了解"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial will also work on AMD ROCm with no additional modifications."
msgstr "本教程在 AMD ROCm 中也可以运行，无需额外修改。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch offers a large library of operators that work on Tensors (e.g. "
"torch.add, torch.sum, etc). However, you may wish to bring a new custom "
"operator to PyTorch. This tutorial demonstrates the blessed path to "
"authoring a custom operator written in C++/CUDA."
msgstr ""
"PyTorch 提供了一个操作符的大型库，这些操作符适用于张量（例如 torch.add、torch.sum "
"等）。然而，您可能希望将新的自定义操作符引入 PyTorch。本教程展示了编写用 C++/CUDA 编写的自定义操作符的推荐路径。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For our tutorial, we’ll demonstrate how to author a fused multiply-add C++ "
"and CUDA operator that composes with PyTorch subsystems. The semantics of "
"the operation are as follows:"
msgstr ""
"在我们的教程中，我们将演示如何编写一个融合了乘法和加法的 C++ 和 CUDA 操作符，并将其与 PyTorch 子系统结合使用。该操作的语义如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can find the end-to-end working example for this tutorial `here "
"<https://github.com/pytorch/extension-cpp>`_ ."
msgstr "您可以在 `此处 <https://github.com/pytorch/extension-cpp>`_ 找到本教程的端到端工作示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Setting up the Build System"
msgstr "设置构建系统"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you are developing custom C++/CUDA code, it must be compiled. Note that "
"if you’re interfacing with a Python library that already has bindings to "
"precompiled C++/CUDA code, you might consider writing a custom Python "
"operator instead (:ref:`python-custom-ops-tutorial`)."
msgstr ""
"如果您正在开发自定义的 C++/CUDA 代码，它必须编译。注意，如果您正在与已绑定到预编译 C++/CUDA 代码的 Python "
"库接口，您可能会考虑编写一个自定义 Python 操作符 (:ref:`python-custom-ops-tutorial`)。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use `torch.utils.cpp_extension "
"<https://pytorch.org/docs/stable/cpp_extension.html>`_ to compile custom "
"C++/CUDA code for use with PyTorch C++ extensions may be built either "
"\"ahead of time\" with setuptools, or \"just in time\" via `load_inline "
"<https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline>`_;"
" we’ll focus on the \"ahead of time\" flavor."
msgstr ""
"使用 `torch.utils.cpp_extension "
"<https://pytorch.org/docs/stable/cpp_extension.html>`_ 编译自定义 C++/CUDA 代码以与 "
"PyTorch 一起使用。C++ 扩展可以通过 setuptools \"提前编译\"，也可以 \"实时编译\" 通过 `load_inline "
"<https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline>`_；我们将重点关注"
" \"提前编译\" 的方式。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Using ``cpp_extension`` is as simple as writing the following ``setup.py``:"
msgstr "使用 ``cpp_extension`` 就像编写以下 ``setup.py`` 一样简单："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you need to compile CUDA code (for example, ``.cu`` files), then instead "
"use `torch.utils.cpp_extension.CUDAExtension "
"<https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension>`_."
" Please see `extension-cpp <https://github.com/pytorch/extension-cpp>`_ for "
"an example for how this is set up."
msgstr ""
"如果您需要编译 CUDA 代码（例如 ``.cu`` 文件），那么请改用 "
"`torch.utils.cpp_extension.CUDAExtension "
"<https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension>`_。有关如何设置的示例，请参阅"
" `extension-cpp <https://github.com/pytorch/extension-cpp>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The above example represents what we refer to as a CPython agnostic wheel, "
"meaning we are building a single wheel that can be run across multiple "
"CPython versions (similar to pure Python packages). CPython agnosticism is "
"desirable in minimizing the number of wheels your custom library needs to "
"support and release. The minimum version we'd like to support is 3.9, since "
"it is the oldest supported version currently, so we use the corresponding "
"hexcode and specifier throughout the setup code. We suggest building the "
"extension in the same environment as the minimum CPython version you'd like "
"to support to minimize unknown behavior, so, here, we build the extension in"
" a CPython 3.9 environment. When built, this single wheel will be runnable "
"in any CPython environment 3.9+. To achieve this, there are three key lines "
"to note."
msgstr ""
"上面的示例代表了我们所称的 CPython 无关轮，这意味着我们正在构建一个可以跨多个 CPython 版本运行的单一轮子（类似于纯 Python "
"包）。CPython 无关性在尽量减少自定义库需要支持和发布的轮子数量方面是理想的。我们希望支持的最低版本是 "
"3.9，因为它是目前支持的最旧版本，所以我们在整个 setup 代码中使用了相应的十六进制代码和说明符。我们建议在您希望支持的最低 CPython "
"版本的同一环境中构建扩展，以最大限度减少未知行为，因此，在这里，我们在 CPython 3.9 环境中构建扩展。当构建完成后，这个单一轮子将在任何 "
"CPython 环境 3.9+ 都能运行。为实现这一目标，有三个关键点需要注意。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The first is the specification of ``Py_LIMITED_API`` in "
"``extra_compile_args`` to the minimum CPython version you would like to "
"support:"
msgstr ""
"第一个是在 ``extra_compile_args`` 中指定您希望支持的最低 CPython 版本的 ``Py_LIMITED_API``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Defining the ``Py_LIMITED_API`` flag helps verify that the extension is in "
"fact only using the `CPython Stable Limited API "
"<https://docs.python.org/3/c-api/stable.html>`_, which is a requirement for "
"the building a CPython agnostic wheel. If this requirement is not met, it is"
" possible to build a wheel that looks CPython agnostic but will crash, or "
"worse, be silently incorrect, in another CPython environment. Take care to "
"avoid using unstable CPython APIs, for example APIs from libtorch_python (in"
" particular pytorch/python bindings,) and to only use APIs from libtorch "
"(ATen objects, operators and the dispatcher). We strongly recommend defining"
" the ``Py_LIMITED_API`` flag to help ascertain the extension is compliant "
"and safe as a CPython agnostic wheel. Note that defining this flag is not a "
"full guarantee that the built wheel is CPython agnostic, but it is better "
"than the wild wild west. There are several caveats mentioned in the `Python "
"docs <https://docs.python.org/3/c-api/stable.html#limited-api-caveats>`_, "
"and you should test and verify yourself that the wheel is truly agnostic for"
" the relevant CPython versions."
msgstr ""
"定义 ``Py_LIMITED_API`` 标志有助于验证扩展实际上仅使用了 `CPython 稳定限制 API "
"<https://docs.python.org/3/c-api/stable.html>`_，这是构建 CPython "
"无关轮的要求。如果不满足此要求，可能会构建一个看似 CPython 无关但在另一个 CPython 环境中崩溃或更糟糕的错误的轮子。务必避免使用不稳定的"
" CPython API，例如来自 libtorch_python 的 API（特别是 pytorch/python 绑定），仅使用来自 "
"libtorch 的 API（ATen 对象、操作符和调度器）。我们强烈建议定义 ``Py_LIMITED_API`` "
"标志，以帮助确定扩展是否符合要求并安全作为 CPython 无关轮构建。注意，定义此标志并不能完全保证构建的轮子符合 CPython "
"无关性，但比没有任何限制要好。Python 文档中提到了一些注意事项 `Python docs "
"<https://docs.python.org/3/c-api/stable.html#limited-api-"
"caveats>`_，您应该自己测试并验证轮子对于相关的 CPython 版本是否真正无关。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The second and third lines specifying ``py_limited_api`` inform setuptools "
"that you intend to build a CPython agnostic wheel and will influence the "
"naming of the wheel accordingly:"
msgstr ""
"第二和第三行指定 ``py_limited_api`` 告诉 setuptools 您打算构建 CPython 无关轮，并将相应地影响轮子的命名："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It is necessary to specify ``py_limited_api=True`` as an argument to "
"CppExtension/ CUDAExtension and also as an option to the ``\"bdist_wheel\"``"
" command with the minimal supported CPython version (in this case, 3.9). "
"Consequently, the ``setup`` in our tutorial would build one properly named "
"wheel that could be installed across multiple CPython versions ``>=3.9``."
msgstr ""
"有必要在 CppExtension/CUDAExtension 的参数中指定 ``py_limited_api=True``，以及在 "
"``\"bdist_wheel\"`` 命令中指定最低支持的 CPython 版本（在本例中为 3.9）。因此，我们教程中的 ``setup`` "
"将构建一个可以跨多个 CPython 版本（``>=3.9``）安装的正确命名的轮子。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If your extension uses CPython APIs outside the stable limited set, then you"
" cannot build a CPython agnostic wheel! You should build one wheel per "
"CPython version instead, like so:"
msgstr ""
"如果您的扩展使用了稳定限制集合外的 CPython API，那么您无法构建 CPython 无关轮！你应该针对每个 CPython "
"版本构建一个轮子，如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining the custom op and adding backend implementations"
msgstr "定义自定义操作符并添加后端实现"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "First, let's write a C++ function that computes ``mymuladd``:"
msgstr "首先，让我们编写一个 C++ 函数，用于计算 ``mymuladd``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In order to use this from PyTorch’s Python frontend, we need to register it "
"as a PyTorch operator using the ``TORCH_LIBRARY`` API. This will "
"automatically bind the operator to Python."
msgstr ""
"为了从 PyTorch 的 Python 前端使用这个操作符，我们需要使用 ``TORCH_LIBRARY`` API 将其注册为 PyTorch "
"的一个操作符。这将自动将该操作符绑定到 Python。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Operator registration is a two step-process:"
msgstr "操作符注册是一个两步流程："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Defining the operator** - This step ensures that PyTorch is aware of the "
"new operator."
msgstr "**定义操作符** - 此步骤确保 PyTorch 知道新的操作符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Registering backend implementations** - In this step, implementations for "
"various backends, such as CPU and CUDA, are associated with the operator."
msgstr "**注册后端实现** - 在此步骤中，后端（如 CPU 和 CUDA）的实现与操作符关联。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining an operator"
msgstr "定义一个操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "To define an operator, follow these steps:"
msgstr "要定义一个操作符，请按照以下步骤操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"select a namespace for an operator. We recommend the namespace be the name "
"of your top-level project; we’ll use \"extension_cpp\" in our tutorial."
msgstr "为操作符选择一个命名空间。我们建议命名空间使用您的顶级项目名称；在我们的教程中，我们使用 \"extension_cpp\"。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"provide a schema string that specifies the input/output types of the "
"operator and if an input Tensors will be mutated. We support more types in "
"addition to Tensor and float; please see `The Custom Operators Manual "
"<https://pytorch.org/docs/main/notes/custom_operators.html>`_ for more "
"details."
msgstr ""
"提供一个规范字符串，用于指定操作符的输入/输出类型以及输入张量是否会被修改。除了 Tensor 和 float，我们还支持更多类型；详情请参考 "
"`自定义操作符手册 <https://pytorch.org/docs/main/notes/custom_operators.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you are authoring an operator that can mutate its input Tensors, please "
"see here (:ref:`mutable-ops`) for how to specify that."
msgstr "如果您正在编写一个可以修改其输入张量的操作符，请参见此处 (:ref:`mutable-ops`) 了解如何进行指定。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This makes the operator available from Python via "
"``torch.ops.extension_cpp.mymuladd``."
msgstr "这使得该操作符可以通过 ``torch.ops.extension_cpp.mymuladd`` 从 Python 中使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Registering backend implementations for an operator"
msgstr "注册操作符的后端实现"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use ``TORCH_LIBRARY_IMPL`` to register a backend implementation for the "
"operator."
msgstr "使用 ``TORCH_LIBRARY_IMPL`` 为操作符注册后端实现。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you also have a CUDA implementation of ``myaddmul``, you can register it "
"in a separate ``TORCH_LIBRARY_IMPL`` block:"
msgstr "如果您还有一个 CUDA 实现的 ``myaddmul``，您可以在单独的 ``TORCH_LIBRARY_IMPL`` 块中注册它："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Adding ``torch.compile`` support for an operator"
msgstr "为一个操作符添加 ``torch.compile`` 支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To add ``torch.compile`` support for an operator, we must add a FakeTensor "
"kernel (also known as a \"meta kernel\" or \"abstract impl\"). FakeTensors "
"are Tensors that have metadata (such as shape, dtype, device) but no data: "
"the FakeTensor kernel for an operator specifies how to compute the metadata "
"of output tensors given the metadata of input tensors. The FakeTensor kernel"
" should return dummy Tensors of your choice with the correct Tensor metadata"
" (shape/strides/``dtype``/device)."
msgstr ""
"要为一个操作符添加 ``torch.compile`` 支持，我们必须为其添加一个 FakeTensor 内核（也称为 \"meta kernel\" "
"或 \"abstract impl\"）。 FakeTensors 是具有元数据（如形状、数据类型、设备）但没有数据的张量：操作符的 "
"FakeTensor 内核指定如何根据输入张量的元数据计算输出张量的元数据。 FakeTensor "
"内核应返回具有正确张量元数据（形状/步幅/``dtype``/设备）的您选择的伪张量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We recommend that this be done from Python via the "
"``torch.library.register_fake`` API, though it is possible to do this from "
"C++ as well (see `The Custom Operators Manual "
"<https://pytorch.org/docs/main/notes/custom_operators.html>`_ for more "
"details)."
msgstr ""
"我们建议通过 Python 中的 ``torch.library.register_fake`` API 完成此操作，尽管也可以从 C++ "
"中完成（请参阅 `自定义操作符手册 "
"<https://pytorch.org/docs/main/notes/custom_operators.html>`_ 获取更多详情）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Setting up hybrid Python/C++ registration"
msgstr "设置混合 Python/C++ 注册"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we defined a custom operator in C++, added CPU/CUDA "
"implementations in C++, and added ``FakeTensor`` kernels and backward "
"formulas in Python. The order in which these registrations are loaded (or "
"imported) matters (importing in the wrong order will lead to an error)."
msgstr ""
"在本教程中，我们在 C++ 中定义了一个自定义操作符，使用 C++ 添加了 CPU/CUDA 实现，并在 Python 中添加了 "
"``FakeTensor`` 内核和反向公式。这些注册的加载（或导入）顺序很重要（错误的导入顺序会导致错误）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To use the custom operator with hybrid Python/C++ registrations, we must "
"first load the C++ library that holds the custom operator definition and "
"then call the ``torch.library`` registration APIs. This can happen in three "
"ways:"
msgstr ""
"为了在混合Python/C++注册中使用自定义操作符，我们必须首先加载包含自定义操作符定义的C++库，然后调用 ``torch.library`` "
"注册API。这可以通过以下三种方式实现："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The first way to load the C++ library that holds the custom operator "
"definition is to define a dummy Python module for _C. Then, in Python, when "
"you import the module with ``import _C``, the ``.so`` files corresponding to"
" the extension will be loaded and the ``TORCH_LIBRARY`` and "
"``TORCH_LIBRARY_IMPL`` static initializers will run. One can create a dummy "
"Python module with ``PYBIND11_MODULE`` like below, but you will notice that "
"this does not compile with ``Py_LIMITED_API``, because ``pybind11`` does not"
" promise to only use the stable limited CPython API! With the below code, "
"you sadly cannot build a CPython agnostic wheel for your extension! "
"(Foreshadowing: I wonder what the second way is ;) )."
msgstr ""
"加载包含自定义操作符定义的C++库的第一种方式是为 _C 定义一个伪Python模块。然后，当在Python中使用 ``import _C`` "
"导入模块时，与扩展对应的 ``.so`` 文件将被加载，并且 ``TORCH_LIBRARY`` 和 ``TORCH_LIBRARY_IMPL`` "
"静态初始化程序将运行。可以像下面这样使用 ``PYBIND11_MODULE`` 创建一个伪Python模块，但你会注意到这无法与 "
"``Py_LIMITED_API`` 一起编译，因为 ``pybind11`` 不保证仅使用稳定有限的CPython "
"API！使用以下代码，你很遗憾无法为扩展构建跨CPython版本的通用轮子！（预示着：不知道第二种方式是什么呢？）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, because we value being able to build a single wheel across"
" multiple CPython versions, we will replace the unstable ``PYBIND11`` call "
"with stable API calls. The below code compiles with "
"``-DPy_LIMITED_API=0x03090000`` and successfully creates a dummy Python "
"module for our ``_C`` extension so that it can be imported from Python. See "
"`extension_cpp/__init__.py <https://github.com/pytorch/extension-"
"cpp/blob/38ec45e/extension_cpp/__init__.py>`_ and "
"`extension_cpp/csrc/muladd.cpp  <https://github.com/pytorch/extension-"
"cpp/blob/38ec45e/extension_cpp/csrc/muladd.cpp>`_ for more details:"
msgstr ""
"在本教程中，由于我们看重能够跨多个CPython版本构建单一轮子，我们将用稳定的API调用替换不稳定的 ``PYBIND11`` "
"调用。下面的代码可以通过 ``-DPy_LIMITED_API=0x03090000`` 编译，并成功为我们的 ``_C`` "
"扩展创建一个伪Python模块，以便可以从Python中导入。有关更多详细信息，请参阅 `extension_cpp/__init__.py "
"<https://github.com/pytorch/extension-"
"cpp/blob/38ec45e/extension_cpp/__init__.py>`_ 和 "
"`extension_cpp/csrc/muladd.cpp <https://github.com/pytorch/extension-"
"cpp/blob/38ec45e/extension_cpp/csrc/muladd.cpp>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you want to avoid ``Python.h`` entirely in your C++ custom operator, you "
"may use ``torch.ops.load_library(\"/path/to/library.so\")`` in Python to "
"load the ``.so`` file(s) compiled from the extension. Note that, with this "
"method, there is no ``_C`` Python module created for the extension so you "
"cannot call ``import _C`` from Python. Instead of relying on the import "
"statement to trigger the custom operators to be registered, "
"``torch.ops.load_library(\"/path/to/library.so\")`` will do the trick. The "
"challenge then is shifted towards understanding where the ``.so`` files are "
"located so that you can load them, which is not always trivial:"
msgstr ""
"如果希望完全避免在C++自定义操作符中使用 ``Python.h``，可以在Python中使用 "
"``torch.ops.load_library(\"/path/to/library.so\")`` 来加载由扩展编译的 ``.so`` "
"文件。需要注意的是，使用此方法不会为扩展创建 ``_C`` Python模块，因此无法通过Python调用 ``import "
"_C``。取而代之的是，通过 ``torch.ops.load_library(\"/path/to/library.so\")`` "
"触发自定义操作符的注册。不过这带来的挑战是理解 ``.so`` 文件的位置，以便你可以加载它们，这并非总是简单的："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Adding training (autograd) support for an operator"
msgstr "为操作符添加训练（自动微分）支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use ``torch.library.register_autograd`` to add training support for an "
"operator. Prefer this over directly using Python ``torch.autograd.Function``"
" or C++ ``torch::autograd::Function``; you must use those in a very specific"
" way to avoid silent incorrectness (see `The Custom Operators Manual "
"<https://pytorch.org/docs/main/notes/custom_operators.html>`_ for more "
"details)."
msgstr ""
"使用 ``torch.library.register_autograd`` 为操作符添加训练支持。建议优先使用该方法，而不是直接使用Python的 "
"``torch.autograd.Function`` 或C++的 "
"``torch::autograd::Function``；如果使用后者，必须以非常特定的方式使用它们以避免静默的不正确结果（详细信息请参阅 "
"`自定义操作符手册 <https://pytorch.org/docs/main/notes/custom_operators.html>`_）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that the backward must be a composition of PyTorch-understood "
"operators. If you wish to use another custom C++ or CUDA kernel in your "
"backwards pass, it must be wrapped into a custom operator."
msgstr ""
"请注意，反向必须是PyTorch可以理解的操作符的组合。如果希望在反向传播中使用其他自定义C++或CUDA内核，则必须将其包装到自定义操作符中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we had our own custom ``mymul`` kernel, we would need to wrap it into a "
"custom operator and then call that from the backward:"
msgstr "如果我们有自己的自定义 ``mymul`` 内核，则需要将其包装到自定义操作符中，然后在反向传播中调用它："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Testing an operator"
msgstr "测试一个操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use ``torch.library.opcheck`` to test that the custom op was registered "
"correctly. Note that this function does not test that the gradients are "
"mathematically correct -- plan to write separate tests for that, either "
"manual ones or by using ``torch.autograd.gradcheck``."
msgstr ""
"使用 ``torch.library.opcheck`` "
"来测试自定义操作符是否已正确注册。请注意，此函数不会测试梯度是否在数学上是正确的——需要为此编写单独的测试，可以是手动测试，也可以通过使用 "
"``torch.autograd.gradcheck``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Creating mutable operators"
msgstr "创建可变操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You may wish to author a custom operator that mutates its inputs. Use "
"``Tensor(a!)`` to specify each mutable Tensor in the schema; otherwise, "
"there will be undefined behavior. If there are multiple mutated Tensors, use"
" different names (for example, ``Tensor(a!)``, ``Tensor(b!)``, "
"``Tensor(c!)``) for each mutable Tensor."
msgstr ""
"您可能希望编写一个可以改变其输入的自定义操作符。使用 ``Tensor(a!)`` "
"在模式中指定每个可变的张量；否则将存在未定义行为。如果存在多个被改变的张量，则使用不同的名称（例如，``Tensor(a!)``，``Tensor(b!)``，``Tensor(c!)``）为每个可变张量命名。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's author a ``myadd_out(a, b, out)`` operator, which writes the contents "
"of ``a+b`` into ``out``."
msgstr "接下来我们将编写一个 ``myadd_out(a, b, out)`` 操作符，它将 ``a+b`` 的内容写入 ``out``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"When defining the operator, we must specify that it mutates the out Tensor "
"in the schema:"
msgstr "在定义操作符时，必须在模式中指定它会改变输出张量："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Do not return any mutated Tensors as outputs of the operator as this will "
"cause incompatibility with PyTorch subsystems like ``torch.compile``."
msgstr "不要将任何被改变的张量作为操作符的输出返回，因为这会导致与PyTorch子系统（如 ``torch.compile``）的不兼容。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we went over the recommended approach to integrating "
"Custom C++ and CUDA operators with PyTorch. The "
"``TORCH_LIBRARY/torch.library`` APIs are fairly low-level. For more "
"information about how to use the API, see `The Custom Operators Manual "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html#the-"
"custom-operators-manual>`_."
msgstr ""
"在本教程中，我们介绍了将自定义C++和CUDA操作符与PyTorch集成的推荐方法。``TORCH_LIBRARY/torch.library`` "
"API 相对底层。有关如何使用该API的更多信息，请参阅 `自定义操作符手册 "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html#the-"
"custom-operators-manual>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Loading a TorchScript Model in C++"
msgstr "在C++中加载TorchScript模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "TorchScript is no longer in active development."
msgstr "TorchScript 不再处于活跃开发状态。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As its name suggests, the primary interface to PyTorch is the Python "
"programming language. While Python is a suitable and preferred language for "
"many scenarios requiring dynamism and ease of iteration, there are equally "
"many situations where precisely these properties of Python are unfavorable. "
"One environment in which the latter often applies is *production* -- the "
"land of low latencies and strict deployment requirements. For production "
"scenarios, C++ is very often the language of choice, even if only to bind it"
" into another language like Java, Rust or Go. The following paragraphs will "
"outline the path PyTorch provides to go from an existing Python model to a "
"serialized representation that can be *loaded* and *executed* purely from "
"C++, with no dependency on Python."
msgstr ""
"顾名思义，PyTorch的主要界面是Python编程语言。虽然Python是满足许多需要动态性和快速迭代场景的合适且首选的语言，但同样有许多情况下，Python的这些特性是不利的。其中一种情况通常是"
" *生产环境* -- "
"低延迟和严格部署要求的领域。在生产场景中，C++往往是首选语言，即使只是为了将其绑定到另一种语言（如Java、Rust或Go）。以下段落将概述PyTorch提供的从现有Python模型到完全可以用C++加载和执行（无须依赖Python）的序列化表示的路径。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Step 1: Converting Your PyTorch Model to Torch Script"
msgstr "步骤1：将PyTorch模型转换为Torch Script"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A PyTorch model's journey from Python to C++ is enabled by `Torch Script "
"<https://pytorch.org/docs/master/jit.html>`_, a representation of a PyTorch "
"model that can be understood, compiled and serialized by the Torch Script "
"compiler. If you are starting out from an existing PyTorch model written in "
"the vanilla \"eager\" API, you must first convert your model to Torch "
"Script. In the most common cases, discussed below, this requires only little"
" effort. If you already have a Torch Script module, you can skip to the next"
" section of this tutorial."
msgstr ""
"PyTorch模型从Python到C++转换的实现由 `Torch Script "
"<https://pytorch.org/docs/master/jit.html>`_ 启用，Torch Script是一种可以被Torch "
"Script编译器理解、编译和序列化的PyTorch模型表示形式。如果您从用原生PyTorch“延迟执行”API编写的现有模型开始，您必须首先将模型转换为Torch"
" Script。在下面讨论的最常见情况下，这只需要少量努力。如果您已经有一个Torch Script模块，可以跳过此教程的下一节。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There exist two ways of converting a PyTorch model to Torch Script. The "
"first is known as *tracing*, a mechanism in which the structure of the model"
" is captured by evaluating it once using example inputs, and recording the "
"flow of those inputs through the model. This is suitable for models that "
"make limited use of control flow. The second approach is to add explicit "
"annotations to your model that inform the Torch Script compiler that it may "
"directly parse and compile your model code, subject to the constraints "
"imposed by the Torch Script language."
msgstr ""
"将PyTorch模型转换为Torch Script有两种方法。第一种方法称为 "
"*跟踪*，这是通过使用示例输入执行一次模型并记录这些输入通过模型的流动来捕获模型结构的机制。这适用于少量使用控制流的模型。第二种方法是在模型中添加显式注释，告知Torch"
" Script编译器可以直接解析和编译您的模型代码，受限于Torch Script语言施加的约束。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can find the complete documentation for both of these methods, as well "
"as further guidance on which to use, in the official `Torch Script reference"
" <https://pytorch.org/docs/master/jit.html>`_."
msgstr ""
"您可以在官方 `Torch Script参考 <https://pytorch.org/docs/master/jit.html>`_ "
"中找到这两种方法的完整文档以及进一步的建议。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Converting to Torch Script via Tracing"
msgstr "通过跟踪转换为Torch Script"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To convert a PyTorch model to Torch Script via tracing, you must pass an "
"instance of your model along with an example input to the "
"``torch.jit.trace`` function. This will produce a ``torch.jit.ScriptModule``"
" object with the trace of your model evaluation embedded in the module's "
"``forward`` method::"
msgstr ""
"要通过跟踪将PyTorch模型转换为Torch Script，您必须将模型实例和示例输入一起传递给 ``torch.jit.trace`` "
"函数。这将生成一个 ``torch.jit.ScriptModule`` 对象，该对象的 ``forward`` 方法中嵌入了模型评估的跟踪信息："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The traced ``ScriptModule`` can now be evaluated identically to a regular "
"PyTorch module::"
msgstr "现在可以使用跟踪后的 ``ScriptModule`` 像普通PyTorch模块一样进行评估："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Converting to Torch Script via Annotation"
msgstr "通过注释转换为Torch Script"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Under certain circumstances, such as if your model employs particular forms "
"of control flow, you may want to write your model in Torch Script directly "
"and annotate your model accordingly. For example, say you have the following"
" vanilla Pytorch model::"
msgstr ""
"在某些情况下，例如当模型使用某些形式的控制流时，可能希望直接在Torch "
"Script中编写模型并进行相应的注释。例如，假设您有以下原生PyTorch模型："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Because the ``forward`` method of this module uses control flow that is "
"dependent on the input, it is not suitable for tracing. Instead, we can "
"convert it to a ``ScriptModule``. In order to convert the module to the "
"``ScriptModule``, one needs to compile the module with ``torch.jit.script`` "
"as follows::"
msgstr ""
"由于该模块的 ``forward`` 方法使用了依赖于输入的控制流，因此不适合跟踪。相应地，我们可以将其转换为一个 "
"``ScriptModule``。为了将模块转换为 ``ScriptModule``，需要使用如下方式通过 ``torch.jit.script`` "
"编译模块："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you need to exclude some methods in your ``nn.Module`` because they use "
"Python features that TorchScript doesn't support yet, you could annotate "
"those with ``@torch.jit.ignore``"
msgstr ""
"如果需要排除 ``nn.Module`` 中的一些方法，因为它们使用了TorchScript尚未支持的Python特性，可以用 "
"``@torch.jit.ignore`` 对这些方法进行注释。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``sm`` is an instance of ``ScriptModule`` that is ready for serialization."
msgstr "``sm`` 是一个 ``ScriptModule`` 实例，已准备好进行序列化。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Step 2: Serializing Your Script Module to a File"
msgstr "步骤2：将脚本模块序列化到文件"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once you have a ``ScriptModule`` in your hands, either from tracing or "
"annotating a PyTorch model, you are ready to serialize it to a file. Later "
"on, you'll be able to load the module from this file in C++ and execute it "
"without any dependency on Python. Say we want to serialize the ``ResNet18`` "
"model shown earlier in the tracing example. To perform this serialization, "
"simply call `save "
"<https://pytorch.org/docs/master/jit.html#torch.jit.ScriptModule.save>`_ on "
"the module and pass it a filename::"
msgstr ""
"一旦手中有一个来自跟踪或注释的 "
"``ScriptModule``，即可将其序列化到文件中。稍后，您可以在C++中从该文件加载模块并执行它，而无需依赖Python。假设我们要序列化跟踪示例中显示的"
" ``ResNet18`` 模型。要执行此序列化，只需在模块上调用 `save "
"<https://pytorch.org/docs/master/jit.html#torch.jit.ScriptModule.save>`_ "
"并传递一个文件名："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This will produce a ``traced_resnet_model.pt`` file in your working "
"directory. If you also would like to serialize ``sm``, call "
"``sm.save(\"my_module_model.pt\")`` We have now officially left the realm of"
" Python and are ready to cross over to the sphere of C++."
msgstr ""
"这将在工作目录中生成一个 ``traced_resnet_model.pt`` 文件。如果还希望序列化 ``sm``，调用 "
"``sm.save(\"my_module_model.pt\")``。我们现在已经正式离开了Python领域，准备进入C++领域。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Step 3: Loading Your Script Module in C++"
msgstr "步骤3：在C++中加载脚本模块"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To load your serialized PyTorch model in C++, your application must depend "
"on the PyTorch C++ API -- also known as *LibTorch*. The LibTorch "
"distribution encompasses a collection of shared libraries, header files and "
"CMake build configuration files. While CMake is not a requirement for "
"depending on LibTorch, it is the recommended approach and will be well "
"supported into the future. For this tutorial, we will be building a minimal "
"C++ application using CMake and LibTorch that simply loads and executes a "
"serialized PyTorch model."
msgstr ""
"要在C++中加载序列化的PyTorch模型，您的应用程序必须依赖于PyTorch C++ API -- 通常称为 "
"*LibTorch*。LibTorch分发版包含共享库、头文件和CMake构建配置文件集合。虽然CMake不是依赖LibTorch的要求，但这是推荐的方法，并将在未来得到充分支持。在本教程中，我们将构建一个简单的C++应用程序，该应用程序使用CMake和LibTorch加载并执行序列化的PyTorch模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "A Minimal C++ Application"
msgstr "一个简单的C++应用程序"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's begin by discussing the code to load a module. The following will "
"already do:"
msgstr "首先讨论加载模块的代码。以下代码即可完成："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``<torch/script.h>`` header encompasses all relevant includes from the "
"LibTorch library necessary to run the example. Our application accepts the "
"file path to a serialized PyTorch ``ScriptModule`` as its only command line "
"argument and then proceeds to deserialize the module using the "
"``torch::jit::load()`` function, which takes this file path as input. In "
"return we receive a ``torch::jit::script::Module`` object. We will examine "
"how to execute it in a moment."
msgstr ""
"``<torch/script.h>`` 头文件包含了运行该示例所需的所有LibTorch库相关文件。我们的应用程序接受序列化PyTorch "
"``ScriptModule`` 文件的路径作为命令行参数，并使用 ``torch::jit::load()`` "
"函数对模块进行反序列化，该函数将该文件路径作为输入。我们将接收到一个 ``torch::jit::script::Module`` "
"对象。稍后我们将研究如何执行它。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Depending on LibTorch and Building the Application"
msgstr "依赖LibTorch并构建应用程序"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Assume we stored the above code into a file called ``example-app.cpp``. A "
"minimal ``CMakeLists.txt`` to build it could look as simple as:"
msgstr ""
"假设我们将上述代码存储在一个名为 ``example-app.cpp`` 的文件中。一个最小的 ``CMakeLists.txt`` "
"文件用于构建它可以简单地如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The last thing we need to build the example application is the LibTorch "
"distribution. You can always grab the latest stable release from the "
"`download page <https://pytorch.org/>`_ on the PyTorch website. If you "
"download and unzip the latest archive, you should receive a folder with the "
"following directory structure:"
msgstr ""
"构建示例应用程序的最后一件事是获取LibTorch分发版。您始终可以从PyTorch官网上的 `下载页面 "
"<https://pytorch.org/>`_ 获取最新稳定版本。如果您下载并解压最新存档，则应看到一个具有以下目录结构的文件夹："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``lib/`` folder contains the shared libraries you must link against,"
msgstr "“lib/” 文件夹包含需要链接的共享库。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``include/`` folder contains header files your program will need to "
"include,"
msgstr "“include/” 文件夹包含程序需要包含的头文件。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``share/`` folder contains the necessary CMake configuration to enable "
"the simple ``find_package(Torch)`` command above."
msgstr "“share/” 文件夹包含所需的 CMake 配置，以支持上述简单的“find_package(Torch)”命令。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"On Windows, debug and release builds are not ABI-compatible. If you plan to "
"build your project in debug mode, please try the debug version of LibTorch. "
"Also, make sure you specify the correct configuration in the ``cmake --build"
" .`` line below."
msgstr ""
"在 Windows 上，调试版本和发布版本的构建不具有 ABI 兼容性。如果计划在调试模式下构建项目，请尝试使用 LibTorch "
"的调试版本。此外，请确保在下面的“cmake --build .”行中指定正确的配置。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The last step is building the application. For this, assume our example "
"directory is laid out like this:"
msgstr "最后一步是构建应用程序。为此，假设我们的示例目录布局如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can now run the following commands to build the application from within "
"the ``example-app/`` folder:"
msgstr "我们现在可以运行以下命令，从“example-app/”文件夹内构建应用程序："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"where ``/path/to/libtorch`` should be the full path to the unzipped LibTorch"
" distribution. If all goes well, it will look something like this:"
msgstr "其中“/path/to/libtorch”应为解压后的 LibTorch 分布的完整路径。如果一切正常，它将显示如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we supply the path to the traced ``ResNet18`` model "
"``traced_resnet_model.pt``  we created earlier to the resulting ``example-"
"app`` binary, we should be rewarded with a friendly \"ok\". Please note, if "
"try to run this example with ``my_module_model.pt`` you will get an error "
"saying that your input is of an incompatible shape. ``my_module_model.pt`` "
"expects 1D instead of 4D."
msgstr ""
"如果我们向生成的“example-"
"app”二进制文件提供之前创建的转存的“ResNet18”模型“traced_resnet_model.pt”，我们应该会得到一个友好的“ok”作为回报。请注意，如果尝试使用“my_module_model.pt”运行此示例，将会收到一个错误提示，指出输入形状不兼容。“my_module_model.pt”期望为"
" 1D 而不是 4D。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Step 4: Executing the Script Module in C++"
msgstr "步骤 4：在 C++ 中执行脚本模块"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Having successfully loaded our serialized ``ResNet18`` in C++, we are now "
"just a couple lines of code away from executing it! Let's add those lines to"
" our C++ application's ``main()`` function:"
msgstr ""
"在 C++ 中成功加载序列化的“ResNet18”后，我们只需几行代码就可以执行它！让我们将这些代码行添加到 C++ 应用程序的“main()”函数中："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The first two lines set up the inputs to our model. We create a vector of "
"``torch::jit::IValue`` (a type-erased value type ``script::Module`` methods "
"accept and return) and add a single input. To create the input tensor, we "
"use ``torch::ones()``, the equivalent to ``torch.ones`` in the C++ API.  We "
"then run the ``script::Module``'s ``forward`` method, passing it the input "
"vector we created. In return we get a new ``IValue``, which we convert to a "
"tensor by calling ``toTensor()``."
msgstr ""
"前两行设置了模型的输入。我们创建一个“torch::jit::IValue”的向量（脚本模块方法接收和返回的类型擦除值类型），并添加一个输入。要创建输入张量，我们使用“torch::ones()”，这是"
" C++ API "
"中“torch.ones”的等效函数。之后我们运行脚本模块的“forward”方法，并传入我们创建的输入向量。返回值为一个新的“IValue”，我们可以通过调用“toTensor()”将其转换为张量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To learn more about functions like ``torch::ones`` and the PyTorch C++ API "
"in general, refer to its documentation at https://pytorch.org/cppdocs. The "
"PyTorch C++ API provides near feature parity with the Python API, allowing "
"you to further manipulate and process tensors just like in Python."
msgstr ""
"要了解更多像“torch::ones”这样的方法以及 PyTorch C++ API "
"的其他内容，请参阅其文档：https://pytorch.org/cppdocs。PyTorch C++ API 提供了与 Python API "
"类似的功能，使您能够像在 Python 中一样进一步操作和处理张量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the last line, we print the first five entries of the output. Since we "
"supplied the same input to our model in Python earlier in this tutorial, we "
"should ideally see the same output. Let's try it out by re-compiling our "
"application and running it with the same serialized model:"
msgstr ""
"在最后一行中，我们打印了输出的前五个条目。由于我们之前在此教程中为模型提供了相同的输入，因此我们应该理想地看到相同的输出。让我们通过重新编译应用程序并使用同样的序列化模型运行它，来尝试一下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "For reference, the output in Python previously was::"
msgstr "作为参考，之前在 Python 中的输出是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Looks like a good match!"
msgstr "看起来匹配得很好！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To move your model to GPU memory, you can write ``model.to(at::kCUDA);``. "
"Make sure the inputs to a model are also living in CUDA memory by calling "
"``tensor.to(at::kCUDA)``, which will return a new tensor in CUDA memory."
msgstr ""
"要将模型移动到 GPU 内存，您可以编写“model.to(at::kCUDA);”。请确保模型的输入也位于 CUDA "
"内存中，方法是调用“tensor.to(at::kCUDA)”，这将返回一个新的 CUDA 内存中的张量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Step 5: Getting Help and Exploring the API"
msgstr "步骤 5：获取帮助并探索 API"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial has hopefully equipped you with a general understanding of a "
"PyTorch model's path from Python to C++. With the concepts described in this"
" tutorial, you should be able to go from a vanilla, \"eager\" PyTorch model,"
" to a compiled ``ScriptModule`` in Python, to a serialized file on disk and "
"-- to close the loop -- to an executable ``script::Module`` in C++."
msgstr ""
"希望本教程已经让您对 PyTorch 模型从 Python 到 C++ "
"的路径有了总体了解。通过本教程中描述的概念，您应该能够从一个普通的“即时”PyTorch 模型，到 Python "
"中编译的“ScriptModule”，再到磁盘上的序列化文件，并最终到可以执行的 C++ 中的“script::Module”。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Of course, there are many concepts we did not cover. For example, you may "
"find yourself wanting to extend your ``ScriptModule`` with a custom operator"
" implemented in C++ or CUDA, and executing this custom operator inside your "
"``ScriptModule`` loaded in your pure C++ production environment. The good "
"news is: this is possible, and well supported! For now, you can explore "
"`this "
"<https://github.com/pytorch/pytorch/tree/master/test/custom_operator>`_ "
"folder for examples, and we will follow up with a tutorial shortly. In the "
"time being, the following links may be generally helpful:"
msgstr ""
"当然，还有很多我们没有介绍的概念。例如，您可能希望通过在 C++ 或 CUDA 中实现自定义操作符，扩展您的“ScriptModule”，并在纯 C++"
" "
"生产环境中加载的“ScriptModule”中执行这个自定义操作符。好消息是：这是可行的，并且支持良好！目前，您可以探索这个文件夹：`https://github.com/pytorch/pytorch/tree/master/test/custom_operator`"
" 来查看更多例子，我们稍后会跟进一个相关教程。同时，以下链接可能对您有所帮助："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The Torch Script reference: https://pytorch.org/docs/master/jit.html"
msgstr "Torch Script 参考文档：https://pytorch.org/docs/master/jit.html"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The PyTorch C++ API documentation: https://pytorch.org/cppdocs/"
msgstr "PyTorch C++ API 文档：https://pytorch.org/cppdocs/"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The PyTorch Python API documentation: https://pytorch.org/docs/"
msgstr "PyTorch Python API 文档：https://pytorch.org/docs/"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As always, if you run into any problems or have questions, you can use our "
"`forum <https://discuss.pytorch.org/>`_ or `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_ to get in touch."
msgstr ""
"如往常，如果您遇到任何问题或有疑问，可以使用论坛 `https://discuss.pytorch.org/` 或 GitHub 问题 "
"`https://github.com/pytorch/pytorch/issues` 与我们联系。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Custom C++ and CUDA Extensions"
msgstr "自定义 C++ 和 CUDA 扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `Peter Goldsborough <https://www.goldsborough.me/>`_"
msgstr "**作者**: `Peter Goldsborough <https://www.goldsborough.me/>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial is deprecated as of PyTorch 2.4. Please see :ref:`custom-ops-"
"landing-page` for the newest up-to-date guides on extending PyTorch with "
"Custom C++/CUDA Extensions."
msgstr ""
"本教程从 PyTorch 2.4 起已被弃用。请参阅 :ref:`custom-ops-landing-page` 了解最新的关于扩展 PyTorch "
"的自定义 C++/CUDA 扩展指南。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch provides a plethora of operations related to neural networks, "
"arbitrary tensor algebra, data wrangling and other purposes. However, you "
"may still find yourself in need of a more customized operation. For example,"
" you might want to use a novel activation function you found in a paper, or "
"implement an operation you developed as part of your research."
msgstr ""
"PyTorch "
"提供了大量涉及神经网络、任意张量代数、数据处理等方面的操作。然而，您可能仍然需要更为定制化的操作。例如，您可能希望使用在论文中发现的一种新型激活函数，或者实现作为研究一部分开发的某个操作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The easiest way of integrating such a custom operation in PyTorch is to "
"write it in Python by extending :class:`Function` and :class:`Module` as "
"outlined `here <https://pytorch.org/docs/master/notes/extending.html>`_. "
"This gives you the full power of automatic differentiation (spares you from "
"writing derivative functions) as well as the usual expressiveness of Python."
" However, there may be times when your operation is better implemented in "
"C++. For example, your code may need to be *really* fast because it is "
"called very frequently in your model or is very expensive even for few "
"calls. Another plausible reason is that it depends on or interacts with "
"other C or C++ libraries. To address such cases, PyTorch provides a very "
"easy way of writing custom *C++ extensions*."
msgstr ""
"在 PyTorch 中集成这样一个定制化操作的最简单方法是通过在 Python 中扩展 :class:`Function` 和 "
":class:`Module` 来实现，如 `这里 "
"<https://pytorch.org/docs/master/notes/extending.html>` "
"所述。这种方式赋予了您全自动求导的强大功能（无需编写导数函数）以及 Python 通常提供的表现力。然而，有时出于某些理由，您的操作可能更适合使用 "
"C++ 实现。例如，代码可能需要*非常*快，因为它在模型中被频繁调用，或者即使被调用的次数较少也非常耗费资源。另一个合理的原因可能是它依赖于其他 C 或"
" C++ 库或与这些库进行交互。为了解决上述情况，PyTorch 提供了一种非常简单的方式来编写自定义的*C++扩展*。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"C++ extensions are a mechanism we have developed to allow users (you) to "
"create PyTorch operators defined *out-of-source*, i.e. separate from the "
"PyTorch backend. This approach is *different* from the way native PyTorch "
"operations are implemented. C++ extensions are intended to spare you much of"
" the boilerplate associated with integrating an operation with PyTorch's "
"backend while providing you with a high degree of flexibility for your "
"PyTorch-based projects. Nevertheless, once you have defined your operation "
"as a C++ extension, turning it into a native PyTorch function is largely a "
"matter of code organization, which you can tackle after the fact if you "
"decide to contribute your operation upstream."
msgstr ""
"C++ 扩展是一种机制，我们开发它是为了让用户（您）能够创建在*外部源文件*中定义的 PyTorch 操作，即与 PyTorch "
"后端分离。这种方法与原生 PyTorch 操作的实现方式*不同*。C++ 扩展旨在减少与 PyTorch 后端整合过程中所需的样板代码，同时为基于 "
"PyTorch 的项目提供极大的灵活性。然而，一旦您将操作定义为 C++ 扩展，将其转换为原生 PyTorch "
"函数主要是代码组织的问题，如果决定为主流行业贡献您的操作，则可以事后解决这个问题。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Motivation and Example"
msgstr "动机与示例"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The rest of this note will walk through a practical example of writing and "
"using a C++ (and CUDA) extension. If you are being chased or someone will "
"fire you if you don't get that op done by the end of the day, you can skip "
"this section and head straight to the implementation details in the next "
"section."
msgstr ""
"本文余下部分将通过一个实际示例讲解如何编写和使用一个 C++（和 "
"CUDA）扩展。如果您需要快速完成某项操作，可以跳过这一部分，直接进入下一部分中的实现细节。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's say you've come up with a new kind of recurrent unit that you found to"
" have superior properties compared to the state of the art. This recurrent "
"unit is similar to an LSTM, but differs in that it lacks a *forget gate* and"
" uses an *Exponential Linear Unit* (ELU) as its internal activation "
"function. Because this unit never forgets, we'll call it *LLTM*, or *Long-"
"Long-Term-Memory* unit."
msgstr ""
"假设您提出了一种全新的循环单元，并发现其性能远优于当前最先进的技术。这种循环单元类似于 "
"LSTM，但不同之处在于它没有*遗忘门*，而是使用*指数线性单元*（ELU）作为其内部激活函数。因为这种单元从不忘记，我们称之为 "
"LLTM，或*长时记忆单元*。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The two ways in which LLTMs differ from vanilla LSTMs are significant enough"
" that we can't configure PyTorch's ``LSTMCell`` for our purposes, so we'll "
"have to create a custom cell. The first and easiest approach for this -- and"
" likely in all cases a good first step -- is to implement our desired "
"functionality in plain PyTorch with Python. For this, we need to subclass "
":class:`torch.nn.Module` and implement the forward pass of the LLTM. This "
"would look something like this::"
msgstr ""
"LLTM 与普通 LSTM 的两个不同之处足够显著，以至于我们无法为自己的目的配置 PyTorch "
"的“LSTMCell”，所以我们必须创建一个自定义单元。第一步也是最简单的方式——这在所有情况下可能都是一个好的第一步——是通过简单的 PyTorch "
"和 Python 实现我们所需的功能。为此，我们需要继承 :class:`torch.nn.Module` 并实现 LLTM "
"的前向传播。这看起来大致如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "which we could then use as expected::"
msgstr "然后我们可以像预期那样使用它："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Naturally, if at all possible and plausible, you should use this approach to"
" extend PyTorch. Since PyTorch has highly optimized implementations of its "
"operations for CPU *and* GPU, powered by libraries such as `NVIDIA cuDNN "
"<https://developer.nvidia.com/cudnn>`_, `Intel MKL "
"<https://software.intel.com/en-us/mkl>`_ or `NNPACK "
"<https://github.com/Maratyszcza/NNPACK>`_, PyTorch code like above will "
"often be fast enough. However, we can also see why, under certain "
"circumstances, there is room for further performance improvements. The most "
"obvious reason is that PyTorch has no knowledge of the *algorithm* you are "
"implementing. It knows only of the individual operations you use to compose "
"your algorithm. As such, PyTorch must execute your operations individually, "
"one after the other. Since each individual call to the implementation (or "
"*kernel*) of an operation, which may involve the launch of a CUDA kernel, "
"has a certain amount of overhead, this overhead may become significant "
"across many function calls. Furthermore, the Python interpreter that is "
"running our code can itself slow down our program."
msgstr ""
"当然，如果可能且可行，您应该使用这种方法来扩展 PyTorch。由于 PyTorch 对其操作的 CPU 和 GPU 实现进行了高度优化，通过使用像 "
"`NVIDIA cuDNN <https://developer.nvidia.com/cudnn>`、`Intel MKL "
"<https://software.intel.com/en-us/mkl>` 或 `NNPACK "
"<https://github.com/Maratyszcza/NNPACK>` 这样的库，像上面这样的 PyTorch "
"代码通常已经足够快。然而，我们也可以看到，在某些情况下仍然有进一步优化的空间。最显而易见的原因是 PyTorch "
"不了解您正在实现的*算法*。它只知道您用于构成算法的个别操作。因此，PyTorch 必须逐个执行您的操作。由于每次调用操作的实现（或*内核*）可能涉及 "
"CUDA 内核的启动，因此这种启动开销可能在多次函数调用中显得较为重要。此外，运行代码的 Python 解释器本身也可能会减慢程序。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A definite method of speeding things up is therefore to rewrite parts in C++"
" (or CUDA) and *fuse* particular groups of operations. Fusing means "
"combining the implementations of many functions into a single function, "
"which profits from fewer kernel launches as well as other optimizations we "
"can perform with increased visibility of the global flow of data."
msgstr ""
"因此，加速代码的一个明确的方法是将部分逻辑重写为 C++（或 "
"CUDA），并*融合*某些特定的操作组。融合意味着将许多功能的实现组合为一个功能，这样可以减少内核启动并在数据流的全局视图方面进行其他优化。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's see how we can use C++ extensions to implement a *fused* version of "
"the LLTM. We'll begin by writing it in plain C++, using the `ATen "
"<https://github.com/zdevito/ATen>`_ library that powers much of PyTorch's "
"backend, and see how easily it lets us translate our Python code. We'll then"
" speed things up even more by moving parts of the model to CUDA kernel to "
"benefit from the massive parallelism GPUs provide."
msgstr ""
"让我们看看如何使用 C++ 扩展来实现 LLTM 的*融合*版本。我们将首先用简单的 C++ 编写它，通过 `ATen "
"<https://github.com/zdevito/ATen>` 库（PyTorch 后端的核心部分）来实现，并看看它是如何轻松地将我们的 "
"Python 代码翻译为实际应用的。然后我们将进一步提速，通过将模型的部分迁移到 CUDA 内核，以利用 GPU 提供的大规模并行计算能力。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Writing a C++ Extension"
msgstr "编写 C++ 扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"C++ extensions come in two flavors: They can be built \"ahead of time\" with"
" :mod:`setuptools`, or \"just in time\" via "
":func:`torch.utils.cpp_extension.load`. We'll begin with the first approach "
"and discuss the latter later."
msgstr ""
"C++ 扩展有两种形式：可以使用 :mod:`setuptools` 进行“预构建”，或者使用 "
":func:`torch.utils.cpp_extension.load` 进行“即时加载”。我们将先讨论第一种方法，然后再说明另一种。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building with :mod:`setuptools`"
msgstr "使用 :mod:`setuptools` 进行构建"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For the \"ahead of time\" flavor, we build our C++ extension by writing a "
"``setup.py`` script that uses setuptools to compile our C++ code. For the "
"LLTM, it looks as simple as this::"
msgstr ""
"对于“预构建”方式，我们通过编写一个使用 setuptools 编译 C++ 代码的“setup.py”脚本来构建 C++ 扩展。对于 "
"LLTM，它看起来非常简单，如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this code, :class:`CppExtension` is a convenience wrapper around "
":class:`setuptools.Extension` that passes the correct include paths and sets"
" the language of the extension to C++. The equivalent vanilla "
":mod:`setuptools` code would simply be::"
msgstr ""
"在这段代码中，:class:`CppExtension` 是 :class:`setuptools.Extension` "
"的一个便利封装器，它会传递正确的包含路径并将扩展语言设置为 C++。等效的基础 :mod:`setuptools` 代码将简单地是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":class:`BuildExtension` performs a number of required configuration steps "
"and checks and also manages mixed compilation in the case of mixed C++/CUDA "
"extensions. And that's all we really need to know about building C++ "
"extensions for now! Let's now take a look at the implementation of our C++ "
"extension, which goes into ``lltm.cpp``."
msgstr ""
":class:`BuildExtension` 执行了一些必要的配置步骤和检查，并且在混合 C++/CUDA "
"扩展的情况下还管理混合编译。而这就是目前关于构建 C++ 扩展我们需要知道的一切！现在让我们来看一下我们的 C++ 扩展的实现，其代码位于 "
"``lltm.cpp``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Writing the C++ Op"
msgstr "编写 C++ 操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's start implementing the LLTM in C++! One function we'll need for the "
"backward pass is the derivative of the sigmoid. This is a small enough piece"
" of code to discuss the overall environment that is available to us when "
"writing C++ extensions:"
msgstr ""
"让我们开始用 C++ 实现 LLTM！反向传播中我们需要实现的一个函数是 sigmoid 的导数。这段代码足够小，可以讨论我们在编写 C++ "
"扩展时可用的整体环境："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``<torch/extension.h>`` is the one-stop header to include all the necessary "
"PyTorch bits to write C++ extensions. It includes:"
msgstr "``<torch/extension.h>`` 是一个一站式头文件，包含了编写 C++ 扩展所需的所有 PyTorch 相关内容。它包括："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The ATen library, which is our primary API for tensor computation,"
msgstr "ATen 库，这是我们进行张量计算的主要 API，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`pybind11 <https://github.com/pybind/pybind11>`_, which is how we create "
"Python bindings for our C++ code,"
msgstr ""
"`pybind11 <https://github.com/pybind/pybind11>`_，它是我们为 C++ 代码创建 Python "
"绑定的方式，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Headers that manage the details of interaction between ATen and pybind11."
msgstr "管理 ATen 和 pybind11 之间交互细节的头文件。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The implementation of :func:`d_sigmoid` shows how to use the ATen API. "
"PyTorch's tensor and variable interface is generated automatically from the "
"ATen library, so we can more or less translate our Python implementation 1:1"
" into C++. Our primary datatype for all computations will be "
":class:`torch::Tensor`. Its full API can be inspected `here "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html>`_. Notice also "
"that we can include ``<iostream>`` or *any other C or C++ header* -- we have"
" the full power of C++11 at our disposal."
msgstr ""
":func:`d_sigmoid` 的实现展示了如何使用 ATen API。PyTorch 的张量和变量接口是从 ATen "
"库中自动生成的，因此我们可以或多或少地将 Python 实现 1:1 翻译为 C++。我们所有计算的主要数据类型将是 "
":class:`torch::Tensor`。其完整的 API 可以在 `这里 "
"<https://pytorch.org/cppdocs/api/classat_1_1_tensor.html>`_ 检查。另外，请注意我们可以包括 "
"``<iostream>`` 或任何其他 C 或 C++ 头文件 —— 我们可使用完整的 C++11 功能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that CUDA-11.5 nvcc will hit internal compiler error while parsing "
"torch/extension.h on Windows. To workaround the issue, move python binding "
"logic to pure C++ file. Example use:"
msgstr ""
"请注意，在 Windows 上使用 CUDA-11.5 的 nvcc 解析 torch/extension.h "
"时会出现内部编译器错误。为了规避此问题，请将 Python 绑定逻辑移动到纯 C++ 文件中。示例用法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Instead of:"
msgstr "代替："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Currently open issue for nvcc bug `here "
"<https://github.com/pytorch/pytorch/issues/69460>`_. Complete workaround "
"code example `here "
"<https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48>`_."
msgstr ""
"目前有关 nvcc 问题的未解决问题 `在这里 "
"<https://github.com/pytorch/pytorch/issues/69460>`_。完整的解决方案代码示例 `在这里 "
"<https://github.com/facebookresearch/pytorch3d/commit/cb170ac024a949f1f9614ffe6af1c38d972f7d48>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Forward Pass"
msgstr "前向传播"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Next we can port our entire forward pass to C++:"
msgstr "接下来，我们可以将整个前向传播移植到 C++："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Backward Pass"
msgstr "反向传播"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The C++ extension API currently does not provide a way of automatically "
"generating a backwards function for us. As such, we have to also implement "
"the backward pass of our LLTM, which computes the derivative of the loss "
"with respect to each input of the forward pass. Ultimately, we will plop "
"both the forward and backward function into a "
":class:`torch.autograd.Function` to create a nice Python binding. The "
"backward function is slightly more involved, so we'll not dig deeper into "
"the code (if you are interested, `Alex Graves' thesis "
"<https://www.cs.toronto.edu/~graves/phd.pdf>`_ is a good read for more "
"information on this):"
msgstr ""
"目前 C++ 扩展 API 并未提供一种方法来自动为我们生成反向函数。因此，我们还必须实现 LLTM "
"的反向传播，该过程计算损失对前向传播每个输入的导数。最终，我们会将前向和反向函数封装到 :class:`torch.autograd.Function`"
" 中，以创建一个优雅的 Python 绑定。反向函数稍微复杂一些，因此我们不会深入探讨代码（如果您感兴趣，可以阅读 `Alex Graves 的学位论文"
" <https://www.cs.toronto.edu/~graves/phd.pdf>`_ 获取更多信息）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Binding to Python"
msgstr "绑定到 Python"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once you have your operation written in C++ and ATen, you can use pybind11 "
"to bind your C++ functions or classes into Python in a very simple manner. "
"Questions or issues you have about this part of PyTorch C++ extensions will "
"largely be addressed by `pybind11 documentation "
"<https://pybind11.readthedocs.io/en/stable/>`_."
msgstr ""
"完成 C++ 和 ATen 操作后，您可以使用 pybind11 以非常简单的方式将 C++ 函数或类绑定到 Python。如果您在这个部分有关 "
"PyTorch 的 C++ 扩展有任何问题，大部分都会在 `pybind11 文档 "
"<https://pybind11.readthedocs.io/en/stable/>`_ 中得到解答。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "For our extensions, the necessary binding code spans only four lines:"
msgstr "对于我们的扩展，必要的绑定代码仅涉及四行："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"One bit to note here is the macro ``TORCH_EXTENSION_NAME``. The torch "
"extension build will define it as the name you give your extension in the "
"``setup.py`` script. In this case, the value of ``TORCH_EXTENSION_NAME`` "
"would be \"lltm_cpp\". This is to avoid having to maintain the name of the "
"extension in two places (the build script and your C++ code), as a mismatch "
"between the two can lead to nasty and hard to track issues."
msgstr ""
"这里需要注意的一点是宏 ``TORCH_EXTENSION_NAME``。torch 扩展构建会将其定义为您在 ``setup.py`` "
"脚本中为扩展指定的名称。在本例中，``TORCH_EXTENSION_NAME`` 的值将是 \"lltm_cpp\"。这是为了避免在构建脚本和 C++"
" 代码中维护扩展名称的两处不一致，因为两者不匹配可能会导致令人头疼且难以跟踪的问题。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using Your Extension"
msgstr "使用您的扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We are now set to import our extension in PyTorch. At this point, your "
"directory structure could look something like this::"
msgstr "现在我们可以在 PyTorch 中导入扩展了。此时，您的目录结构可能如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, run ``python setup.py install`` to build and install your extension. "
"This should look something like this::"
msgstr "现在运行 ``python setup.py install`` 来构建并安装扩展。这可能看起来像这样："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A small note on compilers: Due to ABI versioning issues, the compiler you "
"use to build your C++ extension must be *ABI-compatible* with the compiler "
"PyTorch was built with. In practice, this means that you must use GCC "
"version 4.9 and above on Linux. For Ubuntu 16.04 and other more-recent Linux"
" distributions, this should be the default compiler already. On MacOS, you "
"must use clang (which does not have any ABI versioning issues). In the worst"
" case, you can build PyTorch from source with your compiler and then build "
"the extension with that same compiler."
msgstr ""
"关于编译器的小提示：由于 ABI 版本问题，用于构建 C++ 扩展的编译器必须与 PyTorch 编译时使用的编译器 *ABI "
"兼容*。实际上，这意味着在 Linux 上您必须使用 GCC 版本 4.9 或更高版本。对于 Ubuntu 16.04 和其他较新的 Linux "
"发行版，这应该已经是默认的编译器。在 MacOS 上，您必须使用 clang（它没有任何 ABI "
"版本问题）。在最坏的情况下，您可以使用您的编译器从源码构建 PyTorch，然后使用相同的编译器构建扩展。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once your extension is built, you can simply import it in Python, using the "
"name you specified in your ``setup.py`` script. Just be sure to ``import "
"torch`` first, as this will resolve some symbols that the dynamic linker "
"must see::"
msgstr ""
"扩展构建完成后，您可以直接在 Python 中使用 ``setup.py`` 指定的名称导入它。只需确保先 ``import "
"torch``，因为这会解决动态链接器必须看到的一些符号："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we call ``help()`` on the function or module, we can see that its "
"signature matches our C++ code::"
msgstr "如果我们在函数或模块上调用 ``help()``，可以看到其签名匹配我们的 C++ 代码："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Since we are now able to call our C++ functions from Python, we can wrap "
"them with :class:`torch.autograd.Function` and :class:`torch.nn.Module` to "
"make them first class citizens of PyTorch::"
msgstr ""
"既然我们现在可以从 Python 调用我们的 C++ 函数，我们可以用 :class:`torch.autograd.Function` 和 "
":class:`torch.nn.Module` 将它们封装，使其成为 PyTorch 的一级公民："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Performance Comparison"
msgstr "性能比较"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we are able to use and call our C++ code from PyTorch, we can run a"
" small benchmark to see how much performance we gained from rewriting our op"
" in C++. We'll run the LLTM forwards and backwards a few times and measure "
"the duration::"
msgstr ""
"既然我们能够从 PyTorch 使用并调用我们的 C++ 代码，我们可以运行一个小型基准测试，看看通过用 C++ "
"重写操作获得了多少性能提升。我们将前后运行 LLTM 几次并测量持续时间："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we run this code with the original LLTM we wrote in pure Python at the "
"start of this post, we get the following numbers (on my machine)::"
msgstr "如果我们用本文开头用纯 Python 编写的原始 LLTM 运行这段代码，我们得到以下的数字（在我的机器上）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "and with our new C++ version::"
msgstr "以及使用我们新的 C++ 版本："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can already see a significant speedup for the forward function (more than"
" 30%). For the backward function, a speedup is visible, albeit not a major "
"one. The backward pass I wrote above was not particularly optimized and "
"could definitely be improved. Also, PyTorch's automatic differentiation "
"engine can automatically parallelize computation graphs, may use a more "
"efficient flow of operations overall, and is also implemented in C++, so "
"it's expected to be fast. Nevertheless, this is a good start."
msgstr ""
"我们已经看到前向函数表现出显著的加速（超过 "
"30%）。对于反向函数，虽然可以看到加速，但不是特别明显。上面编写的反向传播并未特别优化，完全可以改进。另外，PyTorch "
"的自动微分引擎可以自动并行化计算图，可能整体上使用更高效的操作流程，并且也用 C++ 实现，因此其本身已经很快了。尽管如此，这是一个不错的开始。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Performance on GPU Devices"
msgstr "GPU 设备上的性能"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A wonderful fact about PyTorch's *ATen* backend is that it abstracts the "
"computing device you are running on. This means the same code we wrote for "
"CPU can *also* run on GPU, and individual operations will correspondingly "
"dispatch to GPU-optimized implementations. For certain operations like "
"matrix multiply (like ``mm`` or ``addmm``), this is a big win. Let's take a "
"look at how much performance we gain from running our C++ code with CUDA "
"tensors. No changes to our implementation are required, we simply need to "
"put our tensors in GPU memory from Python, with either adding "
"``device=cuda_device`` argument at creation time or using "
"``.to(cuda_device)`` after creation::"
msgstr ""
"有关 PyTorch *ATen* 后端的一个美妙事实是，它抽象了您运行的计算设备。这意味着我们为 CPU 编写的代码可以 *也* 在 GPU "
"上运行，而单个操作则会相应地调度到 GPU 优化的实现中。对于某些操作，如矩阵乘法（如 ``mm`` 或 "
"``addmm``），这是一个重大优势。让我们看看通过使用 CUDA 张量运行 C++ "
"代码可以获得多少性能提升。无需对实现进行更改，我们只需要通过在创建时添加 ``device=cuda_device`` 参数，或者在创建后使用 "
"``.to(cuda_device)``，将张量放入 GPU 内存："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once more comparing our plain PyTorch code with our C++ version, now both "
"running on CUDA devices, we again see performance gains. For "
"Python/PyTorch::"
msgstr ""
"再次比较普通 PyTorch 代码与 C++ 版本，现在两者都运行在 CUDA 设备上，我们再次看到了性能提升。对于 Python/PyTorch："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "And C++/ATen::"
msgstr "而对于 C++/ATen："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"That's a great overall speedup compared to non-CUDA code. However, we can "
"pull even more performance out of our C++ code by writing custom CUDA "
"kernels, which we'll dive into soon. Before that, let's discuss another way "
"of building your C++ extensions."
msgstr ""
"与非 CUDA 代码相比，这已经是一个很棒的总体性能提升。不过，通过编写自定义 CUDA 核心，我们可以从 C++ "
"代码中挖掘更多的性能，我们很快会深入研究。在那之前，让我们讨论另一种构建 C++ 扩展的方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "JIT Compiling Extensions"
msgstr "JIT 编译扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Previously, I mentioned there were two ways of building C++ extensions: "
"using :mod:`setuptools` or just in time (JIT). Having covered the former, "
"let's elaborate on the latter. The JIT compilation mechanism provides you "
"with a way of compiling and loading your extensions on the fly by calling a "
"simple function in PyTorch's API called "
":func:`torch.utils.cpp_extension.load`. For the LLTM, this would look as "
"simple as this::"
msgstr ""
"之前，我提到有两种构建 C++ 扩展的方法：使用 :mod:`setuptools` 或即时（JIT）编译。在讲述了前者之后，我们来详细说明后者。JIT"
" 编译机制为您提供了一种通过 PyTorch API 中的简单函数 :func:`torch.utils.cpp_extension.load` "
"随时编译和加载扩展的方法。对于 LLTM，这将变得如此简单："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here, we provide the function with the same information as for "
":mod:`setuptools`. In the background, this will do the following:"
msgstr "在这里，我们为函数提供了与 :mod:`setuptools` 相同的信息。在后台，这将执行以下操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Create a temporary directory ``/tmp/torch_extensions/lltm``,"
msgstr "创建临时目录 ``/tmp/torch_extensions/lltm``，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Emit a `Ninja <https://ninja-build.org/>`_ build file into that temporary "
"directory,"
msgstr "在该临时目录中生成一个 `Ninja <https://ninja-build.org/>`_ 构建文件，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Compile your source files into a shared library,"
msgstr "将您的源文件编译为共享库，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Import this shared library as a Python module."
msgstr "将该共享库作为 Python 模块导入。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In fact, if you pass ``verbose=True`` to :func:`cpp_extension.load`, you "
"will be informed about the process::"
msgstr ""
"实际上，如果您将 ``verbose=True`` 传递给 :func:`cpp_extension.load`，您会看到有关该过程的信息："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The resulting Python module will be exactly the same as produced by "
"setuptools, but removes the requirement of having to maintain a separate "
"``setup.py`` build file. If your setup is more complicated and you do need "
"the full power of :mod:`setuptools`, you *can* write your own ``setup.py`` "
"-- but in many cases this JIT technique will do just fine. The first time "
"you run through this line, it will take some time, as the extension is "
"compiling in the background. Since we use the Ninja build system to build "
"your sources, re-compilation is incremental and thus re-loading the "
"extension when you run your Python module a second time is fast and has low "
"overhead if you didn't change the extension's source files."
msgstr ""
"生成的 Python 模块将与 setuptools 产生的模块完全相同，但无需维护单独的 ``setup.py`` "
"构建文件。如果您的设置更复杂且确实需要 :mod:`setuptools` 的全部功能，您 *可以* 编写自己的 "
"``setup.py``——但在许多情况下，此 JIT 技术完全足够。您首次运行此行代码时会需要一些时间，因为扩展正在后台编译。由于我们使用 Ninja"
" 构建系统来构建源代码，因此重新编译是增量的，因此如果您没有更改扩展的源文件，则第二次运行 Python 模块时重新加载扩展会很快且开销很小。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Writing a Mixed C++/CUDA extension"
msgstr "编写混合 C++/CUDA 扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To really take our implementation to the next level, we can hand-write parts"
" of our forward and backward passes with custom CUDA kernels. For the LLTM, "
"this has the prospect of being particularly effective, as there are a large "
"number of pointwise operations in sequence, that can all be fused and "
"parallelized in a single CUDA kernel. Let's see how we could write such a "
"CUDA kernel and integrate it with PyTorch using this extension mechanism."
msgstr ""
"为了真正在实现中达到新高度，我们可以通过自定义 CUDA 核心手动编写部分的前向和反向传播函数。对于 LLTM "
"来说，这特别有效，因为有大量连续的逐点操作，可以在单个 CUDA 核心中融合并并行化。让我们看看如何编写这样的 CUDA 核心，并使用此扩展机制将其与 "
"PyTorch 集成。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The general strategy for writing a CUDA extension is to first write a C++ "
"file which defines the functions that will be called from Python, and binds "
"those functions to Python with pybind11. Furthermore, this file will also "
"*declare* functions that are defined in CUDA (``.cu``) files. The C++ "
"functions will then do some checks and ultimately forward its calls to the "
"CUDA functions. In the CUDA files, we write our actual CUDA kernels. The "
":mod:`cpp_extension` package will then take care of compiling the C++ "
"sources with a C++ compiler like ``gcc`` and the CUDA sources with NVIDIA's "
"``nvcc`` compiler. This ensures that each compiler takes care of files it "
"knows best to compile. Ultimately, they will be linked into one shared "
"library that is available to us from Python code."
msgstr ""
"编写 CUDA 扩展的一般策略是首先编写一个定义将从 Python 调用的函数的 C++ 文件，并使用 pybind11 将这些函数绑定到 "
"Python。此外，此文件还将 *声明* 在 CUDA（``.cu``）文件中定义的函数。C++ 函数然后会执行一些检查并最终将其调用转发到 CUDA "
"函数。在 CUDA 文件中，我们编写实际的 CUDA 核心。:mod:`cpp_extension` 包随后将负责使用类似于 ``gcc`` 的 C++"
" 编译器编译 C++ 源代码，并使用 NVIDIA 的 ``nvcc`` 编译器编译 CUDA "
"源代码。这确保了每个编译器处理它最擅长的文件。最后，它们将被链接为一个共享库，我们可以从 Python 代码中使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We'll start with the C++ file, which we'll call ``lltm_cuda.cpp``, for "
"example:"
msgstr "我们将从 C++ 文件开始，例如我们称其为 ``lltm_cuda.cpp``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As you can see, it is largely boilerplate, checks and forwarding to "
"functions that we'll define in the CUDA file. We'll name this file "
"``lltm_cuda_kernel.cu`` (note the ``.cu`` extension!). NVCC can reasonably "
"compile C++11, thus we still have ATen and the C++ standard library "
"available to us (but not ``torch.h``). Note that :mod:`setuptools` cannot "
"handle files with the same name but different extensions, so if you use the "
"``setup.py`` method instead of the JIT method, you must give your CUDA file "
"a different name than your C++ file (for the JIT method, ``lltm.cpp`` and "
"``lltm.cu`` would work fine). Let's take a small peek at what this file will"
" look like:"
msgstr ""
"如您所见，这主要是样板代码，用于检查并转发给我们将在CUDA文件中定义的功能。我们将这个文件命名为``lltm_cuda_kernel.cu``（注意``.cu``扩展名！）。NVCC可以合理地编译C++11，因此我们仍然可以使用ATen和C++标准库（但不能使用``torch.h``）。注意：:mod:`setuptools`无法处理具有相同名字但不同扩展名的文件，因此如果您使用``setup.py``方法而不是JIT方法，则必须为您的CUDA文件另取一个名字，而不能与您的C++文件重复命名（对于JIT方法，``lltm.cpp``和``lltm.cu``是可行的）。让我们快速浏览一下这个文件可能的样子："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here we see the headers I just described, as well as the fact that we are "
"using CUDA-specific declarations like ``__device__`` and ``__forceinline__``"
" and functions like ``exp``. Let's continue with a few more helper functions"
" that we'll need:"
msgstr ""
"这里我们看到了我刚描述的头文件，以及我们使用CUDA特定声明如``__device__``和``__forceinline__``以及函数如``exp``。让我们继续讨论一些我们需要的其他辅助函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To now actually implement a function, we'll again need two things: one "
"function that performs operations we don't wish to explicitly write by hand "
"and calls into CUDA kernels, and then the actual CUDA kernel for the parts "
"we want to speed up. For the forward pass, the first function should look "
"like this:"
msgstr ""
"现在要实际实现一个函数，我们仍然需要两件事：一个函数执行我们不希望手动显式编写的操作并调用CUDA内核，然后是实际用于加速部分的CUDA内核。对于前向传播，第一个函数应该看起来像这样："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The main point of interest here is the ``AT_DISPATCH_FLOATING_TYPES`` macro "
"and the kernel launch (indicated by the ``<<<...>>>``). While ATen abstracts"
" away the device and datatype of the tensors we deal with, a tensor will, at"
" runtime, still be backed by memory of a concrete type on a concrete device."
" As such, we need a way of determining at runtime what type a tensor is and "
"then selectively call functions with the corresponding correct type "
"signature. Done manually, this would (conceptually) look something like "
"this:"
msgstr ""
"这里的主要关注点是``AT_DISPATCH_FLOATING_TYPES``宏和内核启动（由``<<<...>>>``指示）。虽然ATen抽象出了张量的设备和数据类型，但张量在运行时仍然由具体类型在具体设备上的内存支持。因此，我们需要一种在运行时确定张量是什么类型的方法，然后选择性地调用具有对应正确类型签名的函数。手动完成，这（概念上）看起来像这样："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The purpose of ``AT_DISPATCH_FLOATING_TYPES`` is to take care of this "
"dispatch for us. It takes a type (``gates.type()`` in our case), a name (for"
" error messages) and a lambda function. Inside this lambda function, the "
"type alias ``scalar_t`` is available and is defined as the type that the "
"tensor actually is at runtime in that context. As such, if we have a "
"template function (which our CUDA kernel will be), we can instantiate it "
"with this ``scalar_t`` alias, and the correct function will be called. In "
"this case, we also want to retrieve the data pointers of the tensors as "
"pointers of that ``scalar_t`` type. If you wanted to dispatch over all types"
" and not just floating point types (``Float`` and ``Double``), you can use "
"``AT_DISPATCH_ALL_TYPES``."
msgstr ""
"``AT_DISPATCH_FLOATING_TYPES``的目的是为我们处理这种调度。它接收一个类型（在我们的例子中为``gates.type()``），一个名字（用于错误消息）以及一个lambda函数。在这个lambda函数内部，类型别名``scalar_t``是可用的，并定义为在相应上下文中张量在运行时实际的类型。因此，如果我们有一个模板函数（我们的CUDA内核将是模板函数），我们可以用这个``scalar_t``别名实例化它，并调用正确的函数。在此情况下，我们还需要将张量的数据指针作为该``scalar_t``类型的指针检索。如果您想要调度所有类型而不仅仅是浮点类型（``Float``和``Double``），可以使用``AT_DISPATCH_ALL_TYPES``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that we perform some operations with plain ATen. These operations will "
"still run on the GPU, but using ATen's default implementations. This makes "
"sense because ATen will use highly optimized routines for things like matrix"
" multiplies (e.g. ``addmm``) or convolutions which would be much harder to "
"implement and improve ourselves."
msgstr ""
"请注意，我们使用ATen进行了一些操作。这些操作仍将在GPU上运行，但使用ATen的默认实现。这是合理的，因为ATen会使用高度优化的例程处理矩阵乘法（例如``addmm``）或卷积，这些实施和优化起来远远要更复杂。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As for the kernel launch itself, we are here specifying that each CUDA block"
" will have 1024 threads, and that the entire GPU grid is split into as many "
"blocks of ``1 x 1024`` threads as are required to fill our matrices with one"
" thread per component. For example, if our state size was 2048 and our batch"
" size 4, we'd launch a total of ``4 x 2 = 8`` blocks with each 1024 threads."
" If you've never heard of CUDA \"blocks\" or \"grids\" before, an "
"`introductory read about CUDA <https://devblogs.nvidia.com/even-easier-"
"introduction-cuda>`_ may help."
msgstr ""
"至于内核启动本身，我们这里指定每个CUDA块有1024个线程，并且整个GPU网格被拆分为尽可能多的``1 x "
"1024``线程的块以填充矩阵，每个元素由一个线程负责。例如，如果状态大小为2048，批量大小为4，我们总共将启动``4 x 2 = "
"8``个块，每块有1024个线程。如果您从未听过CUDA的“块”或“网格”，可以阅读`关于CUDA的介绍文章 "
"<https://devblogs.nvidia.com/even-easier-introduction-cuda>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The actual CUDA kernel is fairly simple (if you've ever programmed GPUs "
"before):"
msgstr "实际的CUDA内核相对简单（如果您曾经有过GPU编程经验）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"What's primarily interesting here is that we are able to compute all of "
"these pointwise operations entirely in parallel for each individual "
"component in our gate matrices. If you imagine having to do this with a "
"giant ``for`` loop over a million elements in serial, you can see why this "
"would be much faster."
msgstr ""
"这里主要有趣的是我们能够以并行方式完全计算这些基于单点的操作，每个独立组件都在我们的门控矩阵中。如果您设想必须用一个巨大的``for``循环序列处理一百万个元素，就可以理解为什么这会快得多。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using accessors"
msgstr "使用访问器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can see in the CUDA kernel that we work directly on pointers with the "
"right type. Indeed, working directly with high level type agnostic tensors "
"inside cuda kernels would be very inefficient."
msgstr "您可以在CUDA内核中看到我们直接操作具有正确类型的指针。事实上，直接在CUDA内核中操作高层类型无关的张量效率会非常低。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"However, this comes at a cost of ease of use and readability, especially for"
" highly dimensional data. In our example, we know for example that the "
"contiguous ``gates`` tensor has 3 dimensions:"
msgstr "然而，这会在易用性和可读性方面造成一定影响，尤其是对于高维数据。在我们的示例中，例如我们知道连续的``gates``张量有3个维度："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "batch, size of ``batch_size`` and stride of ``3*state_size``"
msgstr "批量维度，大小为``batch_size``，步幅为``3*state_size``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "row, size of ``3`` and stride of ``state_size``"
msgstr "行维度，大小为``3``，步幅为``state_size``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "index, size  of ``state_size`` and stride of ``1``"
msgstr "索引维度，大小为``state_size``，步幅为``1``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How can we access the element ``gates[n][row][column]`` inside the kernel "
"then? It turns out that you need the strides to access your element with "
"some simple arithmetic."
msgstr "那么我们如何在内核中访问元素``gates[n][row][column]``呢？事实证明，您需要使用步幅通过一些简单的算术获得您的元素。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In addition to being verbose, this expression needs stride to be explicitly "
"known, and thus passed to the kernel function within its arguments. You can "
"see that in the case of kernel functions accepting multiple tensors with "
"different sizes you will end up with a very long list of arguments."
msgstr ""
"此外，这种表达式不仅冗长，还需要显式知道步幅，因此必须通过参数传递给内核函数。您可以看到，对于接受具有不同尺寸的多个张量的内核函数，您最终会得到一个非常长的参数列表。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Fortunately for us, ATen provides accessors that are created with a single "
"dynamic check that a Tensor is the type and number of dimensions. Accessors "
"then expose an API for accessing the Tensor elements efficiently without "
"having to convert to a single pointer:"
msgstr ""
"幸运的是，ATen提供了访问器，它通过单次动态检查张量的类型和维度数量来创建。访问器随后提供高效访问张量元素的API，而无需转换为单一指针："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Accessor objects have a relatively high level interface, with ``.size()`` "
"and ``.stride()`` methods and multi-dimensional indexing. The "
"``.accessor<>`` interface is designed to access data efficiently on cpu "
"tensor. The equivalent for cuda tensors are ``packed_accessor64<>`` and "
"``packed_accessor32<>``, which produce Packed Accessors with either 64-bit "
"or 32-bit integer indexing."
msgstr ""
"访问器对象具有相对高级的接口，带有``.size()``和``.stride()``方法以及多维索引。用于访问CPU张量的``.accessor<>``接口的等效项是``packed_accessor64<>``和``packed_accessor32<>``，它们分别生成具有64位或32位整数索引的打包访问器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The fundamental difference with Accessor is that a Packed Accessor copies "
"size and stride data inside of its structure instead of pointing to it. It "
"allows us to pass it to a CUDA kernel function and use its interface inside "
"it."
msgstr ""
"与普通访问器的根本区别在于，打包访问器将大小或步幅的数据复制到其结构内，而不是指向它。这使我们能够将其传递给CUDA内核函数并在其中使用其接口。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can design a function that takes Packed Accessors instead of pointers."
msgstr "我们可以设计一个接受打包访问器而不是指针的函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's decompose the template used here. the first two arguments ``scalar_t``"
" and ``2`` are the same as regular Accessor. The argument "
"``torch::RestrictPtrTraits`` indicates that the ``__restrict__`` keyword "
"must be used. Note also that we've used the ``PackedAccessor32`` variant "
"which store the sizes and strides in an ``int32_t``. This is important as "
"using the 64-bit variant (``PackedAccessor64``) can make the kernel slower."
msgstr ""
"让我们分解这里使用的模板。前两个参数``scalar_t``和``2``与普通访问器相同。参数``torch::RestrictPtrTraits``表示必须使用``__restrict__``关键字。还要注意的是，我们使用了``PackedAccessor32``变体，它将尺寸和步幅存储在``int32_t``中。这很重要，因为使用64位变体（``PackedAccessor64``）可能会让内核变慢。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The function declaration becomes"
msgstr "函数声明变成了"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The implementation is much more readable! This function is then called by "
"creating Packed Accessors with the ``.packed_accessor32<>`` method within "
"the host function."
msgstr "实现变得更易读！然后通过在主机函数中使用``.packed_accessor32<>``方法创建打包访问器来调用该函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The backwards pass follows much the same pattern and I won't elaborate "
"further on it:"
msgstr "反向传播遵循大致相同的模式，我不会进一步详细展开："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Integrating a C++/CUDA Operation with PyTorch"
msgstr "将C++/CUDA操作集成到PyTorch中"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Integration of our CUDA-enabled op with PyTorch is again very "
"straightforward. If you want to write a ``setup.py`` script, it could look "
"like this::"
msgstr "将我们的支持CUDA的操作集成到PyTorch中再一次非常简单。如果您想编写一个``setup.py``脚本，它可以像这样："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Instead of :func:`CppExtension`, we now use :func:`CUDAExtension`. We can "
"just specify the ``.cu`` file along with the ``.cpp`` files -- the library "
"takes care of all the hassle this entails for you. The JIT mechanism is even"
" simpler::"
msgstr ""
"我们现在使用:func:`CUDAExtension`而不是:func:`CppExtension`。我们可以简单地指定``.cu``文件以及``.cpp``文件——库会为您处理所有相关的复杂工作。JIT机制甚至更简单："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Our hope was that parallelizing and fusing the pointwise operations of our "
"code with CUDA would improve the performance of our LLTM. Let's see if that "
"holds true. We can run the code I listed earlier to run a benchmark. Our "
"fastest version earlier was the CUDA-based C++ code::"
msgstr ""
"我们的希望是将代码中的基于单点的操作并行化并通过CUDA融合会提升我们LLTM的性能。让我们看看是否确实如此。我们可以运行我之前列出的代码以运行一个基准测试。之前最快的版本是基于CUDA的C++代码："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "And now with our custom CUDA kernel::"
msgstr "现在使用我们自定义的CUDA内核："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "More performance increases!"
msgstr "更多性能提升！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You should now be equipped with a good overview of PyTorch's C++ extension "
"mechanism as well as a motivation for using them. You can find the code "
"examples displayed in this note `here <https://github.com/pytorch/extension-"
"cpp>`_. If you have questions, please use `the forums "
"<https://discuss.pytorch.org>`_. Also be sure to check our `FAQ "
"<https://pytorch.org/cppdocs/notes/faq.html>`_ in case you run into any "
"issues. A blog on writing extensions for AMD ROCm can be found `here "
"<https://rocm.blogs.amd.com/artificial-intelligence/cpp-extn/readme.html>`_."
msgstr ""
"现在您应该对PyTorch的C++扩展机制有了一个很好的概述并了解使用它们的动机。您可以在`这里 "
"<https://github.com/pytorch/extension-cpp>`_找到本笔记中展示的代码示例。如果您有问题，请使用`论坛 "
"<https://discuss.pytorch.org>`_。如果您遇到任何问题，务必查看我们的`FAQ "
"<https://pytorch.org/cppdocs/notes/faq.html>`_。关于为AMD ROCm编写扩展的博客请参阅`这里 "
"<https://rocm.blogs.amd.com/artificial-intelligence/cpp-extn/readme.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using the PyTorch C++ Frontend"
msgstr "使用PyTorch C++前端"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The PyTorch C++ frontend is a pure C++ interface to the PyTorch machine "
"learning framework. While the primary interface to PyTorch naturally is "
"Python, this Python API sits atop a substantial C++ codebase providing "
"foundational data structures and functionality such as tensors and automatic"
" differentiation. The C++ frontend exposes a pure C++11 API that extends "
"this underlying C++ codebase with tools required for machine learning "
"training and inference. This includes a built-in collection of common "
"components for neural network modeling; an API to extend this collection "
"with custom modules; a library of popular optimization algorithms such as "
"stochastic gradient descent; a parallel data loader with an API to define "
"and load datasets; serialization routines and more."
msgstr ""
"PyTorch C++前端是PyTorch机器学习框架的纯C++接口。尽管PyTorch的主要接口是Python，但此Python "
"API是建立在提供诸如张量和自动微分等基础数据结构和功能的大量C++代码库之上的。C++前端通过提供构建机器学习训练和推断所需工具的纯C++11 "
"API扩展了底层的C++代码库。这包括用于神经网络建模的内置常见组件集；一个API以自定义模块扩展该集合；一个包含流行优化算法（如随机梯度下降）的库；一个带有定义和加载数据集API的并行数据加载器；序列化例程等。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial will walk you through an end-to-end example of training a "
"model with the C++ frontend. Concretely, we will be training a `DCGAN "
"<https://arxiv.org/abs/1511.06434>`_ -- a kind of generative model -- to "
"generate images of MNIST digits. While conceptually a simple example, it "
"should be enough to give you a whirlwind overview of the PyTorch C++ "
"frontend and wet your appetite for training more complex models. We will "
"begin with some motivating words for why you would want to use the C++ "
"frontend to begin with, and then dive straight into defining and training "
"our model."
msgstr ""
"本教程将通过一个端到端的模型训练示例来引导您使用C++前端。具体来说，我们将训练一个`DCGAN "
"<https://arxiv.org/abs/1511.06434>`_——一种生成模型——来生成MNIST数字的图像。虽然这是一个概念上简单的示例，但应该足以让您快速了解PyTorch"
" C++前端并激发您训练更复杂模型的兴趣。我们将从一些关于使用C++前端的动机性话语开始，然后直接定义和训练我们的模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Watch `this lightning talk from CppCon 2018 "
"<https://www.youtube.com/watch?v=auRPXMMHJzc>`_ for a quick (and humorous) "
"presentation on the C++ frontend."
msgstr ""
"观看`CppCon 2018的这个快闪演讲 "
"<https://www.youtube.com/watch?v=auRPXMMHJzc>`_，可以快速（和幽默地）了解C++前端。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`This note <https://pytorch.org/cppdocs/frontend.html>`_ provides a sweeping"
" overview of the C++ frontend's components and design philosophy."
msgstr ""
"`这个说明 <https://pytorch.org/cppdocs/frontend.html>`_提供了关于C++前端的组件和设计哲学的全面概述。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Documentation for the PyTorch C++ ecosystem is available at "
"https://pytorch.org/cppdocs. There you can find high level descriptions as "
"well as API-level documentation."
msgstr ""
"可以在https://pytorch.org/cppdocs找到PyTorch C++生态系统的文档。在那里您可以找到高级描述以及API级文档。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Motivation"
msgstr "动机"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Before we embark on our exciting journey of GANs and MNIST digits, let's "
"take a step back and discuss why you would want to use the C++ frontend "
"instead of the Python one to begin with. We (the PyTorch team) created the "
"C++ frontend to enable research in environments in which Python cannot be "
"used, or is simply not the right tool for the job. Examples for such "
"environments include:"
msgstr ""
"在我们踏上激动人心的GAN和MNIST数字之旅之前，让我们退后一步讨论为什么您想要使用C++前端而不是Python前端。我们（PyTorch团队）创建C++前端是为了促进在Python无法使用或者根本不是正确工具的环境中进行研究。这些环境的例子包括："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Low Latency Systems**: You may want to do reinforcement learning research "
"in a pure C++ game engine with high frames-per-second and low latency "
"requirements. Using a pure C++ library is a much better fit to such an "
"environment than a Python library. Python may not be tractable at all "
"because of the slowness of the Python interpreter."
msgstr ""
"**低延迟系统**: "
"您可能希望在具有高帧速率和低延迟要求的纯C++游戏引擎中进行强化学习研究。使用纯C++库更适合这样的环境，而不是Python库。由于Python解释器的速度缓慢，Python可能完全不可行。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Highly Multithreaded Environments**: Due to the Global Interpreter Lock "
"(GIL), Python cannot run more than one system thread at a time. "
"Multiprocessing is an alternative, but not as scalable and has significant "
"shortcomings. C++ has no such constraints and threads are easy to use and "
"create. Models requiring heavy parallelization, like those used in `Deep "
"Neuroevolution <https://www.uber.com/blog/deep-neuroevolution/>`_, can "
"benefit from this."
msgstr ""
"**高度线程化的环境**: "
"由于全局解释器锁（GIL）的存在，Python无法同时运行多个系统线程。虽然多进程是一种替代方案，但其可扩展性较差且存在显著缺点。C++则没有这样的限制，并且线程的创建和使用更加轻松。需要大量并行计算的模型，例如"
" `Deep Neuroevolution <https://www.uber.com/blog/deep-neuroevolution/>`_ "
"中使用的模型，可以从中受益。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Existing C++ Codebases**: You may be the owner of an existing C++ "
"application doing anything from serving web pages in a backend server to "
"rendering 3D graphics in photo editing software, and wish to integrate "
"machine learning methods into your system. The C++ frontend allows you to "
"remain in C++ and spare yourself the hassle of binding back and forth "
"between Python and C++, while retaining much of the flexibility and "
"intuitiveness of the traditional PyTorch (Python) experience."
msgstr ""
"**现有的C++代码库**: "
"如果您拥有一个现有的C++应用程序，无论是用于后端服务器中提供网页服务，还是在照片编辑软件中进行3D图像渲染，并且希望将机器学习方法集成到您的系统中，C++前端允许您继续使用C++，避免在Python和C++之间来回绑定的麻烦，同时保留了传统PyTorch（Python）经验的大部分灵活性和直观性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The C++ frontend is not intended to compete with the Python frontend. It is "
"meant to complement it. We know researchers and engineers alike love PyTorch"
" for its simplicity, flexibility and intuitive API. Our goal is to make sure"
" you can take advantage of these core design principles in every possible "
"environment, including the ones described above. If one of these scenarios "
"describes your use case well, or if you are simply interested or curious, "
"follow along as we explore the C++ frontend in detail in the following "
"paragraphs."
msgstr ""
"C++前端并非旨在与Python前端竞争，而是为其补充。我们知道研究人员和工程师们都喜欢PyTorch，因为它简单、灵活并且API直观。我们的目标是确保您可以在所有可能的环境中利用这些核心设计原则，包括上述描述的环境。如果这些场景之一与您的使用案例相符，或者您仅仅出于兴趣或好奇，请继续阅读，以下段落将详细介绍C++前端。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The C++ frontend tries to provide an API as close as possible to that of the"
" Python frontend. If you are experienced with the Python frontend and ever "
"ask yourself \"how do I do X with the C++ frontend?\", write your code the "
"way you would in Python, and more often than not the same functions and "
"methods will be available in C++ as in Python (just remember to replace dots"
" with double colons)."
msgstr ""
"C++前端试图提供一个尽可能接近Python前端的API。如果您熟悉Python前端并且想知道“如何用C++前端实现X功能？”， "
"按照您在Python中的编码方式编写代码，多数情况下，在C++中您会发现与Python中相同的函数和方法（只是需要将点替换为双冒号）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Writing a Basic Application"
msgstr "编写一个基本的应用程序"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's begin by writing a minimal C++ application to verify that we're on the"
" same page regarding our setup and build environment. First, you will need "
"to grab a copy of the *LibTorch* distribution -- our ready-built zip archive"
" that packages all relevant headers, libraries and CMake build files "
"required to use the C++ frontend. The LibTorch distribution is available for"
" download on the `PyTorch website <https://pytorch.org/get-"
"started/locally/>`_ for Linux, MacOS and Windows. The rest of this tutorial "
"will assume a basic Ubuntu Linux environment, however you are free to follow"
" along on MacOS or Windows too."
msgstr ""
"让我们从编写一个最小的C++应用程序开始，以验证我们在设置和构建环境方面是否一致。首先，您需要获取一个 *LibTorch* 发行版-- "
"一个预先构建的压缩档案，其中包含使用C++前端所需的所有相关头文件、库和CMake构建文件。可以从PyTorch官网上下载适用于Linux、MacOS和Windows的LibTorch发行版：`PyTorch官网"
" <https://pytorch.org/get-started/locally/>`_。本教程其余部分将以基本的Ubuntu "
"Linux环境为例，但是您也可以在MacOS或Windows上跟随学习。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The note on `Installing C++ Distributions of PyTorch "
"<https://pytorch.org/cppdocs/installing.html>`_ describes the following "
"steps in more detail."
msgstr ""
"关于 `安装PyTorch的C++发行版 <https://pytorch.org/cppdocs/installing.html>`_ "
"的说明描述了以下步骤的详细信息。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The first step is to download the LibTorch distribution locally, via the "
"link retrieved from the PyTorch website. For a vanilla Ubuntu Linux "
"environment, this means running:"
msgstr "第一步是通过从PyTorch官网下载的链接，将LibTorch发行版下载到本地。对于一个普通的Ubuntu Linux环境，这意味着运行："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, let's write a tiny C++ file called ``dcgan.cpp`` that includes "
"``torch/torch.h`` and for now simply prints out a three by three identity "
"matrix:"
msgstr ""
"接下来，让我们编写一个名为 ``dcgan.cpp`` 的小型C++文件，包含头文件 "
"``torch/torch.h``，并暂时只打印一个3x3的单位矩阵："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To build this tiny application as well as our full-fledged training script "
"later on we'll use this ``CMakeLists.txt`` file:"
msgstr "为了构建这个小程序以及我们稍后进行的完整训练脚本，我们将使用以下 ``CMakeLists.txt`` 文件："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"While CMake is the recommended build system for LibTorch, it is not a hard "
"requirement. You can also use Visual Studio project files, QMake, plain "
"Makefiles or any other build environment you feel comfortable with. However,"
" we do not provide out-of-the-box support for this."
msgstr ""
"虽然CMake是LibTorch推荐的构建系统，但它并不是强制要求。您也可以使用Visual "
"Studio项目文件、QMake、普通的Makefile或任何您熟悉的构建环境。然而，我们不为这些提供现成的支持。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Make note of line 4 in the above CMake file: ``find_package(Torch "
"REQUIRED)``. This instructs CMake to find the build configuration for the "
"LibTorch library. In order for CMake to know *where* to find these files, we"
" must set the ``CMAKE_PREFIX_PATH`` when invoking ``cmake``. Before we do "
"this, let's agree on the following directory structure for our ``dcgan`` "
"application:"
msgstr ""
"注意上述CMake文件中的第4行：``find_package(Torch "
"REQUIRED)``。这指示CMake查找LibTorch库的构建配置。为了让CMake知道在哪里找到这些文件，我们在调用``cmake``时必须设置``CMAKE_PREFIX_PATH``。在此之前，让我们制定以下目录结构作为"
" ``dcgan`` 应用的结构："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Further, I will refer to the path to the unzipped LibTorch distribution as "
"``/path/to/libtorch``. Note that this **must be an absolute path**. In "
"particular, setting ``CMAKE_PREFIX_PATH`` to something like "
"``../../libtorch`` will break in unexpected ways. Instead, write "
"``$PWD/../../libtorch`` to get the corresponding absolute path. Now, we are "
"ready to build our application:"
msgstr ""
"此外，我将把已经解压的LibTorch发行版路径称为``/path/to/libtorch``。注意，这**必须是一个绝对路径**。特别是，将``CMAKE_PREFIX_PATH``设置为例如``../../libtorch``可能会以意料之外的方式出错。相反，可以使用``$PWD/../../libtorch``来获取对应的绝对路径。现在，我们已经准备好构建我们的应用程序："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Above, we first created a ``build`` folder inside of our ``dcgan`` "
"directory, entered this folder, ran the ``cmake`` command to generate the "
"necessary build (Make) files and finally compiled the project successfully "
"by running ``cmake --build . --config Release``. We are now all set to "
"execute our minimal binary and complete this section on basic project "
"configuration:"
msgstr ""
"在上方的步骤中，我们首先在``dcgan``目录中创建了一个``build``文件夹，然后进入这个文件夹，运行``cmake``命令以生成必要的构建（Make）文件，最后通过运行``cmake"
" --build . --config Release``成功编译项目。现在我们可以执行编译好的小型二进制文件，完成这一部分关于基本项目配置的内容："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Looks like an identity matrix to me!"
msgstr "这看起来确实是一个单位矩阵！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining the Neural Network Models"
msgstr "定义神经网络模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have our basic environment configured, we can dive into the much"
" more interesting parts of this tutorial. First, we will discuss how to "
"define and interact with modules in the C++ frontend. We'll begin with "
"basic, small-scale example modules and then implement a full-fledged GAN "
"using the extensive library of built-in modules provided by the C++ "
"frontend."
msgstr ""
"现在我们已经配置好了基本环境，可以开始本教程中更有趣的部分了。首先，我们将讨论如何在C++前端定义和交互模块。我们将从基础的小规模模块示例开始，然后使用C++前端内置模块的庞大库实现一个完整的GAN。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Module API Basics"
msgstr "模块API基础"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In line with the Python interface, neural networks based on the C++ frontend"
" are composed of reusable building blocks called *modules*. There is a base "
"module class from which all other modules are derived. In Python, this class"
" is ``torch.nn.Module`` and in C++ it is ``torch::nn::Module``. Besides a "
"``forward()`` method that implements the algorithm the module encapsulates, "
"a module usually contains any of three kinds of sub-objects: parameters, "
"buffers and submodules."
msgstr ""
"与Python接口一样，基于C++前端的神经网络是由可重用的构建模块（被称为*模块*）组成的。所有其他模块都以一个基本模块类为基础。在Python中，这个类是``torch.nn.Module``；在C++中，它是``torch::nn::Module``。除了实现模块封装算法的``forward()``方法外，一个模块通常包含三种子对象之一：参数、缓冲区和子模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Parameters and buffers store state in form of tensors. Parameters record "
"gradients, while buffers do not. Parameters are usually the trainable "
"weights of your neural network. Examples of buffers include means and "
"variances for batch normalization. In order to re-use particular blocks of "
"logic and state, the PyTorch API allows modules to be nested. A nested "
"module is termed a *submodule*."
msgstr ""
"参数和缓冲区以张量形式存储状态。参数记录梯度，而缓冲区则不记录。参数通常是神经网络的可训练权重。缓冲区的示例包括用于批归一化的均值和方差。为了重用特定的逻辑块和状态，PyTorch"
" API允许模块嵌套。嵌套的模块被称为*子模块*。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Parameters, buffers and submodules must be explicitly registered. Once "
"registered, methods like ``parameters()`` or ``buffers()`` can be used to "
"retrieve a container of all parameters in the entire (nested) module "
"hierarchy. Similarly, methods like ``to(...)``, where e.g. "
"``to(torch::kCUDA)`` moves all parameters and buffers from CPU to CUDA "
"memory, work on the entire module hierarchy."
msgstr ""
"参数、缓冲区和子模块必须显式注册。一旦注册，像``parameters()``或``buffers()`` "
"这样的方法可以用来检索整个（嵌套）模块层次结构中的所有参数容器。同样，类似``to(...)``的方法，例如``to(torch::kCUDA)``，可以将所有参数和缓冲区从CPU移动到CUDA内存，这些方法作用于整个模块层次结构。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining a Module and Registering Parameters"
msgstr "定义一个模块并注册参数"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To put these words into code, let's consider this simple module written in "
"the Python interface:"
msgstr "将这些文字转换为代码，让我们看一下通过Python接口编写的简单模块示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "In C++, it would look like this:"
msgstr "在C++中，它看起来像这样："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Just like in Python, we define a class called ``Net`` (for simplicity here a"
" ``struct`` instead of a ``class``) and derive it from the module base "
"class. Inside the constructor, we create tensors using ``torch::randn`` just"
" like we use ``torch.randn`` in Python. One interesting difference is how we"
" register the parameters. In Python, we wrap the tensors with the "
"``torch.nn.Parameter`` class, while in C++ we have to pass the tensor "
"through the ``register_parameter`` method instead. The reason for this is "
"that the Python API can detect that an attribute is of type "
"``torch.nn.Parameter`` and automatically registers such tensors. In C++, "
"reflection is very limited, so a more traditional (and less magical) "
"approach is provided."
msgstr ""
"与Python一样，我们定义了一个名为``Net``的类（为简单起见，这里使用``struct``而不是``class``），并将其从模块基类派生。我们在构造函数中使用``torch::randn``创建张量，与在Python中使用``torch.randn``的方法类似。一个有趣的区别是我们如何注册参数。在Python中，我们使用``torch.nn.Parameter``类包装张量，而在C++中，我们必须通过``register_parameter``方法传递张量。其原因在于，Python"
" "
"API可以检测属性是否为``torch.nn.Parameter``类型，并自动注册此类张量。而在C++中，反射能力非常有限，因此提供了一种更为传统（也更直观）的方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Registering Submodules and Traversing the Module Hierarchy"
msgstr "注册子模块并遍历模块层次结构"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the same way we can register parameters, we can also register submodules."
" In Python, submodules are automatically detected and registered when they "
"are assigned as an attribute of a module:"
msgstr "与注册参数的方式相同，我们也可以注册子模块。在Python中，当子模块作为模块的属性分配时，它们会被自动检测并注册："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This allows, for example, to use the ``parameters()`` method to recursively "
"access all parameters in our module hierarchy:"
msgstr "这允许，例如，使用``parameters()``方法递归访问模块层次结构中的所有参数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To register submodules in C++, use the aptly named ``register_module()`` "
"method to register a module like ``torch::nn::Linear``:"
msgstr "在C++中，使用名为``register_module()``的方法注册诸如``torch::nn::Linear``这样的模块："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can find the full list of available built-in modules like "
"``torch::nn::Linear``, ``torch::nn::Dropout`` or ``torch::nn::Conv2d`` in "
"the documentation of the ``torch::nn`` namespace `here "
"<https://pytorch.org/cppdocs/api/namespace_torch__nn.html>`_."
msgstr ""
"您可以在 ``torch::nn`` "
"命名空间的文档中找到所有可用的内置模块列表，比如``torch::nn::Linear``、``torch::nn::Dropout``或``torch::nn::Conv2d``：`这里"
" <https://pytorch.org/cppdocs/api/namespace_torch__nn.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"One subtlety about the above code is why the submodule was created in the "
"constructor's initializer list, while the parameter was created inside the "
"constructor body. There is a good reason for this, which we'll touch upon "
"this in the section on the C++ frontend's *ownership model* further below. "
"The end result, however, is that we can recursively access our module tree's"
" parameters just like in Python. Calling ``parameters()`` returns a "
"``std::vector<torch::Tensor>``, which we can iterate over:"
msgstr ""
"上述代码中的一个细节是，为什么子模块是在构造函数的初始化列表中创建的，而参数是在构造函数主体内部创建的。这样做是有充分理由的，将在稍后关于C++前端 "
"*所有权模型* "
"的部分中详细讨论。不过最终结果是，我们可以像在Python中一样递归访问模块树中的参数。调用``parameters()``会返回一个``std::vector<torch::Tensor>``，我们可以对其进行迭代："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "which prints:"
msgstr "这会打印出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"with three parameters just like in Python. To also see the names of these "
"parameters, the C++ API provides a ``named_parameters()`` method which "
"returns an ``OrderedDict`` just like in Python:"
msgstr ""
"与Python中的三个参数完全一样。为了同时查看这些参数的名称，C++ "
"API提供了一个``named_parameters()``方法，它返回一个类似Python中的``OrderedDict``的结构："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "which we can execute again to see the output:"
msgstr "我们可以再次执行上述代码查看输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`The documentation "
"<https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_module.html#exhale-"
"class-classtorch-1-1nn-1-1-module>`_ for ``torch::nn::Module`` contains the "
"full list of methods that operate on the module hierarchy."
msgstr ""
"``torch::nn::Module`` 的文档 `这里 "
"<https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_module.html#exhale-"
"class-classtorch-1-1nn-1-1-module>`_ 包含了操作模块层次结构的完整方法列表。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Running the Network in Forward Mode"
msgstr "以前向模式运行网络"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To execute the network in C++, we simply call the ``forward()`` method we "
"defined ourselves:"
msgstr "要在C++中执行网络，我们只需调用我们自己定义的``forward()``方法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "which prints something like:"
msgstr "这会打印出类似于："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Module Ownership"
msgstr "模块所有权"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"At this point, we know how to define a module in C++, register parameters, "
"register submodules, traverse the module hierarchy via methods like "
"``parameters()`` and finally run the module's ``forward()`` method. While "
"there are many more methods, classes and topics to devour in the C++ API, I "
"will refer you to `docs "
"<https://pytorch.org/cppdocs/api/namespace_torch__nn.html>`_ for the full "
"menu. We'll also touch upon some more concepts as we implement the DCGAN "
"model and end-to-end training pipeline in just a second. Before we do so, "
"let me briefly touch upon the *ownership model* the C++ frontend provides "
"for subclasses of ``torch::nn::Module``."
msgstr ""
"到目前为止，我们已经知道如何在C++中定义一个模块，注册参数，注册子模块，通过``parameters()``等方法遍历模块层次结构，最终运行模块的``forward()``方法。虽然C++"
" API中还有许多方法、类和主题需要深入了解，但这里我建议您查看 `文档 "
"<https://pytorch.org/cppdocs/api/namespace_torch__nn.html>`_ "
"获取完整选单。接下来我们将在实现DCGAN模型和端到端训练管道时触及更多概念。在此之前，简要提及C++前端为``torch::nn::Module``子类提供的"
" *所有权模型*。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this discussion, the ownership model refers to the way modules are "
"stored and passed around -- which determines who or what *owns* a particular"
" module instance. In Python, objects are always allocated dynamically (on "
"the heap) and have reference semantics. This is very easy to work with and "
"straightforward to understand. In fact, in Python, you can largely forget "
"about where objects live and how they get referenced, and focus on getting "
"things done."
msgstr ""
"在本讨论中，所有权模型指模块被存储和传递的方式——这决定了谁或什么拥有特定的模块实例。在Python中，对象总是动态分配的（在堆上），并具有引用语义。这很容易使用，也容易理解。实际上，在Python中，通常可以完全忘记对象存储在哪里以及如何被引用，而将注意力集中在完成任务上。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"C++, being a lower level language, provides more options in this realm. This"
" increases complexity and heavily influences the design and ergonomics of "
"the C++ frontend. In particular, for modules in the C++ frontend, we have "
"the option of using *either* value semantics *or* reference semantics. The "
"first case is the simplest and was shown in the examples thus far: module "
"objects are allocated on the stack and when passed to a function, can be "
"either copied, moved (with ``std::move``) or taken by reference or by "
"pointer:"
msgstr ""
"C++作为一种较低级别的语言，在这个领域提供了更多选项。这增加了复杂性并对C++前端的设计和易用性产生了深刻影响。特别是对于C++前端的模块，我们可以选择*值语义*或*引用语义*。第一种情况是最简单的，之前的例子已经展示过：模块对象分配在栈上，传递给函数时，可以被复制、移动（使用`std::move`）或者以引用或指针形式传递："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For the second case -- reference semantics -- we can use "
"``std::shared_ptr``. The advantage of reference semantics is that, like in "
"Python, it reduces the cognitive overhead of thinking about how modules must"
" be passed to functions and how arguments must be declared (assuming you use"
" ``shared_ptr`` everywhere)."
msgstr ""
"对于第二种情况--引用语义--"
"我们可以使用`std::shared_ptr`。引用语义的优势在于它减少了思考模块如何传递给函数以及参数如何声明的认知负担（假设你在所有地方都使用`shared_ptr`），这与Python类似。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In our experience, researchers coming from dynamic languages greatly prefer "
"reference semantics over value semantics, even though the latter is more "
"\"native\" to C++. It is also important to note that ``torch::nn::Module``'s"
" design, in order to stay close to the ergonomics of the Python API, relies "
"on shared ownership. For example, take our earlier (here shortened) "
"definition of ``Net``:"
msgstr ""
"根据我们的经验，从动态语言转来的人通常更喜欢引用语义而非值语义，尽管后者更符合C++的“原生”方式。此外，值得注意的是，为了与Python "
"API的易用性保持接近，`torch::nn::Module`的设计依赖于共享所有权。例如，以下是我们之前简化的`Net`定义："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In order to use the ``linear`` submodule, we want to store it directly in "
"our class. However, we also want the module base class to know about and "
"have access to this submodule. For this, it must store a reference to this "
"submodule. At this point, we have already arrived at the need for shared "
"ownership. Both the ``torch::nn::Module`` class and concrete ``Net`` class "
"require a reference to the submodule. For this reason, the base class stores"
" modules as ``shared_ptr``\\s, and therefore the concrete class must too."
msgstr ""
"为了使用`linear`子模块，我们希望直接将其存储在我们的类中。然而，我们还希望模块基类能够知道并能访问该子模块。为此，它必须存储对该子模块的引用。这时，我们已经需要共享所有权了。`torch::nn::Module`基类和具体的`Net`类都需要子模块的引用。因此，基类将模块存储为`shared_ptr`，因此具体类也必须这样做。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"But wait! I don't see any mention of ``shared_ptr`` in the above code! Why "
"is that? Well, because ``std::shared_ptr<MyModule>`` is a hell of a lot to "
"type. To keep our researchers productive, we came up with an elaborate "
"scheme to hide the mention of ``shared_ptr`` -- a benefit usually reserved "
"for value semantics -- while retaining reference semantics. To understand "
"how this works, we can take a look at a simplified definition of the "
"``torch::nn::Linear`` module in the core library (the full definition is "
"`here "
"<https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/nn/modules/linear.h>`_):"
msgstr ""
"但等等！在上述代码中我没有看到任何关于`shared_ptr`的提及！为什么呢？嗯，因为`std::shared_ptr<MyModule>`写起来太繁琐了。为了提高研究人员的工作效率，我们设计了一个复杂的方案来隐藏`shared_ptr`的提及——通常只有值语义才能享受这种便捷——同时保留引用语义。为了理解它的工作原理，我们可以看一下核心库中`torch::nn::Linear`模块的简化版本定义（完整版在此处：`https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/nn/modules/linear.h`）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In brief: the module is not called ``Linear``, but ``LinearImpl``. A macro, "
"``TORCH_MODULE`` then defines the actual ``Linear`` class. This "
"\"generated\" class is effectively a wrapper over a "
"``std::shared_ptr<LinearImpl>``. It is a wrapper instead of a simple typedef"
" so that, among other things, constructors still work as expected, i.e. you "
"can still write ``torch::nn::Linear(3, 4)`` instead of "
"``std::make_shared<LinearImpl>(3, 4)``. We call the class created by the "
"macro the module *holder*. Like with (shared) pointers, you access the "
"underlying object using the arrow operator (like ``model->forward(...)``). "
"The end result is an ownership model that resembles that of the Python API "
"quite closely. Reference semantics become the default, but without the extra"
" typing of ``std::shared_ptr`` or ``std::make_shared``. For our ``Net``, "
"using the module holder API looks like this:"
msgstr ""
"简单来说：模块不叫`Linear`，而是叫`LinearImpl`。一个宏`TORCH_MODULE`定义了实际的`Linear`类。这个“生成”的类实际上是一个`std::shared_ptr<LinearImpl>`的包装器。它是一个包装器而不是简单的typedef，这样构造函数仍然能如期工作，例如你仍然可以写`torch::nn::Linear(3,"
" 4)`而不是`std::make_shared<LinearImpl>(3, "
"4)`。我们称由宏创建的类为模块*持有者*。与（共享）指针类似，你可以使用箭头操作符访问底层对象（比如`model->forward(...)`）。最终的结果是一个与Python"
" "
"API非常相似的所有权模型。引用语义成为默认选项，但无需额外输入`std::shared_ptr`或`std::make_shared`。对于我们的`Net`，使用模块持有者API的代码如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There is one subtle issue that deserves mention here. A default constructed "
"``std::shared_ptr`` is \"empty\", i.e. contains a null pointer. What is a "
"default constructed ``Linear`` or ``Net``? Well, it's a tricky choice. We "
"could say it should be an empty (null) ``std::shared_ptr<LinearImpl>``. "
"However, recall that ``Linear(3, 4)`` is the same as "
"``std::make_shared<LinearImpl>(3, 4)``. This means that if we had decided "
"that ``Linear linear;`` should be a null pointer, then there would be no way"
" to construct a module that does not take any constructor arguments, or "
"defaults all of them. For this reason, in the current API, a default "
"constructed module holder (like ``Linear()``) invokes the default "
"constructor of the underlying module (``LinearImpl()``). If the underlying "
"module does not have a default constructor, you get a compiler error. To "
"instead construct the empty holder, you can pass ``nullptr`` to the "
"constructor of the holder."
msgstr ""
"这里有一个细微的问题值得一提。默认构造的`std::shared_ptr`是“空的”，即包含一个空指针。那么默认构造的`Linear`或`Net`是什么呢？嗯，这个选择比较难。我们可以说它应该是一个空的（null）`std::shared_ptr<LinearImpl>`。然而，请记住，`Linear(3,"
" 4)`等同于`std::make_shared<LinearImpl>(3, 4)`。这意味着如果我们决定`Linear "
"linear;`应该是一个空指针，那么就无法构造一个不带任何构造参数的模块或将所有参数默认化的模块。因此，在当前API中，默认构造的模块持有者（例如`Linear()`）会调用底层模块的默认构造函数（即`LinearImpl()`）。如果底层模块没有默认构造函数，会出现编译器错误。要构造空的持有者，可以将`nullptr`传递给持有者的构造函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In practice, this means you can use submodules either like shown earlier, "
"where the module is registered and constructed in the *initializer list*:"
msgstr "实际上，这意味着你可以像之前展示的那样使用子模块，其中模块在*初始化列表*中注册并构造："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"or you can first construct the holder with a null pointer and then assign to"
" it in the constructor (more familiar for Pythonistas):"
msgstr "或者你可以首先使用空指针构造持有者，然后在构造函数中赋值（对Python用户来说更熟悉）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In conclusion: Which ownership model -- which semantics -- should you use? "
"The C++ frontend's API best supports the ownership model provided by module "
"holders. The only disadvantage of this mechanism is one extra line of "
"boilerplate below the module declaration. That said, the simplest model is "
"still the value semantics model shown in the introduction to C++ modules. "
"For small, simple scripts, you may get away with it too. But you'll find "
"sooner or later that, for technical reasons, it is not always supported. For"
" example, the serialization API (``torch::save`` and ``torch::load``) only "
"supports module holders (or plain ``shared_ptr``). As such, the module "
"holder API is the recommended way of defining modules with the C++ frontend,"
" and we will use this API in this tutorial henceforth."
msgstr ""
"结论：应该使用哪个所有权模型--"
"哪种语义？C++前端的API最支持模块持有者提供的所有权模型。这种机制的唯一缺点是模块声明下面多了一行额外样板代码。不过，最简单的模型仍然是介绍C++模块时提到的值语义模型。对于小型、简单的脚本，它也许可以满足需求。但你会发现迟早由于技术原因它并不总是被支持。例如，序列化API（`torch::save`和`torch::load`）仅支持模块持有者（或普通`shared_ptr`）。因此，模块持有者API是定义C++前端模块的推荐方式，我们将在本教程中继续使用该API。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining the DCGAN Modules"
msgstr "定义DCGAN模块"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We now have the necessary background and introduction to define the modules "
"for the machine learning task we want to solve in this post. To recap: our "
"task is to generate images of digits from the `MNIST dataset "
"<https://huggingface.co/datasets/ylecun/mnist>`_. We want to use a "
"`generative adversarial network (GAN) "
"<https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>`_ to "
"solve this task. In particular, we'll use a `DCGAN architecture "
"<https://arxiv.org/abs/1511.06434>`_ -- one of the first and simplest of its"
" kind, but entirely sufficient for this task."
msgstr ""
"现在我们已经有了定义机器学习任务模块所需的背景知识和介绍。回顾一下：我们的任务是从`MNIST数据集 "
"<https://huggingface.co/datasets/ylecun/mnist>`_生成数字图片。我们想用一个`生成对抗网络（GAN） "
"<https://papers.nips.cc/paper/5423-generative-adversarial-"
"nets.pdf>`_来完成这个任务。特别是，我们将使用一个`DCGAN架构 "
"<https://arxiv.org/abs/1511.06434>`_——这是第一种和最简单的一种，但对于该任务来说完全足够。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can find the full source code presented in this tutorial `in this "
"repository <https://github.com/pytorch/examples/tree/master/cpp/dcgan>`_."
msgstr ""
"本教程中展示的完整源码可以在这个仓库中找到：`https://github.com/pytorch/examples/tree/master/cpp/dcgan`。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "What was a GAN aGAN?"
msgstr "什么是GAN，GAN又是什么？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A GAN consists of two distinct neural network models: a *generator* and a "
"*discriminator*. The generator receives samples from a noise distribution, "
"and its aim is to transform each noise sample into an image that resembles "
"those of a target distribution -- in our case the MNIST dataset. The "
"discriminator in turn receives either *real* images from the MNIST dataset, "
"or *fake* images from the generator. It is asked to emit a probability "
"judging how real (closer to ``1``) or fake (closer to ``0``) a particular "
"image is. Feedback from the discriminator on how real the images produced by"
" the generator are is used to train the generator. Feedback on how good of "
"an eye for authenticity the discriminator has is used to optimize the "
"discriminator. In theory, a delicate balance between the generator and "
"discriminator makes them improve in tandem, leading to the generator "
"producing images indistinguishable from the target distribution, fooling the"
" discriminator's (by then) excellent eye into emitting a probability of "
"``0.5`` for both real and fake images. For us, the end result is a machine "
"that receives noise as input and generates realistic images of digits as its"
" output."
msgstr ""
"GAN由两个独立的神经网络模型组成：一个*生成器*和一个*判别器*。生成器接收来自噪声分布的样本，它的目标是将每个噪声样本转化为一个类似于目标分布图片的图像"
"--"
"在我们这里是MNIST数据集。判别器依次接收MNIST数据集中的*真实*图像或生成器的*伪造*图像。它需要输出一个概率来判断具体图像是多真实（接近`1`）还是多假（接近`0`）。生成器生成的图像多真实的信息由判别器提供的反馈用于训练生成器。判别器对图像真实性的判断能力如何的信息被用于优化判别器。理论上，生成器和判别器之间的微妙平衡使它们能够共同提高，从而生成器会生成和目标分布无法区分的图像，骗取判别器（到那时）优秀的判断能力，从而对真实和伪造图像都输出`0.5`的概率。最终结果是一个接收噪声作为输入并生成真实感数字图片作为输出的机器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The Generator Module"
msgstr "生成器模块"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We begin by defining the generator module, which consists of a series of "
"transposed 2D convolutions, batch normalizations and ReLU activation units. "
"We explicitly pass inputs (in a functional way) between modules in the "
"``forward()`` method of a module we define ourselves:"
msgstr ""
"我们从定义生成器模块开始，它由一系列转置的2D卷积、批量归一化和ReLU激活单元组成。在我们自己定义的模块的`forward()`方法中，我们以函数式方式显式地在模块之间传递输入："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can now invoke ``forward()`` on the ``DCGANGenerator`` to map a noise "
"sample to an image."
msgstr "现在我们可以在`DCGANGenerator`上调用`forward()`将噪声样本映射到图像。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The particular modules chosen, like ``nn::ConvTranspose2d`` and "
"``nn::BatchNorm2d``, follows the structure outlined earlier. The "
"``kNoiseSize`` constant determines the size of the input noise vector and is"
" set to ``100``. Hyperparameters were, of course, found via grad student "
"descent."
msgstr ""
"所选择的具体模块，如`nn::ConvTranspose2d`和`nn::BatchNorm2d`，遵循之前提到的结构。`kNoiseSize`常量确定输入噪声向量的大小，设置为`100`。超参数通过研究生“梯度下降法”找到。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"No grad students were harmed in the discovery of hyperparameters. They were "
"fed Soylent regularly."
msgstr "在发现这些超参数过程中，没有研究生受到伤害。他们定期食用Soylent。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A brief word on the way options are passed to built-in modules like "
"``Conv2d`` in the C++ frontend: Every module has some required options, like"
" the number of features for ``BatchNorm2d``. If you only need to configure "
"the required options, you can pass them directly to the module's "
"constructor, like ``BatchNorm2d(128)`` or ``Dropout(0.5)`` or ``Conv2d(8, 4,"
" 2)`` (for input channel count, output channel count, and kernel size). If, "
"however, you need to modify other options, which are normally defaulted, "
"such as ``bias`` for ``Conv2d``, you need to construct and pass an *options*"
" object. Every module in the C++ frontend has an associated options struct, "
"called ``ModuleOptions`` where ``Module`` is the name of the module, like "
"``LinearOptions`` for ``Linear``. This is what we do for the ``Conv2d`` "
"modules above."
msgstr ""
"关于如何将选项传递给C++前端中内置模块的简要说明：每个模块都有一些必须的选项，如`BatchNorm2d`的特征数。如果你只需要配置这些必须选项，可以直接传递给模块的构造函数，比如`BatchNorm2d(128)`或`Dropout(0.5)`或`Conv2d(8,"
" 4, "
"2)`（用于输入通道计数、输出通道计数和核大小）。然而，如果你需要修改其他通常默认的选项，例如`Conv2d`中的`bias`，则需要构造并传递一个*选项对象*。C++前端的每个模块都有一个相关的选项结构体，称为`ModuleOptions`，其中`Module`是模块名称，例如`Linear`的选项为`LinearOptions`。这就是我们在上述`Conv2d`模块中所做的。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The Discriminator Module"
msgstr "判别器模块"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The discriminator is similarly a sequence of convolutions, batch "
"normalizations and activations. However, the convolutions are now regular "
"ones instead of transposed, and we use a leaky ReLU with an alpha value of "
"0.2 instead of a vanilla ReLU. Also, the final activation becomes a Sigmoid,"
" which squashes values into a range between 0 and 1. We can then interpret "
"these squashed values as the probabilities the discriminator assigns to "
"images being real."
msgstr ""
"判别器同样是一系列卷积、批量归一化和激活构成的序列。然而，卷积现在是常规卷积而非转置卷积，我们使用alpha值为0.2的泄漏ReLU而不是普通ReLU。此外，最终激活变为Sigmoid，将值归一化到0到1的范围内。我们可以将这些归一化值解释为判别器对图像真实性的概率判断。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To build the discriminator, we will try something different: a `Sequential` "
"module. Like in Python, PyTorch here provides two APIs for model definition:"
" a functional one where inputs are passed through successive functions (e.g."
" the generator module example), and a more object-oriented one where we "
"build a `Sequential` module containing the entire model as submodules. Using"
" `Sequential`, the discriminator would look like:"
msgstr ""
"为了构建判别器，我们将尝试使用一种不同的方式：一个`Sequential`模块。与Python中的类似，PyTorch提供了两种模型定义API：一种是函数式的，输入通过连续的函数传递（例如生成器模块示例），另一种是更面向对象的方式，我们使用`Sequential`模块将整个模型作为子模块构建。使用`Sequential`，判别器将如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A ``Sequential`` module simply performs function composition. The output of "
"the first submodule becomes the input of the second, the output of the third"
" becomes the input of the fourth and so on."
msgstr ""
"一个`Sequential`模块简单地执行函数组合。第一个子模块的输出成为第二个子模块的输入，第三个子模块的输出成为第四个子模块的输入，依此类推。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Loading Data"
msgstr "加载数据"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have defined the generator and discriminator model, we need some"
" data we can train these models with. The C++ frontend, like the Python one,"
" comes with a powerful parallel data loader. This data loader can read "
"batches of data from a dataset (which you can define yourself) and provides "
"many configuration knobs."
msgstr ""
"现在我们已经定义了生成器和判别器模型，我们需要一些数据来训练这些模型。与Python前端一样，C++前端也提供了一个强大的并行数据加载器。这个数据加载器可以从数据集（可以自己定义）中读取批量数据，并提供许多配置选项。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"While the Python data loader uses multi-processing, the C++ data loader is "
"truly multi-threaded and does not launch any new processes."
msgstr "虽然Python数据加载器使用多进程，但C++数据加载器是真正的多线程实现，并不会启动任何新进程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The data loader is part of the C++ frontend's ``data`` api, contained in the"
" ``torch::data::`` namespace. This API consists of a few different "
"components:"
msgstr "数据加载器是C++前端``data`` API的一部分，包含在``torch::data::``命名空间中。此API由几个不同的组件组成："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The data loader class,"
msgstr "数据加载器类，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "An API for defining datasets,"
msgstr "定义数据集的API，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "An API for defining *transforms*, which can be applied to datasets,"
msgstr "定义*转换*的API，可以应用于数据集，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"An API for defining *samplers*, which produce the indices with which "
"datasets are indexed,"
msgstr "定义*采样器*的API，生成用于索引数据集的索引，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "A library of existing datasets, transforms and samplers."
msgstr "现有数据集、转换和采样器的库。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this tutorial, we can use the ``MNIST`` dataset that comes with the C++ "
"frontend. Let's instantiate a ``torch::data::datasets::MNIST`` for this, and"
" apply two transformations: First, we normalize the images so that they are "
"in the range of ``-1`` to ``+1`` (from an original range of ``0`` to ``1``)."
" Second, we apply the ``Stack`` *collation*, which takes a batch of tensors "
"and stacks them into a single tensor along the first dimension:"
msgstr ""
"在本教程中，我们可以使用C++前端附带的``MNIST``数据集。让我们实例化一个``torch::data::datasets::MNIST``，并应用两个转换：首先，我们将图像规范化，使其范围在``-1``到``+1``之间（原始范围为``0``到``1``）。其次，我们应用``Stack``*归并*，它将一批张量沿第一维度堆叠成一个张量："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that the MNIST dataset should be located in the ``./mnist`` directory "
"relative to wherever you execute the training binary from. You can use `this"
" script "
"<https://gist.github.com/goldsborough/6dd52a5e01ed73a642c1e772084bcd03>`_ to"
" download the MNIST dataset."
msgstr ""
"注意，MNIST数据集应该位于相对于您执行培训二进制文件的位置的``./mnist``目录中。您可以使用`此脚本 "
"<https://gist.github.com/goldsborough/6dd52a5e01ed73a642c1e772084bcd03>`_下载MNIST数据集。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we create a data loader and pass it this dataset. To make a new data "
"loader, we use ``torch::data::make_data_loader``, which returns a "
"``std::unique_ptr`` of the correct type (which depends on the type of the "
"dataset, the type of the sampler and some other implementation details):"
msgstr ""
"接下来，我们创建一个数据加载器并将此数据集传递给它。要创建一个新的数据加载器，我们使用``torch::data::make_data_loader``，它返回正确类型的``std::unique_ptr``（取决于数据集类型、采样器类型和其他一些实现细节）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The data loader does come with a lot of options. You can inspect the full "
"set `here "
"<https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/dataloader_options.h>`_."
" For example, to speed up the data loading, we can increase the number of "
"workers. The default number is zero, which means the main thread will be "
"used. If we set ``workers`` to ``2``, two threads will be spawned that load "
"data concurrently. We should also increase the batch size from its default "
"of ``1`` to something more reasonable, like ``64`` (the value of "
"``kBatchSize``). So let's create a ``DataLoaderOptions`` object and set the "
"appropriate properties:"
msgstr ""
"数据加载器确实有很多选项。您可以在`这里 "
"<https://github.com/pytorch/pytorch/blob/master/torch/csrc/api/include/torch/data/dataloader_options.h>`_查看完整设置。例如，为了加快数据加载，我们可以增加工作线程数量。默认值为零，这意味着将使用主线程。如果我们将``workers``设置为``2``，将生成两个线程并发加载数据。我们还应该将批量大小从默认值``1``增加到更合理的值，例如``64``（即``kBatchSize``）。因此，让我们创建一个``DataLoaderOptions``对象并设置适当的属性："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can now write a loop to load batches of data, which we'll only print to "
"the console for now:"
msgstr "现在我们可以写一个循环来加载数据批次，目前我们只将其打印到控制台："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The type returned by the data loader in this case is a "
"``torch::data::Example``. This type is a simple struct with a ``data`` field"
" for the data and a ``target`` field for the label. Because we applied the "
"``Stack`` collation earlier, the data loader returns only a single such "
"example. If we had not applied the collation, the data loader would yield "
"``std::vector<torch::data::Example<>>`` instead, with one element per "
"example in the batch."
msgstr ""
"在这种情况下，数据加载器返回的类型是``torch::data::Example``。此类型是一个简单的结构体，包含一个用于数据的``data``字段和一个用于标签的``target``字段。因为我们之前应用了``Stack``归并，所以数据加载器只返回一个这样的例子。如果我们未应用归并，数据加载器将生成``std::vector<torch::data::Example<>>``，其中每个批次中的每个示例都有一个元素。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "If you rebuild and run this code, you should see something like this:"
msgstr "如果你重建并运行此代码，你应该看到类似这样的内容："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Which means we are successfully able to load data from the MNIST dataset."
msgstr "这意味着我们成功地从MNIST数据集中加载了数据。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Writing the Training Loop"
msgstr "编写训练循环"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's now finish the algorithmic part of our example and implement the "
"delicate dance between the generator and discriminator. First, we'll create "
"two optimizers, one for the generator and one for the discriminator. The "
"optimizers we use implement the `Adam "
"<https://arxiv.org/pdf/1412.6980.pdf>`_ algorithm:"
msgstr ""
"现在让我们完成示例的算法部分并实现生成器与判别器之间的微妙舞蹈。首先，我们将创建两个优化器，一个用于生成器，一个用于判别器。我们使用的优化器实现了`Adam"
" <https://arxiv.org/pdf/1412.6980.pdf>`_算法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As of this writing, the C++ frontend provides optimizers implementing "
"Adagrad, Adam, LBFGS, RMSprop and SGD. The `docs "
"<https://pytorch.org/cppdocs/api/namespace_torch__optim.html>`_ have the up-"
"to-date list."
msgstr ""
"截至撰写本文时，C++前端提供了实现Adagrad、Adam、LBFGS、RMSprop和SGD的优化器。`文档 "
"<https://pytorch.org/cppdocs/api/namespace_torch__optim.html>`_包含最新列表。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we need to update our training loop. We'll add an outer loop to "
"exhaust the data loader every epoch and then write the GAN training code:"
msgstr "接下来，我们需要更新我们的训练循环。我们将在外部添加一个循环以每个epoch耗尽数据加载器，然后编写GAN训练代码："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Above, we first evaluate the discriminator on real images, for which it "
"should assign a high probability. For this, we use "
"``torch::empty(batch.data.size(0)).uniform_(0.8, 1.0)`` as the target "
"probabilities."
msgstr ""
"上面，我们首先对真实图像评估判别器，判别器应该分配一个高概率。为此，我们使用``torch::empty(batch.data.size(0)).uniform_(0.8,"
" 1.0)``作为目标概率。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We pick random values uniformly distributed between 0.8 and 1.0 instead of "
"1.0 everywhere in order to make the discriminator training more robust. This"
" trick is called *label smoothing*."
msgstr "我们随机选择介于0.8到1.0之间均匀分布的值，而不是使用始终为1.0的值，以使判别器训练更加稳健。这种技巧称为*标签平滑*。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Before evaluating the discriminator, we zero out the gradients of its "
"parameters. After computing the loss, we back-propagate it through the "
"network by calling ``d_loss.backward()`` to compute new gradients. We repeat"
" this spiel for the fake images. Instead of using images from the dataset, "
"we let the generator create fake images for this by feeding it a batch of "
"random noise. We then forward those fake images to the discriminator. This "
"time, we want the discriminator to emit low probabilities, ideally all "
"zeros. Once we have computed the discriminator loss for both the batch of "
"real and the batch of fake images, we can progress the discriminator's "
"optimizer by one step in order to update its parameters."
msgstr ""
"在评估判别器之前，先将其参数的梯度归零。计算损失后，我们通过调用``d_loss.backward()``对网络进行反向传播以计算新的梯度。我们对假图像重复此操作。我们不是使用数据集中的图像，而是让生成器通过输入一批随机噪声来创建假图像。然后，我们将这些假图像传递给判别器。这一次，我们希望判别器输出低概率，理想情况下全部为零。一旦我们为真实图像批次和假图像批次计算了判别器损失，我们可以通过判别器的优化器前进一步以更新其参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To train the generator, we again first zero its gradients, and then re-"
"evaluate the discriminator on the fake images. However, this time we want "
"the discriminator to assign probabilities very close to one, which would "
"indicate that the generator can produce images that fool the discriminator "
"into thinking they are actually real (from the dataset). For this, we fill "
"the ``fake_labels`` tensor with all ones. We finally step the generator's "
"optimizer to also update its parameters."
msgstr ""
"为了训练生成器，我们再次首先将其梯度归零，然后重新评估假图像上的判别器。然而，这一次我们希望判别器分配非常接近1的概率，这表明生成器可以生成让判别器认为它们实际上是真实的（来自数据集）的图像。为此，我们将``fake_labels``张量填充为全1。最后，我们对生成器的优化器也进行一步调整以更新其参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We should now be ready to train our model on the CPU. We don't have any code"
" yet to capture state or sample outputs, but we'll add this in just a "
"moment. For now, let's just observe that our model is doing *something* -- "
"we'll later verify based on the generated images whether this something is "
"meaningful. Re-building and running should print something like:"
msgstr ""
"我们现在应该可以在CPU上训练我们的模型了。我们还没有任何代码来捕获状态或采样输出，但我们稍后会添加它。目前，让我们仅观察我们的模型正在做某些事情——我们稍后会根据生成的图像验证这些事情是否有意义。重新构建并运行会打印类似如下的信息："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Moving to the GPU"
msgstr "迁移到GPU"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"While our current script can run just fine on the CPU, we all know "
"convolutions are a lot faster on GPU. Let's quickly discuss how we can move "
"our training onto the GPU. We'll need to do two things for this: pass a GPU "
"device specification to tensors we allocate ourselves, and explicitly copy "
"any other tensors onto the GPU via the ``to()`` method all tensors and "
"modules in the C++ frontend have. The simplest way to achieve both is to "
"create an instance of ``torch::Device`` at the top level of our training "
"script, and then pass that device to tensor factory functions like "
"``torch::zeros`` as well as the ``to()`` method. We can start by doing this "
"with a CPU device:"
msgstr ""
"尽管我们当前的脚本可以在CPU上正常运行，但我们都知道卷积在GPU上要快得多。让我们快速讨论如何将我们的训练迁移到GPU。我们需要做两件事：将GPU设备规范传递给我们自己分配的张量，并通过C++前端的所有张量和模块的``to()``方法显式地将任何其他张量复制到GPU。实现这两个目标的最简单方法是，在训练脚本的顶层创建一个``torch::Device``实例，然后将该设备传递给像``torch::zeros``这样的张量工厂函数以及``to()``方法。我们可以从使用CPU设备开始："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "New tensor allocations like"
msgstr "像这样的新张量分配"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "should be updated to take the ``device`` as the last argument:"
msgstr "应该更新以将``device``作为最后一个参数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For tensors whose creation is not in our hands, like those coming from the "
"MNIST dataset, we must insert explicit ``to()`` calls. This means"
msgstr "对于创建并不在我们控制范围内的张量（例如那些来自MNIST数据集的张量），我们必须插入显式``to()``调用。这意味着"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "becomes"
msgstr "变成"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "and also our model parameters should be moved to the correct device:"
msgstr "此外，我们的模型参数也应该移动到正确的设备："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If a tensor already lives on the device supplied to ``to()``, the call is a "
"no-op. No extra copy is made."
msgstr "如果张量已经驻留在传递给``to()``的设备上，则该调用是一个无操作。不会进行额外的复制。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"At this point, we've just made our previous CPU-residing code more explicit."
" However, it is now also very easy to change the device to a CUDA device:"
msgstr "此时，我们只是使之前驻留在CPU上的代码更加明确。但是，现在也可以轻松地将设备更改为CUDA设备："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"And now all tensors will live on the GPU, calling into fast CUDA kernels for"
" all operations, without us having to change any downstream code. If we "
"wanted to specify a particular device index, it could be passed as the "
"second argument to the ``Device`` constructor. If we wanted different "
"tensors to live on different devices, we could pass separate device "
"instances (for example one on CUDA device 0 and the other on CUDA device 1)."
" We can even do this configuration dynamically, which is often useful to "
"make our training scripts more portable:"
msgstr ""
"现在，所有张量都将在GPU上，调用快速CUDA内核执行所有操作，而我们无需更改任何下游代码。如果我们想指定一个特定的设备索引，可以将其作为第二个参数传递给``Device``构造函数。如果我们希望不同的张量驻留在不同的设备上，可以传递单独的设备实例（例如一个在CUDA设备0上，另一个在CUDA设备1上）。我们甚至可以动态地进行这种配置，这对于使我们的训练脚本更便携往往非常有用："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "or even"
msgstr "甚至"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Checkpointing and Recovering the Training State"
msgstr "检查点和恢复训练状态"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The last augmentation we should make to our training script is to "
"periodically save the state of our model parameters, the state of our "
"optimizers as well as a few generated image samples. If our computer were to"
" crash in the middle of the training procedure, the first two will allow us "
"to restore the training state. For long-lasting training sessions, this is "
"absolutely essential. Fortunately, the C++ frontend provides an API to "
"serialize and deserialize both model and optimizer state, as well as "
"individual tensors."
msgstr ""
"我们应该对训练脚本进行的最后一个增强是定期保存模型参数的状态、优化器状态以及一些生成的图像样本。如果我们的计算机在训练过程中崩溃，前两者将允许我们恢复训练状态。对于持续时间较长的训练过程，这绝对是必要的。幸运的是，C++前端提供了一个API来序列化和反序列化模型和优化器状态以及单个张量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The core API for this is ``torch::save(thing,filename)`` and "
"``torch::load(thing,filename)``, where ``thing`` could be a "
"``torch::nn::Module`` subclass or an optimizer instance like the ``Adam`` "
"object we have in our training script. Let's update our training loop to "
"checkpoint the model and optimizer state at a certain interval:"
msgstr ""
"核心API是``torch::save(thing,filename)``和``torch::load(thing,filename)``，其中``thing``可以是``torch::nn::Module``子类或像我们训练脚本中的``Adam``对象这样的优化器实例。让我们更新训练循环以在某个间隔检查点模型和优化器状态："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"where ``kCheckpointEvery`` is an integer set to something like ``100`` to "
"checkpoint every ``100`` batches, and ``checkpoint_counter`` is a counter "
"bumped every time we make a checkpoint."
msgstr ""
"其中``kCheckpointEvery``是一个整数，设置为类似``100``以每``100``个批次进行检查点，而``checkpoint_counter``是每次创建检查点时递增的计数器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To restore the training state, you can add lines like these after all models"
" and optimizers are created, but before the training loop:"
msgstr "要恢复训练状态，您可以在创建所有模型和优化器之后但在训练循环之前添加类似以下的行："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Inspecting Generated Images"
msgstr "查看生成的图像"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Our training script is now complete. We are ready to train our GAN, whether "
"on CPU or GPU. To inspect the intermediary output of our training procedure,"
" for which we added code to periodically save image samples to the "
"``\"dcgan-sample-xxx.pt\"`` file, we can write a tiny Python script to load "
"the tensors and display them with matplotlib:"
msgstr ""
"我们的训练脚本现在已完成。我们可以在CPU或GPU上训练我们的GAN。为了检查我们训练过程的中间输出，我们已添加代码以定期将图像样本保存到``\"dcgan-"
"sample-xxx.pt\"``文件中，我们可以编写一个小的Python脚本来加载这些张量并使用matplotlib显示它们："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let's now train our model for around 30 epochs:"
msgstr "现在让我们训练我们的模型约30个epoch："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "And display the images in a plot:"
msgstr "并在图中显示图像："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Which should look something like this:"
msgstr "应该看起来像这样："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "digits"
msgstr "数字"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Digits! Hooray! Now the ball is in your court: can you improve the model to "
"make the digits look even better?"
msgstr "数字！太棒了！现在机会在你手中：你能改进这个模型让数字看起来更好吗？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial has hopefully given you a digestible digest of the PyTorch C++"
" frontend. A machine learning library like PyTorch by necessity has a very "
"broad and extensive API. As such, there are many concepts we did not have "
"time or space to discuss here. However, I encourage you to try out the API, "
"and consult `our documentation <https://pytorch.org/cppdocs/>`_ and in "
"particular the `Library API "
"<https://pytorch.org/cppdocs/api/library_root.html>`_ section when you get "
"stuck. Also, remember that you can expect the C++ frontend to follow the "
"design and semantics of the Python frontend whenever we could make this "
"possible, so you can leverage this fact to increase your learning rate."
msgstr ""
"本教程希望能够向您介绍PyTorch "
"C++前端的基本概念。像PyTorch这样的机器学习库必然有一个非常广泛和全面的API。因此，有许多概念我们没有时间或空间在此处讨论。然而，我鼓励您尝试使用API，并在您遇到困难时查阅`我们的文档"
" <https://pytorch.org/cppdocs/>`_，特别是`库API "
"<https://pytorch.org/cppdocs/api/library_root.html>`_部分。另外，请记住，C++前端尽可能遵循Python前端的设计和语义，因此您可以利用这一事实来提高您的学习速度。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PyTorch Custom Operators"
msgstr "PyTorch自定义操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch offers a large library of operators that work on Tensors (e.g. "
"``torch.add``, ``torch.sum``, etc). However, you may wish to bring a new "
"custom operation to PyTorch and get it to work with subsystems like "
"``torch.compile``, autograd, and ``torch.vmap``. In order to do so, you must"
" register the custom operation with PyTorch via the Python `torch.library "
"docs <https://pytorch.org/docs/stable/library.html>`_ or C++ "
"``TORCH_LIBRARY`` APIs."
msgstr ""
"PyTorch 提供了一个丰富的运算符库，可以操作张量（例如 ``torch.add``、``torch.sum`` 等）。然而，您可能希望为 "
"PyTorch 引入一个新的自定义操作，并使其与 ``torch.compile``、autograd 和 ``torch.vmap`` "
"等系统协同工作。要实现这一点，您需要通过 Python 的 `torch.library 文档 "
"<https://pytorch.org/docs/stable/library.html>`_ 或 C++ 的 ``TORCH_LIBRARY`` "
"API 在 PyTorch 中注册自定义操作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Authoring a custom operator from Python"
msgstr "从 Python 编写自定义操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Please see :ref:`python-custom-ops-tutorial`."
msgstr "请参阅 :ref:`python-custom-ops-tutorial`。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You may wish to author a custom operator from Python (as opposed to C++) if:"
msgstr "如果有以下需求，您可能希望从 Python（而不是 C++）编写自定义操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"you have a Python function you want PyTorch to treat as an opaque callable, "
"especially with respect to ``torch.compile`` and ``torch.export``."
msgstr ""
"您有一个希望 PyTorch 视为不透明可调用对象的 Python 函数，特别是在涉及 ``torch.compile`` 和 "
"``torch.export`` 时。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"you have some Python bindings to C++/CUDA kernels and want those to compose "
"with PyTorch subsystems (like ``torch.compile`` or ``torch.autograd``)"
msgstr ""
"您有一些 C++/CUDA 内核的 Python 绑定，并希望这些绑定能与 PyTorch 的系统（如 ``torch.compile`` 或 "
"``torch.autograd``）协同工作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"you are using Python (and not a C++-only environment like AOTInductor)."
msgstr "您正在使用 Python（而不是 AOTInductor 这样的仅 C++ 环境）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Integrating custom C++ and/or CUDA code with PyTorch"
msgstr "将自定义 C++ 和/或 CUDA 代码与 PyTorch 集成"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Please see :ref:`cpp-custom-ops-tutorial`."
msgstr "请参阅 :ref:`cpp-custom-ops-tutorial`。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You may wish to author a custom operator from C++ (as opposed to Python) if:"
msgstr "如果有以下需求，您可能希望从 C++（而不是 Python）编写自定义操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "you have custom C++ and/or CUDA code."
msgstr "您有自定义的 C++ 和/或 CUDA 代码。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"you plan to use this code with ``AOTInductor`` to do Python-less inference."
msgstr "您计划使用此代码与 ``AOTInductor`` 一起进行无 Python 环境的推理。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The Custom Operators Manual"
msgstr "自定义操作手册"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For information not covered in the tutorials and this page, please see `The "
"Custom Operators Manual "
"<https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU>`_"
" (we're working on moving the information to our docs site). We recommend "
"that you first read one of the tutorials above and then use the Custom "
"Operators Manual as a reference; it is not meant to be read head to toe."
msgstr ""
"关于教程和此页面未涵盖的信息，请参阅 `自定义操作手册 "
"<https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU>`_（我们正在努力将信息迁移到我们的文档网站上）。我们建议您先阅读上述教程之一，然后将自定义操作手册用作参考；这不是一本从头到尾阅读的手册。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "When should I create a Custom Operator?"
msgstr "什么时候应该创建自定义操作？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If your operation is expressible as a composition of built-in PyTorch "
"operators then please write it as a Python function and call it instead of "
"creating a custom operator. Use the operator registration APIs to create a "
"custom operator if you are calling into some library that PyTorch doesn't "
"understand (e.g. custom C/C++ code, a custom CUDA kernel, or Python bindings"
" to C/C++/CUDA extensions)."
msgstr ""
"如果您的操作可以通过组合内置的 PyTorch 操作符表示，请将其写为 Python 函数并调用它，而不是创建自定义操作。如果您调用的是 PyTorch"
" 无法理解的某些库（例如，自定义的 C/C++ 代码、自定义 CUDA 内核或 Python 对 C/C++/CUDA 扩展的绑定），请使用操作符注册 "
"API 创建自定义操作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Why should I create a Custom Operator?"
msgstr "为什么应该创建自定义操作？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It is possible to use a C/C++/CUDA kernel by grabbing a Tensor's data "
"pointer and passing it to a pybind'ed kernel. However, this approach doesn't"
" compose with PyTorch subsystems like autograd, torch.compile, vmap, and "
"more. In order for an operation to compose with PyTorch subsystems, it must "
"be registered via the operator registration APIs."
msgstr ""
"可以通过获取张量的数据指针并将其传递给 `pybind` 的内核来使用 C/C++/CUDA 内核。然而，这种方法无法与 PyTorch 的子系统（如 "
"autograd、torch.compile、vmap 等）集成。为了让一个操作可以与 PyTorch 的子系统协同工作，必须通过操作符注册 API "
"注册这个操作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Training Transformer models using Distributed Data Parallel and Pipeline "
"Parallelism"
msgstr "使用分布式数据并行和流水线并行训练 Transformer 模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Redirecting to the latest parallelism APIs in 3 seconds..."
msgstr "3 秒后重定向到最新的并行 API..."

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Registering a Dispatched Operator in C++"
msgstr "在 C++ 中注册一个分派操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial is deprecated as of PyTorch 2.4. Please see :ref:`custom-ops-"
"landing-page` for the newest up-to-date guides on extending PyTorch with "
"Custom Operators."
msgstr ""
"自 PyTorch 2.4 起，本教程已被弃用。请参阅 :ref:`custom-ops-landing-page` 获取最新的扩展 PyTorch "
"的自定义操作指南。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The dispatcher is an internal component of PyTorch which is responsible for "
"figuring out what code should actually get run when you call a function like"
" ``torch::add``.  This can be nontrivial, because PyTorch operations need to"
" handle a lot of cross-cutting concerns that are \"layered\" on top of one "
"of another.  Here is a sampling of some of the things it handles:"
msgstr ""
"分派器是 PyTorch 的一个内部组件，用于在您调用像 ``torch::add`` 这样的函数时，决定实际应运行的代码。这可能并不简单，因为 "
"PyTorch 操作需要处理许多 \"叠加\" 在一起的交叉关注点。以下是它处理的一些内容的示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Switching between the CPU and CUDA implementations of an operator, depending"
" on the devices of the input tensors."
msgstr "根据输入张量的设备，在 CPU 和 CUDA 实现之间切换操作符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Switching between the autograd and backend implementations of an operator, "
"depending on whether or not autograd handling is necessary."
msgstr "根据是否需要自动微分处理，在自动微分和后端实现之间切换操作符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Applying autocasting when necessary for automatic mixed precision."
msgstr "在需要时应用自动混合精度的自动类型转换。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Applying batching rules when an operator is run under a ``vmap`` call."
msgstr "当操作符在 ``vmap`` 调用下运行时，应用批处理规则。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Tracing execution of operations, if you are tracing a model for export."
msgstr "如果您正在为导出而跟踪模型，则跟踪操作的执行。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If in your `custom operator code <torch_script_custom_ops>`_ you find "
"yourself manually writing if statements to handle these cases, the "
"dispatcher APIs can help organize your code.  (Conversely, if your custom "
"operator is very simple and is only for CPU inference, you probably don't "
"need to use the dispatcher, just use the basic API.)"
msgstr ""
"如果在 `自定义操作代码 <torch_script_custom_ops>`_ 中发现自己需要手动编写 if 语句来处理这些情况，那么分派器 API "
"可以帮助您组织代码。（相反，如果您的自定义操作非常简单，仅用于 CPU 推理，那么可能不需要使用分派器，仅使用基本 API 即可。）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we will describe how to structure a custom operator "
"registration to use the dispatcher to organize various components.  We'll "
"assume that you are familiar with how to `register an operator "
"<torch_script_custom_ops>`_ and how to write a `custom autograd function "
"<cpp_autograd>`_."
msgstr ""
"在本教程中，我们将描述如何构建一个自定义操作的注册，以使用分派器组织各种组件。我们假设您熟悉如何 `注册一个操作 "
"<torch_script_custom_ops>`_ 以及如何编写一个 `自定义自动微分函数 <cpp_autograd>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining schema and backend implementations"
msgstr "定义架构和后端实现"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The general principle behind the dispatcher is that it divides the "
"implementation of an operator into multiple kernels, each of which "
"implements functionality for a specific *dispatch key*, e.g. CPU, CUDA.  The"
" dispatcher determines what the highest priority dispatch key is at the time"
" you call an operator (this is done by looking at both the tensor arguments "
"as well as some thread local state), and transfers control to the kernel for"
" that dispatch key.  The end effect is that when you call an operator, we "
"first execute the Autograd kernel, and then we redispatch to the backend "
"kernel depending on the device types of the passed in tensors."
msgstr ""
"分派器的基本原则是将操作符的实现划分为多个内核，每个内核为特定的 "
"*分派键*（例如，CPU、CUDA）实现功能。分派器在您调用操作符时确定最高优先级分派键（这是通过查看张量参数以及一些线程本地状态完成的），然后将控制权转移给该分派键的内核。最终效果是，当您调用一个操作符时，我们首先执行自动微分内核，然后根据输入张量的设备类型重新分派到后端内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's take a look at the various parts involved in making this happen.  "
"First, we must define the schema for the operator in question. Unlike simple"
" pybind11-style operator registration, we don't actually provide an "
"implementation of our operator at this point; we just provide a schema "
"string specifying the type signature of the operator that all of our other "
"kernels will abide by:"
msgstr ""
"让我们来看一下实现这一目标所需的各个部分。首先，我们必须定义相关操作符的架构。与简单的 pybind11 "
"风格操作符注册不同，此时我们并未提供操作符的实现；我们只是提供了一个架构字符串，指定了操作符的类型签名，所有其他内核都将遵守该签名："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we need to actually provide some implementations of this operator. For"
" concreteness, here is a really simple implementation of addition on CPU:"
msgstr "接下来，我们需要实际提供一些操作符的实现。以下是 CPU 上一个非常简单的加法实现："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We'd like to register this function as an implementation of "
"``myops::myadd``. However, the simple way of registering it "
"(``def(\"myadd\", myadd_cpu)``) would register the kernel to run in all "
"cases, even if the tensor is not a CPU tensor!  (Internally, we refer to "
"these as \"catch-all\" kernels, since they catch all cases.)  To ensure that"
" ``myadd_cpu`` is only run for CPU tensors, we can use the "
"``TORCH_LIBRARY_IMPL`` macro:"
msgstr ""
"我们希望将此函数注册为 ``myops::myadd`` 的一个实现。然而，简单的注册方式（``def(\"myadd\", "
"myadd_cpu)``）会在所有情况下都注册该内核，即使张量不是一个 CPU 张量！（内部我们将这些称为 \"全捕捉\" "
"内核，因为它们捕捉了所有情况。）为了确保 ``myadd_cpu`` 仅对 CPU 张量运行，我们可以使用 ``TORCH_LIBRARY_IMPL``"
" 宏："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``TORCH_LIBRARY_IMPL`` lets us register implementations for operators on"
" a specific dispatch key (in this case, CPU).  Each call to ``impl`` "
"associates a CPU kernel with the corresponding operator (which we previously"
" defined in the ``TORCH_LIBRARY`` block).  If we also have a CUDA "
"implementation ``myadd_cuda``, we can register it in a separate "
"``TORCH_LIBRARY_IMPL`` block:"
msgstr ""
"``TORCH_LIBRARY_IMPL`` 可让我们为特定分派键（在此情况下为 CPU）注册操作符的实现。每次调用 ``impl`` 都将 CPU "
"内核与对应的操作符相关联（此前在 ``TORCH_LIBRARY`` 块中定义）。如果我们还有一个 CUDA 实现 "
"``myadd_cuda``，可以在一个单独的 ``TORCH_LIBRARY_IMPL`` 块中注册："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"These registrations can be split across files or even across library "
"boundaries; so for example, you could have these two ``TORCH_LIBRARY_IMPL`` "
"blocks compiled into a separate ``myops_cpu`` and ``myops_cuda`` dynamic "
"libraries.  Generally, speaking, the structure of your registrations will "
"look like this:"
msgstr ""
"这些注册可以分散在不同的文件甚至跨库边界；例如，您可以将这些 ``TORCH_LIBRARY_IMPL`` 块分别编译到单独的 "
"``myops_cpu`` 和 ``myops_cuda`` 动态库中。一般来说，注册的结构大致如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A single ``TORCH_LIBRARY`` that lists every custom operator in your "
"namespace in a centralized place."
msgstr "一个 `TORCH_LIBRARY`，在集中地列出命名空间中的每个自定义操作符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A ``TORCH_LIBRARY_IMPL`` per dispatch key that registers implementations for"
" that key (e.g., CPU or CUDA).  If you like, you can further subdivide "
"``TORCH_LIBRARY_IMPL`` blocks into a block per operator. This is convenient "
"if you have a separate file per operator implementation, but don't want to "
"expose the operators in a header; you can just put the registration in the "
"cpp file that defines your operator."
msgstr ""
"每个分派键一个 `TORCH_LIBRARY_IMPL`，为该键（例如 CPU 或 CUDA）注册实现。如果愿意，可以进一步将 "
"``TORCH_LIBRARY_IMPL`` "
"块按每个操作符拆分。这在您为每个操作符实现提供单独文件时非常方便，但不希望在头文件中公开操作符；可以将注册写入定义操作符的 cpp 文件中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Did you know that you can also write ``TORCH_LIBRARY_IMPL`` blocks for "
"existing core operators in PyTorch?  This is how XLA support for PyTorch is "
"implemented: the ``torch_xla`` library contains a ``TORCH_LIBRARY_IMPL`` "
"that provides implementations for all basic operators on the XLA dispatch "
"key."
msgstr ""
"您知道吗？您还可以为 PyTorch 中的现有核心操作符编写 ``TORCH_LIBRARY_IMPL`` 块？这就是 XLA 对 PyTorch "
"的支持实现方式：``torch_xla`` 库包含一个 ``TORCH_LIBRARY_IMPL``，为 XLA 分派键提供所有基本操作的实现。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "For operators that do not need autograd"
msgstr "无需自动微分的操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Note: This section only applies to versions of PyTorch ``>= 1.10``."
msgstr "注意：此部分仅适用于 PyTorch ``>= 1.10`` 的版本。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the next section, we will discuss how to add autograd support to an "
"operator. But for the ops that do not need autograd support, the following "
"kernel should be registered improve useability and make your op behave like "
"PyTorch's built-in operators."
msgstr ""
"在下一节中，我们将讨论如何为操作符添加自动微分支持。但对于无需支持自动微分的操作符，以下内核的注册可以提高可用性，并使您的操作符表现得像 PyTorch"
" 的内置操作符一样。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The above lines registers an ``Autograd`` kernel that appends a dummy "
"``NotImplemented`` node on forward (preserving the ``require_grad``-ness of "
"the inputs). On backward, the ``NotImplemented`` node raises an error. This "
"can be helpful for debugging in larger models where previously it can be "
"hard to pin-point exactly where the ``requires_grad``-ness is lost during "
"the forward pass."
msgstr ""
"以上代码为 ``Autograd`` 内核注册了一行代码，该内核在前向传播时添加了一个占位的 ``NotImplemented`` 节点（保留输入的 "
"``requires_grad`` 性质）。在反向传播时，该 ``NotImplemented`` "
"节点会引发错误。在较大的模型中调试时，这可能非常有用，因为此前可能难以准确定位前向传播过程中哪里失去了 ``requires_grad`` 性质。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "In-place or view ops"
msgstr "就地或视图操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To ensure correctness and best possible performance, if your op mutates an "
"input in-place or returns a tensor that aliases with one of the inputs, two "
"additional steps should be taken:"
msgstr "为了确保正确性和最佳性能，如果您的操作符会就地修改输入或返回与输入之一别名的张量，应采取以下两个附加步骤："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Register an ``ADInplaceOrView`` kernel in addition to the ``Autograd`` "
"kernel above. This kernel handles the necessary bookkeeping to ensure the "
"correctness of in-place or view operations. It is important to note that "
"this ADInplaceOrView kernel should only be used with "
"``autogradNotImplementedFallback``."
msgstr ""
"除了上述 ``Autograd`` 内核外，还需注册一个 ``ADInplaceOrView`` "
"内核。该内核处理必要的记录，以确保就地或视图操作的正确性。需要注意的是，``ADInplaceOrView`` 内核应仅与 "
"``autogradNotImplementedFallback`` 一起使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``Autograd`` or ``ADInplaceOrView`` boxed kernels registered above rely "
"on operator schema information in their logi. If your op mutates an input "
"in-place or returns a tensor that aliases with one of the inputs it is "
"important to ensure that your schema properly reflects this. See `here "
"<https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md>`_"
" for more information on how to annotate the schema."
msgstr ""
"上述注册的 ``Autograd`` 或 ``ADInplaceOrView`` "
"包装内核依赖于操作符架构信息。如果您的操作符会就地修改输入或返回与输入之一别名的张量，则必须确保您的架构正确地反映了这一点。有关如何注释架构的更多信息，请参见"
" `此处 "
"<https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Adding autograd support"
msgstr "添加自动微分支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"At this point, we have an operator with both CPU and CUDA implementations.  "
"How can we add autograd support to it?  As you might guess, we will register"
" an autograd kernel (similar to what's described in the `custom autograd "
"function <cpp_autograd>`_ tutorial)! However, there is a twist: unlike the "
"CPU and CUDA kernels, the autograd kernel needs to *redispatch*: it needs to"
" call back into the dispatcher to get to the inference kernels, e.g. CPU or "
"CUDA implementations."
msgstr ""
"到目前为止，我们已经有了一个具有 CPU 和 CUDA "
"实现的操作符。那么，如何为它添加自动微分支持呢？正如您可能猜测的那样，我们将注册一个自动微分内核（类似于 `自定义自动微分函数 "
"<cpp_autograd>`_ 教程中描述的内容）！然而，这里有一点不同：与 CPU 和 CUDA "
"内核不同，自动微分内核需要重新分派：它需要调用分派器来访问推理内核，例如 CPU 或 CUDA 实现。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Thus, before we write the autograd kernel, let's write a *dispatching "
"function* which calls into the dispatcher to find the right kernel for your "
"operator. This function constitutes the public C++ API for your operators--"
"in fact, all of the tensor functions in PyTorch's C++ API all call the "
"dispatcher in the same way under the hood.  Here's what the dispatching "
"function looks like:"
msgstr ""
"因此，在编写自动微分内核之前，让我们编写一个 *分派函数*，该函数调用分派器以找到您的操作符的正确内核。此函数构成了您的操作符的公共 C++ "
"API——事实上，PyTorch 的 C++ API 中的所有张量函数在内部下都会以相同的方式调用分派器。以下是分派函数的样子："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let's break it down:"
msgstr "让我们将其详细分析："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the first line, we look up a typed operator handle from the dispatcher "
"corresponding to the operator that we are going to dispatch to. "
"``findSchemaOrThrow`` takes two arguments: the (namespace qualified) name of"
" the operator, and the overload name of the operator (typically just the "
"empty string).  ``typed`` casts the dynamically typed handle into a "
"statically typed handle (doing a runtime test to make sure you've given the "
"correct C++ type), so that we can do a normal C++ call on it.  We pass it "
"``decltype(myadd)`` since the type of the dispatching function is the same "
"as the type of the underlying kernels registered to the dispatcher."
msgstr ""
"在第一行中，我们从分派器中查找与我们要分派的运算符对应的已键入的运算符句柄。“findSchemaOrThrow” "
"接受两个参数：运算符的（命名空间限定）名称，以及运算符的过载名称（通常只是空字符串）。“typed” "
"将动态键入的句柄转换为静态键入的句柄（进行运行时测试以确保您提供了正确的 C++ 类型），这样我们就可以对其进行正常的 C++ "
"调用。我们将“decltype(myadd)”传递给它，因为分派函数的类型与注册到分派器的底层内核类型相同。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For performance, this computation is done in a static variable, so that we "
"only need to do the (slow) lookup once.  If you typoed the name of the "
"operator you want to call, this lookup will error the first time you call "
"this function."
msgstr ""
"为了提高性能，此计算是在静态变量中完成的，因此我们只需要执行一次（较慢的）查找。如果您拼错了要调用的运算符名称，此查找将在您第一次调用此函数时出错。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the second line, we simply ``call`` the operator handle with all of the "
"arguments passed into the dispatching function.  This will actually invoke "
"the dispatcher and in the end control will be transferred to whatever kernel"
" is appropriate for this call."
msgstr "在第二行中，我们只需使用传递到分派函数中的所有参数来“call”运算符句柄。这实际上会调用分派器，最终控制将转移到此调用适用的内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"With the dispatch function in hand, we can now write the autograd kernel:"
msgstr "有了分派函数，我们现在可以编写自动梯度内核："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The autograd function is written as normal using "
"``torch::autograd::Function``, except that instead of directly writing the "
"implementation in ``forward()``, we:"
msgstr ""
"自动梯度函数是使用“torch::autograd::Function”正常编写的，只是我们不直接在“forward()”中编写实现，而是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Turn off autograd handling with the ``at::AutoNonVariableTypeMode`` RAII "
"guard, and then"
msgstr "使用“at::AutoNonVariableTypeMode” RAII守卫关闭自动梯度处理，然后"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Call the dispatch function ``myadd`` to call back into the dispatcher."
msgstr "调用分派函数“myadd”以调用分派器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Without (1), your calls will infinite loop (and stack overflow), because "
"``myadd`` will send you back to this function (as the highest priority "
"dispatch key would still be autograd.) With (1), autograd is excluded from "
"the set of dispatch keys under consideration, and we will go to the next "
"handlers, which will either be CPU and CUDA."
msgstr ""
"没有（1），调用会进入无限循环（并导致栈溢出），因为“myadd”会将调用发送回此函数（因为最高优先级的分派键仍然是自动梯度）。通过（1），自动梯度会从考虑的分派键集合中排除，然后我们将转到下一个处理程序（可能是CPU或CUDA）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can now register this function in the same way we registered the CPU/CUDA"
" functions:"
msgstr "我们现在可以用注册 CPU/CUDA 函数时相同的方式注册此函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this example we register the kernel to ``Autograd``, which installs it as"
" the autograd kernel for all backends. You can also register optimized "
"kernels for specific backends by using the corresponding backend-specific "
"dispatch key - for example, ``AutogradCPU`` or ``AutogradCUDA``. To explore "
"these and other dispatch key options in more detail, check out the "
"``PythonDispatcher`` tool provided in `torch/_python_dispatcher.py "
"<https://github.com/pytorch/pytorch/blob/master/torch/_python_dispatcher.py>`_."
msgstr ""
"在此示例中，我们将内核注册为“Autograd”，这会将其安装为所有后端的自动梯度内核。您还可以通过使用相应的后端特定分派键（例如，“AutogradCPU”或“AutogradCUDA”）注册特定后端的优化内核。想更详细了解这些和其他分派键选项，可以查看“torch/_python_dispatcher.py”中的“PythonDispatcher”工具。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Going beyond autograd"
msgstr "超越自动梯度"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In some sense, the dispatcher isn't doing all that much: all it does is "
"implement a glorified if-statement, along the lines of this:"
msgstr "从某种意义上说，分派器并没有做很多事情：它所做的只是实现一个类似这样的高级if语句："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "So why use the dispatcher?  There are a few reasons:"
msgstr "那么为什么要使用分派器？原因有几个："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It is decentralized.  You can assemble all of the pieces of an operator "
"(CPU, CUDA, Autograd) without having to write a single, centralized if "
"statement that refers to all of them.  Importantly, third parties can "
"register extra implementations for other aspects without having to patch the"
" original definition of an operator.  We'll talk more about extending the "
"dispatcher in `extending dispatcher for a new backend <extend_dispatcher>`_."
msgstr ""
"它是分散的。您可以将一个运算符的所有部分（CPU、CUDA、自动梯度）组合在一起，而无需编写单个集中式if语句来引用它们。重要的是，第三方可以为其他方面注册额外的实现，而无需修补运算符的原始定义。我们将在“为新后端扩展分派器”中进一步讨论。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It supports more dispatch keys than CPU, CUDA and Autograd.  You can see a "
"full list of dispatch keys that are currently implemented in PyTorch in "
"``c10/core/DispatchKey.h``.  These dispatch keys implement a variety of "
"optional functionality for operators, and if you decide you want your custom"
" operator to support this functionality, all you have to register a kernel "
"for the appropriate key."
msgstr ""
"它支持的分派键不仅限于CPU、CUDA和自动梯度。您可以在“c10/core/DispatchKey.h”中查看当前在PyTorch中实现的分派键的完整列表。这些分派键为运算符实现了多种可选功能，如果您希望自定义运算符支持这些功能，只需为相应的键注册一个内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The dispatcher implements support for boxed fallback functions, which are "
"functions that can be implemented once and apply to all operators in the "
"system.  Boxed fallbacks can be used to provide default behavior for a "
"dispatch key; if you use the dispatcher to implement your operator, you also"
" opt into the fallbacks for all of these operations."
msgstr ""
"分派器实现了对箱式后备函数的支持，这些函数可以一次实现并应用于系统中的所有运算符。箱式后备函数可用于为分派键提供默认行为；如果使用分派器实现运算符，也会将所有这些操作的后备选项纳入其中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here are some particular dispatch keys which you may need to define an "
"operator for."
msgstr "以下是一些可能需要为其定义运算符的特定分派键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Autocast"
msgstr "自动类型转换 (Autocast)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The Autocast dispatch key implements support for `automatic mixed precision "
"(AMP) <https://pytorch.org/docs/stable/amp.html>`_. An autocast wrapper "
"kernel typically casts incoming ``float16`` or ``float32`` CUDA tensors to "
"some preferred precision before running the op. For example, matmuls and "
"convolutions on floating-point CUDA tensors usually run faster and use less "
"memory in ``float16`` without impairing convergence. Autocast wrappers only "
"have an effect in `autocast-enabled contexts "
"<https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.autocast>`_."
msgstr ""
"自动类型转换 (Autocast) 分派键实现了对“自动混合精度 "
"(AMP)”的支持。自动类型转换包装内核通常会在运行操作前，将传入的“float16”或“float32”CUDA张量转换为某种首选精度。例如，浮点CUDA张量上的矩阵乘法和卷积通常在“float16”中运行得更快且占用更少的内存，而不影响收敛性。只有在启用了自动类型转换的上下文中，自动类型转换包装内核才会生效。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here's an autocast wrapper for a hypothetical custom matmul, along with its "
"registration:"
msgstr "以下是一个假设的自定义矩阵乘法的自动类型转换包装内核及其注册："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``cached_cast(kHalf, tensor)`` casts ``tensor`` to ``float16`` if ``tensor``"
" is CUDA and ``float32``, otherwise, it leaves ``tensor`` unchanged (c.f. "
"the `eligibility policy <https://pytorch.org/docs/stable/amp.html#op-"
"eligibility>`_ for natively autocasted ops). This ensures if the network "
"calls ``mymatmul`` on any mixture of ``float16`` and ``float32`` CUDA "
"tensors, ``mymatmul`` runs in ``float16``.  Meanwhile, calls to ``mymatmul``"
" with non-CUDA, integer-type, or ``float64`` inputs are unaffected.  Using "
"``cached_cast`` to follow the native eligibility policy in your own autocast"
" wrapper is recommended, but not required.  For example, if you wanted to "
"force ``float16`` execution for all input types, you could ``return "
"mymatmul(self.half(), other.half());`` instead of using ``cached_cast``."
msgstr ""
"“cached_cast(kHalf, "
"tensor)”会在“tensor”为CUDA且“float32”时，将其转换为“float16”；否则保持“tensor”不变。这确保了如果网络调用了“mymatmul”且任何张量为“float16”和“float32”的CUDA张量的混合，"
" "
"“mymatmul”会在“float16”中运行。同时，使用非CUDA、整数类型或“float64”输入调用“mymatmul”的行为不受影响。推荐在您自己的自动类型转换包装内核中使用“cached_cast”以遵循本机自动类型转换操作的合规策略，但并非强制。例如，如果想强制对所有输入类型执行“float16”运算，可以直接“return"
" mymatmul(self.half(), other.half());”而不是使用“cached_cast”。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Notice that, like our autograd kernels, we exclude the ``Autocast`` key from"
" dispatch before redispatching."
msgstr "需要注意的是，与自动梯度内核类似，我们在重新分派之前将“Autocast”键从分派中排除。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"By default, if no autocast wrapper is provided, we fallthrough directly to "
"the regular operator implementation (no autocasting occurs).  (We didn't use"
" ``myadd`` for this example, since pointwise addition doesn't need "
"autocasting and should just fall through.)"
msgstr ""
"默认情况下，如果没有提供自动类型转换包装内核，我们会直接进入常规运算符实现（不会发生自动类型转换）。（我们没有在这个示例中使用“myadd”，因为点运算加法不需要自动类型转换，直接可以省略。）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"When should an autocast wrapper be registered? Unfortunately, there aren't "
"cut-and-dried rules for an op's preferred precision.  You can get a sense "
"for some native ops' preferred precisions by looking at the `cast lists "
"<https://pytorch.org/docs/master/amp.html#op-specific-behavior>`_. General "
"guidance:"
msgstr ""
"什么时候应该注册自动类型转换包装？遗憾的是，没有明确的规则来定义操作的优选精度。可以通过查看本机操作的“转换列表”来了解某些操作的优选精度。一般指导原则："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Ops that do reductions should probably execute in ``float32``,"
msgstr "执行归约的操作可能应该以“float32”运行，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Any op that does a convolution or gemm under the hood should probably "
"execute in ``float16``, and"
msgstr "任何底层执行卷积或通用矩阵乘法（gemm）的操作可能应该以“float16”运行，以及"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Other ops with multiple floating-point tensor inputs should standardize them"
" to a common precision (unless the implementation supports inputs with "
"different precisions)."
msgstr "具有多个浮点张量输入的其他操作应将它们标准化为通用精度（除非实现支持不同精度的输入）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If your custom op falls into the third category, the ``promote_type`` "
"template helps figure out the widest floating-point type present among input"
" tensors, which is the safest choice for the execution type:"
msgstr "如果自定义操作属于第三种情况，“promote_type”模板有助于找出输入张量中存在的最宽浮点类型，这也是执行类型的最安全选择："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If your custom op is :ref:`autograd-enabled<autograd-support>`, you only "
"need to write and register an autocast wrapper for the same name onto which "
"the autograd wrapper is registered. For example, if you wanted an autocast "
"wrapper for the ``myadd`` function shown in the autograd section, all you'd "
"need is"
msgstr ""
"如果自定义运算符是支持自动梯度的，您只需要为自动梯度包装注册的同一名称编写并注册一个自动类型转换包装。例如，如果您想为自动梯度部分显示的“myadd”函数编写一个自动类型转换包装，所需的仅仅是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There are no separate gymnastics to make the backward method autocast "
"compatible. However, the backward method defined in your custom autograd "
"function will run in the same dtype as autocast sets for the forward method,"
" so you should choose a ``<desired dtype>`` suitable for both your forward "
"and backward methods."
msgstr ""
"没有单独的额外操作使向后方法兼容自动类型转换。但是，在自定义自动梯度函数中定义的向后方法将在与自动类型转换设置的前向方法相同的dtype下运行，因此您应该为自己的前向和后向方法选择合适的``<desired"
" dtype>``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Batched"
msgstr "批处理模式 (Batched)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Batched tensors allow you to write your code in a per-example manner, and "
"then have them be automatically batched when run under a ``vmap`` "
"invocation.  The API for writing batching rules is currently under "
"development, but once it is stabilized, you can add support for ``vmap`` for"
" your operators by registering a kernel at the Batched dispatch key."
msgstr ""
"批处理张量允许您以每示例方式编写代码，然后在“vmap”调用下运行时自动进行批处理。目前编写批处理规则的API正在开发中，但一旦稳定下来，您可以通过在“Batched”分派键处注册一个内核来为操作添加“vmap”支持。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Tracer"
msgstr "跟踪 (Tracer)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The Tracer dispatch key implements support for recording invocations of "
"operators into a trace when you run ``torch.jit.trace``.  We intend to "
"provide a boxed fallback that will implement tracing for arbitrary "
"operations, see `issue #41478 "
"<https://github.com/pytorch/pytorch/issues/41478>`_ to track progress."
msgstr ""
"跟踪分派键实现了支持在运行“torch.jit.trace”时将运算符调用记录到跟踪中。我们计划提供一个箱式后备以实现对任意操作的跟踪，请参阅Issue"
" #41478以跟踪进展。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here "
"<sphx_glr_download_advanced_dynamic_quantization_tutorial.py>` to download "
"the full example code"
msgstr "点击这里下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "(beta) Dynamic Quantization on an LSTM Word Language Model"
msgstr "（测试版）基于LSTM的动态量化在单词语言模型上的应用"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `James Reed <https://github.com/jamesr66a>`_"
msgstr "**作者**：James Reed"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Edited by**: `Seth Weidman <https://github.com/SethHWeidman/>`_"
msgstr "**编辑**：Seth Weidman"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Introduction"
msgstr "简介"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Quantization involves converting the weights and activations of your model "
"from float to int, which can result in smaller model size and faster "
"inference with only a small hit to accuracy."
msgstr "量化涉及将模型的权重和激活从浮点转换为整数，这可以使模型体积减小，并且推理速度变快，同时对精度的影响很小。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we will apply the easiest form of quantization - `dynamic "
"quantization "
"<https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_"
" - to an LSTM-based next word-prediction model, closely following the `word "
"language model "
"<https://github.com/pytorch/examples/tree/master/word_language_model>`_ from"
" the PyTorch examples."
msgstr "在本教程中，我们将动态量化这一最简单的量化形式应用于基于LSTM的词预测模型，紧随来自PyTorch示例的单词语言模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1. Define the model"
msgstr "1. 定义模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here we define the LSTM model architecture, following the `model "
"<https://github.com/pytorch/examples/blob/master/word_language_model/model.py>`_"
" from the word language model example."
msgstr "在这里，参考单词语言模型示例定义LSTM模型架构。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "2. Load in the text data"
msgstr "2. 加载文本数据"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we load the `Wikitext-2 dataset "
"<https://www.google.com/search?q=wikitext+2+data>`_ into a `Corpus`, again "
"following the `preprocessing "
"<https://github.com/pytorch/examples/blob/master/word_language_model/data.py>`_"
" from the word language model example."
msgstr "接下来，我们将Wikitext-2数据集加载到Corpus中，再次参照单词语言模型示例的预处理。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "3. Load the pretrained model"
msgstr "3. 加载预训练模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This is a tutorial on dynamic quantization, a quantization technique that is"
" applied after a model has been trained. Therefore, we'll simply load some "
"pretrained weights into this model architecture; these weights were obtained"
" by training for five epochs using the default settings in the word language"
" model example."
msgstr ""
"这是关于动态量化的教程，这是一种在模型训练后应用的量化技术。因此，我们简单地加载一些预训练权重到这个模型架构；这些权重是通过在单词语言模型示例中使用默认设置训练五个周期获得的。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Before running this tutorial, download the required pre-trained model:"
msgstr "运行此教程之前，下载所需的预训练模型："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Place the downloaded file in the data directory or update the "
"model_data_filepath accordingly."
msgstr "将下载的文件放在数据目录中，或者相应地更新model_data_filepath。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's generate some text to ensure that the pretrained model is working "
"properly - similarly to before, we follow `here "
"<https://github.com/pytorch/examples/blob/master/word_language_model/generate.py>`_"
msgstr "我们现在生成一些文本以确保预训练模型正常工作——同样参照此代码。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It's no GPT-2, but it looks like the model has started to learn the "
"structure of language!"
msgstr "这并不是GPT-2，但看起来模型已经开始学习语言的结构！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We're almost ready to demonstrate dynamic quantization. We just need to "
"define a few more helper functions:"
msgstr "我们差不多就绪可以演示动态量化了。我们只需要定义一些辅助函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "4. Test dynamic quantization"
msgstr "4. 测试动态量化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, we can call ``torch.quantization.quantize_dynamic`` on the model! "
"Specifically,"
msgstr "最后，我们对模型调用“torch.quantization.quantize_dynamic”！具体："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We specify that we want the ``nn.LSTM`` and ``nn.Linear`` modules in our "
"model to be quantized"
msgstr "我们指定希望模型中的“nn.LSTM”和“nn.Linear”模块进行量化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We specify that we want weights to be converted to ``int8`` values"
msgstr "我们指定希望权重转换为“int8”值"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The model looks the same; how has this benefited us? First, we see a "
"significant reduction in model size:"
msgstr "模型看起来与以前一样；这是如何使我们受益的呢？首先，我们看到模型体积大幅减少："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Second, we see faster inference time, with no difference in evaluation loss:"
msgstr "其次，我们看到推理时间更快，并且评估损失没有变化："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note: we set the number of threads to one for single threaded comparison, "
"since quantized models run single threaded."
msgstr "注意：我们将线程数设置为1，用于单线程比较，因为量化模型在单线程下运行。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Running this locally on a MacBook Pro, without quantization, inference takes"
" about 200 seconds, and with quantization it takes just about 100 seconds."
msgstr "在本地运行（使用 MacBook Pro，无量化），推理时间大约是200秒，而使用量化后则仅需约100秒。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Dynamic quantization can be an easy way to reduce model size while only "
"having a limited effect on accuracy."
msgstr "动态量化可以是减少模型大小的一种简单方法，同时对准确性的影响有限。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Thanks for reading! As always, we welcome any feedback, so please create an "
"issue `here <https://github.com/pytorch/pytorch/issues>`_ if you have any."
msgstr ""
"感谢阅读！如往常一样，我们欢迎任何反馈，因此如果您有任何问题，请在`此处 "
"<https://github.com/pytorch/pytorch/issues>`_创建问题。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: dynamic_quantization_tutorial.py "
"<dynamic_quantization_tutorial.py>`"
msgstr ""
":download:`下载 Python 源代码: dynamic_quantization_tutorial.py "
"<dynamic_quantization_tutorial.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: dynamic_quantization_tutorial.ipynb "
"<dynamic_quantization_tutorial.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: dynamic_quantization_tutorial.ipynb "
"<dynamic_quantization_tutorial.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Extending dispatcher for a new backend in C++"
msgstr "在 C++ 中为新后端扩展调度器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial we will walk through all necessary steps to extend the "
"dispatcher to add a new device living outside ``pytorch/pytorch`` repo and "
"maintain it to keep in sync with native PyTorch devices.  Here we'll assume "
"that you're familiar with how to `register a dispatched operator in C++ "
"<dispatcher>`_ and how to write a `custom autograd function "
"<cpp_autograd>`_."
msgstr ""
"在本教程中，我们将逐步介绍扩展调度器的所有必要步骤，以添加一个新的设备（生活在``pytorch/pytorch``库之外），并维护它以与原生 "
"PyTorch 设备保持同步。在这里我们假设你已经熟悉如何`在 C++ 中注册调度操作符 <dispatcher>`_以及如何编写`自定义自动求导函数 "
"<cpp_autograd>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial touches a lot of internal components inside PyTorch which are "
"being actively improved, please expect changes to APIs if you decide to "
"follow this tutorial.  We'll keep this tutorial up to date with the latest "
"APIs."
msgstr ""
"本教程涵盖了 PyTorch 内部许多正在积极改进的组件，如果您决定遵循本教程，请预期 API 会有所变化。我们将根据最新的 API 更新本教程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "What's a new backend?"
msgstr "什么是新后端？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Adding a new backend to PyTorch requires a lot of development and "
"maintenance from backend extenders. Before adding a new backend, let's first"
" consider a few common use cases and recommended solutions for them:"
msgstr "向 PyTorch 添加新后端需要后端扩展者进行大量开发和维护。在添加新后端之前，我们先考虑一些常见用例及推荐的解决方案："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you have new algorithms for an existing PyTorch operator, send a PR to "
"PyTorch."
msgstr "如果您对现有 PyTorch 操作符有新的算法，请向 PyTorch 提交 PR。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you want to propose a new operator, send a feature request/PR to PyTorch."
msgstr "如果您想提出一个新的操作符，请向 PyTorch 提交功能请求/PR。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you want to add support for a new device/hardware like Google TPU and "
"customized chips, which often requires using hardware-specific API to write "
"kernels, follow this tutorial and add a out-of-tree backend to PyTorch."
msgstr ""
"如果您想为新设备/硬件添加支持，例如 Google TPU 和定制芯片，这通常需要使用硬件特定的 API 来编写内核，请遵循此教程并为 PyTorch "
"添加一个树外后端。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you want to add support for existing operators but with a different "
"Tensor layout/representation like sparse and quantized, which enforces your "
"kernels to be written in a way that's more efficient given the "
"layout/representation limitation, follow this tutorial and add a out-of-tree"
" backend to PyTorch."
msgstr ""
"如果您想为现有操作符添加支持，但具有不同的张量布局/表示形式，例如稀疏和量化，这会要求您以一种给定布局/表示形式限制下更高效的方式编写内核，请遵循此教程并为"
" PyTorch 添加一个树外后端。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial we'll mainly focus on adding a new out-of-tree device "
"below.  Adding out-of-tree support for a different tensor layout might share"
" many common steps with devices, but we haven't seen an example of such "
"integrations yet so it might require additional work from PyTorch to support"
" it."
msgstr ""
"在本教程中我们主要聚焦于添加一个新的树外设备。针对不同张量布局添加树外支持可能与设备共享许多共同步骤，但我们尚未看到此类集成的例子，因此可能需要 "
"PyTorch 进行额外的工作来支持它。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Get a dispatch key for your backend"
msgstr "为您的后端获取调度键"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch operators are implemented in C++ and made available in Python "
"frontend through Python bindings. The PyTorch dispatcher divides the "
"implementation of an operator into multiple kernels, each of which is "
"associated with a specific dispatch key.  Supporting a new backend in "
"PyTorch essentially means writing a kernel for each PyTorch operator in C++ "
"and then registering them to a dispatch key representing your customized "
"backend in the dispatcher."
msgstr ""
"PyTorch 的操作符是在 C++ 中实现的，并通过 Python 绑定在前端提供。PyTorch "
"调度器将操作符的实现分成多个内核，每个内核与一个特定的调度键相关联。在 PyTorch 中支持新后端实际上意味着为每个 PyTorch 操作符在 C++"
" 中编写内核，然后将它们注册到调度器中代表自定义后端的调度键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Dispatch key is your identifier in the dispatcher system. The dispatcher "
"looks at the dispatch keys carried on input tensors and calls the right "
"kernel accordingly.  PyTorch provides three reserved dispatch keys (and "
"their corresponding Autograd keys) for prototyping out-of-tree backend "
"extensions:"
msgstr ""
"调度键是您在调度器系统中的标识符。调度器会查看输入张量所携带的调度键，并调用相应的内核。PyTorch "
"提供了三个预留的调度键（及其对应的自动求导键），用于树外后端扩展的原型设计："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PrivateUse1/AutogradPrivateUse1"
msgstr "PrivateUse1/AutogradPrivateUse1"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PrivateUse2/AutogradPrivateUse2"
msgstr "PrivateUse2/AutogradPrivateUse2"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PrivateUse3/AutogradPrivateUse3"
msgstr "PrivateUse3/AutogradPrivateUse3"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can choose any of keys above to prototype your customized backend. To "
"create a Tensor on ``PrivateUse1`` backend, you need to set dispatch key in "
"``TensorImpl`` constructor."
msgstr ""
"您可以选择上述任意键来设计您的自定义后端原型。要在``PrivateUse1``后端上创建一个张量，您需要在``TensorImpl``构造函数中设置调度键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that ``TensorImpl`` class above assumes your Tensor is backed by a "
"storage like CPU/CUDA. We also provide ``OpaqueTensorImpl`` for backends "
"without a storage. And you might need to tweak/override certain methods to "
"fit your customized hardware. One example in pytorch repo is `Vulkan "
"TensorImpl "
"<https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h>`_."
msgstr ""
"请注意，上述的``TensorImpl``类假设您的张量以存储（例如 "
"CPU/CUDA）为基础。我们还提供``OpaqueTensorImpl``用于没有存储的后端。您可能需要调整/覆盖某些方法以适配您的自定义硬件。PyTorch"
" 仓库中的一个例子是`Vulkan TensorImpl "
"<https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/vulkan/VulkanOpaqueTensorImpl.h>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once the prototype is done and you plan to do regular releases for your "
"backend extension,  please feel free to submit a PR to ``pytorch/pytorch`` "
"to reserve a dedicated dispatch key for your backend."
msgstr "一旦原型完成并计划为您的后端扩展进行定期发布，请随时提交 PR 至``pytorch/pytorch``以为您的后端保留一个专用的调度键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Get the full list of PyTorch operators"
msgstr "获取 PyTorch 操作符的完整列表"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch provides a full list of extensible C++ operators in generated file "
"``build/aten/src/ATen/RegistrationDeclarations.h``. This file is only "
"available after building PyTorch from source. Here's a snippet of the file:"
msgstr ""
"PyTorch 在生成的文件``build/aten/src/ATen/RegistrationDeclarations.h``中提供了可扩展的 C++"
" 操作符的完整列表。该文件仅在从源码构建 PyTorch 后可用。以下是该文件的一个片段："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There're multiple fields associated with a single operator. Let's break it "
"down using ``abs_out`` as an example:"
msgstr "一个操作符有多个相关字段。以下使用``abs_out``作为示例来拆解："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``Tensor & abs_out(Tensor & out, const Tensor & self);`` is the C++ "
"signature of the operator, your C++ kernel should match this signature "
"exactly."
msgstr ""
"``Tensor & abs_out(Tensor & out, const Tensor & self);``是该操作符的 C++ 签名，您的 C++"
" 内核必须与该签名完全匹配。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)`` is the "
"unique schema representing the operator, which also contains aliasing and "
"mutation annotations compared to the C++ signature.  This is the unique "
"identifier the dispatcher uses to find an operator."
msgstr ""
"``aten::abs.out(Tensor self, *, Tensor(a!) out) -> "
"Tensor(a!)``是表示该操作符的唯一架构，除了 C++ 签名之外，它还包含别名和变异注释。这是调度器用于查找操作符的唯一标识符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``dispatch`` and ``default`` are boolean fields that provide information "
"about what native PyTorch kernels can do, thus implies whether it's required"
" for backend extenders to implement the kernel. More details can be found in"
" :ref:`register kernels for the new backend<register-kernel>`."
msgstr ""
"``dispatch``和``default``是布尔字段，它们提供有关原生 PyTorch "
"内核能力的信息，因此暗示是否需要后端扩展者实现该内核。更多细节请参见:ref:`为新后端注册内核<register-kernel>`。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register kernels for the new backend"
msgstr "为新后端注册内核"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To register your kernels to PyTorch dispatcher, you can use the "
"``TORCH_LIBRARY_IMPL`` API described in `Registering a Dispatched Operator "
"in C++ <dispatcher>`_:"
msgstr ""
"要将您的内核注册到 PyTorch 调度器，您可以使用``TORCH_LIBRARY_IMPL`` API，该 API在`注册一个调度操作符 "
"<dispatcher>`_中有所描述："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's zoom in and what operator requires a kernel from a customized "
"backend and what's inside the kernels exactly."
msgstr "现在让我们深入研究哪个操作符需要自定义后端的内核以及内核内部究竟包含什么。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch currently has more than 1600 operators and it’s still growing.  It’s"
" unrealistic for backend extensions to keep up with this speed.  Even for "
"native backends like CPU or CUDA, it often requires a lot of work to write "
"dedicated kernels for every new op."
msgstr ""
"PyTorch 目前有超过1600个操作符，数量还在增长。让后端扩展者跟上这样的速度是不现实的。即使是像 CPU 或 CUDA "
"这样的原生后端，也需要大量工作来为每个新操作符编写专用内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Fortunately, some native PyTorch kernels are written in a way that they "
"decompose to combination of several known operators. In other words, you "
"only need to implement a set of known operators (ops that require "
"registration below) instead of all PyTorch operators."
msgstr ""
"幸运的是，某些原生 PyTorch "
"内核被编写为可以分解为几个已知操作符的组合。换句话说，您只需要实现一组已知操作符（下方列出的需要注册的操作符），而不是实现所有 PyTorch 操作符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PyTorch operators can be classified into two categories:"
msgstr "PyTorch 操作符可以分为两类："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Ops that require registration: PyTorch native implementation for these ops "
"is backend specific and thus it’s required to provide a kernel for "
"customized backend.  Otherwise calling such op on the customized backend "
"will error out."
msgstr ""
"需要注册的操作符：这些操作符的 PyTorch 原生实现是后端特定的，因此需要为自定义后端提供内核。否则，在自定义后端上调用这样的操作符会出现错误。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In ``RegistrationDeclarations.h`` these operators have ``dispatch`` set to "
"True *and* ``default`` set to False in the metadata found in their "
"accompanying comments."
msgstr ""
"在``RegistrationDeclarations.h``中，这些操作符在其伴随的注释中具有``dispatch``设置为True且``default``设置为False。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Registration is optional: backend extenders can skip registering to these "
"ops without sacrificing any support. However, if a backend extender wants to"
" override the default kernel provided by PyTorch, they can still register "
"their customized kernel to their backend and the dispatcher will use it for "
"your backend only. For example, current implementation of PyTorch's "
"``max_pool2d`` returns ``indices`` as part of forward outputs which creates "
"overhead in torch_xla, so torch_xla registers its own kernel for "
"``max_pool2d`` instead."
msgstr ""
"可选注册：后端扩展者可以跳过注册这些操作符而不会牺牲任何支持。然而，如果后端扩展者想要覆盖 PyTorch "
"提供的默认内核，他们仍然可以将自定义内核注册到他们的后端，调度器将只在您的后端上使用它。例如，当前 PyTorch "
"的``max_pool2d``实现返回``indices``作为前向输出的一部分，这在torch_xla 中会创建开销，因此 torch_xla "
"为``max_pool2d``注册了自己的内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In ``RegistrationDeclarations.h`` these operators have ``dispatch`` set to "
"False *or* ``default`` set to True in the metadata found in their "
"accompanying comments."
msgstr ""
"在``RegistrationDeclarations.h``中，这些操作符在其伴随的注释中具有``dispatch``设置为False或``default``设置为True。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Autograd support for the new backend"
msgstr "为新后端提供自动求导支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Gradient formulas are mostly purely mathematical and thus are general for "
"all backends. PyTorch often registers a kernel to alias dispatch key "
"Autograd, which means it can be used by all backends."
msgstr ""
"梯度公式主要是纯数学的，因此对所有后端都是通用的。PyTorch 通常注册一个内核到别名调度键Autograd，这意味着它可以供所有后端使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For these operators you don't have to worry about their derivative formulas,"
" you can just write forward definitions for operators in "
"``RegistrationDeclarations.h`` and PyTorch handles backward for you "
"automatically."
msgstr ""
"对于这些操作符，您无需担心它们的导数公式，只需在``RegistrationDeclarations.h``中编写操作符的正向定义，PyTorch "
"会自动为您处理反向计算。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In some cases, PyTorch backward kernel implementations are also device "
"specific so that they can squeeze out max performance out of each backend. "
"For those operators you’ll see op_backward showing up in "
"``RegistrationDeclarations.h`` as *required registration* as well."
msgstr ""
"在某些情况下，PyTorch "
"的反向内核实现也会是设备特定的，因此它可以为每个后端发挥最大性能。对于那些操作符，您会看到``RegistrationDeclarations.h``中的op_backward显示为*需要注册*。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In a few *rare* cases, PyTorch’s gradient formula for certain operators may "
"have assumptions that don’t generalize for all backends. In those cases "
"backend extenders can optionally override PyTorch Autograd layer by "
"registering a kernel from torch::autograd::Function to the corresponding "
"dispatch key (for example, AutogradPrivateUse1 if you're using PrivateUse1 "
"for your backend):"
msgstr ""
"在少数*罕见*情况下，PyTorch "
"某些操作符的梯度公式可能有不适用于所有后端的假设。在这些情况下，后端扩展者可以选择通过从torch::autograd::Function注册一个内核到相应的调度键（例如，如果您使用PrivateUse1作为后端，则"
" AutogradPrivateUse1）来覆盖 PyTorch 的自动求导层："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"With this trick you have full control over both training and inference "
"behavior for ``my_add`` operator in your backend. Here's `an example "
"<https://github.com/pytorch/xla/blob/r1.7/torch_xla/csrc/aten_autograd_ops.h>`_"
" in the ``pytorch/xla`` repository."
msgstr ""
"通过此技巧，您可以完全控制``my_add``操作符在您后端中的训练和推理行为。以下是``pytorch/xla``仓库中的`一个例子 "
"<https://github.com/pytorch/xla/blob/r1.7/torch_xla/csrc/aten_autograd_ops.h>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Build an extension"
msgstr "构建扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Out-of-tree backend is supported by adding a C++ extension to PyTorch. Once "
"you have kernels and registrations ready, you can build a C++ extension by "
"writing a ``setup.py`` script that uses ``setuptools`` to compile C++ code."
"  Here's a simplified example from `pytorch/xla repo "
"<https://github.com/pytorch/xla/blob/master/setup.py>`_::"
msgstr ""
"通过向 PyTorch 添加 C++ "
"扩展支持树外后端。一旦您准备好内核和注册，可以通过编写使用``setuptools``编译C++代码的``setup.py``脚本构建一个C++扩展。以下是`pytorch/xla仓库"
" <https://github.com/pytorch/xla/blob/master/setup.py>`_中的一个简化示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"See `our C++ extension tutorial "
"<https://pytorch.org/tutorials/advanced/cpp_extension.html#building-with-"
"setuptools>`_ for more details."
msgstr ""
"有关更多详情，请参阅`我们的 C++ 扩展教程 "
"<https://pytorch.org/tutorials/advanced/cpp_extension.html#building-with-"
"setuptools>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Custom operator support"
msgstr "自定义操作符支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Your new backend should work seamlessly with `customized operators extended "
"in python <https://pytorch.org/docs/stable/notes/extending.html>`_ without "
"writing any new kernels as long as the customized operator is composed of "
"existing PyTorch operators (which are already supported by your backend)."
msgstr ""
"您的新后端应能与`Python中扩展的自定义操作符 "
"<https://pytorch.org/docs/stable/notes/extending.html>`_无缝工作，而无需编写任何新的内核，只要自定义操作符由现有"
" PyTorch 操作符组成（这些已经被您的后端支持）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For `custom operators extended in C++ <cpp_autograd>`_ they often come with "
"a `backend specific C++ kernel implementation e.g. nms kernel in torchvsion "
"<https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/cuda/nms_kernel.cu>`_"
" as well as `a customized Python API e.g. torch.ops.torchvision.nms "
"<https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/nms.cpp#L18>`_."
" To support these operators, backend extenders will need to write a C++ "
"kernel for your backend and properly register it to the corresponding "
"namespace in the dispatcher similar to supporting PyTorch native operators. "
"Alternatively you could also add a customized API in your extension e.g "
"``torch_xla.core.functions.nms`` for these adhoc requests."
msgstr ""
"对于在 C++ 中扩展的`自定义操作符 <cpp_autograd>`_，通常会附带`具体后端的 C++ 内核实现，例如 torchvision 中的 "
"nms 内核 "
"<https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/cuda/nms_kernel.cu>`_，以及`自定义的"
" Python API，例如 torch.ops.torchvision.nms "
"<https://github.com/pytorch/vision/blob/master/torchvision/csrc/ops/nms.cpp#L18>`_。为了支持这些操作符，后端扩展者需要为其后端编写一个"
" C++ 内核，并将其正确注册到调度器中的对应命名空间，与支持 PyTorch 原生操作符类似。或者，您也可以在扩展中添加自定义的 API，例如 "
"``torch_xla.core.functions.nms`` 以满足这些临时请求。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "JIT support"
msgstr "JIT 支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As we mentioned in `Registering a Dispatched Operator in C++ <dispatcher>`_,"
" kernels registered through `m.impl()` API support being called in both "
"unboxed and boxed ways. In other words your customized backend can also work"
" with our JIT tracing/scripting frontend just like the in-tree backends like"
" CPU or CUDA do.  You could potentially also write specialized optimization "
"passes for your backend on a JIT graph.  But we will not discuss it here "
"since we haven't finalized the integration point in JIT, so the current "
"backend support will focus on the eager frontend for now."
msgstr ""
"正如我们在`在 C++ 中注册调度操作符 <dispatcher>`_中提到的，通过 `m.impl()` API "
"注册的内核支持以未封装方式和封装方式调用。换句话说，您的自定义后端也可以像内置的后端（如 CPU 或 CUDA）一样，与我们的 JIT "
"跟踪/脚本前端协同工作。您甚至可以为您的后端在 JIT 图上编写专门的优化流程。不过，我们不会在这里讨论，因为我们尚未在 JIT "
"中确定集成点，因此目前的后端支持将主要聚焦于即时前端。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Testing your backend against native PyTorch backends"
msgstr "根据 PyTorch 原生后端测试您的后端"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch lets tests run on multiple device types using its `generic device "
"type testing framework "
"<https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_device_type.py>`_."
" You can find details about `how tests use it "
"<https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L23>`_"
" and information about `how to add a new device type "
"<https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L369>`_."
" Once added, PyTorch tests using the generic device type testing framework "
"will be run using your device type, too. See `this Wiki page "
"<https://github.com/pytorch/pytorch/wiki/Writing-tests-that-run-on-all-"
"available-device-types>`_ for an example of how tests are instantiated."
msgstr ""
"PyTorch 允许使用其`通用设备类型测试框架 "
"<https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_device_type.py>`_在多个设备类型上运行测试。您可以在`测试如何使用它"
" "
"<https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L23>`_和`如何添加新的设备类型的信息"
" "
"<https://github.com/pytorch/pytorch/blob/5a8198eb3c594aa18352930fd21f3c25bd7b7100/torch/testing/_internal/common_device_type.py#L369>`_中找到详细信息。一旦添加了新的设备类型，使用通用设备类型测试框架的"
" PyTorch 测试也将在您的设备类型上运行。有关如何实例化测试的示例，请参阅`这篇 Wiki 页面 "
"<https://github.com/pytorch/pytorch/wiki/Writing-tests-that-run-on-all-"
"available-device-types>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Running PyTorch’s existing test suites with your device type is important to"
" ensure correctness, but not all PyTorch features are supported by every "
"device type.  The generic device type testing framework allows for "
"considerable customization so that device types can select which tests to "
"run, which dtypes they support, and even which precisions to use when "
"comparing tensors for equality."
msgstr ""
"使用您的设备类型运行 PyTorch 的现有测试套件对于确保正确性至关重要，但并不是所有 PyTorch "
"功能都支持每个设备类型。通用设备类型测试框架允许相当程度的自定义，因此设备类型可以选择运行哪些测试、支持哪些数据类型甚至在比较张量是否相等时使用哪些精度。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"An example device type that uses the generic device type testing framework "
"and doesn’t ship with PyTorch is XLA.  See `its extension of the generic "
"device type testing framework "
"<https://github.com/pytorch/xla/blob/master/test/pytorch_test_base.py>`_, "
"which contains examples of block listing tests, block listing dtypes, and "
"overriding test precision."
msgstr ""
"一个不随 PyTorch 一起发布但使用通用设备类型测试框架的示例设备类型是 XLA。请参阅其`对通用设备类型测试框架的扩展 "
"<https://github.com/pytorch/xla/blob/master/test/pytorch_test_base.py>`_，其中包含屏蔽测试、屏蔽数据类型以及覆盖测试精度的示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The generic device type testing framework is actively developed. To request "
"a feature please file an issue on PyTorch’s Github."
msgstr "通用设备类型测试框架正在积极开发中。请在 PyTorch 的 GitHub 上提交问题来请求功能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Backward Compatibility"
msgstr "向后兼容性"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Currently PyTorch can’t guarantee backward compatibility for registered "
"operators. Operators, as well as their schemas, might be "
"added/modified/deleted as needed.  Registered kernels must be *exactly* the "
"same as PyTorch version.  If PyTorch adds more parameters ( even with "
"defaults) for an operator, your old registration won't work until it's "
"updated to match PyTorch's new signature."
msgstr ""
"目前，PyTorch 无法保证注册操作符的向后兼容性。操作符及其架构可能会根据需要添加、修改或删除。注册的内核必须与 PyTorch "
"的版本*完全一致*。如果 PyTorch 为某个操作符添加了更多参数（即使是带默认值的参数），您的旧注册将无法工作，直到它更新为符合 PyTorch "
"的新签名。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As a result, we *highly recommend* out-of-tree backend extenders only sync "
"with major PyTorch releases to minimize interruptions in development.  "
"PyTorch is on a quarterly release cadence. Backend extenders should join the"
" *#announcement* channel at `pytorch.slack.com <http://pytorch.slack.com/>`_"
" to get latest updates on releases."
msgstr ""
"因此，我们*强烈建议*扩展的外部后端仅与主要 PyTorch 版本同步，以最大限度地减少开发中的中断。PyTorch "
"是一个季度发布周期。后端扩展者应加入`pytorch.slack.com <http://pytorch.slack.com/>`_ 的 "
"*#announcement* 频道以获取最新的发布更新。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Known issues & additional notes"
msgstr "已知问题和其他注意事项"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Not all test suites are device generic yet. Extensible test classes can be "
"found by searching ``instantiate_device_type_tests`` in PyTorch codebase, "
"e.g ``TestTorchDeviceType, TestViewOps, TestTensorDeviceOps, "
"TestTypePromotion`` etc."
msgstr ""
"并不是所有测试套件都是设备通用的。可以通过在 PyTorch 代码库中搜索 ``instantiate_device_type_tests`` "
"找到可扩展的测试类，例如 ``TestTorchDeviceType, TestViewOps, TestTensorDeviceOps, "
"TestTypePromotion`` 等。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There's no extension point in C++ for serializing a python Tensor object on "
"customized backend. Currently you can only extend it by modifying `PyTorch "
"Tensor __reduce_ex__ method "
"<https://github.com/pytorch/pytorch/blob/5640b79bf8a5412a0209a919c05c811d5427cc12/torch/tensor.py#L83-L150>`_"
" or monkey patching in out-of-tree repository."
msgstr ""
"在 C++ 中没有扩展点可以序列化自定义后端上的 python Tensor 对象。目前只能通过修改 `PyTorch Tensor "
"__reduce_ex__ 方法 "
"<https://github.com/pytorch/pytorch/blob/5640b79bf8a5412a0209a919c05c811d5427cc12/torch/tensor.py#L83-L150>`_"
" 或在外部存储库中补丁猴子来扩展。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If your backend doesn't allow direct memory access, you should pay "
"additional attention to supporting view ops since they're supposed to share "
"storage. Changes to view tensor need to propagated to its base tensor and "
"vice versa."
msgstr "如果您的后端不允许直接内存访问，则需要特别注意支持视图操作符，因为它们提供共享存储功能。对视图张量的更改需要传播到其基础张量，反之亦然。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There's no extension point in C++ for Optimizer if your backend doesn't work"
" with the native PyTorch Optimizers, e.g. need to carry the states to be "
"updated in backward like torch-xla. Such use cases currently can only be "
"done through adding customized API or monkey patching in out-of-tree "
"repository."
msgstr ""
"如果您的后端不能与原生 PyTorch 优化器协作，例如需要携带状态以在反向过程中更新（如 torch-xla），那么在 C++ "
"中没有优化器的扩展点。目前这种使用情况只能通过添加自定义的 API 或在外部存储库中补丁猴子来完成。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Future Work"
msgstr "未来工作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Making every component in PyTorch extensible for an out-of-tree backend "
"seamless requires a lot of changes to PyTorch internals.  Here are a few "
"items that we're actively working on might improve the experience in the "
"future:"
msgstr ""
"使 PyTorch 中的每个组件都能够无缝扩展到外部后端需对 PyTorch 内部进行大量更改。以下是我们正在积极研究的一些可能改善未来体验的项目："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Improve test coverage of generic testing framework."
msgstr "改进通用测试框架的测试覆盖率。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Improve ``Math`` kernel coverage and more comprehensive tests to make sure "
"``Math`` kernel behavior matches other backends like ``CPU/CUDA``."
msgstr "改进 ``Math`` 内核覆盖率以及更全面的测试，以确保 ``Math`` 内核行为与其他后端（如 ``CPU/CUDA``）一致。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Refactor ``RegistrationDeclarations.h`` to carry the minimal information and"
" reuse PyTorch's codegen as much as possible."
msgstr "重构 ``RegistrationDeclarations.h`` 以包含最少信息并尽可能重复使用 PyTorch 的代码生成。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Support a backend fallback kernel to automatic convert inputs to CPU and "
"convert the result back to the customized backend. This will allow \"full\" "
"operator coverage even though you don't have kernels written for every "
"operator."
msgstr "支持后端回退内核以自动将输入转换为 CPU 并将结果转换回自定义后端。即使您没有为每个操作符编写内核，这也将允许“完整”的操作符覆盖。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Stay in touch"
msgstr "保持联系"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Please use `PyTorch dev discussions <https://dev-discuss.pytorch.org/>`_ for"
" questions and discussions. If you have any feature requests or bug reports,"
" please `file an issue on github "
"<https://github.com/pytorch/pytorch/issues>`_."
msgstr ""
"请使用`PyTorch 开发讨论 <https://dev-"
"discuss.pytorch.org/>`_来提问和讨论。如果您有任何功能请求或错误报告，请在 GitHub 上`提交问题 "
"<https://github.com/pytorch/pytorch/issues>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you're interested in helping in any of the future work items above (e.g "
"adding more ``Math`` kernels for PyTorch operators in C++), please reach out"
" to us through Github or Slack!"
msgstr ""
"如果您对上述未来工作项目感兴趣（例如为 PyTorch 操作符在 C++ 中添加更多的 ``Math`` 内核），请通过 GitHub 或 Slack "
"与我们联系！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Distributed Training with Uneven Inputs Using the Join Context Manager"
msgstr "使用 Join 上下文管理器进行具有不均衡输入的分布式训练"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**\\ : `Andrew Gu <https://github.com/andwgu>`_"
msgstr "**作者**\\ : `Andrew Gu <https://github.com/andwgu>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst>`__."
msgstr ""
"|编辑| 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst>`__"
" 中查看和编辑此教程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``Join`` is introduced in PyTorch 1.10 as a prototype feature. This API is "
"subject to change."
msgstr "``Join`` 在 PyTorch 1.10 中作为原型功能引入。此 API 可能会有变更。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "In this tutorial, you will see:"
msgstr "在本教程中，您将看到："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "An overview of the `Join`_ context manager."
msgstr "对 `Join`_ 上下文管理器的概述。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"An example of how to use the context manager with "
"``DistributedDataParallel``."
msgstr "一个使用 ``DistributedDataParallel`` 的上下文管理器示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"An example of how to use the context manager with both "
"``DistributedDataParallel`` and ``ZeroRedundancyOptimizer``."
msgstr ""
"一个使用 ``DistributedDataParallel`` 和 ``ZeroRedundancyOptimizer`` 的上下文管理器示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "An example of passing in keyword arguments to the context manager."
msgstr "一个传递关键字参数给上下文管理器的示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "A dive into how the `Join`_ context manager works."
msgstr "深入探讨 `Join`_ 上下文管理器的工作原理。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"An example showing how to make a toy class compatible with the context "
"manager."
msgstr "一个示例展示如何使一个玩具类兼容上下文管理器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Requirements"
msgstr "需求"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PyTorch 1.10+"
msgstr "PyTorch 1.10+"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "`Getting Started with Distributed Data Parallel`_"
msgstr "`使用分布式数据并行入门`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "`Shard Optimizer States with ZeroRedundancyOptimizer`_"
msgstr "`使用 ZeroRedundancyOptimizer 切分优化器状态`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "What is ``Join``?"
msgstr "什么是 ``Join``？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In `Getting Started with Distributed Data Parallel - Basic Use Case`_, you "
"saw the general skeleton for using `DistributedDataParallel`_ to perform "
"data parallel training. This implicitly schedules all-reduces in each "
"backward pass to synchronize gradients across ranks. Such `collective "
"communications <https://pytorch.org/docs/stable/distributed.html>`__ require"
" participation from all ranks in the process group, so if a rank has fewer "
"inputs, then the other ranks will hang or error (depending on the backend). "
"More generally, this problem persists for any class that performs per-"
"iteration synchronous collective communications."
msgstr ""
"在 `使用分布式数据并行入门 - 基础应用场景`_ 中，您看到了使用 `DistributedDataParallel`_ "
"进行数据并行训练的一般框架。这会隐式地在每次反向传递中调度所有归约操作以同步各个设备的梯度。这类`集合通信 "
"<https://pytorch.org/docs/stable/distributed.html>`__需要该流程组中的所有设备参与，因此如果某个设备拥有较少的输入，其他设备就会停滞或出错（取决于后端）。更一般而言，对于任何执行每轮同步集合通信的类，这个问题都会持续存在。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``Join`` is a context manager to be used around your per-rank training loop "
"to facilitate training with uneven inputs. The context manager allows the "
"ranks that exhaust their inputs early (i.e. *join* early) to shadow the "
"collective communications performed by those that have not yet joined. The "
"ways in which the communications are shadowed are specified by hooks."
msgstr ""
"``Join`` 是一个上下文管理器，用于围绕每设备的训练循环，以便在输入不均衡情况下进行训练。上下文管理器允许提前耗尽其输入的设备（即 "
"*提前加入*）影子执行尚未加入的设备进行的集合通信。影子操作的方式由钩子指定。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using ``Join`` with ``DistributedDataParallel``"
msgstr "使用 ``Join`` 和 ``DistributedDataParallel``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch's `DistributedDataParallel`_ works out-of-the-box with the ``Join`` "
"context manager. Here is an example usage:"
msgstr ""
"PyTorch 的 `DistributedDataParallel`_ 可以直接与 ``Join`` 上下文管理器一起使用。以下是一个使用示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This produces the following output (where the ``print()`` s from rank 0 and "
"rank 1 may be arbitrarily ordered):"
msgstr "这将生成如下输出（其中，来自设备 0 和设备 1 的 ``print()`` 顺序可能是任意的）："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`DistributedDataParallel`_ provided its own `join()`_ context manager prior "
"to the introduction of this generic ``Join`` context manager. In the above "
"example, using ``with Join([model]):`` is equivalent to using ``with "
"model.join():``. One limitation of the existing "
"``DistributedDataParallel.join()`` is that it does not allow multiple "
"participating classes, e.g. ``DistributedDataParallel`` and "
"`ZeroRedundancyOptimizer`_ together."
msgstr ""
"`DistributedDataParallel`_ 在引入这个通用 ``Join`` 上下文管理器之前已经提供了自己的 `join()`_ "
"上下文管理器。在上面的示例中，使用 ``with Join([model]):`` 等同于使用 ``with model.join():``。现有的 "
"``DistributedDataParallel.join()`` 的一个限制是它不允许多个参与类，例如同时使用 "
"``DistributedDataParallel`` 和 `ZeroRedundancyOptimizer`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Using ``Join`` with ``DistributedDataParallel`` and "
"``ZeroRedundancyOptimizer``"
msgstr ""
"使用 ``Join``，结合 ``DistributedDataParallel`` 和 ``ZeroRedundancyOptimizer``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``Join`` context manager works not only with a single class but also "
"with multiple classes together. PyTorch's ``ZeroRedundancyOptimizer`` is "
"also compatible with the context manager, so here, we examine how to modify "
"the previous example to use both ``DistributedDataParallel`` and "
"``ZeroRedundancyOptimizer``:"
msgstr ""
"``Join`` 上下文管理器不仅可以与单个类一起工作，还可以与多个类一起工作。PyTorch 的 "
"``ZeroRedundancyOptimizer`` 也与上下文管理器兼容。因此，在这里，我们研究如何修改以前的示例以同时使用 "
"``DistributedDataParallel`` 和 ``ZeroRedundancyOptimizer``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This will yield the same output as before. The notable change was "
"additionally passing in the ``ZeroRedundancyOptimizer`` instance into "
"``Join()``."
msgstr "这将生成与之前相同的输出。显著的变化是将 ``ZeroRedundancyOptimizer`` 实例也传递给 ``Join()``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Passing Keyword Arguments"
msgstr "传递关键字参数"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Classes may provide keyword arguments that modify their behavior in the "
"context manager at run time. For example, ``DistributedDataParallel`` "
"provides an argument ``divide_by_initial_world_size``, which determines if "
"gradients are divided by the initial world size or by the effective world "
"size (i.e. number of non-joined ranks). Such keyword arguments can be passed"
" directly into the context manager."
msgstr ""
"类可以提供关键字参数，在运行时修改其在上下文管理器中的行为。例如，``DistributedDataParallel`` 提供了一个参数 "
"``divide_by_initial_world_size``，它决定梯度是除以初始设备数还是除以有效设备数（即未加入的设备数）。这样的关键字参数可以直接传递到上下文管理器中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The keyword arguments passed into the context manager are shared across all "
"participating classes. This should not be a limitation since we do not "
"expect cases where multiple ``Joinable`` s need differing settings of the "
"same argument. Nonetheless, this is something to keep in mind."
msgstr ""
"传递到上下文管理器中的关键字参数在所有参与类之间共享。这不应成为限制，因为我们不预期在同一参数的不同设置上需要不同的 ``Joinable`` "
"的情况。然而，这是需要注意的一点。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How Does ``Join`` Work?"
msgstr "``Join`` 的工作原理"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have seen some preliminary examples of how to use the ``Join`` "
"context manager, let us delve deeper into how it works. This will provide a "
"greater insight into the full capability that it offers and prepare you to "
"make your own custom classes compatible. Here, we will go over the ``Join`` "
"class as well as the supporting classes ``Joinable`` and ``JoinHook``."
msgstr ""
"现在我们已经看到了一些如何使用 ``Join`` "
"上下文管理器的初步示例，让我们更深入地探讨它的工作原理。这将提供对其全面功能的更深入了解，并准备好使您自己的自定义类兼容。接下来，我们将介绍 "
"``Join`` 类以及支持类 ``Joinable`` 和 ``JoinHook``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``Joinable``"
msgstr "``Joinable``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To begin, classes compatible with the ``Join`` context manager must inherit "
"from the abstract base class ``Joinable``. In particular, a ``Joinable`` "
"must implement:"
msgstr ""
"首先，与 ``Join`` 上下文管理器兼容的类必须继承抽象基类 ``Joinable``。特别是，一个 ``Joinable`` 必须实现："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``join_hook(self, **kwargs) -> JoinHook``"
msgstr "``join_hook(self, **kwargs) -> JoinHook``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This returns the ``JoinHook`` instance for the ``Joinable``, determining how"
" joined processes should shadow the per-iteration collective communications "
"performed by the ``Joinable``."
msgstr ""
"这会返回``Joinable``的``JoinHook``实例，以确定参与训练的进程如何跟踪``Joinable``在每次迭代中进行的广播通信。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``join_device(self) -> torch.device``"
msgstr "``join_device(self) -> torch.device``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This returns a device to be used by the ``Join`` context manager to perform "
"collective communications, e.g. ``torch.device(\"cuda:0\")`` or "
"``torch.device(\"cpu\")``."
msgstr ""
"这会返回一个设备供``Join``上下文管理器用于执行广播通信，例如``torch.device(\"cuda:0\")``或``torch.device(\"cpu\")``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``join_process_group(self) -> ProcessGroup``"
msgstr "``join_process_group(self) -> ProcessGroup``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This returns the process group to be used by the ``Join`` context manager to"
" perform collective communications."
msgstr "这会返回一个进程组供``Join``上下文管理器用于执行广播通信。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In particular, the ``join_device`` and ``join_process_group`` are required "
"attributes to ensure that the context manager can schedule collective "
"communications between joined and non-joined processes. One usage is to "
"count the number of non-joined processes on each iteration using an all-"
"reduce. Another usage is for implementing the mechanism required for "
"``throw_on_early_termination=True``, which we will explain later below."
msgstr ""
"特别是，``join_device``和``join_process_group``是必需属性，以确保上下文管理器能够在已加入和未加入的进程之间计划广播通信。一种用途是使用全归约在每次迭代中计算未加入进程的数量。另一种用途是实现``throw_on_early_termination=True``所需的机制，稍后我们会对此进行解释。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``DistributedDataParallel`` and ``ZeroRedundancyOptimizer`` already inherit "
"from ``Joinable`` and implement the above methods, which is why we could "
"directly use them in the previous examples."
msgstr ""
"``DistributedDataParallel``和``ZeroRedundancyOptimizer``已经继承了``Joinable``并实现了上述方法，这就是为什么我们可以直接在之前的示例中使用它们。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``Joinable`` classes should make sure to call the ``Joinable`` constructor "
"since it initializes a ``JoinConfig`` instance, which is used internally by "
"the context manager to ensure correctness. This will be saved in each "
"``Joinable`` as a field ``_join_config``."
msgstr ""
"``Joinable``类应确保调用``Joinable``构造函数，因为它会初始化一个``JoinConfig``实例，用于上下文管理器内部确保正确性。这将作为字段``_join_config``保存于每个``Joinable``中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``JoinHook``"
msgstr "``JoinHook``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, let us break down the ``JoinHook`` class. A ``JoinHook`` provides two "
"entry points into a context manager:"
msgstr "接下来，让我们详细说明``JoinHook``类。一个``JoinHook``为上下文管理器提供了两个入口点："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``main_hook(self) -> None``"
msgstr "``main_hook(self) -> None``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This hook is called repeatedly by each joined rank while there exists a rank"
" that has not yet joined. It is meant to shadow the collective "
"communications performed by the ``Joinable`` in each training iteration "
"(e.g. in one forward pass, backward pass, and optimizer step)."
msgstr ""
"在仍有尚未加入的进程存在的情况下，此钩子会被每个已加入的进程重复调用，目的是跟踪``Joinable``在每次训练迭代中执行的广播通信（例如一次前向传播、后向传播和优化步骤）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``post_hook(self, is_last_joiner: bool) -> None``"
msgstr "``post_hook(self, is_last_joiner: bool) -> None``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This hook is called once all ranks have joined. It is passed an additional "
"``bool`` argument ``is_last_joiner``, which indicates if the rank was one of"
" the last to join. The argument may be useful for synchronization."
msgstr ""
"此钩子会在所有进程已加入之后被调用。它会传递一个额外的``bool``参数``is_last_joiner``，指示该进程是否是最后一个加入的进程之一。此参数可能对同步有帮助。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To give concrete examples of what these hooks may look like, the provided "
"``ZeroRedundancyOptimizer`` main hook performs an optimizer step per normal "
"since the joined rank is still responsible for updating and synchronizing "
"its shard of the parameters, and the provided ``DistributedDataParallel`` "
"post-hook broadcasts the final updated model from one of the last joining "
"ranks to ensure that it is the same across all ranks."
msgstr ""
"为了给出这些钩子的具体示例，所提供的``ZeroRedundancyOptimizer``主钩子正常执行优化步骤，因为已加入的进程仍然负责更新和同步其参数的分片，而所提供的``DistributedDataParallel``后钩子从最后加入的进程之一广播最终更新的模型，以确保所有进程中的模型一致。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``Join``"
msgstr "``Join``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Finally, let us examine how these fit into the ``Join`` class itself."
msgstr "最后，让我们检查这些如何适应到``Join``类本身。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``__init__(self, joinables: List[Joinable], enable: bool = True, "
"throw_on_early_termination: bool = False)``"
msgstr ""
"``__init__(self, joinables: List[Joinable], enable: bool = True, "
"throw_on_early_termination: bool = False)``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As we saw in the previous examples, the constructor takes in a list of the "
"``Joinable`` s that participate in the training loop. These should be the "
"classes that perform collective communications in each iteration."
msgstr "如我们在之前的示例中所见，构造函数接受参与训练循环的``Joinable``列表。这些应该是每次迭代中执行广播通信的类。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``enable`` is a ``bool`` that can be set to ``False`` if you know that there"
" will not be uneven inputs, in which case the context manager becomes "
"vacuous similar to ``contextlib.nullcontext()``. This also may disable join-"
"related computation in the participating ``Joinable`` s."
msgstr ""
"``enable``是一个``bool``，如果知道不会有不平衡输入，可以将其设置为``False``，此时上下文管理器变得无效，类似于``contextlib.nullcontext()``。这也可能在参与的``Joinable``中禁用与加入相关的计算。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``throw_on_early_termination`` is a ``bool`` that can be set to ``True`` to "
"have each rank raise an exception the moment that uneven inputs are "
"detected. This is useful for cases that do not conform to the context "
"manager's requirements, which is most typically when there are collective "
"communications from different classes that may be arbitrarily interleaved, "
"such as when using ``DistributedDataParallel`` with a model that has "
"``SyncBatchNorm`` layers. In such cases, this argument should be set to "
"``True`` so that the application logic can catch the exception and determine"
" how to proceed."
msgstr ""
"``throw_on_early_termination``是一个``bool``，可以设置为``True``以使每个进程在检测到输入不平衡时立刻抛出异常。这对于不符合上下文管理器要求的情况非常有用，通常在有多个类的广播通信可能任意交错时，例如使用包含``SyncBatchNorm``层的模型与``DistributedDataParallel``一起使用。在这种情况下，此参数应设置为``True``，以便应用逻辑可以捕获异常并决定如何继续。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The core logic occurs in the ``__exit__()`` method, which loops while there "
"exists a non-joined rank, calling each ``Joinable`` 's main hook, and then "
"once all ranks have joined, calls their post hooks. Both the main hooks and "
"post-hooks are iterated over in the order that the ``Joinable`` s are passed"
" in."
msgstr ""
"核心逻辑发生在``__exit__()``方法中，该方法在仍存在未加入的进程时循环调用每个``Joinable``的主钩子，并且在所有进程已加入之后调用它们的后钩子。无论是主钩子还是后钩子，都会按照传递的``Joinable``列表中的顺序进行迭代。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The context manager requires a heartbeat from non-joined processes. As such,"
" each ``Joinable`` class should make a call to "
"``Join.notify_join_context()`` before its per-iteration collective "
"communications. The context manager will ensure that only the first "
"``Joinable`` passed in actually sends the heartbeat."
msgstr ""
"上下文管理器需要来自未加入进程的心跳。因此，每个``Joinable``类应在每次迭代的广播通信之前调用``Join.notify_join_context()``。上下文管理器会确保只有传递进来的第一个``Joinable``实际发送心跳。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As mentioned above regarding ``throw_on_early_termination``, the ``Join`` "
"context manager is not compatible with certain compositions of classes. The "
"``Joinable`` 's ``JoinHook`` s must be serializable since each hook is fully"
" executed before proceeding to the next. In other words, two hooks cannot "
"overlap. Moreover, currently, both the main hooks and post- hooks are "
"iterated over in the same deterministic order. If this appears to be a major"
" limitation, we may modify the API to permit a customizable ordering."
msgstr ""
"如上所述关于``throw_on_early_termination``，``Join``上下文管理器与某些类组合不兼容。``Joinable``的``JoinHook``必须是可序列化的，因为每个钩子都需要在执行下一个钩子之前完全执行完毕。换句话说，两个钩子不能重叠。此外，目前主钩子和后钩子都按照相同的确定性顺序进行迭代。如果这似乎是一个主要限制，我们可能会修改API以允许自定义排序。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Making a Toy Class Work with ``Join``"
msgstr "使一个简单类兼容``Join``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Since the previous section introduced several concepts, let us see them in "
"practice with a toy example. Here, we will implement a class that counts the"
" number of inputs that are seen across all ranks before its rank joins. This"
" should provide a basic idea of how you may make your own class compatible "
"with the ``Join`` context manager."
msgstr ""
"由于上一节介绍了几个概念，让我们通过一个简单示例来实践它们。这里，我们将实现一个类，该类统计所有进程在首次加入之前看到的输入数量。这应该提供如何使自己的类兼容``Join``上下文管理器的基本思路。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Specifically, the following code has each rank print out (1) the number of "
"inputs across all ranks that seen before it joins and (2) the total number "
"of inputs across all ranks."
msgstr "具体来说，以下代码会让每个进程打印出：(1)在加入之前所有进程看到的输入数量，以及(2)所有进程看到的输入总数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Since rank 0 sees 5 inputs and rank 1 sees 6, this yields the output:"
msgstr "由于进程0看到5个输入，进程1看到6个输入，这会产生如下输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Some key points to highlight:"
msgstr "一些需要注意的关键点："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A ``Counter`` instance performs a single all-reduce per iteration, so the "
"main hook performs a single all-reduce as well to shadow it."
msgstr "一个``Counter``实例每次迭代执行一次全归约，因此主钩子也执行一次全归约以跟踪它。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``Counter`` class makes a call to ``Join.notify_join_context()`` at the "
"beginning of its ``__call__()`` method since that is a place before its per-"
" iteration collective communications (i.e. its all-reduce)."
msgstr ""
"``Counter``类在其``__call__()``方法的开始处调用``Join.notify_join_context()``，因为这是它进行每次迭代广播通信（即其全归约）之前的地方。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The ``is_last_joiner`` argument is used to determine the broadcast source in"
" the post-hooks."
msgstr "``is_last_joiner``参数用于在后钩子中确定广播源。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We pass in the ``sync_max_count`` keyword argument to the context manager, "
"which is then forwarded to ``Counter`` 's join hook."
msgstr "我们将``sync_max_count``关键字参数传递给上下文管理器，然后转发到``Counter``的加入钩子。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_neural_style_tutorial.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_advanced_neural_style_tutorial.py>` 下载完整示例代码。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Neural Transfer Using PyTorch"
msgstr "使用PyTorch实现神经迁移"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `Alexis Jacq <https://alexis-jacq.github.io>`_"
msgstr "**作者**: `Alexis Jacq <https://alexis-jacq.github.io>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Edited by**: `Winston Herring <https://github.com/winston6>`_"
msgstr "**编辑**: `Winston Herring <https://github.com/winston6>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial explains how to implement the `Neural-Style algorithm "
"<https://arxiv.org/abs/1508.06576>`__ developed by Leon A. Gatys, Alexander "
"S. Ecker and Matthias Bethge. Neural-Style, or Neural-Transfer, allows you "
"to take an image and reproduce it with a new artistic style. The algorithm "
"takes three images, an input image, a content-image, and a style-image, and "
"changes the input to resemble the content of the content-image and the "
"artistic style of the style-image."
msgstr ""
"本教程解释如何实现由Leon A. Gatys, Alexander S. Ecker和Matthias Bethge开发的`Neural-"
"Style算法 <https://arxiv.org/abs/1508.06576>`__。Neural-"
"Style或神经迁移算法使您可以将一张图像转换为具有新的艺术风格。算法接受三个图像：输入图像、内容图像和风格图像，并改变输入图像以同时保留内容图像的内容和风格图像的艺术风格。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "content1"
msgstr "内容1"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Underlying Principle"
msgstr "基本原理"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The principle is simple: we define two distances, one for the content "
"(:math:`D_C`) and one for the style (:math:`D_S`). :math:`D_C` measures how "
"different the content is between two images while :math:`D_S` measures how "
"different the style is between two images. Then, we take a third image, the "
"input, and transform it to minimize both its content-distance with the "
"content-image and its style-distance with the style-image. Now we can import"
" the necessary packages and begin the neural transfer."
msgstr ""
"原理很简单：我们定义两个距离，一个用于内容（:math:`D_C`）一个用于风格（:math:`D_S`）。:math:`D_C`测量两张图像之间内容的差异，而:math:`D_S`测量两张图像之间风格的差异。然后，我们使用第三张图片即输入图像，通过调整它来同时最小化它与内容图像的内容距离和与风格图像的风格距离。现在我们可以导入必要的包并开始神经迁移。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Importing Packages and Selecting a Device"
msgstr "导入包并选择设备"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Below is a  list of the packages needed to implement the neural transfer."
msgstr "以下是实现神经迁移所需的包列表。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch``, ``torch.nn``, ``numpy`` (indispensables packages for neural "
"networks with PyTorch)"
msgstr "``torch``, ``torch.nn``, ``numpy``（PyTorch中神经网络所不可或缺的包）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.optim`` (efficient gradient descents)"
msgstr "``torch.optim``（高效梯度下降）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``PIL``, ``PIL.Image``, ``matplotlib.pyplot`` (load and display images)"
msgstr "``PIL``, ``PIL.Image``, ``matplotlib.pyplot``（加载和显示图像）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torchvision.transforms`` (transform PIL images into tensors)"
msgstr "``torchvision.transforms``（将PIL图像转换为张量）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torchvision.models`` (train or load pretrained models)"
msgstr "``torchvision.models``（训练或加载预训练模型）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``copy`` (to deep copy the models; system package)"
msgstr "``copy``（用于深度复制模型；系统包）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we need to choose which device to run the network on and import the "
"content and style images. Running the neural transfer algorithm on large "
"images takes longer and will go much faster when running on a GPU. We can "
"use ``torch.cuda.is_available()`` to detect if there is a GPU available. "
"Next, we set the ``torch.device`` for use throughout the tutorial. Also the "
"``.to(device)`` method is used to move tensors or modules to a desired "
"device."
msgstr ""
"接下来，我们需要选择在哪个设备上运行网络，并导入内容图像和风格图像。在大型图像上运行神经迁移算法耗时较长，但运行在GPU上会快得多。我们可以使用``torch.cuda.is_available()``来检测是否有可用的GPU。接下来，我们设置``torch.device``供整个教程使用。此外，通过使用``.to(device)``方法，可以将张量或模块移动到目标设备。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Loading the Images"
msgstr "加载图像"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now we will import the style and content images. The original PIL images "
"have values between 0 and 255, but when transformed into torch tensors, "
"their values are converted to be between 0 and 1. The images also need to be"
" resized to have the same dimensions. An important detail to note is that "
"neural networks from the torch library are trained with tensor values "
"ranging from 0 to 1. If you try to feed the networks with 0 to 255 tensor "
"images, then the activated feature maps will be unable to sense the intended"
" content and style. However, pretrained networks from the Caffe library are "
"trained with 0 to 255 tensor images."
msgstr ""
"现在我们将导入风格和内容图像。原始PIL图像的值范围为0到255，但转换为torch张量后，其值被转换为0到1之间。图像还需要调整为相同的尺寸。需要注意的重要细节是：torch库中的神经网络使用范围为0到1的张量值进行训练。如果尝试使用范围为0到255的张量图像训练网络，则活化的特征映射将无法感知预期的内容和风格。然而，来自Caffe库的预训练网络是使用0到255的张量图像进行训练的。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here are links to download the images required to run the tutorial: "
"`picasso.jpg <https://pytorch.org/tutorials/_static/img/neural-"
"style/picasso.jpg>`__ and `dancing.jpg "
"<https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg>`__. "
"Download these two images and add them to a directory with name ``images`` "
"in your current working directory."
msgstr ""
"以下是运行教程所需图像的下载链接：`picasso.jpg "
"<https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg>`__ 和 "
"`dancing.jpg <https://pytorch.org/tutorials/_static/img/neural-"
"style/dancing.jpg>`__。下载这两个图像并将其添加到当前工作目录中名为``images``的文件夹中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, let's create a function that displays an image by reconverting a copy "
"of it to PIL format and displaying the copy using ``plt.imshow``. We will "
"try displaying the content and style images to ensure they were imported "
"correctly."
msgstr ""
"现在，我们创建一个函数，通过将其复制重新转换为PIL格式并使用``plt.imshow``显示该副本来显示图像。我们将尝试显示内容图像和风格图像以确保它们成功导入。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Loss Functions"
msgstr "损失函数"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Content Loss"
msgstr "内容损失"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The content loss is a function that represents a weighted version of the "
"content distance for an individual layer. The function takes the feature "
"maps :math:`F_{XL}` of a layer :math:`L` in a network processing input "
":math:`X` and returns the weighted content distance "
":math:`w_{CL}.D_C^L(X,C)` between the image :math:`X` and the content image "
":math:`C`. The feature maps of the content image(:math:`F_{CL}`) must be "
"known by the function in order to calculate the content distance. We "
"implement this function as a torch module with a constructor that takes "
":math:`F_{CL}` as an input. The distance :math:`\\|F_{XL} - F_{CL}\\|^2` is "
"the mean square error between the two sets of feature maps, and can be "
"computed using ``nn.MSELoss``."
msgstr ""
"内容损失是一种函数，用于表示每一层的内容距离的加权版本。该函数接收输入 X 在某层 L 的特征图 F_{XL}，并返回图像 X 和内容图像 C "
"之间的加权内容距离 w_{CL}·D_C^L(X, "
"C)。为了计算内容距离，函数必须已经知道内容图像的特征图（F_{CL）。我们将此函数实现为一个包含构造函数的 torch 模块，该构造函数以 "
"F_{CL} 作为输入。距离 \\|F_{XL} - F_{CL}\\|^2 是两组特征图之间的均方误差，可以使用 nn.MSELoss 计算。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We will add this content loss module directly after the convolution layer(s)"
" that are being used to compute the content distance. This way each time the"
" network is fed an input image the content losses will be computed at the "
"desired layers and because of auto grad, all the gradients will be computed."
" Now, in order to make the content loss layer transparent we must define a "
"``forward`` method that computes the content loss and then returns the "
"layer’s input. The computed loss is saved as a parameter of the module."
msgstr ""
"我们将直接在用于计算内容距离的卷积层之后添加这一内容损失模块。这样每次网络接收输入图像时，内容损失都会在期望的层上计算，并且由于自动梯度，所有梯度都会被计算出来。为了使内容损失层透明，我们必须定义一个"
" ``forward`` 方法来计算内容损失，然后返回该层的输入。计算得到的损失作为模块的参数保存。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Important detail**: although this module is named ``ContentLoss``, it is "
"not a true PyTorch Loss function. If you want to define your content loss as"
" a PyTorch Loss function, you have to create a PyTorch autograd function to "
"recompute/implement the gradient manually in the ``backward`` method."
msgstr ""
"**重要细节**：虽然该模块被命名为 ``ContentLoss``，它并不是一个真正的 PyTorch 损失函数。如果您想将内容损失定义为 "
"PyTorch 损失函数，则需要创建一个 PyTorch 自动梯度函数，并在 ``backward`` 方法中手动重新计算/实现梯度。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Style Loss"
msgstr "样式损失"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The style loss module is implemented similarly to the content loss module. "
"It will act as a transparent layer in a network that computes the style loss"
" of that layer. In order to calculate the style loss, we need to compute the"
" gram matrix :math:`G_{XL}`. A gram matrix is the result of multiplying a "
"given matrix by its transposed matrix. In this application the given matrix "
"is a reshaped version of the feature maps :math:`F_{XL}` of a layer "
":math:`L`. :math:`F_{XL}` is reshaped to form :math:`\\hat{F}_{XL}`, a "
":math:`K`\\ x\\ :math:`N` matrix, where :math:`K` is the number of feature "
"maps at layer :math:`L` and :math:`N` is the length of any vectorized "
"feature map :math:`F_{XL}^k`. For example, the first line of "
":math:`\\hat{F}_{XL}` corresponds to the first vectorized feature map "
":math:`F_{XL}^1`."
msgstr ""
"样式损失模块的实现与内容损失模块类似。它将在网络中作为一个透明层，用于计算该层的样式损失。为了计算样式损失，我们需要计算 Gram 矩阵 "
"G_{XL}。Gram 矩阵是将一个给定矩阵乘以其转置矩阵所得的结果。在此应用中，给定的矩阵是某层 L 的特征图 F_{XL} 的重塑版本。F_{XL}"
" 被重塑形成 \\hat{F}_{XL}，一个 K\\ x\\ N 矩阵，其中 K 是层 L 中特征图的数量，N 是任何矢量化特征图 F_{XL}^k "
"的长度。例如，\\hat{F}_{XL} 的第一行对应于第一个矢量化特征图 F_{XL}^1。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, the gram matrix must be normalized by dividing each element by the "
"total number of elements in the matrix. This normalization is to counteract "
"the fact that :math:`\\hat{F}_{XL}` matrices with a large :math:`N` "
"dimension yield larger values in the Gram matrix. These larger values will "
"cause the first layers (before pooling layers) to have a larger impact "
"during the gradient descent. Style features tend to be in the deeper layers "
"of the network so this normalization step is crucial."
msgstr ""
"最后，Gram 矩阵必须通过将矩阵中的每个元素除以矩阵中元素的总数来进行归一化。该归一化是为了抵消具有大 N 维度的 \\hat{F}_{XL} 矩阵在"
" Gram "
"矩阵中生成更大的值。这些较大的值会导致梯度下降过程中前几层（池化层之前）产生更大影响。样式特征倾向于在网络的更深层次，因此这个归一化步骤至关重要。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now the style loss module looks almost exactly like the content loss module."
" The style distance is also computed using the mean square error between "
":math:`G_{XL}` and :math:`G_{SL}`."
msgstr "现在样式损失模块看起来几乎和内容损失模块一样。样式距离也使用 G_{XL} 和 G_{SL} 之间的均方误差进行计算。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Importing the Model"
msgstr "导入模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now we need to import a pretrained neural network. We will use a 19 layer "
"VGG network like the one used in the paper."
msgstr "现在我们需要导入一个预训练的神经网络。我们将使用论文中使用的19层VGG网络。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch’s implementation of VGG is a module divided into two child "
"``Sequential`` modules: ``features`` (containing convolution and pooling "
"layers), and ``classifier`` (containing fully connected layers). We will use"
" the ``features`` module because we need the output of the individual "
"convolution layers to measure content and style loss. Some layers have "
"different behavior during training than evaluation, so we must set the "
"network to evaluation mode using ``.eval()``."
msgstr ""
"PyTorch 中的 VGG 实现是一个模块，由两个子 ``Sequential`` 模块组成：“features”（包含卷积和池化层）和 "
"“classifier”（包含全连接层）。我们将使用 ``features`` "
"模块，因为我们需要单个卷积层的输出来度量内容和样式损失。一些层在训练与评估阶段行为不同，因此我们必须使用 ``.eval()`` 将网络设置为评估模式。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Additionally, VGG networks are trained on images with each channel "
"normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. We "
"will use them to normalize the image before sending it into the network."
msgstr ""
"此外，VGG 网络是基于每个通道通过均值=[0.485, 0.456, 0.406] 和标准差=[0.229, 0.224, 0.225] "
"归一化的图像进行训练的。我们将在网络处理图像前使用它们对图像进行归一化。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A ``Sequential`` module contains an ordered list of child modules. For "
"instance, ``vgg19.features`` contains a sequence (``Conv2d``, ``ReLU``, "
"``MaxPool2d``, ``Conv2d``, ``ReLU``…) aligned in the right order of depth. "
"We need to add our content loss and style loss layers immediately after the "
"convolution layer they are detecting. To do this we must create a new "
"``Sequential`` module that has content loss and style loss modules correctly"
" inserted."
msgstr ""
"一个 ``Sequential`` 模块包含子模块的有序列表。例如，``vgg19.features`` "
"包含一组序列（``Conv2d``、``ReLU``、``MaxPool2d``、``Conv2d``、``ReLU``……）按照深度的正确顺序排列。我们需要在检测卷积层之后立即添加内容损失和样式损失层。为此，我们必须创建一个新的"
" ``Sequential`` 模块，其中正确插入了内容损失和样式损失模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we select the input image. You can use a copy of the content image or "
"white noise."
msgstr "接下来，我们选择输入图像。可以使用内容图像的副本或白噪声。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Gradient Descent"
msgstr "梯度下降"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As Leon Gatys, the author of the algorithm, suggested `here "
"<https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-"
"artistic-style/336/20?u=alexis-jacq>`__, we will use L-BFGS algorithm to run"
" our gradient descent. Unlike training a network, we want to train the input"
" image in order to minimize the content/style losses. We will create a "
"PyTorch L-BFGS optimizer ``optim.LBFGS`` and pass our image to it as the "
"tensor to optimize."
msgstr ""
"正如算法作者 Leon Gatys 在此处建议的，我们将使用 L-BFGS "
"算法进行梯度下降。与训练网络不同，我们希望训练输入图像以最小化内容/样式损失。我们将创建一个 PyTorch L-BFGS 优化器 "
"``optim.LBFGS`` 并将图像传递给它作为要优化的张量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, we must define a function that performs the neural transfer. For "
"each iteration of the networks, it is fed an updated input and computes new "
"losses. We will run the ``backward`` methods of each loss module to "
"dynamically compute their gradients. The optimizer requires a “closure” "
"function, which reevaluates the module and returns the loss."
msgstr ""
"最后，必须定义一个执行神经传递的函数。对于网络的每次迭代，它都会接收更新后的输入并计算新的损失。我们将运行每个损失模块的 ``backward`` "
"方法以动态计算其梯度。优化器需要一个“闭包”函数，该函数重新评估模块并返回损失。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We still have one final constraint to address. The network may try to "
"optimize the input with values that exceed the 0 to 1 tensor range for the "
"image. We can address this by correcting the input values to be between 0 to"
" 1 each time the network is run."
msgstr ""
"我们仍需解决一个最终约束。网络可能试图优化超出图像张量范围 (0 到 1) 的输入值。我们可以通过在网络每次运行时将输入值修正到 0 到 1 "
"范围来解决这个问题。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Finally, we can run the algorithm."
msgstr "最终，我们可以运行算法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: neural_style_tutorial.py "
"<neural_style_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: neural_style_tutorial.py <neural_style_tutorial.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: neural_style_tutorial.ipynb "
"<neural_style_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: neural_style_tutorial.ipynb "
"<neural_style_tutorial.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_numpy_extensions_tutorial.py>` "
"to download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_advanced_numpy_extensions_tutorial.py>` "
"下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Creating Extensions Using NumPy and SciPy"
msgstr "使用 NumPy 和 SciPy 创建扩展"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `Adam Paszke <https://github.com/apaszke>`_"
msgstr "**作者**: `Adam Paszke <https://github.com/apaszke>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Updated by**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_"
msgstr "**更新作者**: `Adam Dziedzic <https://github.com/adam-dziedzic>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "In this tutorial, we shall go through two tasks:"
msgstr "在本教程中，我们将完成两个任务："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Create a neural network layer with no parameters."
msgstr "创建一个没有参数的神经网络层。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "This calls into **numpy** as part of its implementation"
msgstr "部分实现调用 **numpy**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Create a neural network layer that has learnable weights"
msgstr "创建一个具有可学习权重的神经网络层。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "This calls into **SciPy** as part of its implementation"
msgstr "部分实现调用 **SciPy**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Parameter-less example"
msgstr "无参示例"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This layer doesn’t particularly do anything useful or mathematically "
"correct."
msgstr "这个层没有做任何有用或数学上正确的事情。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "It is aptly named ``BadFFTFunction``"
msgstr "它被恰当地命名为 ``BadFFTFunction``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Layer Implementation**"
msgstr "**层实现**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Example usage of the created layer:**"
msgstr "**创建层的示例使用**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Parametrized example"
msgstr "带参数的示例"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In deep learning literature, this layer is confusingly referred to as "
"convolution while the actual operation is cross-correlation (the only "
"difference is that filter is flipped for convolution, which is not the case "
"for cross-correlation)."
msgstr "在深度学习文献中，这个层被混淆地称为卷积，而实际操作是交叉相关（唯一的区别在于，对于卷积，滤波器被翻转，而交叉相关不会翻转）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Implementation of a layer with learnable weights, where cross-correlation "
"has a filter (kernel) that represents weights."
msgstr "实现具有可学习权重的层，其中交叉相关的滤波器（kernel）表示权重。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The backward pass computes the gradient ``wrt`` the input and the gradient "
"``wrt`` the filter."
msgstr "反向传播计算输入的梯度 ``wrt`` 和滤波器的梯度 ``wrt``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Example usage:**"
msgstr "**示例使用**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Check the gradients:**"
msgstr "**检查梯度**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: numpy_extensions_tutorial.py "
"<numpy_extensions_tutorial.py>`"
msgstr ""
":download:`下载Python源代码: numpy_extensions_tutorial.py "
"<numpy_extensions_tutorial.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: numpy_extensions_tutorial.ipynb "
"<numpy_extensions_tutorial.ipynb>`"
msgstr ""
":download:`下载Jupyter notebook: numpy_extensions_tutorial.ipynb "
"<numpy_extensions_tutorial.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_pendulum.py>` to download the "
"full example code"
msgstr "点击 :ref:`这里 <sphx_glr_download_advanced_pendulum.py>` 下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Pendulum: Writing your environment and transforms with TorchRL"
msgstr "摆：使用 TorchRL 编写环境和变换"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Creating an environment (a simulator or an interface to a physical control "
"system) is an integrative part of reinforcement learning and control "
"engineering."
msgstr "创建环境（模拟器或物理控制系统的接口）是强化学习和控制工程的重要组成部分。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL provides a set of tools to do this in multiple contexts. This "
"tutorial demonstrates how to use PyTorch and TorchRL code a pendulum "
"simulator from the ground up. It is freely inspired by the Pendulum-v1 "
"implementation from `OpenAI-Gym/Farama-Gymnasium control library "
"<https://github.com/Farama-Foundation/Gymnasium>`__."
msgstr ""
"TorchRL 提供一套工具，用于在多种场景中实现这一目标。本教程展示如何使用 PyTorch 和 TorchRL "
"从头开始编写一个摆动模拟器。它灵感来源于 `OpenAI-Gym/Farama-Gymnasium 控制库 "
"<https://github.com/Farama-Foundation/Gymnasium>`__ 中的 Pendulum-v1。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Pendulum"
msgstr "摆"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Simple Pendulum"
msgstr "简单摆"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Key learnings:"
msgstr "关键学习点："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How to design an environment in TorchRL: - Writing specs (input, observation"
" and reward); - Implementing behavior: seeding, reset and step."
msgstr "如何设计 TorchRL 中的环境： - 编写规范（输入、观察和奖励）； - 实现行为：种子、重置和步骤。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Transforming your environment inputs and outputs, and writing your own "
"transforms;"
msgstr "转换环境输入和输出，并编写自己定义的变换；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How to use :class:`~tensordict.TensorDict` to carry arbitrary data "
"structures through the ``codebase``."
msgstr "如何使用 :class:`~tensordict.TensorDict` 在 ``代码库`` 中传递任意数据结构。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "In the process, we will touch three crucial components of TorchRL:"
msgstr "在过程中，我们将触及 TorchRL 的三个关键组件："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "`environments <https://pytorch.org/rl/stable/reference/envs.html>`__"
msgstr "`环境 <https://pytorch.org/rl/stable/reference/envs.html>`__"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`transforms "
"<https://pytorch.org/rl/stable/reference/envs.html#transforms>`__"
msgstr "`变换 <https://pytorch.org/rl/stable/reference/envs.html#transforms>`__"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`models (policy and value function) "
"<https://pytorch.org/rl/stable/reference/modules.html>`__"
msgstr ""
"`模型（策略和价值函数） <https://pytorch.org/rl/stable/reference/modules.html>`__"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To give a sense of what can be achieved with TorchRL's environments, we will"
" be designing a *stateless* environment. While stateful environments keep "
"track of the latest physical state encountered and rely on this to simulate "
"the state-to-state transition, stateless environments expect the current "
"state to be provided to them at each step, along with the action undertaken."
" TorchRL supports both types of environments, but stateless environments are"
" more generic and hence cover a broader range of features of the environment"
" API in TorchRL."
msgstr ""
"为了展示使用 TorchRL "
"环境可以实现的目标，我们将设计一个*无状态*环境。虽然有状态环境跟踪最新的物理状态并依赖于这一点来模拟状态间的转移，但无状态环境期望每一步提供当前状态以及执行的动作。TorchRL"
" 支持两种类型的环境，但无状态环境更为通用，因此涵盖了 TorchRL 环境 API 的更多功能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Modeling stateless environments gives users full control over the input and "
"outputs of the simulator: one can reset an experiment at any stage or "
"actively modify the dynamics from the outside. However, it assumes that we "
"have some control over a task, which may not always be the case: solving a "
"problem where we cannot control the current state is more challenging but "
"has a much wider set of applications."
msgstr ""
"建模无状态环境使用户可以完全控制模拟器的输入和输出：可以在实验的任何阶段进行重置或者从外部主动改变动力学。然而，这假设我们对任务有一定控制，而这可能并不总是实际情况：解决无法控制当前状态的问题更具挑战性，但应用范围更广。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Another advantage of stateless environments is that they can enable batched "
"execution of transition simulations. If the backend and the implementation "
"allow it, an algebraic operation can be executed seamlessly on scalars, "
"vectors, or tensors. This tutorial gives such examples."
msgstr ""
"无状态环境的另一个优势是它们可以实现转移模拟的批量执行。如果后端和实现允许，将可以无缝地对标量、向量或张量执行代数操作。本教程给出了这样的示例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "This tutorial will be structured as follows:"
msgstr "本教程将结构化地包括以下部分："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We will first get acquainted with the environment properties: its shape "
"(``batch_size``), its methods (mainly :meth:`~torchrl.envs.EnvBase.step`, "
":meth:`~torchrl.envs.EnvBase.reset` and "
":meth:`~torchrl.envs.EnvBase.set_seed`) and finally its specs."
msgstr ""
"首先，我们将熟悉环境属性：其形状（``batch_size``）、其方法（主要是 "
":meth:`~torchrl.envs.EnvBase.step`、:meth:`~torchrl.envs.EnvBase.reset` 和 "
":meth:`~torchrl.envs.EnvBase.set_seed`）以及其规范。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After having coded our simulator, we will demonstrate how it can be used "
"during training with transforms."
msgstr "在编写模拟器之后，我们将展示如何在训练期间使用变换。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We will explore new avenues that follow from the TorchRL's API, including: "
"the possibility of transforming inputs, the vectorized execution of the "
"simulation and the possibility of backpropagation through the simulation "
"graph."
msgstr "我们将探索基于 TorchRL API 的新方法，包括：输入转换的可能性、模拟的矢量化执行以及通过模拟图进行反向传播的可能性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, we will train a simple policy to solve the system we implemented."
msgstr "最后，我们将训练一个简单的策略以解决我们实现的系统。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There are four things you must take care of when designing a new environment"
" class:"
msgstr "在设计新环境类时必须注意以下四点："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":meth:`EnvBase._reset`, which codes for the resetting of the simulator at a "
"(potentially random) initial state;"
msgstr ":meth:`EnvBase._reset`，用于将模拟器重置到一个（可能随机的）初始状态；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ":meth:`EnvBase._step` which codes for the state transition dynamic;"
msgstr ":meth:`EnvBase._step`，用于编码状态转移动力学；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ":meth:`EnvBase._set_seed`` which implements the seeding mechanism;"
msgstr ":meth:`EnvBase._set_seed`，实现设定种子机制；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "the environment specs."
msgstr "环境规范。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let us first describe the problem at hand: we would like to model a simple "
"pendulum over which we can control the torque applied on its fixed point. "
"Our goal is to place the pendulum in upward position (angular position at 0 "
"by convention) and having it standing still in that position. To design our "
"dynamic system, we need to define two equations: the motion equation "
"following an action (the torque applied) and the reward equation that will "
"constitute our objective function."
msgstr ""
"我们首先来描述当前的问题：我们希望对一个简单的摆模型进行建模，其中可以控制其固定点施加的扭矩。我们的目标是将摆置于向上位置（按照约定，角位置为0）并在该位置保持静止。为了设计我们的动态系统，我们需要定义两个方程：随着动作（施加的扭矩）的运动方程和构成目标函数的奖励方程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For the motion equation, we will update the angular velocity following:"
msgstr "对于运动方程，我们将按照以下公式更新角速度："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"\\dot{\\theta}_{t+1} = \\dot{\\theta}_t + (3 * g / (2 * L) * "
"\\sin(\\theta_t) + 3 / (m * L^2) * u) * dt"
msgstr ""
"\\dot{\\theta}_{t+1} = \\dot{\\theta}_t + (3 * g / (2 * L) * "
"\\sin(\\theta_t) + 3 / (m * L^2) * u) * dt"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"where :math:`\\dot{\\theta}` is the angular velocity in rad/sec, :math:`g` "
"is the gravitational force, :math:`L` is the pendulum length, :math:`m` is "
"its mass, :math:`\\theta` is its angular position and :math:`u` is the "
"torque. The angular position is then updated according to"
msgstr ""
"其中 :math:`\\dot{\\theta}` 是以弧度每秒表示的角速度，:math:`g` 是重力，:math:`L` "
"是摆的长度，:math:`m` 是摆的质量，:math:`\\theta` 是摆的角位置，:math:`u` 是扭矩。然后根据以下公式更新角位置："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "\\theta_{t+1} = \\theta_{t} + \\dot{\\theta}_{t+1} dt"
msgstr "\\theta_{t+1} = \\theta_{t} + \\dot{\\theta}_{t+1} dt"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We define our reward as"
msgstr "我们定义奖励为："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "r = -(\\theta^2 + 0.1 * \\dot{\\theta}^2 + 0.001 * u^2)"
msgstr "r = -(\\theta^2 + 0.1 * \\dot{\\theta}^2 + 0.001 * u^2)"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"which will be maximized when the angle is close to 0 (pendulum in upward "
"position), the angular velocity is close to 0 (no motion) and the torque is "
"0 too."
msgstr "当角度接近0（摆在向上位置）、角速度接近0（无运动）且扭矩也为0时，这个奖励会达到最大。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Coding the effect of an action: :func:`~torchrl.envs.EnvBase._step`"
msgstr "动作效果编码：:func:`~torchrl.envs.EnvBase._step`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The step method is the first thing to consider, as it will encode the "
"simulation that is of interest to us. In TorchRL, the "
":class:`~torchrl.envs.EnvBase` class has a :meth:`EnvBase.step` method that "
"receives a :class:`tensordict.TensorDict` instance with an ``\"action\"`` "
"entry indicating what action is to be taken."
msgstr ""
"step方法是我们首先需要考虑的，它将会编码我们感兴趣的模拟。在TorchRL中，:class:`~torchrl.envs.EnvBase`类有一个:meth:`EnvBase.step`方法，该方法接收一个带有“action”条目的:class:`tensordict.TensorDict`实例，指示要执行的动作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To facilitate the reading and writing from that ``tensordict`` and to make "
"sure that the keys are consistent with what's expected from the library, the"
" simulation part has been delegated to a private abstract method "
":meth:`_step` which reads input data from a ``tensordict``, and writes a "
"*new*  ``tensordict`` with the output data."
msgstr ""
"为了便于从该“tensordict”读取和写入，并确保键与库期望的一致，模拟部分已委托给一个私有抽象方法:meth:`_step`，它从一个“tensordict”读取输入数据，并使用输出数据写入一个*新的*“tensordict”。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The :func:`_step` method should do the following:"
msgstr ":func:`_step` 方法应执行以下操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Read the input keys (such as ``\"action\"``) and execute the simulation "
"based on these;"
msgstr "读取输入键（例如“action”）并根据这些执行模拟；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Retrieve observations, done state and reward;"
msgstr "检索观测值、完成状态和奖励；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Write the set of observation values along with the reward and done state at "
"the corresponding entries in a new :class:`TensorDict`."
msgstr "将观测值集合与奖励和完成状态写入到新的:class:`TensorDict`的相应条目中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, the :meth:`~torchrl.envs.EnvBase.step` method will merge the output of"
" :meth:`~torchrl.envs.EnvBase.step` in the input ``tensordict`` to enforce "
"input/output consistency."
msgstr ""
"接着，:meth:`~torchrl.envs.EnvBase.step`方法将会合并:meth:`~torchrl.envs.EnvBase.step`的输出到输入的“tensordict”中，以强制输入/输出一致性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Typically, for stateful environments, this will look like this:"
msgstr "通常，对于有状态环境来说，这看起来是这样的："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Notice that the root ``tensordict`` has not changed, the only modification "
"is the appearance of a new ``\"next\"`` entry that contains the new "
"information."
msgstr "注意，根“tensordict”没有改变，唯一的修改是新增了一个“next”条目，其中包含新信息。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the Pendulum example, our :meth:`_step` method will read the relevant "
"entries from the input ``tensordict`` and compute the position and velocity "
"of the pendulum after the force encoded by the ``\"action\"`` key has been "
"applied onto it. We compute the new angular position of the pendulum "
"``\"new_th\"`` as the result of the previous position ``\"th\"`` plus the "
"new velocity ``\"new_thdot\"`` over a time interval ``dt``."
msgstr ""
"在摆模型的例子中，我们的:meth:`_step`方法将从输入的“tensordict”读取相关条目，并计算在“action”键代表的力作用后摆的运动和速度。我们将新的摆角位置“new_th”计算为此前位置“th”加上覆盖时间间隔“dt”中的新速度“new_thdot”的结果。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Since our goal is to turn the pendulum up and maintain it still in that "
"position, our ``cost`` (negative reward) function is lower for positions "
"close to the target and low speeds. Indeed, we want to discourage positions "
"that are far from being \"upward\" and/or speeds that are far from 0."
msgstr ""
"鉴于我们的目标是让摆向上并保持静止，我们的“cost”（负奖励）函数在位置靠近目标且速度较低时更低。确实，我们希望减少远离“向上”的位置和远离0的速度。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In our example, :meth:`EnvBase._step` is encoded as a static method since "
"our environment is stateless. In stateful settings, the ``self`` argument is"
" needed as the state needs to be read from the environment."
msgstr ""
"在我们的例子中，:meth:`EnvBase._step`是作为一个静态方法编码的，因为我们的环境是无状态的。在有状态设置中，需要使用“self”参数，因为状态需要从环境中读取。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Resetting the simulator: :func:`~torchrl.envs.EnvBase._reset`"
msgstr "重置模拟器：:func:`~torchrl.envs.EnvBase._reset`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The second method we need to care about is the "
":meth:`~torchrl.envs.EnvBase._reset` method. Like "
":meth:`~torchrl.envs.EnvBase._step`, it should write the observation entries"
" and possibly a done state in the ``tensordict`` it outputs (if the done "
"state is omitted, it will be filled as ``False`` by the parent method "
":meth:`~torchrl.envs.EnvBase.reset`). In some contexts, it is required that "
"the ``_reset`` method receives a command from the function that called it "
"(for example, in multi-agent settings we may want to indicate which agents "
"need to be reset). This is why the :meth:`~torchrl.envs.EnvBase._reset` "
"method also expects a ``tensordict`` as input, albeit it may perfectly be "
"empty or ``None``."
msgstr ""
"我们需要关注的第二个方法是:meth:`~torchrl.envs.EnvBase._reset`方法。像:meth:`~torchrl.envs.EnvBase._step`一样，它应在输出的“tensordict”中写入观测条目以及可能的完成状态（如果省略完成状态，父方法:meth:`~torchrl.envs.EnvBase.reset`会将其填充为“False”）。在某些情况下，_reset方法需要接收到调用它的函数的命令（例如，在多智能体设置中我们可能需要指示需要重置的智能体）。这就是:meth:`~torchrl.envs.EnvBase._reset`方法还需要一个作为输入的“tensordict”的原因，尽管它完全可以是空的或“None”。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The parent :meth:`EnvBase.reset` does some simple checks like the "
":meth:`EnvBase.step` does, such as making sure that a ``\"done\"`` state is "
"returned in the output ``tensordict`` and that the shapes match what is "
"expected from the specs."
msgstr ""
"父:meth:`EnvBase.reset`做了一些简单的检查，如:meth:`EnvBase.step`所做的检查，包括确保在输出“tensordict”中返回一个“done”状态，以及形状符合规范的期望。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For us, the only important thing to consider is whether "
":meth:`EnvBase._reset` contains all the expected observations. Once more, "
"since we are working with a stateless environment, we pass the configuration"
" of the pendulum in a nested ``tensordict`` named ``\"params\"``."
msgstr ""
"对我们来说，唯一重要的事情是确保:meth:`EnvBase._reset`包含所有期望的观测值。再次强调，由于我们工作的是一个无状态环境，我们将摆的配置传递到一个嵌套的“tensordict”中，命名为“params”。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this example, we do not pass a done state as this is not mandatory for "
":meth:`_reset` and our environment is non-terminating, so we always expect "
"it to be ``False``."
msgstr ""
"在这个例子中，我们没有传递完成状态，因为这对于:meth:`_reset`来说不是强制性的，并且我们的环境是非终止的，因此我们总是期望它为“False”。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Environment metadata: ``env.*_spec``"
msgstr "环境元数据：``env.*_spec``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The specs define the input and output domain of the environment. It is "
"important that the specs accurately define the tensors that will be received"
" at runtime, as they are often used to carry information about environments "
"in multiprocessing and distributed settings. They can also be used to "
"instantiate lazily defined neural networks and test scripts without actually"
" querying the environment (which can be costly with real-world physical "
"systems for instance)."
msgstr ""
"规格定义了环境的输入和输出域。重要的是规格准确地定义了运行时将要接收的张量，因为它们通常用于多进程和分布式设置中的环境信息承载。它们还可以用于实例化惰性定义的神经网络并测试脚本而无需实际查询环境（例如，这对于真实物理系统来说可能会很昂贵）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "There are four specs that we must code in our environment:"
msgstr "我们必须在环境中编写四个规格："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":obj:`EnvBase.observation_spec`: This will be a "
":class:`~torchrl.data.CompositeSpec` instance where each key is an "
"observation (a :class:`CompositeSpec` can be viewed as a dictionary of "
"specs)."
msgstr ""
":obj:`EnvBase.observation_spec`：它会是:class:`~torchrl.data.CompositeSpec`实例，其中每个键是一个观测值（:class:`CompositeSpec`可视为规范的字典）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":obj:`EnvBase.action_spec`: It can be any type of spec, but it is required "
"that it corresponds to the ``\"action\"`` entry in the input ``tensordict``;"
msgstr ""
":obj:`EnvBase.action_spec`：它可以是任何类型的规格，但要求它与输入“tensordict”中的“action”条目相对应；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":obj:`EnvBase.reward_spec`: provides information about the reward space;"
msgstr ":obj:`EnvBase.reward_spec`：提供关于奖励空间的信息；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":obj:`EnvBase.done_spec`: provides information about the space of the done "
"flag."
msgstr ":obj:`EnvBase.done_spec`：提供关于完成标志空间的信息。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL specs are organized in two general containers: ``input_spec`` which "
"contains the specs of the information that the step function reads (divided "
"between ``action_spec`` containing the action and ``state_spec`` containing "
"all the rest), and ``output_spec`` which encodes the specs that the step "
"outputs (``observation_spec``, ``reward_spec`` and ``done_spec``). In "
"general, you should not interact directly with ``output_spec`` and "
"``input_spec`` but only with their content: ``observation_spec``, "
"``reward_spec``, ``done_spec``, ``action_spec`` and ``state_spec``. The "
"reason if that the specs are organized in a non-trivial way within "
"``output_spec`` and ``input_spec`` and neither of these should be directly "
"modified."
msgstr ""
"TorchRL规格组织在两个通用容器中：``input_spec``包含step函数读取的信息规范（分为包含动作的``action_spec``和包含所有其他内容的``state_spec``），而``output_spec``编码step输出的规范（``observation_spec``、``reward_spec``和``done_spec``）。通常，你不应该直接与``output_spec``和``input_spec``交互，而仅与其内容交互：``observation_spec``、``reward_spec``、``done_spec``、``action_spec``和``state_spec``。原因是规格在``output_spec``和``input_spec``中组织方式不够直观，这两个容器中的内容不应直接修改。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In other words, the ``observation_spec`` and related properties are "
"convenient shortcuts to the content of the output and input spec containers."
msgstr "换句话说，``observation_spec``及相关属性是输出和输入规格容器内容的便捷快捷方式。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL offers multiple :class:`~torchrl.data.TensorSpec` `subclasses "
"<https://pytorch.org/rl/stable/reference/data.html#tensorspec>`_ to encode "
"the environment's input and output characteristics."
msgstr ""
"TorchRL提供了多个:class:`~torchrl.data.TensorSpec` `子类 "
"<https://pytorch.org/rl/stable/reference/data.html#tensorspec>`_用于编码环境的输入和输出特征。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Specs shape"
msgstr "规格形状"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The environment specs leading dimensions must match the environment batch-"
"size. This is done to enforce that every component of an environment "
"(including its transforms) have an accurate representation of the expected "
"input and output shapes. This is something that should be accurately coded "
"in stateful settings."
msgstr ""
"环境规格的领先维度必须与环境批量大小匹配。这是为了确保环境的每个组件（包括其变换）准确地表示期望的输入和输出形状。这是有状态设置中应准确编码的内容。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For non batch-locked environments, such as the one in our example (see "
"below), this is irrelevant as the environment batch size will most likely be"
" empty."
msgstr "对于无批量锁定的环境，例如示例中的环境（见下文），这与之无关，因为环境批量大小很可能是空的。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Reproducible experiments: seeding"
msgstr "可重复的实验：设置种子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Seeding an environment is a common operation when initializing an "
"experiment. The only goal of :func:`EnvBase._set_seed` is to set the seed of"
" the contained simulator. If possible, this operation should not call "
"``reset()`` or interact with the environment execution. The parent "
":func:`EnvBase.set_seed` method incorporates a mechanism that allows seeding"
" multiple environments with a different pseudo-random and reproducible seed."
msgstr ""
"为环境设置种子是初始化实验时的常见操作。:func:`EnvBase._set_seed`的唯一目标是设置包含的模拟器的种子。如果可能，该操作不应调用``reset()``或与环境执行交互。父:func:`EnvBase.set_seed`方法集成了一个机制，可使用不同的伪随机且可重复种子为多个环境设置种子。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Wrapping things together: the :class:`~torchrl.envs.EnvBase` class"
msgstr "将内容整合在一起：:class:`~torchrl.envs.EnvBase`类"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can finally put together the pieces and design our environment class. The"
" specs initialization needs to be performed during the environment "
"construction, so we must take care of calling the :func:`_make_spec` method "
"within :func:`PendulumEnv.__init__`."
msgstr ""
"我们终于可以将这些部分整合在一起并设计我们的环境类了。规格初始化需要在环境构造期间完成，因此我们必须在:func:`PendulumEnv.__init__`中调用:func:`_make_spec`方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We add a static method :meth:`PendulumEnv.gen_params` which "
"deterministically generates a set of hyperparameters to be used during "
"execution:"
msgstr "我们添加一个静态方法:meth:`PendulumEnv.gen_params`，它确定性地生成一组在执行期间使用的超参数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We define the environment as non-``batch_locked`` by turning the "
"``homonymous`` attribute to ``False``. This means that we will **not** "
"enforce the input ``tensordict`` to have a ``batch-size`` that matches the "
"one of the environment."
msgstr ""
"我们通过将其“同名”属性设置为``False``，定义环境为非``batch_locked``。这意味着我们**不会**强制输入``tensordict``具有与环境相匹配的``batch-"
"size``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The following code will just put together the pieces we have coded above."
msgstr "以下代码只是将之前编写的部分整合在一起。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Testing our environment"
msgstr "测试我们的环境"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL provides a simple function "
":func:`~torchrl.envs.utils.check_env_specs` to check that a (transformed) "
"environment has an input/output structure that matches the one dictated by "
"its specs. Let us try it out:"
msgstr ""
"TorchRL提供了一个简单的函数:func:`~torchrl.envs.utils.check_env_specs`，用于检查（已变换）环境的输入/输出结构是否与其规格规定的匹配。让我们尝试一下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can have a look at our specs to have a visual representation of the "
"environment signature:"
msgstr "我们可以查看规格以获得环境签名的视觉表示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can execute a couple of commands too to check that the output structure "
"matches what is expected."
msgstr "我们可以执行几个命令来检查输出结构是否与预期相符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can run the :func:`env.rand_step` to generate an action randomly from the"
" ``action_spec`` domain. A ``tensordict`` containing the hyperparameters and"
" the current state **must** be passed since our environment is stateless. In"
" stateful contexts, ``env.rand_step()`` works perfectly too."
msgstr ""
"我们可以运行:func:`env.rand_step`从``action_spec``域随机生成一个动作。由于我们的环境是无状态的，因此**必须**传递一个包含超参数和当前状态的``tensordict``。在有状态上下文中，``env.rand_step()``也能正常运行。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Transforming an environment"
msgstr "变换环境"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Writing environment transforms for stateless simulators is slightly more "
"complicated than for stateful ones: transforming an output entry that needs "
"to be read at the following iteration requires to apply the inverse "
"transform before calling :func:`meth.step` at the next step. This is an "
"ideal scenario to showcase all the features of TorchRL's transforms!"
msgstr ""
"为无状态模拟器编写环境变换比有状态的稍微复杂一些：变换一个需要在下一次迭代中读取的输出条目要求在接下来的步骤调用:func:`meth.step`之前应用反变换。这是展示TorchRL变换所有特性的理想场景！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For instance, in the following transformed environment we ``unsqueeze`` the "
"entries ``[\"th\", \"thdot\"]`` to be able to stack them along the last "
"dimension. We also pass them as ``in_keys_inv`` to squeeze them back to "
"their original shape once they are passed as input in the next iteration."
msgstr ""
"例如，在下面的变换环境中，我们对条目``[\"th\", "
"\"thdot\"]``执行``unsqueeze``以便能够在最后一个维度上堆叠它们。我们也将它们作为``in_keys_inv``传递，以便在下一次迭代时将它们恢复到原始形状后再作为输入传递。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Writing custom transforms"
msgstr "编写自定义变换"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchRL's transforms may not cover all the operations one wants to execute "
"after an environment has been executed. Writing a transform does not require"
" much effort. As for the environment design, there are two steps in writing "
"a transform:"
msgstr "TorchRL的变换可能无法涵盖人们希望在环境执行后执行的所有操作。编写一个变换并不需要太大努力。和环境设计一样，编写变换有两个步骤："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Getting the dynamics right (forward and inverse);"
msgstr "完成正确的动力学（正向和逆向）；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Adapting the environment specs."
msgstr "调整环境规范。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A transform can be used in two settings: on its own, it can be used as a "
":class:`~torch.nn.Module`. It can also be used appended to a "
":class:`~torchrl.envs.transforms.TransformedEnv`. The structure of the class"
" allows to customize the behavior in the different contexts."
msgstr ""
"一个变换可以在两种设置下使用：单独使用时，它可以作为一个 :class:`~torch.nn.Module` 使用。它还可以附加到 "
":class:`~torchrl.envs.transforms.TransformedEnv`。该类的结构允许在不同的上下文中自定义行为。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A :class:`~torchrl.envs.transforms.Transform` skeleton can be summarized as "
"follows:"
msgstr "一个 :class:`~torchrl.envs.transforms.Transform` 的框架可以总结如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There are three entry points (:func:`forward`, :func:`_step` and "
":func:`inv`) which all receive :class:`tensordict.TensorDict` instances. The"
" first two will eventually go through the keys indicated by "
":obj:`~tochrl.envs.transforms.Transform.in_keys` and call "
":meth:`~torchrl.envs.transforms.Transform._apply_transform` to each of "
"these. The results will be written in the entries pointed by "
":obj:`Transform.out_keys` if provided (if not the ``in_keys`` will be "
"updated with the transformed values). If inverse transforms need to be "
"executed, a similar data flow will be executed but with the "
":func:`Transform.inv` and :func:`Transform._inv_apply_transform` methods and"
" across the ``in_keys_inv`` and ``out_keys_inv`` list of keys. The following"
" figure summarized this flow for environments and replay buffers."
msgstr ""
"有三个入口点 (:func:`forward`, :func:`_step` 和 :func:`inv`)，它们都接收 "
":class:`tensordict.TensorDict` 实例。前两个入口点最终将处理 "
":obj:`~tochrl.envs.transforms.Transform.in_keys` 指示的键并调用 "
":meth:`~torchrl.envs.transforms.Transform._apply_transform` 方法。这些结果将被写入 "
":obj:`Transform.out_keys` 指向的条目中（如果未提供，则将更新 ``in_keys`` "
"的值）。如果需要执行逆向变换，一个类似的数据流将会执行，但使用的是 :func:`Transform.inv` 和 "
":func:`Transform._inv_apply_transform` 方法，并利用 ``in_keys_inv`` 和 "
"``out_keys_inv`` 键列表。下图总结了这种环境和重放缓存的流程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Transform API"
msgstr "变换 API"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In some cases, a transform will not work on a subset of keys in a unitary "
"manner, but will execute some operation on the parent environment or work "
"with the entire input ``tensordict``. In those cases, the :func:`_call` and "
":func:`forward` methods should be re-written, and the "
":func:`_apply_transform` method can be skipped."
msgstr ""
"在某些情况下，变换不会以单一的方式作用于键的子集，而是会对父环境执行某些操作或处理整个输入 ``tensordict``。在这些情况下，应该重写 "
":func:`_call` 和 :func:`forward` 方法，并且可以跳过 :func:`_apply_transform` 方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let us code new transforms that will compute the ``sine`` and ``cosine`` "
"values of the position angle, as these values are more useful to us to learn"
" a policy than the raw angle value:"
msgstr "让我们编写新的变换来计算位置角度的 ``正弦`` 和 ``余弦`` 值，因为这些值比原始角度值更有助于我们学习策略："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Concatenates the observations onto an \"observation\" entry. "
"``del_keys=False`` ensures that we keep these values for the next iteration."
msgstr "将观测值拼接到 \"observation\" 条目中。``del_keys=False`` 确保我们在下一次迭代中保留这些值。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once more, let us check that our environment specs match what is received:"
msgstr "同样，让我们检查我们的环境规范是否与接收到的内容匹配："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Executing a rollout"
msgstr "执行回合"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Executing a rollout is a succession of simple steps:"
msgstr "执行回合是简单步骤的连续体："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "reset the environment"
msgstr "重置环境"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "while some condition is not met:"
msgstr "在某些条件未满足时："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "compute an action given a policy"
msgstr "根据策略计算行动"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "execute a step given this action"
msgstr "根据此行动执行一步操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "collect the data"
msgstr "收集数据"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "make a ``MDP`` step"
msgstr "进行 ``MDP`` 步骤"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "gather the data and return"
msgstr "收集数据并返回"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"These operations have been conveniently wrapped in the "
":meth:`~torchrl.envs.EnvBase.rollout` method, from which we provide a "
"simplified version here below."
msgstr "这些操作已方便地封装在 :meth:`~torchrl.envs.EnvBase.rollout` 方法中，我们在这里提供一个简化版本。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Batching computations"
msgstr "批量计算"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The last unexplored end of our tutorial is the ability that we have to batch"
" computations in TorchRL. Because our environment does not make any "
"assumptions regarding the input data shape, we can seamlessly execute it "
"over batches of data. Even better: for non-batch-locked environments such as"
" our Pendulum, we can change the batch size on the fly without recreating "
"the environment. To do this, we just generate parameters with the desired "
"shape."
msgstr ""
"本教程的最后一个未探索部分是我们在 TorchRL "
"中批量计算的能力。因为我们的环境并不对输入数据的形状作任何假设，所以可以无缝地对数据批次执行操作。更妙的是：对于像我们的 Pendulum "
"这样不受批处理限制的环境，我们可以随意更改批量大小而无需重新创建环境。为此，我们只需生成具有所需形状的参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Executing a rollout with a batch of data requires us to reset the "
"environment out of the rollout function, since we need to define the "
"batch_size dynamically and this is not supported by "
":meth:`~torchrl.envs.EnvBase.rollout`:"
msgstr ""
"用数据批次执行回合需要我们在回合函数之外重置环境，因为我们需要动态定义批量大小，而这并不被 "
":meth:`~torchrl.envs.EnvBase.rollout` 支持："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Training a simple policy"
msgstr "训练简单的策略"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this example, we will train a simple policy using the reward as a "
"differentiable objective, such as a negative loss. We will take advantage of"
" the fact that our dynamic system is fully differentiable to backpropagate "
"through the trajectory return and adjust the weights of our policy to "
"maximize this value directly. Of course, in many settings many of the "
"assumptions we make do not hold, such as differentiable system and full "
"access to the underlying mechanics."
msgstr ""
"在这个例子中，我们将使用奖励作为可微分的目标（如负损失）来训练一个简单的策略。我们将利用动态系统完全可微的特点，通过轨迹返回进行反向传播并调整策略的权重以直接最大化该值。当然，在许多情况下，我们做的假设并不成立，例如可微分系统以及对底层力学的完全访问。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Still, this is a very simple example that showcases how a training loop can "
"be coded with a custom environment in TorchRL."
msgstr "尽管如此，这仍是一个非常简单的例子，展示了如何在 TorchRL 中使用自定义环境编写训练循环。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let us first write the policy network:"
msgstr "首先让我们编写策略网络："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "and our optimizer:"
msgstr "以及我们的优化器："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Training loop"
msgstr "训练循环"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We will successively:"
msgstr "我们将依次："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "generate a trajectory"
msgstr "生成一个轨迹"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "sum the rewards"
msgstr "累计奖励"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "backpropagate through the graph defined by these operations"
msgstr "通过定义这些操作的图进行反向传播"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "clip the gradient norm and make an optimization step"
msgstr "剪裁梯度范数并进行优化步骤"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "repeat"
msgstr "重复操作"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"At the end of the training loop, we should have a final reward close to 0 "
"which demonstrates that the pendulum is upward and still as desired."
msgstr "在训练循环的最后，我们应获得接近 0 的最终奖励，这表明钟摆处于向上且静止状态。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we have learned how to code a stateless environment from "
"scratch. We touched the subjects of:"
msgstr "在本教程中，我们学习了如何从零开始编写无状态的环境内容。我们涉及的主题包括："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The four essential components that need to be taken care of when coding an "
"environment (``step``, ``reset``, seeding and building specs). We saw how "
"these methods and classes interact with the :class:`~tensordict.TensorDict` "
"class;"
msgstr ""
"构建环境时需要处理的四个基本组件（``step``，``reset``，播种和构建规范）。我们看到了这些方法和类如何与 "
":class:`~tensordict.TensorDict` 交互；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How to test that an environment is properly coded using "
":func:`~torchrl.envs.utils.check_env_specs`;"
msgstr "如何使用 :func:`~torchrl.envs.utils.check_env_specs` 测试环境是否编写正确；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"How to append transforms in the context of stateless environments and how to"
" write custom transformations;"
msgstr "如何在无状态环境的上下文中添加变换以及如何编写自定义变换；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to train a policy on a fully differentiable simulator."
msgstr "如何在完全可微的仿真器上训练策略。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ":download:`Download Python source code: pendulum.py <pendulum.py>`"
msgstr ":download:`下载 Python 源代码: pendulum.py <pendulum.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ":download:`Download Jupyter notebook: pendulum.ipynb <pendulum.ipynb>`"
msgstr ":download:`下载 Jupyter 笔记本: pendulum.ipynb <pendulum.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Facilitating New Backend Integration by PrivateUse1"
msgstr "通过 PrivateUse1 便捷整合新后端"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial we will walk through some necessary steps to integrate a "
"new backend living outside ``pytorch/pytorch`` repo by ``PrivateUse1``. Note"
" that this tutorial assumes that you already have a basic understanding of "
"PyTorch. you are an advanced user of PyTorch."
msgstr ""
"在本教程中，我们将展示一些必要的步骤，以通过 ``PrivateUse1`` 将 ``pytorch/pytorch`` "
"仓库之外的新后端集成进来。请注意，本教程假定您已经对 PyTorch 有基本了解，并且是 PyTorch 的高级用户。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial only involves the parts related to the PrivateUse1 mechanism "
"that facilitates the integration of new devices, and other parts will not be"
" covered. At the same time, not all the modules involved in this tutorial "
"are required, and you can choose the modules that are helpful to you "
"according to your actual needs."
msgstr ""
"本教程仅涉及与 PrivateUse1 "
"机制相关的部分，该机制便于新设备的集成，其他部分不会涉及。同时，本教程所涉及的模块并非全部必需，您可以根据实际需要选择对您有帮助的模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "What is PrivateUse1?"
msgstr "什么是 PrivateUse1？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Prior to Pytorch 2.0, PyTorch provided three reserved dispatch keys (and "
"their corresponding Autograd keys) for prototyping out-of-tree backend "
"extensions, the three dispatch keys are as follows:"
msgstr ""
"在 PyTorch 2.0 之前，PyTorch 提供了三个保留的分派键（及其对应的 Autograd 键）用于原型外部后端扩展，这三个分派键如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``PrivateUse1/AutogradPrivateUse1``"
msgstr "``PrivateUse1/AutogradPrivateUse1``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``PrivateUse2/AutogradPrivateUse2``"
msgstr "``PrivateUse2/AutogradPrivateUse2``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``PrivateUse3/AutogradPrivateUse3``"
msgstr "``PrivateUse3/AutogradPrivateUse3``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After the prototype verification is passed, you can apply for a private key "
"for the new backend, such as CUDA, XLA, MPS, and so on."
msgstr "原型验证通过后，您可以为新后端申请私有键，例如 CUDA、XLA、MPS 等。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"However, with the rapid development of PyTorch, more and more hardware "
"manufacturers are trying to integrate their backends into PyTorch, which "
"might cause the following problems:"
msgstr "但是，随着 PyTorch 的快速发展，越来越多的硬件制造商试图将后端集成到 PyTorch 中，这可能导致以下问题："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Every new backend integration involves a lot of file modification"
msgstr "每次新后端集成都涉及大量文件修改"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There is currently a hard limit on the number of Dispatch Keys "
"(``DispatchKeySet`` 64-bit limit)"
msgstr "目前，分派键的数量有硬性限制（``DispatchKeySet`` 为 64 位限制）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There is also a problem with integrating the new backend into PyTorch "
"through the PrivateUse1 Key, as it is impossible to integrate many backends "
"at the same time. Fortunately, these out-of-tree backends are rarely used "
"simultaneously."
msgstr ""
"通过 PrivateUse1 键将新后端集成到 PyTorch 中也存在问题，因为无法同时集成多个后端。幸运的是，这些树外后端很少同时使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In view of the above reasons, the community began to recommend new backend "
"to be integrated into the PyTorch via ``PrivateUse1``."
msgstr "基于上述原因，社区开始推荐通过 ``PrivateUse1`` 将新后端集成到 PyTorch 中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"However, the previous ``PrivateUse1`` mechanism is not fully capable of "
"integrating with the new backend, because it lacks some related support in "
"certain modules, such as Storage, AMP, Distributed, and so on."
msgstr ""
"然而，之前的 ``PrivateUse1`` 机制并不能完全支持新后端的集成，因为它在某些模块中缺乏支持，比如 Storage、AMP 和分布式等。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"With the arrival of Pytorch 2.1.0, a series of optimizations and "
"enhancements have been made for ``PrivateUse1`` in terms of new backend "
"integration, and it is now possible to support the integration of new "
"devices rapidly and efficiently."
msgstr ""
"随着 PyTorch 2.1.0 的到来，``PrivateUse1`` 在新后端集成方面进行了一系列优化和增强，现在可以快速高效地支持新设备的集成。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to integrate new backend via PrivateUse1"
msgstr "如何通过 PrivateUse1 集成新后端"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this section, we will discuss the details of integrating the new backend "
"into Pytorch via ``PrivateUse1``, which mainly consists of the following "
"parts:"
msgstr "在本节中，我们将讨论通过 ``PrivateUse1`` 将新后端集成到 PyTorch 中的细节，这主要由以下几个部分组成："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register kernels for the new backend."
msgstr "为新后端注册内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register generator for the new backend."
msgstr "为新后端注册生成器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register device guard for the new backend."
msgstr "为新后端注册设备管理器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Register serialization and deserialization functions for new backend "
"metadata."
msgstr "为新后端的元数据注册序列化和反序列化功能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Other Modules."
msgstr "其他模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The new backend may have some high-performance implementations of operator, "
"which can be registered to the dispatcher by ``TORCH_LIBRARY_IMPL`` API "
"described in `Registering a Dispatched Operator in C++ <dispatcher>`_. This "
"involves several situations:"
msgstr ""
"新后端可能有一些高性能的运算符实现，这些可以通过使用 ``TORCH_LIBRARY_IMPL`` API（描述于 `Registering a "
"Dispatched Operator in C++ <dispatcher>`_）注册到分派器中。这涉及几种情况："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Register all the forward operators supported by the new backend to the "
"dispatcher, and register the fallback at the same time, so that when the new"
" backend does not support some operators, these operators can fall back to "
"the CPU for execution to ensure the availability of functions."
msgstr ""
"将新后端支持的所有前向运算符注册到分派器，同时注册后备机制，这样在新后端不支持某些运算符时，这些运算符可以回退到 CPU 执行，以确保功能的可用性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Register kernels from ``torch::autograd::Function`` to the dispatcher by "
"``AutogradPrivateUse1``, if it is necessary for new backend to override "
"``PyTorch Autograd layer``, the dispatcher and autograd system will "
"automatically call the forward and backward implementations of these "
"operators."
msgstr ""
"通过 ``AutogradPrivateUse1`` 将 ``torch::autograd::Function`` "
"的内核注册到分派器中，如果新后端需要覆盖 ``PyTorch Autograd 层``，分派器和自动求导系统将自动调用这些运算符的前向和后向实现。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Register kernels which want to support `automatic mixed precision (AMP) "
"<https://pytorch.org/docs/stable/amp.html>`_ and fallback mechanism to the "
"dispatcher by ``AutocastPrivateUse1``, the autocast system will "
"automatically call these kernels when needed."
msgstr ""
"通过 ``AutocastPrivateUse1`` 将希望支持 `自动混合精度 (AMP) "
"<https://pytorch.org/docs/stable/amp.html>`_ "
"和后备机制的内核注册到分派器中，当需要时，自动转换系统将自动调用这些内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"What needs to be added is that if you want to support AMP in a new backend, "
"you need to register a new ``BackendModule`` by "
"``torch._register_device_module(\"backend_name\", BackendModule)``, and the "
"``BackendModule`` needs to have the following APIs:"
msgstr ""
"需要补充的是，如果您想在新后端中支持 AMP，则需要通过 "
"``torch._register_device_module(\"backend_name\", BackendModule)`` 注册一个新的 "
"``BackendModule``，且 ``BackendModule`` 需要包含以下 API："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``get_amp_supported_dtype() -> List[torch.dtype]``"
msgstr "``get_amp_supported_dtype() -> List[torch.dtype]``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"get the supported dtypes on the new backend in AMP, which might support one "
"more ``dtype``."
msgstr "获取新后端支持的 AMP 中的数据类型，可能支持额外的 ``dtype``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``is_autocast_enabled() -> bool``"
msgstr "``is_autocast_enabled() -> bool``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "check the AMP is enabled or not on the new backend."
msgstr "检查新后端的 AMP 是否启用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``get_autocast_dtype() -> torch.dtype``"
msgstr "``get_autocast_dtype() -> torch.dtype``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"get the supported ``dtype`` on the new backend in AMP, which is set by "
"``set_autocast_dtype`` or the default ``dtype``, and the default ``dtype`` "
"is ``torch.float16``."
msgstr ""
"获取新后端的 AMP 中支持的 ``dtype``，此值由 ``set_autocast_dtype`` 或默认 ``dtype`` 设置，而默认 "
"``dtype`` 是 ``torch.float16``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``set_autocast_enabled(bool) -> None``"
msgstr "``set_autocast_enabled(bool) -> None``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "enable or disable AMP on the new backend."
msgstr "启用或禁用新后端的 AMP。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``set_autocast_dtype(dtype) -> None``"
msgstr "``set_autocast_dtype(dtype) -> None``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"set the supported ``dtype`` on the new backend in AMP, and the ``dtype`` be "
"contained in the ``dtypes`` got from ``get_amp_supported_dtype``."
msgstr ""
"设置新后端的 AMP 中支持的 ``dtype``，此 ``dtype`` 必须包含在 ``get_amp_supported_dtype`` 返回的 "
"``dtypes`` 中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register generator for the new backend"
msgstr "为新后端注册生成器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It is necessary to support generators corresponding to new devices. "
"Currently, ``PrivateUse1`` can dynamically register custom generators, which"
" are mainly divided into the following steps."
msgstr "需要支持与新设备相对应的生成器。目前，``PrivateUse1`` 可以动态注册自定义生成器，主要分为以下几个步骤。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Inherit the ``GeneratorImpl`` class to implement the generator class "
"corresponding to the new backend, and implement various general methods."
msgstr "继承 ``GeneratorImpl`` 类以实现与新后端对应的生成器类，并实现各种通用方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Define a new backend ``builder`` with a single parameter: ``device index``."
msgstr "定义一个参数为 ``device index`` 的新后端 ``builder``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Call ``REGISTER_GENERATOR_PRIVATEUSE1`` macro to complete dynamic "
"registration."
msgstr "调用 ``REGISTER_GENERATOR_PRIVATEUSE1`` 宏完成动态注册。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register device guard for the new backend"
msgstr "为新的后端注册设备保护。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch provides functionalities related to device, stream, and event "
"switching via ``DeviceGuard``. This function is also applicable to "
"``PrivateUse1`` Key."
msgstr ""
"PyTorch 通过 ``DeviceGuard`` 提供与设备、流和事件切换相关的功能。此功能同样适用于 ``PrivateUse1`` 键。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Inherit the ``DeviceGuardImplInterface`` class to implement the various "
"general methods corresponding to the new backend."
msgstr "继承 ``DeviceGuardImplInterface`` 类以实现与新后端对应的各种通用方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Call ``C10_REGISTER_GUARD_IMPL`` macro to complete dynamic registration."
msgstr "调用 ``C10_REGISTER_GUARD_IMPL`` 宏以完成动态注册。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Register serialization and deserialization functions for new backend "
"metadata"
msgstr "为新的后端元数据注册序列化和反序列化功能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch is currently able to dynamically register "
"serialization/deserialization functions to support the serialization and "
"deserialization of new backend additional metadata named ``backend_meta_`` "
"in class ``TensorImpl.ExtraMeta``. You can refer to the following steps:"
msgstr ""
"PyTorch 目前能够动态注册序列化/反序列化功能，以支持类 ``TensorImpl.ExtraMeta`` 中名为 "
"``backend_meta_`` 的新的后端附加元数据的序列化和反序列化。您可以参考以下步骤："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Inherit the ``BackendMeta`` class to implement ``CustomBackendMetadata`` "
"corresponding to the new backend and various fields of the new backend can "
"be customized in the class."
msgstr ""
"继承 ``BackendMeta`` 类以实现与新后端对应的 ``CustomBackendMetadata``，并且可以在类中自定义新后端的各个字段。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Implement the serialization and deserialization functions of the new "
"backend, the function signatures are ``void(const at::Tensor&, "
"std::unordered_map<std::string, bool>&)``."
msgstr ""
"实现新后端的序列化和反序列化功能，函数签名为 ``void(const at::Tensor&, "
"std::unordered_map<std::string, bool>&)``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Call the ``TensorBackendMetaRegistry`` macro to complete dynamic "
"registration."
msgstr "调用 ``TensorBackendMetaRegistry`` 宏以完成动态注册。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Other Modules"
msgstr "其他模块"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In addition to the above-mentioned parts, there are some other modules that "
"can be expanded through ``PrivateUse1``, such as ``distributed collective "
"communication``, ``benchmark timer``, and others, which will be added in the"
" future. One example about ``PrivateUse1`` integration is `Ascend NPU "
"<https://github.com/ascend/pytorch>`_."
msgstr ""
"除了上述部分以外，还有一些其他模块可以通过 ``PrivateUse1`` 扩展，例如 ``分布式集体通信``、``基准计时器`` "
"等，这些模块将在未来添加。关于 ``PrivateUse1`` 集成的一个例子是 `Ascend NPU "
"<https://github.com/ascend/pytorch>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to Improve User Experience with Privateuse1"
msgstr "如何利用 PrivateUse1 提升用户体验"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The primary goal of integrating new devices through ``PrivateUse1`` is to "
"meet the basic functional requirements, and the next thing to do is to "
"improve usability, which mainly involves the following aspects."
msgstr "通过 ``PrivateUse1`` 集成新的设备的主要目标是满足基本功能需求，接下来的任务是提升可用性，这主要涉及以下几个方面。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register new backend module to Pytorch."
msgstr "将新的后端模块注册到 PyTorch。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Rename PrivateUse1 to a custom name for the new backend."
msgstr "将 PrivateUse1 重命名为新后端的自定义名称。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Generate methods and properties related to the new backend."
msgstr "生成与新后端相关的方法和属性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Register new backend module to Pytorch"
msgstr "将新的后端模块注册到 PyTorch"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Some CUDA-related interfaces in PyTorch can be called through the following "
"form: ``torch.cuda.xxx``. Therefore, in order to comply with user habits, "
"the new backend implemented through the ``PrivateUse1`` mechanism should "
"also provide similar interfaces."
msgstr ""
"在 PyTorch 中，某些 CUDA 相关接口可以通过以下形式调用：``torch.cuda.xxx``。因此，为符合用户习惯，应该为通过 "
"``PrivateUse1`` 机制实现的新后端也提供类似的接口。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "For example, using ``Ascend NPU``:"
msgstr "例如，使用 ``Ascend NPU``:"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After doing the above operations, users can call some exclusive APIs of "
"``Ascend NPU`` through ``torch.npu.xxx``"
msgstr "执行上述操作后，用户可以通过 ``torch.npu.xxx`` 调用 ``Ascend NPU`` 的某些专属 API。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Rename PrivateUse1 to a custom name for the new backend"
msgstr "将 PrivateUse1 重命名为新后端的自定义名称"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``PrivateUse1`` Key is the internal mechanism of the new backend integrated "
"into PyTorch. For users, compared with ``PrivateUse1``, the custom name "
"strongly related to the new backend should be more friendly."
msgstr ""
"``PrivateUse1`` 键是集成到 PyTorch 的新后端的内部机制。对于用户而言，相较于 "
"``PrivateUse1``，与新后端强相关的自定义名称会更友好。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Taking the ``Ascend NPU`` as an example, the first usage will be more user-"
"friendly."
msgstr "以 ``Ascend NPU`` 为例，第一种使用方式会更符合用户友好性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, PyTorch provides a new C++/Python API for the self-named "
"``PrivateUse1`` backend, which is very simple to use."
msgstr "现在，PyTorch 提供了一个简单易用的新 C++/Python API，用于自行命名 ``PrivateUse1`` 后端。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Generate methods and properties related to the new backend"
msgstr "生成与新后端相关的方法和属性"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After renaming ``PrivateUse1`` to a custome name, automatically generate "
"properties and methods related to the new backend name in the ``Tensor, nn, "
"Storage`` modules for the new backend."
msgstr ""
"在将 ``PrivateUse1`` 重命名为自定义名称后，可以在 ``Tensor, nn, Storage`` "
"模块中为新后端自动生成与新后端名称相关的属性和方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Here is an example for ``Ascend NPU``:"
msgstr "以下是关于 ``Ascend NPU`` 的一个示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Then, you can use the following methods and properties:"
msgstr "然后，您可以使用以下方法和属性："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The improvement of the ``PrivateUse1`` mechanism is still in progress, so "
"the integration method of ``PrivateUse1`` of the new module will be added in"
" turn. Here are a few items that we are actively working on:"
msgstr ""
"``PrivateUse1`` 机制的改进仍在进行中，因此新模块的``PrivateUse1`` 集成方法将依次添加。以下是我们正在积极开展的工作项目："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Add the integration method of ``distributed collective communication``."
msgstr "添加 ``分布式集体通信`` 的集成方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Add the integration method of ``benchmark timer``."
msgstr "添加 ``基准计时器`` 的集成方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial walked you through the process of integrating new backends "
"into PyTorch via ``PrivateUse1``, including but not limited to operator "
"registration, generator registration, device guard registration, and so on. "
"At the same time, some methods are introduced to improve the user "
"experience."
msgstr ""
"本教程向您介绍了通过 ``PrivateUse1`` 集成新后端到 PyTorch "
"的过程，包括但不限于算子注册、生成器注册、设备保护注册等。同时介绍了一些方法以提升用户体验。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_python_custom_ops.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_advanced_python_custom_ops.py>` 下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Custom Python Operators"
msgstr "自定义 Python 算子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "How to integrate custom operators written in Python with PyTorch"
msgstr "如何将用 Python 编写的自定义算子集成到 PyTorch 中"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"PyTorch offers a large library of operators that work on Tensors (e.g. "
"``torch.add``, ``torch.sum``, etc). However, you might wish to use a new "
"customized operator with PyTorch, perhaps written by a third-party library. "
"This tutorial shows how to wrap Python functions so that they behave like "
"PyTorch native operators. Reasons why you may wish to create a custom "
"operator in PyTorch include:"
msgstr ""
"PyTorch 提供了一个非常丰富的算子库，这些算子可用于 Tensor（例如 ``torch.add``、``torch.sum`` "
"等）。然而，您可能希望在 PyTorch 中使用一个由第三方库编写的新自定义算子。本教程展示了如何封装 Python 函数，使其行为类似 PyTorch"
" 原生算子。您可能希望在 PyTorch 中创建自定义算子的原因包括："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Treating an arbitrary Python function as an opaque callable with respect to "
"``torch.compile`` (that is, prevent ``torch.compile`` from tracing into the "
"function)."
msgstr ""
"将任意 Python 函数作为 ``torch.compile`` 的不透明可调用（即防止 ``torch.compile`` 跟踪函数内部）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Adding training support to an arbitrary Python function"
msgstr "为任意 Python 函数添加训练支持。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use :func:`torch.library.custom_op` to create Python custom operators. Use "
"the C++ ``TORCH_LIBRARY`` APIs to create C++ custom operators (these work in"
" Python-less environments). See the `Custom Operators Landing Page "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html>`_ for "
"more details."
msgstr ""
"使用 :func:`torch.library.custom_op` 创建 Python 自定义算子。在无 Python 环境中，可以使用 C++ "
"``TORCH_LIBRARY`` API 来创建 C++ 自定义算子。有关更多详情，请参见 `自定义算子简介 "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Please note that if your operation can be expressed as a composition of "
"existing PyTorch operators, then there is usually no need to use the custom "
"operator API -- everything (for example ``torch.compile``, training support)"
" should just work."
msgstr ""
"请注意，如果您的操作可以用现有 PyTorch 算子的组合来表达，则通常无需使用自定义算子 API —— 所有内容（例如 "
"``torch.compile``、训练支持）应该都能正常工作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Example: Wrapping PIL's crop into a custom operator"
msgstr "示例：将 PIL 的 crop 封装为一个自定义算子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let's say that we are using PIL's ``crop`` operation."
msgstr "假设我们正在使用 PIL 的 ``crop`` 操作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``crop`` is not handled effectively out-of-the-box by ``torch.compile``: "
"``torch.compile`` induces a `\"graph break\" "
"<https://pytorch.org/docs/stable/torch.compiler_faq.html#graph-breaks>`_ on "
"functions it is unable to handle and graph breaks are bad for performance. "
"The following code demonstrates this by raising an error (``torch.compile`` "
"with ``fullgraph=True`` raises an error if a graph break occurs)."
msgstr ""
"``crop`` 无法开箱即用地被 ``torch.compile`` 处理：当它无法处理时，``torch.compile`` 导致 `\"图分裂\""
" <https://pytorch.org/docs/stable/torch.compiler_faq.html#graph-"
"breaks>`_，而图分裂对性能不利。以下代码通过报错来展示这一点（``fullgraph=True`` 时 ``torch.compile`` "
"引发错误，如果发生图分裂）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In order to black-box ``crop`` for use with ``torch.compile``, we need to do"
" two things:"
msgstr "为了将 ``crop`` 黑箱化并与 ``torch.compile`` 一起使用，我们需要做两件事："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "wrap the function into a PyTorch custom operator."
msgstr "将函数封装为一个 PyTorch 自定义算子。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"add a \"``FakeTensor`` kernel\" (aka \"meta kernel\") to the operator. Given"
" some ``FakeTensors`` inputs (dummy Tensors that don't have storage), this "
"function should return dummy Tensors of your choice with the correct Tensor "
"metadata (shape/strides/``dtype``/device)."
msgstr ""
"为算子添加一个 \"``FakeTensor`` 内核\"（也称为 \"元内核\"）。给定一些 ``FakeTensors`` 输入（不包含存储的虚拟 "
"Tensor），该函数应该返回具有正确 Tensor 元数据（形状/步幅/``dtype``/设备）的虚拟 Tensor。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "After this, ``crop`` now works without graph breaks:"
msgstr "完成后，``crop`` 现在可以无图分裂地工作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Adding training support for crop"
msgstr "为 crop 添加训练支持"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use ``torch.library.register_autograd`` to add training support for an "
"operator. Prefer this over directly using ``torch.autograd.Function``; some "
"compositions of ``autograd.Function`` with PyTorch operator registration "
"APIs can lead to (and has led to) silent incorrectness when composed with "
"``torch.compile``."
msgstr ""
"使用 ``torch.library.register_autograd`` 为算子添加训练支持。优先使用此方法而非直接用 "
"``torch.autograd.Function``；有些情况与 PyTorch 算子注册 API 组合可能导致（并且确实导致）与 "
"``torch.compile`` 组合时发生静默错误。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you don't need training support, there is no need to use "
"``torch.library.register_autograd``. If you end up training with a "
"``custom_op`` that doesn't have an autograd registration, we'll raise an "
"error message."
msgstr ""
"如果您不需要训练支持，则无需使用 "
"``torch.library.register_autograd``。如果您最终用一个``custom_op``进行训练，而它没有 autograd "
"注册，我们会抛出一条错误消息。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The gradient formula for ``crop`` is essentially ``PIL.paste`` (we'll leave "
"the derivation as an exercise to the reader). Let's first wrap ``paste`` "
"into a custom operator:"
msgstr ""
"``crop`` 的梯度公式本质上是 ``PIL.paste``（推导过程留给读者练习）。首先将 ``paste`` 封装为一个自定义算子："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"And now let's use ``register_autograd`` to specify the gradient formula for "
"``crop``:"
msgstr "然后使用 ``register_autograd`` 为 ``crop`` 指定梯度公式："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that the backward must be a composition of PyTorch-understood "
"operators, which is why we wrapped paste into a custom operator instead of "
"directly using PIL's paste."
msgstr "注意反向必须是 PyTorch 可理解操作的组合，因此我们将 paste 封装为一个自定义算子，而不是直接使用 PIL 的 paste。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This is the correct gradient, with 1s (white) in the cropped region and 0s "
"(black) in the unused region."
msgstr "这是正确的梯度，裁剪区域为 1（白色），未使用的区域为 0（黑色）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Testing Python Custom operators"
msgstr "测试 Python 自定义算子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use ``torch.library.opcheck`` to test that the custom operator was "
"registered correctly. This does not test that the gradients are "
"mathematically correct; please write separate tests for that (either manual "
"ones or ``torch.autograd.gradcheck``)."
msgstr ""
"使用 ``torch.library.opcheck`` 测试自定义算子是否注册正确。这不会测试梯度是否数学上正确；请单独编写测试（可以是手动测试或 "
"``torch.autograd.gradcheck``）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To use ``opcheck``, pass it a set of example inputs to test against. If your"
" operator supports training, then the examples should include Tensors that "
"require grad. If your operator supports multiple devices, then the examples "
"should include Tensors from each device."
msgstr ""
"使用 ``opcheck`` 时，传入一组示例输入以进行测试。如果您的算子支持训练，则示例如中应该包含需要梯度的 "
"Tensor。如果算子支持多设备，则示例应该包括来自每个设备的 Tensor。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Mutable Python Custom operators"
msgstr "可变 Python 自定义算子"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can also wrap a Python function that mutates its inputs into a custom "
"operator. Functions that mutate inputs are common because that is how many "
"low-level kernels are written; for example, a kernel that computes ``sin`` "
"may take in the input and an output tensor and write ``input.sin()`` to the "
"output tensor."
msgstr ""
"您还可以将一个会改变其输入的 Python 函数封装为自定义算子。这种会改变输入的函数很常见，因为这是许多底层内核的编写方式；例如，一个计算 "
"``sin`` 的内核可能会接收输入 Tensor 和输出 Tensor，并将 ``input.sin()`` 写入输出 Tensor。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We'll use ``numpy.sin`` to demonstrate an example of a mutable Python custom"
" operator."
msgstr "我们用 ``numpy.sin`` 举例说明一个可变 Python 自定义算子。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Because the operator doesn't return anything, there is no need to register a"
" ``FakeTensor`` kernel (meta kernel) to get it to work with "
"``torch.compile``."
msgstr ""
"由于该算子不返回任何内容，因此无须为其注册 ``FakeTensor`` 内核（元内核）以使其与 ``torch.compile`` 一起工作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"And here's an ``opcheck`` run telling us that we did indeed register the "
"operator correctly. ``opcheck`` would error out if we forgot to add the "
"output to ``mutates_args``, for example."
msgstr ""
"以下是一个 ``opcheck`` 运行，告诉我们确实正确地注册了算子。例如，如果我们忘记将输出包含到 ``mutates_args`` 中，则 "
"``opcheck`` 会抛出错误。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we learned how to use ``torch.library.custom_op`` to "
"create a custom operator in Python that works with PyTorch subsystems such "
"as ``torch.compile`` and autograd."
msgstr ""
"在本教程中，我们学习了如何使用 ``torch.library.custom_op`` 在 Python 中创建一个自定义算子，使其可以与 "
"PyTorch 子系统（例如 ``torch.compile`` 和autograd）一起工作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial provides a basic introduction to custom operators. For more "
"detailed information, see:"
msgstr "本教程提供了自定义算子基本介绍。更详细的信息，请参见："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`the torch.library documentation "
"<https://pytorch.org/docs/stable/library.html>`_"
msgstr "`torch.library 文档 <https://pytorch.org/docs/stable/library.html>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`the Custom Operators Manual "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html#the-"
"custom-operators-manual>`_"
msgstr ""
"`自定义算子手册 "
"<https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html#the-"
"custom-operators-manual>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: python_custom_ops.py "
"<python_custom_ops.py>`"
msgstr ":download:`下载 Python 源代码：python_custom_ops.py <python_custom_ops.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: python_custom_ops.ipynb "
"<python_custom_ops.ipynb>`"
msgstr ""
":download:`下载 Jupyter 笔记本：python_custom_ops.ipynb <python_custom_ops.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Combining Distributed DataParallel with Distributed RPC Framework"
msgstr "结合分布式数据并行与分布式 RPC 框架"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Authors**: `Pritam Damania <https://github.com/pritamdamania87>`_ and `Yi "
"Wang <https://github.com/wayi1>`_"
msgstr ""
"**作者**：`Pritam Damania <https://github.com/pritamdamania87>`_ 和 `Yi Wang "
"<https://github.com/wayi1>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"|edit| View and edit this tutorial in `github "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst>`__."
msgstr ""
"| 编辑 | 在 `github "
"<https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst>`__"
" 中查看和编辑此教程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial uses a simple example to demonstrate how you can combine "
"`DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__"
" (DDP) with the `Distributed RPC framework "
"<https://pytorch.org/docs/master/rpc.html>`__ to combine distributed data "
"parallelism with distributed model parallelism to train a simple model. "
"Source code of the example can be found `here "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc>`__."
msgstr ""
"本教程使用一个简单的示例演示如何结合 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__"
" (DDP) 与 `分布式 RPC 框架 "
"<https://pytorch.org/docs/master/rpc.html>`__，将分布式数据并行与分布式模型并行结合，用于训练一个简单模型。示例的源代码可以在"
" `这里 "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc>`__"
" 找到。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Previous tutorials, `Getting Started With Distributed Data Parallel "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`__ and "
"`Getting Started with Distributed RPC Framework "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__, described"
" how to perform distributed data parallel and distributed model parallel "
"training respectively. Although, there are several training paradigms where "
"you might want to combine these two techniques. For example:"
msgstr ""
"之前的教程，`入门分布式数据并行 "
"<https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`__ 和 "
"`入门分布式RPC框架 "
"<https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__，分别介绍了如何进行分布式数据并行和分布式模型并行训练。然而，在某些训练范式中，可能需要结合这两种技术。例如："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we have a model with a sparse part (large embedding table) and a dense "
"part (FC layers), we might want to put the embedding table on a parameter "
"server and replicate the FC layer across multiple trainers using "
"`DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__."
" The `Distributed RPC framework "
"<https://pytorch.org/docs/master/rpc.html>`__ can be used to perform "
"embedding lookups on the parameter server."
msgstr ""
"如果我们有一个包含稀疏部分（大型嵌入表）和密集部分（全连接层）的模型，我们可能希望将嵌入表放在参数服务器上，并使用 "
"`DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__"
" 在多个训练器之间复制全连接层。`分布式RPC框架 <https://pytorch.org/docs/master/rpc.html>`__ "
"可用于对参数服务器上的嵌入表进行查找。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Enable hybrid parallelism as described in the `PipeDream "
"<https://arxiv.org/abs/1806.03377>`__ paper. We can use the `Distributed RPC"
" framework <https://pytorch.org/docs/master/rpc.html>`__ to pipeline stages "
"of the model across multiple workers and replicate each stage (if needed) "
"using `DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__."
msgstr ""
"启用如 `PipeDream <https://arxiv.org/abs/1806.03377>`__ 论文中所述的混合并行机制。我们可以使用 "
"`分布式RPC框架 <https://pytorch.org/docs/master/rpc.html>`__ "
"在多个工作者之间创建模型阶段的流水线，并在需要时使用 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__"
" 复制每一个阶段。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial we will cover case 1 mentioned above. We have a total of 4 "
"workers in our setup as follows:"
msgstr "在本教程中，我们将深入探讨上述的案例1。我们在设置中总共有4个工作者，如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"1 Master, which is responsible for creating an embedding table "
"(nn.EmbeddingBag) on the parameter server. The master also drives the "
"training loop on the two trainers."
msgstr "1个Master，负责在参数服务器上创建嵌入表（nn.EmbeddingBag）。Master还负责驱动两个训练器上的训练循环。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"1 Parameter Server, which basically holds the embedding table in memory and "
"responds to RPCs from the Master and Trainers."
msgstr "1个参数服务器，主要负责将嵌入表保存在内存中，并回应Master和训练器的RPC请求。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"2 Trainers, which store an FC layer (nn.Linear) which is replicated amongst "
"themselves using `DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__."
" The trainers are also responsible for executing the forward pass, backward "
"pass and optimizer step."
msgstr ""
"2个训练器，存储一个全连接层（nn.Linear），使用 `DistributedDataParallel "
"<https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel>`__"
" 在训练器之间进行复制。这些训练器还负责执行前向传播、反向传播和优化器步骤。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The entire training process is executed as follows:"
msgstr "整个训练过程按以下方式运行："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The master creates a `RemoteModule "
"<https://pytorch.org/docs/master/rpc.html#remotemodule>`__ that holds an "
"embedding table on the Parameter Server."
msgstr ""
"Master在参数服务器上创建了一个包含嵌入表的 `RemoteModule "
"<https://pytorch.org/docs/master/rpc.html#remotemodule>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The master, then kicks off the training loop on the trainers and passes the "
"remote module to the trainers."
msgstr "然后，Master启动了训练器上的训练循环，并将远程模块传递给训练器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The trainers create a ``HybridModel`` which first performs an embedding "
"lookup using the remote module provided by the master and then executes the "
"FC layer which is wrapped inside DDP."
msgstr "训练器创建了一个 ``HybridModel``，首先使用Master提供的远程模块执行嵌入查找，然后执行包含在DDP中的全连接层。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The trainer executes the forward pass of the model and uses the loss to "
"execute the backward pass using `Distributed Autograd "
"<https://pytorch.org/docs/master/rpc.html#distributed-autograd-"
"framework>`__."
msgstr ""
"训练器执行模型的前向传播，并利用损失执行反向传播，使用 `Distributed Autograd "
"<https://pytorch.org/docs/master/rpc.html#distributed-autograd-"
"framework>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As part of the backward pass, the gradients for the FC layer are computed "
"first and synced to all trainers via allreduce in DDP."
msgstr "在反向传播过程中，首先计算全连接层的梯度，并通过DDP的allreduce机制同步到所有训练器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, Distributed Autograd propagates the gradients to the parameter server,"
" where the gradients for the embedding table are updated."
msgstr "接下来，Distributed Autograd将梯度传播到参数服务器，在那里更新嵌入表的梯度。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, the `Distributed Optimizer "
"<https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim>`__"
" is used to update all the parameters."
msgstr ""
"最后，使用 `分布式优化器 <https://pytorch.org/docs/master/rpc.html#module-"
"torch.distributed.optim>`__ 更新所有参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You should always use `Distributed Autograd "
"<https://pytorch.org/docs/master/rpc.html#distributed-autograd-framework>`__"
" for the backward pass if you're combining DDP and RPC."
msgstr ""
"如果你将DDP与RPC结合使用，反向传播始终应该使用 `Distributed Autograd "
"<https://pytorch.org/docs/master/rpc.html#distributed-autograd-"
"framework>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, let's go through each part in detail. Firstly, we need to setup all of "
"our workers before we can perform any training. We create 4 processes such "
"that ranks 0 and 1 are our trainers, rank 2 is the master and rank 3 is the "
"parameter server."
msgstr ""
"现在，我们详细讨论每个部分。首先，在进行任何训练之前，我们需要设置所有工作者。我们创建了4个进程，其中rank 0和1是训练器，rank "
"2是Master，rank 3是参数服务器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We initialize the RPC framework on all 4 workers using the TCP init_method. "
"Once RPC initialization is done, the master creates a remote module that "
"holds an `EmbeddingBag "
"<https://pytorch.org/docs/master/generated/torch.nn.EmbeddingBag.html>`__ "
"layer on the Parameter Server using `RemoteModule "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule>`__."
" The master then loops through each trainer and kicks off the training loop "
"by calling ``_run_trainer`` on each trainer using `rpc_async "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.rpc_async>`__."
" Finally, the master waits for all training to finish before exiting."
msgstr ""
"我们使用TCP初始化方法在所有4个工作者上初始化RPC框架。一旦RPC初始化完成，Master创建了一个包含在参数服务器上的 `EmbeddingBag"
" <https://pytorch.org/docs/master/generated/torch.nn.EmbeddingBag.html>`__ "
"层的远程模块 (RemoteModule)。然后，Master通过对每个训练器使用 `rpc_async "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.rpc.rpc_async>`__"
" 调用``_run_trainer``方法，逐一启动训练循环。最后，Master等待所有训练完成后退出。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The trainers first initialize a ``ProcessGroup`` for DDP with world_size=2 "
"(for two trainers) using `init_process_group "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group>`__."
" Next, they initialize the RPC framework using the TCP init_method. Note "
"that the ports are different in RPC initialization and ProcessGroup "
"initialization. This is to avoid port conflicts between initialization of "
"both frameworks. Once the initialization is done, the trainers just wait for"
" the ``_run_trainer`` RPC from the master."
msgstr ""
"训练器首先使用 `init_process_group "
"<https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group>`__"
" 为DDP初始化一个 "
"``ProcessGroup``，其world_size为2（即两个训练器）。接下来，他们使用TCP初始化方法初始化RPC框架。请注意RPC初始化和ProcessGroup初始化中的端口不同，这是为了避免两个框架初始化过程中出现端口冲突。一旦初始化完成，训练器仅需等待来自Master的``_run_trainer``"
" RPC请求。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The parameter server just initializes the RPC framework and waits for RPCs "
"from the trainers and master."
msgstr "参数服务器仅需初始化RPC框架，并等待来自训练器和Master的RPC请求。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Before we discuss details of the Trainer, let's introduce the "
"``HybridModel`` that the trainer uses. As described below, the "
"``HybridModel`` is initialized using a remote module that holds an embedding"
" table (``remote_emb_module``) on the parameter server and the ``device`` to"
" use for DDP. The initialization of the model wraps an `nn.Linear "
"<https://pytorch.org/docs/master/generated/torch.nn.Linear.html>`__ layer "
"inside DDP to replicate and synchronize this layer across all trainers."
msgstr ""
"在讨论训练器的细节之前，我们先介绍训练器使用的 ``HybridModel``。如下面所述，``HybridModel`` "
"的初始化需要一个保存嵌入表（``remote_emb_module``）的远程模块，以及用于DDP的``设备``。模型的初始化将一个 "
"`nn.Linear "
"<https://pytorch.org/docs/master/generated/torch.nn.Linear.html>`__ "
"层封装在DDP中，以复制和同步该层到所有训练器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The forward method of the model is pretty straightforward. It performs an "
"embedding lookup on the parameter server using RemoteModule's ``forward`` "
"and passes its output onto the FC layer."
msgstr "模型的forward方法相对简单。它使用远程模块的``forward``在参数服务器上执行嵌入查找，并将其输出传递给全连接层。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, let's look at the setup on the Trainer. The trainer first creates the "
"``HybridModel`` described above using a remote module that holds the "
"embedding table on the parameter server and its own rank."
msgstr ""
"接下来，我们来看训练器的设置。训练器首先利用远程模块创建上述的 ``HybridModel``，该模块保存参数服务器上的嵌入表，以及自身的rank。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, we need to retrieve a list of RRefs to all the parameters that we would"
" like to optimize with `DistributedOptimizer "
"<https://pytorch.org/docs/master/rpc.html#module-"
"torch.distributed.optim>`__. To retrieve the parameters for the embedding "
"table from the parameter server, we can call RemoteModule's "
"`remote_parameters "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters>`__,"
" which basically walks through all the parameters for the embedding table "
"and returns a list of RRefs. The trainer calls this method on the parameter "
"server via RPC to receive a list of RRefs to the desired parameters. Since "
"the DistributedOptimizer always takes a list of RRefs to parameters that "
"need to be optimized, we need to create RRefs even for the local parameters "
"for our FC layers. This is done by walking ``model.fc.parameters()``, "
"creating an RRef for each parameter and appending it to the list returned "
"from ``remote_parameters()``. Note that we cannnot use "
"``model.parameters()``, because it will recursively call "
"``model.remote_emb_module.parameters()``, which is not supported by "
"``RemoteModule``."
msgstr ""
"现在，我们需要通过 `DistributedOptimizer "
"<https://pytorch.org/docs/master/rpc.html#module-torch.distributed.optim>`__"
" 检索需要优化的所有参数的RRef列表。要从参数服务器检索嵌入表的参数，我们可以调用远程模块的`remote_parameters "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters>`__，基本上遍历嵌入表的所有参数并返回一个RRef列表。训练器通过RPC调用该方法，从参数服务器接收需要的参数的RRef列表。由于分布式优化器总是需要优化参数的RRef列表，我们需要为局部的全连接层参数创建RRef。这是通过遍历``model.fc.parameters()``、为每个参数创建RRef并将其附加到``remote_parameters()``返回的列表中完成的。请注意，我们不能使用``model.parameters()``，因为它会递归调用``model.remote_emb_module.parameters()``，这不被``RemoteModule``支持。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, we create our DistributedOptimizer using all the RRefs and define a"
" CrossEntropyLoss function."
msgstr "最后，我们使用所有的RRef创建分布式优化器，并定义了一个交叉熵损失函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now we're ready to introduce the main training loop that is run on each "
"trainer. ``get_next_batch`` is just a helper function to generate random "
"inputs and targets for training. We run the training loop for multiple "
"epochs and for each batch:"
msgstr ""
"现在我们介绍在每个训练器上运行的主要训练循环。``get_next_batch`` "
"只是一个生成随机输入和目标用于训练的辅助函数。我们运行多次epoch的训练循环，对于每一个批次："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Setup a `Distributed Autograd Context "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.autograd.context>`__"
" for Distributed Autograd."
msgstr ""
"设置分布式自动微分的 `Distributed Autograd Context "
"<https://pytorch.org/docs/master/rpc.html#torch.distributed.autograd.context>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Run the forward pass of the model and retrieve its output."
msgstr "运行模型的forward传播并获取其输出。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Compute the loss based on our outputs and targets using the loss function."
msgstr "使用损失函数根据输出和目标计算损失。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use Distributed Autograd to execute a distributed backward pass using the "
"loss."
msgstr "使用分布式自动微分通过损失执行分布式反向传播。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, run a Distributed Optimizer step to optimize all the parameters."
msgstr "最后，运行分布式优化器步骤以优化所有参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Source code for the entire example can be found `here "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc>`__."
msgstr ""
"整个示例的源代码可以在 `此处 "
"<https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc>`__"
" 找到。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_semi_structured_sparse.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`此处 <sphx_glr_download_advanced_semi_structured_sparse.py>` 下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "(beta) Accelerating BERT with semi-structured (2:4) sparsity"
msgstr "(测试版) 使用半结构化（2:4）稀疏加速BERT"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `Jesse Cai <https://github.com/jcaip>`_"
msgstr "**作者**: `Jesse Cai <https://github.com/jcaip>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Like other forms of sparsity, **semi-structured sparsity** is a model "
"optimization technique that seeks to reduce the memory overhead and latency "
"of a neural network at the expense of some model accuracy. It is also known "
"as **fine-grained structured sparsity** or **2:4 structured sparsity**."
msgstr ""
"与其他形式的稀疏性类似，**半结构化稀疏**是一种模型优化技术，其目的是在提升模型效率的同时，以牺牲一定的模型准确率为代价降低神经网络的内存开销和延迟。它也被称为**精细粒度结构化稀疏**或**2:4结构化稀疏**。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Semi-structured sparsity derives its name from its unique sparsity pattern, "
"where n out of every 2n elements are pruned. We most often see n=2, hence "
"2:4 sparsity Semi-structured sparsity is particularly interesting because it"
" can be efficiently accelerated on GPUs and doesn’t degrade model accuracy "
"as much as other sparsity patterns."
msgstr ""
"半结构化稀疏因其独特的稀疏模式而得名，其中每2n个元素中有n个被剪枝。最常见的是n=2，即2:4稀疏。半结构化稀疏之所以特别有趣，是因为它可以在GPU上高效加速，并且其对模型准确率的影响比其他稀疏模式更小。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"With the introduction of `semi-structured sparsity support "
"<https://pytorch.org/docs/2.1/sparse.html#sparse-semi-structured-tensors>`_,"
" it is possible to prune and accelerate a semi-structured sparse model "
"without leaving PyTorch. We will explain this process in this tutorial."
msgstr ""
"随着对 `半结构化稀疏支持 <https://pytorch.org/docs/2.1/sparse.html#sparse-semi-"
"structured-tensors>`_ 的引入，现在可以在完全使用PyTorch的情况下剪枝并加速半结构化稀疏模型。在本教程中，我们将解释这一过程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"By the end of this tutorial, we will have sparsified a BERT question-"
"answering model to be 2:4 sparse, fine-tuning it to recover nearly all F1 "
"loss (86.92 dense vs 86.48 sparse). Finally, we will accelerate this 2:4 "
"sparse model for inference, yielding a 1.3x speedup."
msgstr ""
"在本教程结束时，我们将把一个BERT问答模型稀疏化为2:4稀疏，对其进行微调以恢复几乎所有F1损失（86.92稠密模型对比86.48稀疏模型）。最后，我们将加速该2:4稀疏模型进行推断，实现1.3倍速度提升。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "PyTorch >= 2.1."
msgstr "PyTorch >= 2.1。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A NVIDIA GPU with semi-structured sparsity support (Compute Capability "
"8.0+)."
msgstr "支持半结构化稀疏的NVIDIA GPU（计算能力8.0+）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial is designed for beginners to semi-structured sparsity and "
"sparsity in general. For users with existing 2:4 sparse models, accelerating"
" ``nn.Linear`` layers for inference with ``to_sparse_semi_structured`` is "
"quite straightforward. Here is an example:"
msgstr ""
"本教程面向半结构化稀疏和稀疏性的初学者设计。对于已有2:4稀疏模型的用户，用``to_sparse_semi_structured``加速``nn.Linear``层进行推断非常简单。以下是一个示例："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "What problem does semi-structured sparsity solve?"
msgstr "半结构化稀疏解决了什么问题？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The general motivation behind sparsity is simple: if there are zeros in your"
" network, you can optimize efficiency by not storing or computing those "
"parameters. However, the specifics of sparsity are tricky. Zeroing out "
"parameters doesn’t affect the latency / memory overhead of our model out of "
"the box."
msgstr ""
"稀疏性的基本动机很简单：如果网络中存在零值，则通过不存储或计算这些参数来优化效率。然而，稀疏性的具体细节非常棘手。只对参数置零本身并不会显著减少模型的延迟或内存开销。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This is because the dense tensor still contains the pruned (zero) elements, "
"which the dense matrix multiplication kernel will still operate on this "
"elements. In order to realize performance gains, we need to swap out dense "
"kernels for sparse kernels, which skip calculation involving pruned "
"elements."
msgstr ""
"这是因为稠密张量仍然包含剪枝（零值）元素，稠密矩阵乘法内核仍会在这些元素上运算。为了实现性能提升，我们需要将稠密内核换成稀疏内核，这些稀疏内核会跳过与剪枝元素相关的计算。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To do this, these kernels work on sparse matrices, which do not store the "
"pruned elements and store the specified elements in a compressed format."
msgstr "为此，这些内核处理稀疏矩阵，稀疏矩阵不存储剪枝元素，而以压缩格式存储指定的元素。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For semi-structured sparsity, we store exactly half of the original "
"parameters along with some compressed metadata about how the elements were "
"arranged."
msgstr "对于半结构化稀疏性，我们存储了原始参数的一半，以及一些关于元素排列的压缩元数据。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There are many different sparse layouts, each with their own benefits and "
"drawbacks. The 2:4 semi-structured sparse layout is particularly interesting"
" for two reasons:"
msgstr "有许多不同的稀疏布局，每种布局都有其自身的优点和缺点。2:4半结构化稀疏布局特别引人注目有两个原因："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Unlike previous sparse formats, semi-structured sparsity was designed to be "
"efficiently accelerated on GPUs. In 2020, NVIDIA introduced hardware support"
" for semi-structured sparsity with their Ampere architecture, and have also "
"released fast sparse kernels via CUTLASS `cuSPARSELt "
"<https://docs.nvidia.com/cuda/cusparselt/index.html>`__."
msgstr ""
"与之前的稀疏格式不同，半结构化稀疏性设计为能够在GPU上高效加速。2020年，NVIDIA通过其Ampere架构引入了对半结构化稀疏性的硬件支持，并通过CUTLASS发布了快速稀疏内核"
" `cuSPARSELt <https://docs.nvidia.com/cuda/cusparselt/index.html>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"At the same time, semi-structured sparsity tends to have a milder impact on "
"model accuracy compared to other sparse formats, especially when accounting "
"for more advanced pruning / fine-tuning methods. NVIDIA has shown in their "
"`white paper <https://arxiv.org/abs/2104.08378>`_ that a simple paradigm of "
"magnitude pruning once to be 2:4 sparse and then retraining the model yields"
" nearly identical model accuracies."
msgstr ""
"同时，与其他稀疏格式相比，半结构化稀疏性对模型准确性的影响较小，特别是在考虑更高级的剪枝/微调方法时。NVIDIA在其 `白皮书 "
"<https://arxiv.org/abs/2104.08378>`_ "
"中显示，通过一次简单的幅度剪枝使模型达到2:4稀疏并重新训练模型后，可获得几乎相同的模型准确性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Semi-structured exists in a sweet spot, providing a 2x (theoretical) speedup"
" at a much lower sparsity level (50%), while still being granular enough to "
"preserve model accuracy."
msgstr "半结构化稀疏性处于甜蜜点，在一个较低的稀疏水平（50%）提供了2倍（理论上）加速，同时仍然具备足够的细粒度以保留模型准确性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Network"
msgstr "网络"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Data Set"
msgstr "数据集"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Metric"
msgstr "度量指标"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Dense FP16"
msgstr "Dense FP16"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Sparse FP16"
msgstr "Sparse FP16"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "ResNet-50"
msgstr "ResNet-50"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "ImageNet"
msgstr "ImageNet"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Top-1"
msgstr "Top-1"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "76.1"
msgstr "76.1"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "76.2"
msgstr "76.2"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "ResNeXt-101_32x8d"
msgstr "ResNeXt-101_32x8d"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "79.3"
msgstr "79.3"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Xception"
msgstr "Xception"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "79.2"
msgstr "79.2"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "SSD-RN50"
msgstr "SSD-RN50"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "COCO2017"
msgstr "COCO2017"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "bbAP"
msgstr "bbAP"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "24.8"
msgstr "24.8"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "MaskRCNN-RN50"
msgstr "MaskRCNN-RN50"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "37.9"
msgstr "37.9"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "FairSeq Transformer"
msgstr "FairSeq Transformer"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "EN-DE WMT14"
msgstr "EN-DE WMT14"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "BLEU"
msgstr "BLEU"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "28.2"
msgstr "28.2"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "28.5"
msgstr "28.5"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "BERT-Large"
msgstr "BERT-Large"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "SQuAD v1.1"
msgstr "SQuAD v1.1"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "F1"
msgstr "F1"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "91.9"
msgstr "91.9"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Semi-structured sparsity has an additional advantage from a workflow "
"perspective. Because the sparsity level is fixed at 50%, it is easier to "
"decompose the problem of sparsifying a model into two distinct subproblems:"
msgstr "从工作流程角度来看，半结构化稀疏性具有额外的优势。由于稀疏水平固定为50%，将模型稀疏化的问题分解为两个不同的子问题变得更加容易："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Accuracy - How can we find a set of 2:4 sparse weights that minimize the "
"accuracy degradation of our model?"
msgstr "准确性 - 我们如何找到一组2:4稀疏权重以最小化模型的准确性下降？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Performance - How can we accelerate our 2:4 sparse weights for inference and"
" reduced memory overhead?"
msgstr "性能 - 我们如何加速模型推理并减少内存开销的2:4稀疏权重？"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"\\begin{bmatrix}\n"
"   1 & 1 & 0 & 0 \\\\\n"
"   0 & 0 & 1 & 1 \\\\\n"
"   1 & 0 & 0 & 0 \\\\\n"
"   0 & 0 & 1 & 1 \\\\\n"
"   \\end{bmatrix}"
msgstr ""
"\\begin{bmatrix}\n"
"   1 & 1 & 0 & 0 \\\\\n"
"   0 & 0 & 1 & 1 \\\\\n"
"   1 & 0 & 0 & 0 \\\\\n"
"   0 & 0 & 1 & 1 \\\\\n"
"   \\end{bmatrix}"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The natural handoff point between these two problems are zeroed-out dense "
"tensors. Our inference solution is designed to compress and accelerate "
"tensors in this format. We anticipate many users coming up with custom "
"masking solution, as this is an active area of research."
msgstr ""
"这两个问题之间的自然交接点是被清零的密集张量。我们的推理解决方案旨在压缩并加速这种格式的张量。我们预计许多用户将提出定制的掩码解决方案，因为这是一个活跃的研究领域。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we’ve learned a little more about semi-structured sparsity, let’s "
"apply it to a BERT model trained on a question answering task, SQuAD."
msgstr "现在我们已经更多了解了半结构化稀疏性，让我们将它应用到一个训练在问答任务SQuAD上的BERT模型中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Intro & Setup"
msgstr "介绍与设置"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let’s start by importing all the packages we need."
msgstr "让我们先导入所有需要的包。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We’ll also need to define some helper functions that are specific to the "
"dataset / task at hand. These were adapted from `this "
"<https://huggingface.co/learn/nlp-course/chapter7/7?fw=pt>`__ Hugging Face "
"course as a reference."
msgstr ""
"我们还需要定义一些特定于数据集/任务的辅助函数。这些函数是从 `此处 <https://huggingface.co/learn/nlp-"
"course/chapter7/7?fw=pt>`__ 的Hugging Face课程中改编而来的参考。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that those are defined, we just need one additional helper function, "
"which will help us benchmark our model."
msgstr "既然定义完这些函数，我们只需要再定义一个辅助函数来帮助我们基准测试我们的模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We will get started by loading our model and tokenizer, and then setting up "
"our dataset."
msgstr "我们将从加载模型和标记器开始，然后设置数据集。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Establishing a baseline"
msgstr "建立基准"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we’ll train a quick baseline of our model on SQuAD. This task asks our"
" model to identify spans, or segments of text, in a given context (Wikipedia"
" articles) that answer a given question. Running the following code gives me"
" an F1 score of 86.9. This is quite close to the reported NVIDIA score and "
"the difference is likely due to BERT-base vs. BERT-large or fine-tuning "
"hyperparameters."
msgstr ""
"接下来，我们将在SQuAD上快速训练模型的基准。这项任务要求我们的模型在给定的上下文（维基百科文章）中识别回答某个问题的文本片段或段落。运行以下代码可以得到一个F1分数为86.9。这非常接近NVIDIA报告的分数，差异可能是由于BERT-"
"base与BERT-large或者微调超参数的不同。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Pruning BERT to be 2:4 sparse"
msgstr "将BERT剪枝为2:4稀疏"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have our baseline, it’s time we prune BERT. There are many "
"different pruning strategies, but one of the most common is **magnitude "
"pruning**, which seeks to remove the weights with the lowest L1 norm. "
"Magnitude pruning was used by NVIDIA in all their results and is a common "
"baseline."
msgstr ""
"现在我们有了基线，是时候剪枝BERT了。有许多不同的剪枝策略，但最常见的一种是**幅度剪枝**，它试图移除具有最低L1范数的权重。NVIDIA在所有结果中使用了幅度剪枝，这也是一种常见的基线。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To do this, we will use the ``torch.ao.pruning`` package, which contains a "
"weight-norm (magnitude) sparsifier. These sparsifiers work by applying mask "
"parametrizations to the weight tensors in a model. This lets them simulate "
"sparsity by masking out the pruned weights."
msgstr ""
"为此，我们将使用``torch.ao.pruning``包，该包包含一个权重范数（幅度）稀疏器。这些稀疏器通过对模型中的权重张量应用掩码参数化来工作。这使它们能够通过屏蔽剪枝权重来模拟稀疏性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We’ll also have to decide what layers of the model to apply sparsity to, "
"which in this case is all of the ``nn.Linear`` layers, except for the task-"
"specific head outputs. That’s because semi-structured sparsity has `shape "
"constraints <https://pytorch.org/docs/2.1/sparse.html#constructing-sparse-"
"semi-structured-tensors>`_, and the task-specific ``nn.Linear`` layers do "
"not satisfy them."
msgstr ""
"我们还需要决定将稀疏性应用于模型的哪些层，在这种情况下，是除了任务特定的头部输出之外的所有``nn.Linear``层。这是因为半结构化稀疏性具有`形状约束"
" <https://pytorch.org/docs/2.1/sparse.html#constructing-sparse-semi-"
"structured-tensors>`_，而任务特定的``nn.Linear``层不满足这些约束。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The first step for pruning the model is to insert parametrizations for "
"masking the weights of the model. This is done by the prepare step. Anytime "
"we try to access the ``.weight`` we will get ``mask * weight`` instead."
msgstr ""
"剪枝模型的第一步是插入掩码化参数化，用于掩盖模型权重。这通过准备步骤完成。任何时候我们尝试访问``.weight``，我们将获取``mask * "
"weight``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Then, we’ll take a single pruning step. All pruners implement a "
"``update_mask()`` method that updates the mask with the logic being "
"determined by the pruner implementation. The step method calls this "
"``update_mask`` functions for the weights specified in the sparse config."
msgstr ""
"然后，我们将进行单个剪枝步骤。所有剪枝器都实现了一个``update_mask()``方法，使用剪枝器实现定义的逻辑更新掩码。步骤方法调用此``update_mask``函数，针对稀疏配置中指定的权重。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We will also evaluate the model to show the accuracy degradation of zero-"
"shot pruning, or pruning without fine-tuning / retraining."
msgstr "我们还将评估模型，以展示零样本剪枝（即无微调/重新训练剪枝）的准确性下降。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this state, we can start fine-tuning the model, updating the elements "
"that wouldn’t be pruned to better account for the accuracy loss. Once we’ve "
"reached a satisfied state, we can call ``squash_mask`` to fuse the mask and "
"the weight together. This will remove the parametrizations and we are left "
"with a zeroed-out 2:4 dense model."
msgstr ""
"在这种状态下，我们可以开始微调模型，更新不会被剪枝的元素以更好地补偿准确性损失。一旦达到满意的状态，我们可以调用``squash_mask``来将掩码和权重合并在一起。这将删除参数化，我们得到一个被清零的2:4密集模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Accelerating 2:4 sparse models for inference"
msgstr "加速2:4稀疏模型的推理"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have a model in this format, we can accelerate it for inference "
"just like in the QuickStart Guide."
msgstr "现在我们有了这种格式的模型，我们可以像快速入门指南中一样加速其推理性能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Retraining our model after magnitude pruning has recovered nearly all of the"
" F1 that has been lost when the model was pruned. At the same time we have "
"achieved a 1.28x speedup for ``bs=16``. Note that not all shapes are "
"amenable to performance improvements. When batch sizes are small and limited"
" time is spent in compute sparse kernels may be slower than their dense "
"counterparts."
msgstr ""
"在幅度剪枝后重新训练我们的模型已恢复几乎所有剪枝时丢失的F1分数。同时，对于``bs=16``我们实现了1.28倍的加速。请注意，并非所有形状都适合性能改进。在批量大小较小且计算稀疏内核的时间有限时，稀疏内核可能比其密集对应物更慢。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Because semi-structured sparsity is implemented as a tensor subclass, it is "
"compatible with ``torch.compile``. When composed with "
"``to_sparse_semi_structured``, we are able to achieve a total 2x speedup on "
"BERT."
msgstr ""
"由于半结构化稀疏性作为张量子类实现，它与``torch.compile``是兼容的。当与``to_sparse_semi_structured``组合时，我们能够在BERT上实现总计2倍的加速。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Metrics"
msgstr "指标"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "fp16"
msgstr "fp16"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "2:4 sparse"
msgstr "2:4稀疏"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "delta / speedup"
msgstr "变化 / 加速"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "compiled"
msgstr "已编译"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Exact Match (%)"
msgstr "精确匹配率（%）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "78.53"
msgstr "78.53"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "78.44"
msgstr "78.44"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "-0.09"
msgstr "-0.09"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "F1 (%)"
msgstr "F1（%）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "86.93"
msgstr "86.93"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "86.49"
msgstr "86.49"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "-0.44"
msgstr "-0.44"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Time (bs=4)"
msgstr "时间（bs=4）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "11.10"
msgstr "11.10"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "15.54"
msgstr "15.54"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "0.71x"
msgstr "0.71x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "no"
msgstr "否"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Time (bs=16)"
msgstr "时间（bs=16）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "19.35"
msgstr "19.35"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "15.74"
msgstr "15.74"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.23x"
msgstr "1.23x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Time (bs=64)"
msgstr "时间（bs=64）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "72.71"
msgstr "72.71"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "59.41"
msgstr "59.41"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.22x"
msgstr "1.22x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Time (bs=256)"
msgstr "时间（bs=256）"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "286.65"
msgstr "286.65"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "247.63"
msgstr "247.63"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.14x"
msgstr "1.14x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "7.59"
msgstr "7.59"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "7.46"
msgstr "7.46"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.02x"
msgstr "1.02x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "yes"
msgstr "是"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "11.47"
msgstr "11.47"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "9.68"
msgstr "9.68"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.18x"
msgstr "1.18x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "41.57"
msgstr "41.57"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "36.92"
msgstr "36.92"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.13x"
msgstr "1.13x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "159.22"
msgstr "159.22"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "142.23"
msgstr "142.23"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1.12x"
msgstr "1.12x"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we have shown how to prune BERT to be 2:4 sparse and how "
"to accelerate a 2:4 sparse model for inference. By taking advantage of our "
"``SparseSemiStructuredTensor`` subclass, we were able to achieve a 1.3x "
"speedup over the fp16 baseline, and up to 2x with ``torch.compile``. We also"
" demonstrated the benefits of 2:4 sparsity by fine-tuning BERT to recover "
"any lost F1 (86.92 dense vs 86.48 sparse)."
msgstr ""
"在本教程中，我们展示了如何将BERT剪枝为2:4稀疏以及如何加速2:4稀疏模型的推理性能。通过利用我们的``SparseSemiStructuredTensor``子类，我们比fp16基线实现了1.3倍的加速，并通过``torch.compile``实现了最高2倍的加速。我们还通过微调BERT展示了2:4稀疏性的优势，以恢复任何丢失的F1（86.92密集与86.48稀疏）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: semi_structured_sparse.py "
"<semi_structured_sparse.py>`"
msgstr ""
":download:`下载Python源码: semi_structured_sparse.py "
"<semi_structured_sparse.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: semi_structured_sparse.ipynb "
"<semi_structured_sparse.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: semi_structured_sparse.ipynb "
"<semi_structured_sparse.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Exploring TorchRec sharding"
msgstr "探索TorchRec的分片"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial will mainly cover the sharding schemes of embedding tables via"
" ``EmbeddingPlanner`` and ``DistributedModelParallel`` API and explore the "
"benefits of different sharding schemes for the embedding tables by "
"explicitly configuring them."
msgstr ""
"本教程将主要介绍通过``EmbeddingPlanner``和``DistributedModelParallel``API对嵌入表进行分片方案，并通过明确配置它们探索不同分片方案对嵌入表的优势。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Installation"
msgstr "安装"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Requirements: - python >= 3.7"
msgstr "要求: - python >= 3.7"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We highly recommend CUDA when using torchRec. If using CUDA: - cuda >= 11.0"
msgstr "我们强烈推荐在使用torchRec时使用CUDA。如果使用CUDA: - cuda >= 11.0"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Installing torchRec will also install `FBGEMM "
"<https://github.com/pytorch/fbgemm>`__, a collection of CUDA kernels and GPU"
" enabled operations to run"
msgstr ""
"安装torchRec时也会安装`FBGEMM "
"<https://github.com/pytorch/fbgemm>`__，这是一个包含CUDA内核和支持GPU的操作的集合"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Install multiprocess which works with ipython to for multi-processing "
"programming within colab"
msgstr "安装multiprocess，它可以与ipython一起用于Colab中的多进程编程"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The following steps are needed for the Colab runtime to detect the added "
"shared libraries. The runtime searches for shared libraries in /usr/lib, so "
"we copy over the libraries which were installed in /usr/local/lib/. **This "
"is a very necessary step, only in the colab runtime**."
msgstr ""
"以下步骤是让Colab运行时检测到添加的共享库所需的。运行时会在/usr/lib中搜索共享库，因此我们将已安装在/usr/local/lib/中的库复制到/usr/lib中。"
" **这在Colab运行时中是非常必要的步骤**。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Restart your runtime at this point for the newly installed packages to be "
"seen.** Run the step below immediately after restarting so that python knows"
" where to look for packages. **Always run this step after restarting the "
"runtime.**"
msgstr ""
"**此时重新启动运行时以使新安装的包可见。** 在重启后立即运行以下步骤，让python知道在哪里寻找包。 **每次在重启运行时后都要运行此步骤。**"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Distributed Setup"
msgstr "分布式设置"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Due to the notebook enviroment, we cannot run `SPMD "
"<https://en.wikipedia.org/wiki/SPMD>`_ program here but we can do "
"multiprocessing inside the notebook to mimic the setup. Users should be "
"responsible for setting up their own `SPMD "
"<https://en.wikipedia.org/wiki/SPMD>`_ launcher when using Torchrec. We "
"setup our environment so that torch distributed based communication backend "
"can work."
msgstr ""
"由于笔记本环境，我们无法在此运行 `SPMD <https://en.wikipedia.org/wiki/SPMD>`_ "
"程序，但我们可以在笔记本中进行多进程编程以模拟设置。在使用Torchrec时，用户应负责设置自己的`SPMD "
"<https://en.wikipedia.org/wiki/SPMD>`_启动器。我们设置了环境，使得Torch的分布式通信后端能够正常工作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Constructing our embedding model"
msgstr "构建我们的嵌入模型"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here we use TorchRec offering of `EmbeddingBagCollection "
"<https://github.com/facebookresearch/torchrec/blob/main/torchrec/modules/embedding_modules.py#L59>`_"
" to construct our embedding bag model with embedding tables."
msgstr ""
"这里我们使用TorchRec提供的`EmbeddingBagCollection "
"<https://github.com/facebookresearch/torchrec/blob/main/torchrec/modules/embedding_modules.py#L59>`_构建带有嵌入表的嵌入包模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here, we create an EmbeddingBagCollection (EBC) with four embedding bags. We"
" have two types of tables: large tables and small tables differentiated by "
"their row size difference: 4096 vs 1024. Each table is still represented by "
"64 dimension embedding."
msgstr ""
"我们在这里创建了一个具有四个嵌入包的EmbeddingBagCollection "
"(EBC)。我们有两种类型的表：大表和小表，它们通过行大小的差异来区分：4096 vs 1024。每张表仍然由64维嵌入表示。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We configure the ``ParameterConstraints`` data structure for the tables, "
"which provides hints for the model parallel API to help decide the sharding "
"and placement strategy for the tables. In TorchRec, we support \\* ``table-"
"wise``: place the entire table on one device; \\* ``row-wise``: shard the "
"table evenly by row dimension and place one shard on each device of the "
"communication world; \\* ``column-wise``: shard the table evenly by "
"embedding dimension, and place one shard on each device of the communication"
" world; \\* ``table-row-wise``: special sharding optimized for intra-host "
"communication for available fast intra-machine device interconnect, e.g. "
"NVLink; \\* ``data_parallel``: replicate the tables for every device;"
msgstr ""
"我们为表配置了``ParameterConstraints``数据结构，它为模型并行API提供提示，以帮助决定表的分片和放置策略。在TorchRec中，我们支持\\*"
" ``表级``: 将整个表放置在一个设备上；\\* ``行级``: 按行维度均匀分片表，并将每个分片放置在通信域的一个设备上；\\* ``列级``: "
"按嵌入维度均匀分片表，并将每个分片放置在通信域的一个设备上；\\* ``表行级``: "
"优化用于主机内通信的特殊分片，以利用可用的快速机器内设备互连，例如NVLink；\\* ``数据并行``: 每个设备复制表；"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note how we initially allocate the EBC on device \"meta\". This will tell "
"EBC to not allocate memory yet."
msgstr "请注意我们最初如何在设备\"meta\"上分配EBC。这将告诉EBC暂时不分配内存。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "DistributedModelParallel in multiprocessing"
msgstr "进程内分布式模型并行"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, we have a single process execution function for mimicking one rank's "
"work during `SPMD <https://en.wikipedia.org/wiki/SPMD>`_ execution."
msgstr ""
"现在，我们有一个单进程执行函数，用于模拟`SPMD "
"<https://en.wikipedia.org/wiki/SPMD>`_执行期间一个rank的工作。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This code will shard the model collectively with other processes and "
"allocate memories accordingly. It first sets up process groups and do "
"embedding table placement using planner and generate sharded model using "
"``DistributedModelParallel``."
msgstr ""
"此代码将与其他进程共同分片模型并相应地分配内存。它首先设置进程组并使用规划器进行嵌入表放置，然后使用``DistributedModelParallel``生成分片模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Multiprocessing Execution"
msgstr "多进程执行"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's execute the code in multi-processes representing multiple GPU "
"ranks."
msgstr "现在让我们在代表多个GPU排名的多进程中执行代码。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Table Wise Sharding"
msgstr "按表分片"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's execute the code in two processes for 2 GPUs. We can see in the "
"plan print that how our tables are sharded across GPUs. Each node will have "
"one large table and one small which shows our planner tries for load balance"
" for the embedding tables. Table-wise is the de-factor go-to sharding "
"schemes for many small-medium size tables for load balancing over the "
"devices."
msgstr ""
"现在让我们在两个进程中为两个GPU执行代码。我们可以在规划打印中看到表如何在GPU之间分片。每个节点将拥有一个大的表和一个小的表，这表明我们的规划器尝试在设备之间为嵌入表进行负载平衡。按表分片是针对许多中小型表的负载平衡设备的默认分片方案。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Explore other sharding modes"
msgstr "探索其他分片模式"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We have initially explored what table-wise sharding would look like and how "
"it balances the tables placement. Now we explore sharding modes with finer "
"focus on load balance: row-wise. Row-wise is specifically addressing large "
"tables which a single device cannot hold due to the memory size increase "
"from large embedding row numbers. It can address the placement of the super "
"large tables in your models. Users can see that in the ``shard_sizes`` "
"section in the printed plan log, the tables are halved by row dimension to "
"be distributed onto two GPUs."
msgstr ""
"我们初步探索了按表分片的表现以及如何平衡表的放置。现在我们探索分片模式，并更加专注于负载平衡：按行分片。按行分片专门解决由于嵌入行数增加导致内存不足而无法单设备容纳的大型表。它可以解决模型中超大表的放置问题。用户可以在打印出的规划日志中的``shard_sizes``部分看到，表通过行维度减半并分布到两个GPU上。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Column-wise on the other hand, address the load imbalance problems for "
"tables with large embedding dimensions. We will split the table vertically. "
"Users can see that in the ``shard_sizes`` section in the printed plan log, "
"the tables are halved by embedding dimension to be distributed onto two "
"GPUs."
msgstr ""
"另一方面，按列分片解决了具有大嵌入维度的表的负载不平衡问题。我们将表垂直切分。用户可以在打印出的规划日志中的``shard_sizes``部分看到，表通过嵌入维度减半并分布到两个GPU上。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For ``table-row-wise``, unfortuately we cannot simulate it due to its nature"
" of operating under multi-host setup. We will present a python `SPMD "
"<https://en.wikipedia.org/wiki/SPMD>`_ example in the future to train models"
" with ``table-row-wise``."
msgstr ""
"对于``table-row-wise``，由于其在多主机设置下运行的性质，我们无法模拟。我们将在未来呈现一个Python `SPMD "
"<https://en.wikipedia.org/wiki/SPMD>`_ 示例，用于使用``table-row-wise``训练模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "With data parallel, we will repeat the tables for all devices."
msgstr "使用数据并行时，我们将对所有设备重复表。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "(beta) Static Quantization with Eager Mode in PyTorch"
msgstr "(测试版) PyTorch中使用Eager模式进行静态量化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"**Author**: `Raghuraman Krishnamoorthi <https://github.com/raghuramank100>`_"
" **Edited by**: `Seth Weidman <https://github.com/SethHWeidman/>`_, `Jerry "
"Zhang <https:github.com/jerryzh168>`_"
msgstr ""
"**作者**：`Raghuraman Krishnamoorthi <https://github.com/raghuramank100>`_ "
"**编辑者**：`Seth Weidman <https://github.com/SethHWeidman/>`_, `Jerry Zhang "
"<https:github.com/jerryzh168>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial shows how to do post-training static quantization, as well as "
"illustrating two more advanced techniques - per-channel quantization and "
"quantization-aware training - to further improve the model's accuracy. Note "
"that quantization is currently only supported for CPUs, so we will not be "
"utilizing GPUs / CUDA in this tutorial. By the end of this tutorial, you "
"will see how quantization in PyTorch can result in significant decreases in "
"model size while increasing speed. Furthermore, you'll see how to easily "
"apply some advanced quantization techniques shown `here "
"<https://arxiv.org/abs/1806.08342>`_ so that your quantized models take much"
" less of an accuracy hit than they would otherwise. Warning: we use a lot of"
" boilerplate code from other PyTorch repos to, for example, define the "
"``MobileNetV2`` model architecture, define data loaders, and so on. We of "
"course encourage you to read it; but if you want to get to the quantization "
"features, feel free to skip to the \"4. Post-training static quantization\" "
"section. We'll start by doing the necessary imports:"
msgstr ""
"本教程展示了如何进行后训练静态量化，并说明了两种更高级的技术——逐通道量化和量化感知训练，以进一步提高模型的准确性。请注意，量化目前仅支持CPU，因此本教程中我们不会使用GPU/CUDA。通过本教程结束时，您将看到PyTorch中的量化如何显著减少模型大小，同时提高速度。此外，您将看到如何轻松应用一些高级量化技术，参见`这里"
" "
"<https://arxiv.org/abs/1806.08342>`_，以使量化模型在精度损失上减少到非常低的程度。警告：我们使用了大量的样板代码，这些代码来源于其他PyTorch仓库，例如定义``MobileNetV2``模型架构，定义数据加载器等。我们当然鼓励您阅读；但如果您想直接学习量化功能，可以跳到“4."
" 后训练静态量化”部分。我们将首先进行必要的导入操作："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "1. Model architecture"
msgstr "1. 模型架构"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We first define the MobileNetV2 model architecture, with several notable "
"modifications to enable quantization:"
msgstr "我们首先定义MobileNetV2模型架构，并进行了几处显著的修改以支持量化："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Replacing addition with ``nn.quantized.FloatFunctional``"
msgstr "用``nn.quantized.FloatFunctional``替换加法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Insert ``QuantStub`` and ``DeQuantStub`` at the beginning and end of the "
"network."
msgstr "在网络的开始和结束处插入``QuantStub``和``DeQuantStub``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Replace ReLU6 with ReLU"
msgstr "用ReLU替换ReLU6"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note: this code is taken from `here "
"<https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_."
msgstr ""
"注意：此代码来源于`此处 "
"<https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "2. Helper functions"
msgstr "2. 辅助函数"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We next define several helper functions to help with model evaluation. These"
" mostly come from `here "
"<https://github.com/pytorch/examples/blob/master/imagenet/main.py>`_."
msgstr ""
"接下来我们定义几个辅助函数来帮助评估模型。这些辅助函数大多来源于`此处 "
"<https://github.com/pytorch/examples/blob/master/imagenet/main.py>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "3. Define dataset and data loaders"
msgstr "3. 定义数据集和数据加载器"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As our last major setup step, we define our dataloaders for our training and"
" testing set."
msgstr "作为最后一个主要设置步骤，我们为训练集和测试集定义数据加载器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "ImageNet Data"
msgstr "ImageNet数据"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To run the code in this tutorial using the entire ImageNet dataset, first "
"download imagenet by following the instructions at here `ImageNet Data "
"<http://www.image-net.org/download>`_. Unzip the downloaded file into the "
"'data_path' folder."
msgstr ""
"要使用整个ImageNet数据集运行本教程中的代码，请首先按照`ImageNet数据 <http://www.image-"
"net.org/download>`_中的说明下载Imagenet。将下载的文件解压到&apos;data_path&apos;文件夹中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"With the data downloaded, we show functions below that define dataloaders "
"we'll use to read in this data. These functions mostly come from `here "
"<https://github.com/pytorch/vision/blob/master/references/detection/train.py>`_."
msgstr ""
"下载数据后，我们展示函数定义加载器以读取数据。这些函数大多来源于`此处 "
"<https://github.com/pytorch/vision/blob/master/references/detection/train.py>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we'll load in the pre-trained MobileNetV2 model. We provide the URL to"
" download the model `here "
"<https://download.pytorch.org/models/mobilenet_v2-b0353104.pth>`_."
msgstr ""
"接下来，我们加载预训练的MobileNetV2模型。我们提供下载模型的URL`在这里 "
"<https://download.pytorch.org/models/mobilenet_v2-b0353104.pth>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally to get a \"baseline\" accuracy, let's see the accuracy of our un-"
"quantized model with fused modules"
msgstr "最后，为获得“基准”准确性，让我们看看未量化模型带有融合模块的准确性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"On the entire model, we get an accuracy of 71.9% on the eval dataset of "
"50,000 images."
msgstr "对于整个模型，我们在包含50,000张图片的评估数据集上获得了71.9%的准确性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This will be our baseline to compare to. Next, let's try different "
"quantization methods"
msgstr "这将是我们进行比较的基准。接下来，让我们尝试不同的量化方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "4. Post-training static quantization"
msgstr "4. 后训练静态量化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Post-training static quantization involves not just converting the weights "
"from float to int, as in dynamic quantization, but also performing the "
"additional step of first feeding batches of data through the network and "
"computing the resulting distributions of the different activations "
"(specifically, this is done by inserting `observer` modules at different "
"points that record this data). These distributions are then used to "
"determine how the specifically the different activations should be quantized"
" at inference time (a simple technique would be to simply divide the entire "
"range of activations into 256 levels, but we support more sophisticated "
"methods as well). Importantly, this additional step allows us to pass "
"quantized values between operations instead of converting these values to "
"floats - and then back to ints - between every operation, resulting in a "
"significant speed-up."
msgstr ""
"后训练静态量化不仅包括将权重从浮点转换为整数（动态量化所做的），还包括额外的步骤，首先向网络中输入数据批次并计算不同激活的分布（具体来说，这通过在不同点插入`observer`模块记录数据完成）。这些分布然后用于确定不同激活在推理时应如何量化（简单技术是将激活的整个范围分成256级，但我们也支持更复杂的方法）。重要的是，这一额外步骤允许操作之间传递量化值，而不是在每个操作之间将这些值转换为浮点数然后再转换为整数，从而显著加快速度。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this quantized model, we see an accuracy of 56.7% on the eval dataset. "
"This is because we used a simple min/max observer to determine quantization "
"parameters. Nevertheless, we did reduce the size of our model down to just "
"under 3.6 MB, almost a 4x decrease."
msgstr ""
"对于这个量化模型，我们在评估数据集上获得了56.7%的准确性。这是因为我们使用了简单的最小/最大观察器来确定量化参数。即便如此，我们还是将模型大小减少到仅3.6"
" MB以下，减少了约4倍。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In addition, we can significantly improve on the accuracy simply by using a "
"different quantization configuration. We repeat the same exercise with the "
"recommended configuration for quantizing for x86 architectures. This "
"configuration does the following:"
msgstr "此外，简单地改变量化配置，我们可以显著提高准确性。我们使用推荐的用于x86架构量化的配置重复相同的实验。这种配置做了以下事情："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Quantizes weights on a per-channel basis"
msgstr "对权重进行逐通道量化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Uses a histogram observer that collects a histogram of activations and then "
"picks quantization parameters in an optimal manner."
msgstr "使用统计直方图观察器收集激活的直方图，并以最佳方式选择量化参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Changing just this quantization configuration method resulted in an increase"
" of the accuracy to over 67.3%! Still, this is 4% worse than the baseline of"
" 71.9% achieved above. So lets try quantization aware training."
msgstr "此量化配置方法变化仅产生的准确性提高到了67.3%以上！不过，与71.9%的基准相比还有4%的差距。所以让我们尝试量化感知训练。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "5. Quantization-aware training"
msgstr "5. 量化感知训练"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Quantization-aware training (QAT) is the quantization method that typically "
"results in the highest accuracy. With QAT, all weights and activations are "
"“fake quantized” during both the forward and backward passes of training: "
"that is, float values are rounded to mimic int8 values, but all computations"
" are still done with floating point numbers. Thus, all the weight "
"adjustments during training are made while “aware” of the fact that the "
"model will ultimately be quantized; after quantizing, therefore, this method"
" will usually yield higher accuracy than either dynamic quantization or "
"post-training static quantization."
msgstr ""
"量化感知训练(QAT)通常是实现最高准确性的量化方法。在QAT中，所有权重和激活在训练的前向和后向传播过程中都执行“假量化”：即，浮点值被四舍五入以模拟int8值，但所有计算仍然使用浮点数进行。因此，在训练过程中的所有权重调整是在“意识到”模型最终会被量化的情况下进行的；因此，经过量化后，该方法通常比动态量化或后训练量化产生更高的准确性。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The overall workflow for actually performing QAT is very similar to before:"
msgstr "实际执行QAT的总体工作流与以前非常相似："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can use the same model as before: there is no additional preparation "
"needed for quantization-aware training."
msgstr "我们可以使用之前相同的模型：量化感知训练无需额外准备。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We need to use a ``qconfig`` specifying what kind of fake-quantization is to"
" be inserted after weights and activations, instead of specifying observers"
msgstr "我们需要使用``qconfig``来指定将在权重和激活后插入的假量化，而不是指定观察器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We first define a training function:"
msgstr "我们首先定义一个训练函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We fuse modules as before"
msgstr "我们像以前一样融合模块。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, ``prepare_qat`` performs the \"fake quantization\", preparing the "
"model for quantization-aware training"
msgstr "最后，``prepare_qat``执行“假量化”，为量化感知训练准备模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Training a quantized model with high accuracy requires accurate modeling of "
"numerics at inference. For quantization aware training, therefore, we modify"
" the training loop by:"
msgstr "用高准确性训练量化模型需要准确模拟推理时的数值行为。因此，对于量化感知训练，我们通过以下方式修改训练循环："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Switch batch norm to use running mean and variance towards the end of "
"training to better match inference numerics."
msgstr "在训练的后期阶段切换批归一化以使用运行中的均值和方差，以更好地匹配推理的数值表现。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We also freeze the quantizer parameters (scale and zero-point) and fine tune"
" the weights."
msgstr "我们还冻结量化器参数（比例和零点）并微调权重。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Quantization-aware training yields an accuracy of over 71.5% on the entire "
"imagenet dataset, which is close to the floating point accuracy of 71.9%."
msgstr "量化感知训练在整个imagenet数据集上产生了71.5%以上的准确性，这接近浮点准确性71.9%。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "More on quantization-aware training:"
msgstr "更多关于量化感知训练："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"QAT is a super-set of post training quant techniques that allows for more "
"debugging. For example, we can analyze if the accuracy of the model is "
"limited by weight or activation quantization."
msgstr "QAT是后训练量化技术的超集，允许更多的调试。例如，我们可以分析模型的准确性是否受到权重或激活量化的限制。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can also simulate the accuracy of a quantized model in floating point "
"since we are using fake-quantization to model the numerics of actual "
"quantized arithmetic."
msgstr "我们还可以模拟浮点数中量化模型的准确性，因为我们使用假量化来模拟实际量化算法的数值表现。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We can mimic post training quantization easily too."
msgstr "我们还可以轻松模拟后训练量化。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Speedup from quantization"
msgstr "量化带来的加速"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, let's confirm something we alluded to above: do our quantized "
"models actually perform inference faster? Let's test:"
msgstr "最后让我们确认我们之前提到的内容：量化模型是否真的进行推理更快？让我们测试一下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Running this locally on a MacBook pro yielded 61 ms for the regular model, "
"and just 20 ms for the quantized model, illustrating the typical 2-4x "
"speedup we see for quantized models compared to floating point ones."
msgstr ""
"在一台MacBook Pro上本地运行此代码时，常规模型的推理耗时为61毫秒，而量化模型仅耗时20毫秒，展示了量化模型通常比浮点模型快2到4倍。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we showed two quantization methods - post-training static "
"quantization, and quantization-aware training - describing what they do "
"\"under the hood\" and how to use them in PyTorch."
msgstr "在本教程中，我们展示了两种量化方法——后训练静态量化和量化感知训练，描述了这些方法的“底层原理”以及如何在PyTorch中使用它们。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here "
"<sphx_glr_download_advanced_super_resolution_with_onnxruntime.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 "
"<sphx_glr_download_advanced_super_resolution_with_onnxruntime.py>` 下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX "
"Runtime"
msgstr "(可选) 从PyTorch导出模型到ONNX并使用ONNX运行时运行"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "As of PyTorch 2.1, there are two versions of ONNX Exporter."
msgstr "截至PyTorch 2.1，有两个版本的ONNX导出器。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch.onnx.dynamo_export`` is the newest (still in beta) exporter based on"
" the TorchDynamo technology released with PyTorch 2.0."
msgstr ""
"``torch.onnx.dynamo_export``是基于PyTorch 2.0推出的TorchDynamo技术的新导出器（仍处于测试版阶段）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch.onnx.export`` is based on TorchScript backend and has been available"
" since PyTorch 1.2.0."
msgstr "``torch.onnx.export``基于TorchScript后端，自PyTorch 1.2.0以来已可用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we describe how to convert a model defined in PyTorch into"
" the ONNX format using the TorchScript ``torch.onnx.export`` ONNX exporter."
msgstr ""
"在本教程中，我们介绍了如何使用TorchScript``torch.onnx.export`` "
"ONNX导出器将PyTorch定义的模型转换为ONNX格式。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The exported model will be executed with ONNX Runtime. ONNX Runtime is a "
"performance-focused engine for ONNX models, which inferences efficiently "
"across multiple platforms and hardware (Windows, Linux, and Mac and on both "
"CPUs and GPUs). ONNX Runtime has proved to considerably increase performance"
" over multiple models as explained `here "
"<https://cloudblogs.microsoft.com/opensource/2019/05/22/onnx-runtime-"
"machine-learning-inferencing-0-4-release>`__"
msgstr ""
"导出的模型将使用ONNX Runtime执行。ONNX "
"Runtime是一个针对ONNX模型的性能优化引擎，可以在多个平台和硬件上高效推断（Windows、Linux、Mac，以及CPU和GPU）。正如`这里"
" <https://cloudblogs.microsoft.com/opensource/2019/05/22/onnx-runtime-"
"machine-learning-inferencing-0-4-release>`__所解释的，ONNX "
"Runtime已证明在多个模型上大幅提高性能。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this tutorial, you will need to install `ONNX "
"<https://github.com/onnx/onnx>`__ and `ONNX Runtime "
"<https://github.com/microsoft/onnxruntime>`__. You can get binary builds of "
"ONNX and ONNX Runtime with"
msgstr ""
"本教程需要安装`ONNX <https://github.com/onnx/onnx>`__和`ONNX Runtime "
"<https://github.com/microsoft/onnxruntime>`__。您可以通过以下方式获取ONNX和ONNX "
"Runtime的二进制版本："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "ONNX Runtime recommends using the latest stable runtime for PyTorch."
msgstr "ONNX Runtime建议为PyTorch使用最新稳定版运行时。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Super-resolution is a way of increasing the resolution of images, videos and"
" is widely used in image processing or video editing. For this tutorial, we "
"will use a small super-resolution model."
msgstr "超分辨率是一种增加图像和视频分辨率的方法，广泛用于图像处理或视频编辑。在本教程中，我们将使用一个小型的超分辨率模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"First, let's create a ``SuperResolution`` model in PyTorch. This model uses "
"the efficient sub-pixel convolution layer described in `\"Real-Time Single "
"Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional "
"Neural Network\" - Shi et al <https://arxiv.org/abs/1609.05158>`__ for "
"increasing the resolution of an image by an upscale factor. The model "
"expects the Y component of the ``YCbCr`` of an image as an input, and "
"outputs the upscaled Y component in super resolution."
msgstr ""
"首先，让我们在PyTorch中创建一个``SuperResolution``模型。该模型使用一种高效的子像素卷积层（详见`\"实时单图像和视频超分辨率使用高效子像素卷积神经网络\""
" - Shi等人 "
"<https://arxiv.org/abs/1609.05158>`__)，通过一个放大因子增加图像的分辨率。该模型以图像的``YCbCr``的Y分量为输入，并输出经过超分辨率处理的Y分量。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"`The model "
"<https://github.com/pytorch/examples/blob/master/super_resolution/model.py>`__"
" comes directly from PyTorch's examples without modification:"
msgstr ""
"`该模型 "
"<https://github.com/pytorch/examples/blob/master/super_resolution/model.py>`__直接来自于PyTorch的示例，未作修改："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Ordinarily, you would now train this model; however, for this tutorial, we "
"will instead download some pretrained weights. Note that this model was not "
"trained fully for good accuracy and is used here for demonstration purposes "
"only."
msgstr ""
"通常情况下，您现在会训练该模型；然而，为了本教程的目的，我们将改为下载一些预训练权重。请注意，该模型并未完全训练以获得良好的准确性，此处仅为演示使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It is important to call ``torch_model.eval()`` or "
"``torch_model.train(False)`` before exporting the model, to turn the model "
"to inference mode. This is required since operators like dropout or "
"batchnorm behave differently in inference and training mode."
msgstr ""
"导出模型前调用``torch_model.eval()``或``torch_model.train(False)``非常重要，以将模型设置为推断模式。这是因为诸如dropout或batchnorm等算子在推断模式和训练模式下的行为不同。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Exporting a model in PyTorch works via tracing or scripting. This tutorial "
"will use as an example a model exported by tracing. To export a model, we "
"call the ``torch.onnx.export()`` function. This will execute the model, "
"recording a trace of what operators are used to compute the outputs. Because"
" ``export`` runs the model, we need to provide an input tensor ``x``. The "
"values in this can be random as long as it is the right type and size. Note "
"that the input size will be fixed in the exported ONNX graph for all the "
"input's dimensions, unless specified as a dynamic axes. In this example we "
"export the model with an input of batch_size 1, but then specify the first "
"dimension as dynamic in the ``dynamic_axes`` parameter in "
"``torch.onnx.export()``. The exported model will thus accept inputs of size "
"[batch_size, 1, 224, 224] where batch_size can be variable."
msgstr ""
"在PyTorch中可以通过追踪或脚本方式导出模型。本教程将以追踪方式导出的模型作为示例。要导出模型，我们调用``torch.onnx.export()``函数。该函数将执行模型，记录计算输出所使用的算子。由于``export``会运行模型，我们需要提供一个输入张量``x``。只要类型和尺寸正确，该张量的值可以是随机的。请注意，除非指定为动态轴，导出的ONNX图将为所有输入维度的尺寸固定输入大小。在本示例中，我们将使用batch_size为1的输入导出模型，但随后在``torch.onnx.export()``里的``dynamic_axes``参数中指定第一个维度为动态。因此，导出的模型将接受尺寸为[batch_size,"
" 1, 224, 224]的输入，其中batch_size可以是可变的。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To learn more details about PyTorch's export interface, check out the "
"`torch.onnx documentation <https://pytorch.org/docs/master/onnx.html>`__."
msgstr ""
"要了解有关PyTorch导出接口的更多细节，请查看`torch.onnx文档 "
"<https://pytorch.org/docs/master/onnx.html>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We also computed ``torch_out``, the output after of the model, which we will"
" use to verify that the model we exported computes the same values when run "
"in ONNX Runtime."
msgstr "我们还计算了``torch_out``，模型的输出，我们将用它来验证导出的模型在ONNX Runtime中计算的是相同值。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"But before verifying the model's output with ONNX Runtime, we will check the"
" ONNX model with ONNX API. First, ``onnx.load(\"super_resolution.onnx\")`` "
"will load the saved model and will output a ``onnx.ModelProto`` structure (a"
" top-level file/container format for bundling a ML model. For more "
"information `onnx.proto documentation "
"<https://github.com/onnx/onnx/blob/master/onnx/onnx.proto>`__.). Then, "
"``onnx.checker.check_model(onnx_model)`` will verify the model's structure "
"and confirm that the model has a valid schema. The validity of the ONNX "
"graph is verified by checking the model's version, the graph's structure, as"
" well as the nodes and their inputs and outputs."
msgstr ""
"在使用ONNX Runtime验证模型的输出之前，我们将使用ONNX "
"API检查ONNX模型。首先，``onnx.load(\"super_resolution.onnx\")``将加载保存的模型并输出一个``onnx.ModelProto``结构（一种用于打包ML模型的顶级文件/容器格式。有关更多信息，请参考`onnx.proto文档"
" "
"<https://github.com/onnx/onnx/blob/master/onnx/onnx.proto>`__）。然后，``onnx.checker.check_model(onnx_model)``将验证模型的结构，并确认模型具有有效的模式。ONNX图的有效性通过检查模型版本、图结构以及节点及其输入和输出来验证。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's compute the output using ONNX Runtime's Python APIs. This part can"
" normally be done in a separate process or on another machine, but we will "
"continue in the same process so that we can verify that ONNX Runtime and "
"PyTorch are computing the same value for the network."
msgstr ""
"现在，让我们使用ONNX Runtime的Python "
"API计算输出。这部分通常可以在单独的进程或另一台机器上完成，但我们将继续在同一进程中进行，以验证ONNX "
"Runtime和PyTorch正在为网络计算相同的值。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In order to run the model with ONNX Runtime, we need to create an inference "
"session for the model with the chosen configuration parameters (here we use "
"the default config). Once the session is created, we evaluate the model "
"using the run() API. The output of this call is a list containing the "
"outputs of the model computed by ONNX Runtime."
msgstr ""
"为了使用ONNX Runtime运行模型，我们需要根据选择的配置参数（此处使用默认配置）为模型创建推断会话。一旦会话被创建，我们使用run() "
"API对模型进行评估。此调用的输出是一个列表，包含由ONNX Runtime计算的模型输出。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We should see that the output of PyTorch and ONNX Runtime runs match "
"numerically with the given precision (``rtol=1e-03`` and ``atol=1e-05``). As"
" a side-note, if they do not match then there is an issue in the ONNX "
"exporter, so please contact us in that case."
msgstr ""
"我们应该能够看到PyTorch和ONNX "
"Runtime运行的输出在给定精度下数值匹配（``rtol=1e-03``和``atol=1e-05``）。作为附注，如果它们不匹配，则表示ONNX导出器存在问题，请在这种情况下与我们联系。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Timing Comparison Between Models"
msgstr "模型之间的时间对比"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Since ONNX models optimize for inference speed, running the same data on an "
"ONNX model instead of a native pytorch model should result in an improvement"
" of up to 2x. Improvement is more pronounced with higher batch sizes."
msgstr ""
"由于ONNX模型针对推断速度进行了优化，使用ONNX模型而不是原生PyTorch模型运行相同数据应该会带来高达2倍的提升。提升在较大的batch_size中更为显著。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Running the model on an image using ONNX Runtime"
msgstr "使用ONNX Runtime运行模型时加载图像"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"So far we have exported a model from PyTorch and shown how to load it and "
"run it in ONNX Runtime with a dummy tensor as an input."
msgstr "目前我们已经从PyTorch导出了一个模型，并展示了如何加载它以及使用一个虚拟张量作为输入在ONNX Runtime中运行它。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this tutorial, we will use a famous cat image used widely which looks "
"like below"
msgstr "本教程将使用一张著名的猫图片，该图片被广泛使用，效果如下"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "cat"
msgstr "猫"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"First, let's load the image, preprocess it using standard PIL python "
"library. Note that this preprocessing is the standard practice of processing"
" data for training/testing neural networks."
msgstr "首先，让我们加载图像，并使用标准的Python库PIL进行预处理。请注意，这种预处理是用于训练/测试神经网络的标准数据处理方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We first resize the image to fit the size of the model's input (224x224). "
"Then we split the image into its Y, Cb, and Cr components. These components "
"represent a grayscale image (Y), and the blue-difference (Cb) and red-"
"difference (Cr) chroma components. The Y component being more sensitive to "
"the human eye, we are interested in this component which we will be "
"transforming. After extracting the Y component, we convert it to a tensor "
"which will be the input of our model."
msgstr ""
"我们首先将图像调整为与模型输入尺寸（224x224）相符。然后，我们将图像分解为其Y、Cb和Cr分量。这些分量分别表示灰度图像（Y）以及蓝色差异（Cb）和红色差异（Cr）的色度分量。由于Y分量对人眼更为敏感，我们关注这个分量并对其进行转化。在提取Y分量后，我们将其转换为张量，它将作为模型的输入。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, as a next step, let's take the tensor representing the grayscale "
"resized cat image and run the super-resolution model in ONNX Runtime as "
"explained previously."
msgstr "接下来，让我们将表示灰度调整后的猫图像的张量输入到ONNX Runtime中的超分辨率模型，并按照之前的解释运行。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"At this point, the output of the model is a tensor. Now, we'll process the "
"output of the model to construct back the final output image from the output"
" tensor, and save the image. The post-processing steps have been adopted "
"from PyTorch implementation of super-resolution model `here "
"<https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py>`__."
msgstr ""
"此时，模型的输出是一个张量。现在，我们将处理模型的输出，以从输出张量构造最终的输出图像，并保存该图像。这些后处理步骤采用了PyTorch超分辨率模型的实现`这里"
" "
"<https://github.com/pytorch/examples/blob/master/super_resolution/super_resolve.py>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Here is the comparison between the two images:"
msgstr "以下是两张图片的对比："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Low-resolution image"
msgstr "低分辨率图像"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Image after super-resolution"
msgstr "超分辨率处理后的图像"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"ONNX Runtime being a cross platform engine, you can run it across multiple "
"platforms and on both CPUs and GPUs."
msgstr "ONNX Runtime作为一个跨平台引擎，可以在多个平台上运行，并支持CPU和GPU。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"ONNX Runtime can also be deployed to the cloud for model inferencing using "
"Azure Machine Learning Services. More information `here "
"<https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-"
"onnx>`__."
msgstr ""
"ONNX Runtime还可以通过Azure机器学习服务部署到云端进行模型推断。更多信息请参考`这里 "
"<https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-"
"onnx>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"More information about ONNX Runtime's performance `here "
"<https://onnxruntime.ai/docs/performance>`__."
msgstr ""
"关于ONNX Runtime性能的更多信息请参考`这里 <https://onnxruntime.ai/docs/performance>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For more information about ONNX Runtime `here "
"<https://github.com/microsoft/onnxruntime>`__."
msgstr ""
"关于ONNX Runtime的更多信息请参考`这里 <https://github.com/microsoft/onnxruntime>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: super_resolution_with_onnxruntime.py"
" <super_resolution_with_onnxruntime.py>`"
msgstr ""
":download:`下载Python源代码: super_resolution_with_onnxruntime.py "
"<super_resolution_with_onnxruntime.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: "
"super_resolution_with_onnxruntime.ipynb "
"<super_resolution_with_onnxruntime.ipynb>`"
msgstr ""
":download:`下载Jupyter笔记本: super_resolution_with_onnxruntime.ipynb "
"<super_resolution_with_onnxruntime.ipynb>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Dynamic Parallelism in TorchScript"
msgstr "TorchScript中的动态并行性"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we introduce the syntax for doing *dynamic inter-op "
"parallelism* in TorchScript. This parallelism has the following properties:"
msgstr "在本教程中，我们介绍了在TorchScript中进行*动态互操作并行性*的语法。这种并行性具有以下特点："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"dynamic - The number of parallel tasks created and their workload can depend"
" on the control flow of the program."
msgstr "动态 - 创建的并行任务数量及其工作量可以依赖于程序的控制流。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"inter-op - The parallelism is concerned with running TorchScript program "
"fragments in parallel. This is distinct from *intra-op parallelism*, which "
"is concerned with splitting up individual operators and running subsets of "
"the operator's work in parallel."
msgstr ""
"互操作 - 这种并行性关注于并行运行TorchScript程序片段。这与*内操作并行性*不同，后者关注于将单个算子分解，并并行运行算子的子集。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Basic Syntax"
msgstr "基本语法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "The two important APIs for dynamic parallelism are:"
msgstr "动态并行性的两个重要API是："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -> "
"torch.jit.Future[T]``"
msgstr ""
"``torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -> "
"torch.jit.Future[T]``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``torch.jit.wait(fut : torch.jit.Future[T]) -> T``"
msgstr "``torch.jit.wait(fut : torch.jit.Future[T]) -> T``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "A good way to demonstrate how these work is by way of an example:"
msgstr "通过示例可以很好地演示这些API的工作方式："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``fork()`` takes the callable ``fn`` and arguments to that callable ``args``"
" and ``kwargs`` and creates an asynchronous task for the execution of "
"``fn``. ``fn`` can be a function, method, or Module instance. ``fork()`` "
"returns a reference to the value of the result of this execution, called a "
"``Future``. Because ``fork`` returns immediately after creating the async "
"task, ``fn`` may not have been executed by the time the line of code after "
"the ``fork()`` call is executed. Thus, ``wait()`` is used to wait for the "
"async task to complete and return the value."
msgstr ""
"``fork()``接受可调用对象``fn``及其参数``args``和``kwargs``，并创建一个异步任务来执行``fn``。``fn``可以是函数、方法或模块实例。``fork()``返回一个对执行结果值的引用，称为``Future``。由于``fork``在创建异步任务后立即返回，``fn``可能还未在执行完毕时就执行了``fork()``调用后的代码行。因此，需要使用``wait()``等待异步任务完成并返回值。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"These constructs can be used to overlap the execution of statements within a"
" function (shown in the worked example section) or be composed with other "
"language constructs like loops:"
msgstr "这些结构可以用于重叠函数中的语句执行（见实际示例章节）或与循环等其他语言结构组合："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"When we initialized an empty list of Futures, we needed to add an explicit "
"type annotation to ``futures``. In TorchScript, empty containers default to "
"assuming they contain Tensor values, so we annotate the list constructor # "
"as being of type ``List[torch.jit.Future[torch.Tensor]]``"
msgstr ""
"当我们初始化一个空的Futures列表时，需要对``futures``添加显式的类型注解。在TorchScript中，空容器默认假定它们包含Tensor值，因此我们在列表构造函数中注解类型为``List[torch.jit.Future[torch.Tensor]]``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This example uses ``fork()`` to launch 100 instances of the function "
"``foo``, waits on the 100 tasks to complete, then sums the results, "
"returning ``-100.0``."
msgstr "此示例使用``fork()``启动函数``foo``的100个实例，等待100个任务完成，然后求和结果，返回``-100.0``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Applied Example: Ensemble of Bidirectional LSTMs"
msgstr "应用示例：双向LSTM的集成"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's try to apply parallelism to a more realistic example and see what sort"
" of performance we can get out of it. First, let's define the baseline "
"model: an ensemble of bidirectional LSTM layers."
msgstr "让我们尝试将并行性应用于一个更实际的示例，并看看我们能从中获得什么样的性能。首先，让我们定义基准模型：一个双向LSTM层的集成。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"On my machine, this network runs in ``2.05`` seconds. We can do a lot "
"better!"
msgstr "在我的机器上，这种网络运行需要``2.05``秒。我们可以做得更好！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Parallelizing Forward and Backward Layers"
msgstr "并行化前向和后向层"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"A very simple thing we can do is parallelize the forward and backward layers"
" within ``BidirectionalRecurrentLSTM``. For this, the structure of the "
"computation is static, so we don't actually even need any loops. Let's "
"rewrite the ``forward`` method of ``BidirectionalRecurrentLSTM`` like so:"
msgstr ""
"一个非常简单的操作是将``BidirectionalRecurrentLSTM``中的前向和后向层进行并行化。对于此操作，计算结构是静态的，因此实际上我们甚至不需要任何循环。让我们像这样重写``BidirectionalRecurrentLSTM``的``forward``方法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this example, ``forward()`` delegates execution of ``cell_f`` to another "
"thread, while it continues to execute ``cell_b``. This causes the execution "
"of both the cells to be overlapped with each other."
msgstr ""
"在此示例中，``forward()``将``cell_f``的执行委托给另一个线程，同时继续执行``cell_b``。这使得两个单元的执行相互重叠。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Running the script again with this simple modification yields a runtime of "
"``1.71`` seconds for an improvement of ``17%``!"
msgstr "重新运行修改后的脚本后，运行时间降低到``1.71``秒，提升了``17%``！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Aside: Visualizing Parallelism"
msgstr "附注：并行性可视化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We're not done optimizing our model but it's worth introducing the tooling "
"we have for visualizing performance. One important tool is the `PyTorch "
"profiler <https://pytorch.org/docs/stable/autograd.html#profiler>`_."
msgstr ""
"我们尚未完成对模型的优化，但值得介绍我们用于可视化性能的工具。其中一个重要工具是 `PyTorch profiler "
"<https://pytorch.org/docs/stable/autograd.html#profiler>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's use the profiler along with the Chrome trace export functionality to "
"visualize the performance of our parallelized model:"
msgstr "让我们用分析器以及 Chrome 的追踪导出功能来可视化我们的并行化模型的性能："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This snippet of code will write out a file named ``parallel.json``. If you "
"navigate Google Chrome to ``chrome://tracing``, click the ``Load`` button, "
"and load in that JSON file, you should see a timeline like the following:"
msgstr ""
"这段代码将写出一个名为 ``parallel.json`` 的文件。如果你在 Google Chrome 中导航到 "
"``chrome://tracing``，点击 ``Load`` 按钮并加载该 JSON 文件，你应该能看到如下的时间线："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The horizontal axis of the timeline represents time and the vertical axis "
"represents threads of execution. As we can see, we are running two ``lstm`` "
"instances at a time. This is the result of our hard work parallelizing the "
"bidirectional layers!"
msgstr "时间线的水平轴表示时间，垂直轴表示执行线程。正如我们所见，我们一次运行两个 ``lstm`` 实例。这是我们辛苦工作的结果并行化双向层！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Parallelizing Models in the Ensemble"
msgstr "对模型集中的模型进行并行化"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You may have noticed that there is a further parallelization opportunity in "
"our code: we can also run the models contained in ``LSTMEnsemble`` in "
"parallel with each other. The way to do that is simple enough, this is how "
"we should change the ``forward`` method of ``LSTMEnsemble``:"
msgstr ""
"你可能注意到我们的代码中还有另一个并行化的机会：我们可以同时并行运行 ``LSTMEnsemble`` 中的模型。实现方法很简单，这就是我们应该如何修改"
" ``LSTMEnsemble`` 的 ``forward`` 方法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Or, if you value brevity, we can use list comprehensions:"
msgstr "或者，如果你喜欢简洁，我们可以使用列表推导式："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Like described in the intro, we've used loops to fork off tasks for each of "
"the models in our ensemble. We've then used another loop to wait for all of "
"the tasks to be completed. This provides even more overlap of computation."
msgstr "正如介绍中所述，我们使用循环来为模型集中每个模型创建任务。然后我们使用另一个循环等待所有任务完成。这提供了更多的计算重叠。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"With this small update, the script runs in ``1.4`` seconds, for a total "
"speedup of ``32%``! Pretty good for two lines of code."
msgstr "通过这个小更新，脚本运行时间缩短至 ``1.4`` 秒，总提速达到 ``32%``！仅两行代码就能取得不错的效果。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "We can also use the Chrome tracer again to see where's going on:"
msgstr "我们还可以再次使用 Chrome 追踪器来查看具体发生了什么："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can now see that all ``LSTM`` instances are being run fully in parallel."
msgstr "现在我们可以看到所有 ``LSTM`` 实例完全处于并行运行状态。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In this tutorial, we learned about ``fork()`` and ``wait()``, the basic APIs"
" for doing dynamic, inter-op parallelism in TorchScript. We saw a few "
"typical usage patterns for using these functions to parallelize the "
"execution of functions, methods, or ``Modules`` in TorchScript code. "
"Finally, we worked through an example of optimizing a model using this "
"technique and explored the performance measurement and visualization tooling"
" available in PyTorch."
msgstr ""
"在本教程中，我们了解了``fork()`` 和 ``wait()``，即在 TorchScript 中进行动态、操作并行性的基本 "
"API。我们看到了使用这些函数并行化函数、方法或``Module``执行的几种典型使用模式。最后，我们通过一个示例展示了如何使用这种技术优化模型，并探索了"
" PyTorch 提供的性能测量和可视化工具。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Extending TorchScript with Custom C++ Classes"
msgstr "使用自定义 C++ 类扩展 TorchScript"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial is a follow-on to the :doc:`custom operator "
"<torch_script_custom_ops>` tutorial, and introduces the API we've built for "
"binding C++ classes into TorchScript and Python simultaneously. The API is "
"very similar to `pybind11 <https://github.com/pybind/pybind11>`_, and most "
"of the concepts will transfer over if you're familiar with that system."
msgstr ""
"本教程是 :doc:`自定义操作符 <torch_script_custom_ops>` 教程的后续内容，介绍了我们构建的 API，用于同时将 C++ "
"类绑定到 TorchScript 和 Python。这个 API 非常类似于 `pybind11 "
"<https://github.com/pybind/pybind11>`_，如果你熟悉该系统，许多概念都可以直接迁移过来。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Implementing and Binding the Class in C++"
msgstr "在 C++ 中实现并绑定类"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this tutorial, we are going to define a simple C++ class that maintains "
"persistent state in a member variable."
msgstr "在本教程中，我们将定义一个简单的 C++ 类，该类在成员变量中保持持久状态。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "There are several things to note:"
msgstr "需要注意以下几点："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch/custom_class.h`` is the header you need to include to extend "
"TorchScript with your custom class."
msgstr "``torch/custom_class.h`` 是扩展 TorchScript 所需包含的头文件。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Notice that whenever we are working with instances of the custom class, we "
"do it via instances of ``c10::intrusive_ptr<>``. Think of ``intrusive_ptr`` "
"as a smart pointer like ``std::shared_ptr``, but the reference count is "
"stored directly in the object, as opposed to a separate metadata block (as "
"is done in ``std::shared_ptr``.  ``torch::Tensor`` internally uses the same "
"pointer type; and custom classes have to also use this pointer type so that "
"we can consistently manage different object types."
msgstr ""
"注意，每当我们使用自定义类的实例时，我们都是通过 ``c10::intrusive_ptr<>`` 实例来操作。可将 ``intrusive_ptr``"
" 类比作 ``std::shared_ptr`` 的智能指针，但它的引用计数直接存储在对象中，而不是像 ``std::shared_ptr`` "
"那样存储在一个单独的元数据块中。 ``torch::Tensor`` "
"内部使用同样的指针类型；而自定义类也必须使用此指针类型，以便我们能够一致地管理不同对象类型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The second thing to notice is that the user-defined class must inherit from "
"``torch::CustomClassHolder``. This ensures that the custom class has space "
"to store the reference count."
msgstr ""
"第二点需要注意的是用户定义的类必须继承自 ``torch::CustomClassHolder``。这样可以确保自定义类有空间存储引用计数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's take a look at how we will make this class visible to TorchScript,"
" a process called *binding* the class:"
msgstr "现在让我们看看如何使此类对 TorchScript 可见，这一过程称为 *绑定* 类："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building the Example as a C++ Project With CMake"
msgstr "使用 CMake 将示例构建为 C++ 项目"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, we're going to build the above C++ code with the `CMake "
"<https://cmake.org>`_ build system. First, take all the C++ code we've "
"covered so far and place it in a file called ``class.cpp``. Then, write a "
"simple ``CMakeLists.txt`` file and place it in the same directory. Here is "
"what ``CMakeLists.txt`` should look like:"
msgstr ""
"现在，我们将使用 `CMake <https://cmake.org>`_ 构建系统来构建上述 C++代码。首先，将迄今为止介绍的所有 "
"C++代码放入一个名为 ``class.cpp`` 的文件中。然后，编写一个简单的 ``CMakeLists.txt`` "
"文件，并将其放在相同目录下。以下是 ``CMakeLists.txt`` 的内容："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Also, create a ``build`` directory. Your file tree should look like this::"
msgstr "此外，创建一个 ``build`` 目录。你的文件树应该像这样::"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We assume you've setup your environment in the same way as described in the "
":doc:`previous tutorial <torch_script_custom_ops>`. Go ahead and invoke "
"cmake and then make to build the project:"
msgstr ""
"我们假设你按照 :doc:`之前的教程 <torch_script_custom_ops>` 设置了你的环境。继续调用 cmake，然后使用 make "
"来构建项目："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"What you'll find is there is now (among other things) a dynamic library file"
" present in the build directory. On Linux, this is probably named "
"``libcustom_class.so``. So the file tree should look like::"
msgstr ""
"你会发现现在（以及其他文件）在 build 目录下出现了一个动态库文件。在 Linux 上，这个文件很可能名为 "
"``libcustom_class.so``。因此文件树应该看起来像这样::"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using the C++ Class from Python and TorchScript"
msgstr "在 Python 和 TorchScript 中使用 C++ 类"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have our class and its registration compiled into an ``.so`` "
"file, we can load that `.so` into Python and try it out. Here's a script "
"that demonstrates that:"
msgstr "现在我们已经将类及其注册编译成一个 ``.so`` 文件，我们可以将此 `.so` 文件加载到 Python 中并试验它。以下是演示代码："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Saving, Loading, and Running TorchScript Code Using Custom Classes"
msgstr "使用自定义类保存、加载和运行 TorchScript代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can also use custom-registered C++ classes in a C++ process using "
"libtorch. As an example, let's define a simple ``nn.Module`` that "
"instantiates and calls a method on our MyStackClass class:"
msgstr ""
"我们还可以在使用 libtorch 的 C++ 进程中使用自定义注册的 "
"C++类。作为示例，让我们定义一个简单的``nn.Module``，它实例化并调用我们 MyStackClass 类中的方法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``foo.pt`` in our filesystem now contains the serialized TorchScript program"
" we've just defined."
msgstr "现在我们的文件系统中已经包含了我们刚刚定义的序列化 TorchScript 程序 ``foo.pt``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, we're going to define a new CMake project to show how you can load this"
" model and its required .so file. For a full treatment of how to do this, "
"please have a look at the `Loading a TorchScript Model in C++ Tutorial "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_."
msgstr ""
"现在，我们将定义一个新的 CMake 项目，以展示如何加载这个模型及其所需的 .so 文件。关于如何全面实现这一点，请参考 `在 C++ 中加载 "
"TorchScript 模型的教程 "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Similarly to before, let's create a file structure containing the "
"following::"
msgstr "与之前类似，让我们创建一个包含以下内容的文件结构::"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Notice we've copied over the serialized ``foo.pt`` file, as well as the "
"source tree from the ``custom_class_project`` above. We will be adding the "
"``custom_class_project`` as a dependency to this C++ project so that we can "
"build the custom class into the binary."
msgstr ""
"注意我们已经复制了序列化的 ``foo.pt`` 文件，以及上述 ``custom_class_project`` 的源代码树。我们将添加 "
"``custom_class_project`` 作为此C++ 项目的依赖项，以便将自定义类构建到二进制文件中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Let's populate ``infer.cpp`` with the following:"
msgstr "让我们用以下内容填充 ``infer.cpp``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "And similarly let's define our CMakeLists.txt file:"
msgstr "类似地让我们定义我们的 CMakeLists.txt 文件："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "You know the drill: ``cd build``, ``cmake``, and ``make``:"
msgstr "按照流程： ``cd build``，``cmake``，然后运行 ``make``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "And now we can run our exciting C++ binary:"
msgstr "现在我们可以运行令人激动的 C++ 二进制文件了："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Incredible!"
msgstr "不可思议！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Moving Custom Classes To/From IValues"
msgstr "将自定义类移动到/从 IValues 中"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"It's also possible that you may need to move custom classes into or out of "
"``IValue``s, such as when you take or return ``IValue``s from TorchScript "
"methods or you want to instantiate a custom class attribute in C++. For "
"creating an ``IValue`` from a custom C++ class instance:"
msgstr ""
"你可能还需要将自定义类移动到或移出 ``IValue`` 中，例如当你从 TorchScript 方法中获取或返回 ``IValue``，或者希望在 "
"C++ 中实例化一个自定义类属性时。对于从自定义 C++ 类实例创建 ``IValue``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``torch::make_custom_class<T>()`` provides an API similar to "
"c10::intrusive_ptr<T> in that it will take whatever set of arguments you "
"provide to it, call the constructor of T that matches that set of arguments,"
" and wrap that instance up and return it. However, instead of returning just"
" a pointer to a custom class object, it returns an ``IValue`` wrapping the "
"object. You can then pass this ``IValue`` directly to TorchScript."
msgstr ""
"``torch::make_custom_class<T>()`` 提供了一个类似于 c10::intrusive_ptr<T> 的 "
"API，它会接收你提供给它的任意参数集，调用匹配该参数集的 T "
"的构造函数，并包装该实例后返回。然而，它不是仅返回指向自定义类对象的指针，而是返回一个带有对象包装的 ``IValue``。然后你可以直接将这个 "
"``IValue`` 传递给 TorchScript。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In the event that you already have an ``intrusive_ptr`` pointing to your "
"class, you can directly construct an IValue from it using the constructor "
"``IValue(intrusive_ptr<T>)``."
msgstr ""
"如果你已经有一个指向你类的 ``intrusive_ptr``，你可以使用构造函数 ``IValue(intrusive_ptr<T>)`` "
"直接从它构造一个 IValue。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "For converting ``IValue`` back to custom classes:"
msgstr "对于将 ``IValue`` 转换回自定义类："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``IValue::toCustomClass<T>()`` will return an ``intrusive_ptr<T>`` pointing "
"to the custom class that the ``IValue`` contains. Internally, this function "
"is checking that ``T`` is registered as a custom class and that the "
"``IValue`` does in fact contain a custom class. You can check whether the "
"``IValue`` contains a custom class manually by calling ``isCustomClass()``."
msgstr ""
"``IValue::toCustomClass<T>()`` 会返回一个 ``intrusive_ptr<T>``，指向 ``IValue`` "
"包含的自定义类。内部，这个函数会检查 ``T`` 是否已经注册为自定义类，并检查 ``IValue`` 是否确实包含一个自定义类。你可以通过调用 "
"``isCustomClass()`` 手动检查 ``IValue`` 是否包含自定义类。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining Serialization/Deserialization Methods for Custom C++ Classes"
msgstr "为自定义 C++ 类定义序列化/反序列化方法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If you try to save a ``ScriptModule`` with a custom-bound C++ class as an "
"attribute, you'll get the following error:"
msgstr "如果你尝试保存一个以自定义绑定 C++ 类为属性的 ``ScriptModule``，你会收到如下错误消息："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This is because TorchScript cannot automatically figure out what information"
" save from your C++ class. You must specify that manually. The way to do "
"that is to define ``__getstate__`` and ``__setstate__`` methods on the class"
" using the special ``def_pickle`` method on ``class_``."
msgstr ""
"这是因为 TorchScript 无法自动确定要从你的 C++ 类中保存哪些信息。你必须手动指定。实现方式是为类定义 ``__getstate__`` "
"和 ``__setstate__`` 方法，使用 ``class_`` 的特殊 ``def_pickle`` 方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The semantics of ``__getstate__`` and ``__setstate__`` in TorchScript are "
"equivalent to that of the Python pickle module. You can `read more "
"<https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md#getstate-"
"and-setstate>`_ about how we use these methods."
msgstr ""
"TorchScript 中 ``__getstate__`` 和 ``__setstate__`` 的语义与 Python pickle "
"模块的语义相同。关于我们如何使用这些方法，可以 `阅读更多 "
"<https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md#getstate-"
"and-setstate>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here is an example of the ``def_pickle`` call we can add to the registration"
" of ``MyStackClass`` to include serialization methods:"
msgstr "以下是我们可以添加到 ``MyStackClass`` 注册中的 ``def_pickle`` 调用示例，以包含序列化方法："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We take a different approach from pybind11 in the pickle API. Whereas "
"pybind11 as a special function ``pybind11::pickle()`` which you pass into "
"``class_::def()``, we have a separate method ``def_pickle`` for this "
"purpose. This is because the name ``torch::jit::pickle`` was already taken, "
"and we didn't want to cause confusion."
msgstr ""
"我们在 pickle API 上采用了不同于 pybind11 的方法。pybind11 提供了一个名为 ``pybind11::pickle()`` "
"的特殊函数，你可以将其传递给 ``class_::def()``；而我们为了这个目的有一个单独的方法 "
"``def_pickle``。这是因为``torch::jit::pickle`` 这个名称已经被占用了，我们不希望引起混乱。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once we have defined the (de)serialization behavior in this way, our script "
"can now run successfully:"
msgstr "一旦以这种方式定义了（反）序列化行为，我们的脚本现在可以成功运行："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Defining Custom Operators that Take or Return Bound C++ Classes"
msgstr "定义采用或返回绑定 C++ 类的自定义操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once you've defined a custom C++ class, you can also use that class as an "
"argument or return from a custom operator (i.e. free functions). Suppose you"
" have the following free function:"
msgstr "一旦你定义了一个自定义 C++ 类，你也可以使用该类作为参数或返回值来自定义操作符（即自由函数）。假设你有如下自由函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can register it running the following code inside your ``TORCH_LIBRARY``"
" block:"
msgstr "你可以在你的 ``TORCH_LIBRARY`` 块内运行以下代码进行注册："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Refer to the `custom op tutorial "
"<https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_ for "
"more details on the registration API."
msgstr ""
"有关注册 API 的详细信息，请参考 `自定义操作符教程 "
"<https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Once this is done, you can use the op like the following example:"
msgstr "完成后，你可以像以下示例一样使用该操作符："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Registration of an operator that takes a C++ class as an argument requires "
"that the custom class has already been registered.  You can enforce this by "
"making sure the custom class registration and your free function definitions"
" are in the same ``TORCH_LIBRARY`` block, and that the custom class "
"registration comes first.  In the future, we may relax this requirement, so "
"that these can be registered in any order."
msgstr ""
"注册一个以 C++ 类为参数的操作符要求该自定义类已被注册。你可以通过确保自定义类注册与你的自由函数定义在同一个 ``TORCH_LIBRARY`` "
"块中，并且自定义类注册优先来强制执行这一点。在未来，我们可能会放松这个要求，以允许这些可以按任何顺序注册。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial walked you through how to expose a C++ class to TorchScript "
"(and by extension Python), how to register its methods, how to use that "
"class from Python and TorchScript, and how to save and load code using the "
"class and run that code in a standalone C++ process. You are now ready to "
"extend your TorchScript models with C++ classes that interface with third "
"party C++ libraries or implement any other use case that requires the lines "
"between Python, TorchScript and C++ to blend smoothly."
msgstr ""
"本教程向你介绍了如何将 C++ 类公开给 TorchScript（以及扩展到 Python）、如何注册其方法、如何从 Python 和 "
"TorchScript 使用该类，以及如何使用该类保存和加载代码，并在独立的 C++ 进程中运行该代码。你现在已经准备好通过接口第三方 "
"C++库或实现任何其他需要在 Python、TorchScript 和 C++之间平滑交互的用例，使用 C++ 类扩展你的 TorchScript "
"模型了。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"As always, if you run into any problems or have questions, you can use our "
"`forum <https://discuss.pytorch.org/>`_ or `GitHub issues "
"<https://github.com/pytorch/pytorch/issues>`_ to get in touch. Also, our "
"`frequently asked questions (FAQ) page "
"<https://pytorch.org/cppdocs/notes/faq.html>`_ may have helpful information."
msgstr ""
"一如既往，如果您遇到任何问题或有任何疑问，可以使用我们的 `论坛 <https://discuss.pytorch.org/>`_ 或 "
"`GitHub问题 <https://github.com/pytorch/pytorch/issues>`_ 联系我们。此外，我们的 `常见问题解答 "
"(FAQ) 页 <https://pytorch.org/cppdocs/notes/faq.html>`_ 可能也有有用的信息。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Extending TorchScript with Custom C++ Operators"
msgstr "使用自定义C++操作符扩展TorchScript"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial is deprecated as of PyTorch 2.4. Please see :ref:`custom-ops-"
"landing-page` for the newest up-to-date guides on PyTorch Custom Operators."
msgstr ""
"本教程自PyTorch 2.4起已被弃用。请参阅 :ref:`custom-ops-landing-page` "
"了解最新的PyTorch自定义操作符指南。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The PyTorch 1.0 release introduced a new programming model to PyTorch called"
" `TorchScript <https://pytorch.org/docs/master/jit.html>`_. TorchScript is a"
" subset of the Python programming language which can be parsed, compiled and"
" optimized by the TorchScript compiler. Further, compiled TorchScript models"
" have the option of being serialized into an on-disk file format, which you "
"can subsequently load and run from pure C++ (as well as Python) for "
"inference."
msgstr ""
"PyTorch 1.0发布引入了一种新的编程模型，称为 `TorchScript "
"<https://pytorch.org/docs/master/jit.html>`_。TorchScript是Python编程语言的一个子集，可以通过TorchScript编译器进行解析、编译并优化。此外，编译后的TorchScript模型可以选择序列化为一种磁盘文件格式，之后可以直接从纯C++（以及Python）中加载和运行用于推理。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"TorchScript supports a large subset of operations provided by the ``torch`` "
"package, allowing you to express many kinds of complex models purely as a "
"series of tensor operations from PyTorch's \"standard library\". "
"Nevertheless, there may be times where you find yourself in need of "
"extending TorchScript with a custom C++ or CUDA function. While we recommend"
" that you only resort to this option if your idea cannot be expressed "
"(efficiently enough) as a simple Python function, we do provide a very "
"friendly and simple interface for defining custom C++ and CUDA kernels using"
" `ATen <https://pytorch.org/cppdocs/#aten>`_, PyTorch's high performance C++"
" tensor library. Once bound into TorchScript, you can embed these custom "
"kernels (or \"ops\") into your TorchScript model and execute them both in "
"Python and in their serialized form directly in C++."
msgstr ""
"TorchScript支持由``torch``包提供的大量操作，允许您仅通过PyTorch的“标准库”中的一系列张量操作来表达多种复杂模型。然而，有时您可能需要使用自定义的C++或CUDA函数来扩展TorchScript。虽然我们建议您仅在无法以简单的Python函数有效表达您的想法时才选择此选项，但我们确实提供了一个非常友好和简单的接口，可以使用"
" `ATen "
"<https://pytorch.org/cppdocs/#aten>`_，即PyTorch高性能C++张量库，定义自定义C++和CUDA内核。一旦绑定到TorchScript中，您可以将这些自定义内核（或“操作符”）嵌入到您的TorchScript模型中，并直接在Python和C++中调用它们的序列化形式运行。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The following paragraphs give an example of writing a TorchScript custom op "
"to call into `OpenCV <https://www.opencv.org>`_, a computer vision library "
"written in C++. We will discuss how to work with tensors in C++, how to "
"efficiently convert them to third party tensor formats (in this case, OpenCV"
" ``Mat``), how to register your operator with the TorchScript runtime and "
"finally how to compile the operator and use it in Python and C++."
msgstr ""
"以下段落给出了一个通过TorchScript自定义操作符调用 `OpenCV <https://www.opencv.org>`_ "
"（一个用C++编写的计算机视觉库）的示例。我们将讨论如何在C++中处理张量，如何高效地将它们转换为第三方张量格式（在本例中为OpenCV "
"``Mat``），如何将操作符注册到TorchScript运行时，以及如何编译操作符并在Python和C++中使用它。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Implementing the Custom Operator in C++"
msgstr "在C++中实现自定义操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"For this tutorial, we'll be exposing the `warpPerspective "
"<https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective>`_"
" function, which applies a perspective transformation to an image, from "
"OpenCV to TorchScript as a custom operator. The first step is to write the "
"implementation of our custom operator in C++. Let's call the file for this "
"implementation ``op.cpp`` and make it look like this:"
msgstr ""
"在本教程中，我们将会将 `warpPerspective "
"<https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective>`_"
" "
"函数从OpenCV暴露为TorchScript的自定义操作符，该函数对图像应用透视变换。第一步是使用C++编写我们自定义操作符的实现。我们将这个实现的文件命名为"
" ``op.cpp``，并让其代码如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The code for this operator is quite short. At the top of the file, we "
"include the OpenCV header file, ``opencv2/opencv.hpp``, alongside the "
"``torch/script.h`` header which exposes all the necessary goodies from "
"PyTorch's C++ API that we need to write custom TorchScript operators. Our "
"function ``warp_perspective`` takes two arguments: an input ``image`` and "
"the ``warp`` transformation matrix we wish to apply to the image. The type "
"of these inputs is ``torch::Tensor``, PyTorch's tensor type in C++ (which is"
" also the underlying type of all tensors in Python). The return type of our "
"``warp_perspective`` function will also be a ``torch::Tensor``."
msgstr ""
"该操作符的代码非常简短。在文件的顶部，我们引入了OpenCV的头文件``opencv2/opencv.hpp``，以及``torch/script.h``头文件，该文件包含了必要的PyTorch"
" C++ "
"API功能，可以用来编写自定义TorchScript操作符。我们的函数``warp_perspective``接受两个参数：输入的``image``和我们希望应用到图像上的``warp``变换矩阵。这些输入的类型是``torch::Tensor``，即PyTorch在C++中的张量类型（也是Python中所有张量的底层类型）。我们的``warp_perspective``函数的返回类型也是``torch::Tensor``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"See `this note <https://pytorch.org/cppdocs/notes/tensor_basics.html>`_ for "
"more information about ATen, the library that provides the ``Tensor`` class "
"to PyTorch. Further, `this tutorial "
"<https://pytorch.org/cppdocs/notes/tensor_creation.html>`_ describes how to "
"allocate and initialize new tensor objects in C++ (not required for this "
"operator)."
msgstr ""
"有关ATen（为PyTorch提供``Tensor``类的库）更多信息，请参阅 `此说明 "
"<https://pytorch.org/cppdocs/notes/tensor_basics.html>`_。此外，`本教程 "
"<https://pytorch.org/cppdocs/notes/tensor_creation.html>`_ "
"描述了如何在C++中分配和初始化新的张量对象（本操作符中不需要这部分内容）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The TorchScript compiler understands a fixed number of types. Only these "
"types can be used as arguments to your custom operator. Currently these "
"types are: ``torch::Tensor``, ``torch::Scalar``, ``double``, ``int64_t`` and"
" ``std::vector`` s of these types. Note that *only* ``double`` and *not* "
"``float``, and *only* ``int64_t`` and *not* other integral types such as "
"``int``, ``short`` or ``long`` are supported."
msgstr ""
"TorchScript编译器支持固定数量的类型。只有这些类型可以用作自定义操作符的参数。这些类型目前是：``torch::Tensor``、``torch::Scalar``、``double``、``int64_t``以及这些类型的``std::vector``。请注意，*只有*"
" ``double`` 支持，而不是 ``float``，和*只有* ``int64_t`` 支持，而不是其他整数类型，例如 "
"``int``、``short`` 或 ``long``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Inside of our function, the first thing we need to do is convert our PyTorch"
" tensors to OpenCV matrices, as OpenCV's ``warpPerspective`` expects "
"``cv::Mat`` objects as inputs. Fortunately, there is a way to do this "
"**without copying any** data. In the first few lines,"
msgstr ""
"在我们的函数内部，我们首先需要将PyTorch张量转换为OpenCV矩阵，因为OpenCV的``warpPerspective``期望输入为``cv::Mat``对象。幸运的是，有一种方法可以执行此操作**而无需复制任何**数据。在前几行中，"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"we are calling `this constructor "
"<https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a922de793eabcec705b3579c5f95a643e>`_"
" of the OpenCV ``Mat`` class to convert our tensor to a ``Mat`` object. We "
"pass it the number of rows and columns of the original ``image`` tensor, the"
" datatype (which we'll fix as ``float32`` for this example), and finally a "
"raw pointer to the underlying data -- a ``float*``. What is special about "
"this constructor of the ``Mat`` class is that it does not copy the input "
"data. Instead, it will simply reference this memory for all operations "
"performed on the ``Mat``. If an in-place operation is performed on the "
"``image_mat``, this will be reflected in the original ``image`` tensor (and "
"vice-versa). This allows us to call subsequent OpenCV routines with the "
"library's native matrix type, even though we're actually storing the data in"
" a PyTorch tensor. We repeat this procedure to convert the ``warp`` PyTorch "
"tensor to the ``warp_mat`` OpenCV matrix:"
msgstr ""
"我们通过调用OpenCV ``Mat`` 类的 `此构造函数 "
"<https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a922de793eabcec705b3579c5f95a643e>`_，将我们的张量转换为``Mat``对象。我们传递了原始``image``张量的行数和列数、数据类型（在此示例中我们固定为``float32``），以及底层数据的原始指针——一个``float*``。此``Mat``类的构造函数的特殊之处在于，它不会复制输入数据。相反，它只是引用此内存来执行有关``Mat``的所有操作。如果对``image_mat``执行了就地操作，这将在原始``image``张量中反映出来（反之亦然）。这使我们可以调用库的本地矩阵类型的后续OpenCV例程，尽管我们实际上将数据存储在PyTorch张量中。我们重复此步骤，将``warp``"
" PyTorch张量转换为``warp_mat`` OpenCV矩阵："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, we are ready to call the OpenCV function we were so eager to use in "
"TorchScript: ``warpPerspective``. For this, we pass the OpenCV function the "
"``image_mat`` and ``warp_mat`` matrices, as well as an empty output matrix "
"called ``output_mat``. We also specify the size ``dsize`` we want the output"
" matrix (image) to be. It is hardcoded to ``8 x 8`` for this example:"
msgstr ""
"接下来，我们可以调用我们在TorchScript中如此渴望使用的OpenCV函数：``warpPerspective``。为此，我们向OpenCV函数传递了``image_mat``和``warp_mat``矩阵，以及一个名为``output_mat``的空输出矩阵。我们还指定了希望输出矩阵（图像）具有的大小参数``dsize``。在本例中，我们将其硬编码为``8"
" x 8``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The final step in our custom operator implementation is to convert the "
"``output_mat`` back into a PyTorch tensor, so that we can further use it in "
"PyTorch. This is strikingly similar to what we did earlier to convert in the"
" other direction. In this case, PyTorch provides a ``torch::from_blob`` "
"method. A *blob* in this case is intended to mean some opaque, flat pointer "
"to memory that we want to interpret as a PyTorch tensor. The call to "
"``torch::from_blob`` looks like this:"
msgstr ""
"我们自定义操作符实现的最后一步是将``output_mat``转换回PyTorch张量，以便我们可以在PyTorch中进一步使用。这和我们之前将数据转换到另一个方向类似。在此情况下，PyTorch提供了一个``torch::from_blob``方法。本例中的*blob*指的是我们希望解释为PyTorch张量的一些不透明平面内存指针。调用``torch::from_blob``的方法如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We use the ``.ptr<float>()`` method on the OpenCV ``Mat`` class to get a raw"
" pointer to the underlying data (just like ``.data_ptr<float>()`` for the "
"PyTorch tensor earlier). We also specify the output shape of the tensor, "
"which we hardcoded as ``8 x 8``. The output of ``torch::from_blob`` is then "
"a ``torch::Tensor``, pointing to the memory owned by the OpenCV matrix."
msgstr ""
"我们在OpenCV ``Mat`` "
"类上使用``.ptr<float>()``方法，获取指向底层数据的原始指针（就像之前的PyTorch张量中的``.data_ptr<float>()``）。我们还指定了张量的输出形状，在这里我们将其硬编码为``8"
" x 8``。``torch::from_blob``的输出是一个``torch::Tensor``，它指向由OpenCV矩阵拥有的内存。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Before returning this tensor from our operator implementation, we must call "
"``.clone()`` on the tensor to perform a memory copy of the underlying data. "
"The reason for this is that ``torch::from_blob`` returns a tensor that does "
"not own its data. At that point, the data is still owned by the OpenCV "
"matrix. However, this OpenCV matrix will go out of scope and be deallocated "
"at the end of the function. If we returned the ``output`` tensor as-is, it "
"would point to invalid memory by the time we use it outside the function. "
"Calling ``.clone()`` returns a new tensor with a copy of the original data "
"that the new tensor owns itself. It is thus safe to return to the outside "
"world."
msgstr ""
"在将此张量从我们的操作符实现中返回之前，我们必须在张量上调用``.clone()``以执行底层数据的内存复制。这样做的原因是``torch::from_blob``返回的张量并不拥有其数据。在这一点上，数据仍然由OpenCV矩阵拥有。然而，这个OpenCV矩阵将在函数结束时超出作用域并被释放。如果我们原样返回``output``张量，那么当我们在函数外部使用它时，它将指向无效内存。调用``.clone()``会返回一个新的张量，该张量拥有原始数据的副本并自身拥有数据。因此可以安全地返回到外部使用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Registering the Custom Operator with TorchScript"
msgstr "将自定义操作符注册到TorchScript"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that have implemented our custom operator in C++, we need to *register* "
"it with the TorchScript runtime and compiler. This will allow the "
"TorchScript compiler to resolve references to our custom operator in "
"TorchScript code. If you have ever used the pybind11 library, our syntax for"
" registration resembles the pybind11 syntax very closely.  To register a "
"single function, we write:"
msgstr ""
"现在我们已经在C++中实现了自定义操作符，我们需要将其*注册*到TorchScript运行时和编译器中。这将允许TorchScript编译器解析TorchScript代码中对我们自定义操作符的引用。如果您曾经使用过pybind11库，注册的语法与pybind11的语法非常相似。要注册单个函数，我们可以这样编写："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"somewhere at the top level of our ``op.cpp`` file.  The ``TORCH_LIBRARY`` "
"macro creates a function that will be called when your program starts.  The "
"name of your library (``my_ops``) is given as the first argument (it should "
"not be in quotes).  The second argument (``m``) defines a variable of type "
"``torch::Library`` which is the main interface to register your operators. "
"The method ``Library::def`` actually creates an operator named "
"``warp_perspective``, exposing it to both Python and TorchScript.  You can "
"define as many operators as you like by making multiple calls to ``def``."
msgstr ""
"在``op.cpp``文件的顶级位置。``TORCH_LIBRARY`` "
"宏创建了一个将在程序启动时调用的函数。库的名称（``my_ops``）作为第一个参数（不需要加引号）。第二个参数``m``定义了一个类型为``torch::Library``的变量，这是注册操作符的主要接口。``Library::def``方法实际上创建了一个名为``warp_perspective``的操作符，使其在Python和TorchScript中都可用。您可以通过多次调用``def``定义任意多的操作符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Behinds the scenes, the ``def`` function is actually doing quite a bit of "
"work: it is using template metaprogramming to inspect the type signature of "
"your function and translate it into an operator schema which specifies the "
"operators type within TorchScript's type system."
msgstr ""
"在幕后，``def``函数实际上做了很多工作：它使用模板元编程来检查您函数的类型签名，并将其翻译为一个操作符模式（schema），该模式指定了操作符在TorchScript类型系统中的类型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building the Custom Operator"
msgstr "构建自定义操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now that we have implemented our custom operator in C++ and written its "
"registration code, it is time to build the operator into a (shared) library "
"that we can load into Python for research and experimentation, or into C++ "
"for inference in a no-Python environment. There exist multiple ways to build"
" our operator, using either pure CMake, or Python alternatives like "
"``setuptools``. For brevity, the paragraphs below only discuss the CMake "
"approach. The appendix of this tutorial dives into other alternatives."
msgstr ""
"现在我们已经在C++中实现了自定义操作符并编写了其注册代码，是时候将操作符构建为（共享）库了，这样我们就可以在Python中加载它用于研究和实验，或者在无Python环境中用C++进行推理了。存在多种构建操作符的方法，可以使用纯CMake或Python替代方案如``setuptools``。为了简洁，下文仅讨论CMake方法。本教程的附录深入探讨了其他替代方案。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Environment setup"
msgstr "环境设置"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We need an installation of PyTorch and OpenCV.  The easiest and most "
"platform independent way to get both is to via Conda::"
msgstr "我们需要安装PyTorch和OpenCV。获取这两者的最简单和与平台无关的方式是通过Conda::"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building with CMake"
msgstr "使用CMake构建"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To build our custom operator into a shared library using the `CMake "
"<https://cmake.org>`_ build system, we need to write a short "
"``CMakeLists.txt`` file and place it with our previous ``op.cpp`` file. For "
"this, let's agree on a a directory structure that looks like this::"
msgstr ""
"要使用 `CMake <https://cmake.org>`_ "
"构建系统将我们的自定义操作符构建为共享库，我们需要编写一个简短的``CMakeLists.txt``文件，并将其放置于我们之前的``op.cpp``文件旁。为此，让我们确定一个目录结构，如下所示::"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The contents of our ``CMakeLists.txt`` file should then be the following:"
msgstr "我们的``CMakeLists.txt``文件的内容应如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To now build our operator, we can run the following commands from our "
"``warp_perspective`` folder:"
msgstr "现在要构建操作符，我们可以从``warp_perspective``文件夹运行以下命令："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"which will place a ``libwarp_perspective.so`` shared library file in the "
"``build`` folder. In the ``cmake`` command above, we use the helper variable"
" ``torch.utils.cmake_prefix_path`` to conveniently tell us where the cmake "
"files for our PyTorch install are."
msgstr ""
"这将在``build``文件夹中生成一个名为``libwarp_perspective.so``的共享库文件。在上文的``cmake``命令中，我们使用了帮助变量``torch.utils.cmake_prefix_path``来方便地找到我们安装的PyTorch的cmake文件位置。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We will explore how to use and call our operator in detail further below, "
"but to get an early sensation of success, we can try running the following "
"code in Python:"
msgstr "我们将在下文详细探索如何使用和调用操作符，但为了一开始获得一些成功的直观感受，我们可以尝试在Python中运行以下代码："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "If all goes well, this should print something like::"
msgstr "如果一切顺利，应该会打印如下内容::"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"which is the Python function we will later use to invoke our custom "
"operator."
msgstr "这是我们稍后用来调用自定义操作符的Python函数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using the TorchScript Custom Operator in Python"
msgstr "在Python中使用TorchScript自定义操作符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Once our custom operator is built into a shared library  we are ready to use"
" this operator in our TorchScript models in Python. There are two parts to "
"this: first loading the operator into Python, and second using the operator "
"in TorchScript code."
msgstr ""
"一旦我们将自定义运算符构建为一个共享库，我们就可以在 Python 中将此运算符用于 TorchScript 模型。这包括两部分：首先将运算符加载到 "
"Python 中，然后在 TorchScript 代码中使用该运算符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You already saw how to import your operator into Python: "
"``torch.ops.load_library()``. This function takes the path to a shared "
"library containing custom operators, and loads it into the current process. "
"Loading the shared library will also execute the ``TORCH_LIBRARY`` block. "
"This will register our custom operator with the TorchScript compiler and "
"allow us to use that operator in TorchScript code."
msgstr ""
"你已经见过如何将你的运算符导入到 Python "
"中：``torch.ops.load_library()``。此函数接收包含自定义运算符的共享库路径，并将其加载到当前进程中。加载共享库还会执行 "
"``TORCH_LIBRARY`` 块，这会将我们的自定义运算符注册到 TorchScript 编译器中，并允许我们在 TorchScript "
"代码中使用该运算符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"You can refer to your loaded operator as "
"``torch.ops.<namespace>.<function>``, where ``<namespace>`` is the namespace"
" part of your operator name, and ``<function>`` the function name of your "
"operator. For the operator we wrote above, the namespace was ``my_ops`` and "
"the function name ``warp_perspective``, which means our operator is "
"available as ``torch.ops.my_ops.warp_perspective``. While this function can "
"be used in scripted or traced TorchScript modules, we can also just use it "
"in vanilla eager PyTorch and pass it regular PyTorch tensors:"
msgstr ""
"你可以通过 ``torch.ops.<namespace>.<function>`` 引用加载的运算符，其中 ``<namespace>`` "
"是运算符名称的命名空间部分，``<function>`` 是运算符的函数名称。对于我们上面编写的运算符，命名空间是 ``my_ops``，函数名称为 "
"``warp_perspective``，这意味着我们的运算符可以通过 ``torch.ops.my_ops.warp_perspective`` "
"调用。虽然该函数可用于脚本化或跟踪的 TorchScript 模块，我们也可以直接在普通的 eager 模式 PyTorch 中使用并传递常规的 "
"PyTorch 张量："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "producing:"
msgstr "生成结果："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"What happens behind the scenes is that the first time you access "
"``torch.ops.namespace.function`` in Python, the TorchScript compiler (in C++"
" land) will see if a function ``namespace::function`` has been registered, "
"and if so, return a Python handle to this function that we can subsequently "
"use to call into our C++ operator implementation from Python. This is one "
"noteworthy difference between TorchScript custom operators and C++ "
"extensions: C++ extensions are bound manually using pybind11, while "
"TorchScript custom ops are bound on the fly by PyTorch itself. Pybind11 "
"gives you more flexibility with regards to what types and classes you can "
"bind into Python and is thus recommended for purely eager code, but it is "
"not supported for TorchScript ops."
msgstr ""
"幕后发生的事情是，当你第一次在 Python 中访问 ``torch.ops.namespace.function`` 时，TorchScript "
"编译器（C++层面）将检查是否注册了一个 ``namespace::function`` 的函数。如果注册了，它将返回一个 Python "
"句柄，通过此句柄我们可以调用从 Python 到 C++ 运算符实现的接口。这是 TorchScript 自定义运算符和 C++ "
"扩展之间的一个显著差异：C++ 扩展需要使用 pybind11 手动绑定，而 TorchScript 自定义运算符是由 PyTorch "
"动态绑定的。pybind11 在绑定类型和类到 Python 时提供更多灵活性，因此更适合纯粹的 eager 代码，但不支持 TorchScript "
"运算符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"From here on, you can use your custom operator in scripted or traced code "
"just as you would other functions from the ``torch`` package. In fact, "
"\"standard library\" functions like ``torch.matmul`` go through largely the "
"same registration path as custom operators, which makes custom operators "
"really first-class citizens when it comes to how and where they can be used "
"in TorchScript.  (One difference, however, is that standard library "
"functions have custom written Python argument parsing logic that differs "
"from ``torch.ops`` argument parsing.)"
msgstr ""
"从这里开始，你可以像使用 ``torch`` 包中的其他函数一样，在脚本化或跟踪代码中使用自定义运算符。事实上，像 ``torch.matmul`` "
"这样的“标准库”函数基本上通过与自定义运算符相同的注册路径，因此在 TorchScript "
"中使用自定义运算符就像使用其他函数一样（不过，标准库函数拥有自定义编写的 Python 参数解析逻辑，这与 ``torch.ops`` "
"的参数解析有所不同）。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using the Custom Operator with Tracing"
msgstr "在跟踪中使用自定义运算符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's start by embedding our operator in a traced function. Recall that for "
"tracing, we start with some vanilla Pytorch code:"
msgstr "让我们从将自定义运算符嵌入到跟踪函数开始。回顾一下，对于跟踪，我们从一些普通的 PyTorch 代码开始："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"and then call ``torch.jit.trace`` on it. We further pass ``torch.jit.trace``"
" some example inputs, which it will forward to our implementation to record "
"the sequence of operations that occur as the inputs flow through it. The "
"result of this is effectively a \"frozen\" version of the eager PyTorch "
"program, which the TorchScript compiler can further analyze, optimize and "
"serialize:"
msgstr ""
"然后调用 ``torch.jit.trace``，并传递一些示例输入，它将把输入传递给我们的实现，记录输入流经的操作序列。结果实际上是 eager "
"PyTorch 程序的一个“冻结”版本，TorchScript 编译器可以进一步分析、优化和序列化它："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Producing::"
msgstr "生成结果："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, the exciting revelation is that we can simply drop our custom operator "
"into our PyTorch trace as if it were ``torch.relu`` or any other ``torch`` "
"function:"
msgstr ""
"现在令人兴奋的发现是，我们可以像使用 ``torch.relu`` 或任何其他 ``torch`` 函数一样，轻松将自定义运算符直接插入 PyTorch"
" 跟踪中："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "and then trace it as before:"
msgstr "然后像之前一样进行跟踪："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Integrating TorchScript custom ops into traced PyTorch code is as easy as "
"this!"
msgstr "将 TorchScript 自定义运算符集成到跟踪的 PyTorch 代码中就是这么简单！"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using the Custom Operator with Script"
msgstr "在脚本中使用自定义运算符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Besides tracing, another way to arrive at a TorchScript representation of a "
"PyTorch program is to directly write your code *in* TorchScript. TorchScript"
" is largely a subset of the Python language, with some restrictions that "
"make it easier for the TorchScript compiler to reason about programs. You "
"turn your regular PyTorch code into TorchScript by annotating it with "
"``@torch.jit.script`` for free functions and ``@torch.jit.script_method`` "
"for methods in a class (which must also derive from "
"``torch.jit.ScriptModule``). See `here "
"<https://pytorch.org/docs/master/jit.html>`_ for more details on TorchScript"
" annotations."
msgstr ""
"除了跟踪，另一种获取 PyTorch 程序的 TorchScript 表示的方法是直接 *用* TorchScript 编写代码。TorchScript"
" 基本上是 Python 语言的一个子集，带有一些限制以便 TorchScript 编译器更容易推理程序。你可以通过给自由函数使用 "
"``@torch.jit.script`` 注解，或给类中的方法使用 ``@torch.jit.script_method`` 注解（这些类还必须继承自"
" ``torch.jit.ScriptModule``）将常规的 PyTorch 代码转换为 TorchScript。更多关于 TorchScript "
"注解的细节参见 `此处 <https://pytorch.org/docs/master/jit.html>`_。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"One particular reason to use TorchScript instead of tracing is that tracing "
"is unable to capture control flow in PyTorch code. As such, let us consider "
"this function which does use control flow:"
msgstr ""
"使用 TorchScript 而非跟踪的一个特殊原因是，跟踪无法捕获 PyTorch 代码中的控制流。因此，让我们考虑一个用到了控制流的函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"To convert this function from vanilla PyTorch to TorchScript, we annotate it"
" with ``@torch.jit.script``:"
msgstr "为了将这个函数从普通 PyTorch 转换为 TorchScript，我们用 ``@torch.jit.script`` 注解它："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This will just-in-time compile the ``compute`` function into a graph "
"representation, which we can inspect in the ``compute.graph`` property:"
msgstr "这会即时编译 ``compute`` 函数为图形表示形式，我们可以在 ``compute.graph`` 属性中检查："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"And now, just like before, we can use our custom operator like any other "
"function inside of our script code:"
msgstr "现在，与之前一样，我们可以像其他任何函数一样，在脚本代码中使用自定义运算符："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"When the TorchScript compiler sees the reference to "
"``torch.ops.my_ops.warp_perspective``, it will find the implementation we "
"registered via the ``TORCH_LIBRARY`` function in C++, and compile it into "
"its graph representation:"
msgstr ""
"当 TorchScript 编译器在图中检测到对 ``torch.ops.my_ops.warp_perspective`` 的引用时，它将找到我们通过"
" C++ 中的 ``TORCH_LIBRARY`` 注册的实现，并将其编译为图形表示形式："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Notice in particular the reference to ``my_ops::warp_perspective`` at the "
"end of the graph."
msgstr "特别注意图的末尾对 ``my_ops::warp_perspective`` 的引用。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The TorchScript graph representation is still subject to change. Do not rely"
" on it looking like this."
msgstr "TorchScript 的图形表示仍可能变化。请不要依赖其当前的展现形式。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"And that's really it when it comes to using our custom operator in Python. "
"In short, you import the library containing your operator(s) using "
"``torch.ops.load_library``, and call your custom op like any other ``torch``"
" operator from your traced or scripted TorchScript code."
msgstr ""
"这基本上就是在 Python 中使用自定义运算符的全部内容。简而言之，你使用 ``torch.ops.load_library`` "
"导入包含运算符的库，并像在跟踪或脚本化的 TorchScript 代码中调用其他 ``torch`` 运算符一样调用自定义运算符。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Using the TorchScript Custom Operator in C++"
msgstr "在 C++ 中使用 TorchScript 自定义运算符"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"One useful feature of TorchScript is the ability to serialize a model into "
"an on-disk file. This file can be sent over the wire, stored in a file "
"system or, more importantly, be dynamically deserialized and executed "
"without needing to keep the original source code around. This is possible in"
" Python, but also in C++. For this, PyTorch provides `a pure C++ API "
"<https://pytorch.org/cppdocs/>`_ for deserializing as well as executing "
"TorchScript models. If you haven't yet, please read `the tutorial on loading"
" and running serialized TorchScript models in C++ "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_, on which the "
"next few paragraphs will build."
msgstr ""
"TorchScript "
"的一个有用功能是可以将模型序列化为磁盘文件。这个文件可以通过网络发送、存储在文件系统中，或者更重要的是可以动态反序列化并执行，而无需保留原始源代码。这在"
" Python 中适用，也在 C++ 中适用。为此，PyTorch 提供了 `一个纯 C++ API "
"<https://pytorch.org/cppdocs/>`_ 用于反序列化和执行 TorchScript 模型。如果你还没有，请阅读 "
"`加载和运行序列化 TorchScript 模型的 C++ 教程 "
"<https://pytorch.org/tutorials/advanced/cpp_export.html>`_，接下来的几段解释的是基于该内容的。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"In short, custom operators can be executed just like regular ``torch`` "
"operators even when deserialized from a file and run in C++. The only "
"requirement for this is to link the custom operator shared library we built "
"earlier with the C++ application in which we execute the model. In Python, "
"this worked simply calling ``torch.ops.load_library``. In C++, you need to "
"link the shared library with your main application in whatever build system "
"you are using. The following example will showcase this using CMake."
msgstr ""
"简而言之，即使从文件中反序列化并在 C++ 中运行，自定义运算符也可以像常规 ``torch`` "
"运算符一样执行。这唯一的要求是将之前构建的自定义运算符共享库与执行模型的 C++ 应用程序链接起来。在 Python 中，这通过简单地调用 "
"``torch.ops.load_library`` 实现。在 C++ 中，你需要在所使用的构建系统中将共享库与主应用程序链接。以下示例将使用 "
"CMake 演示这一点。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Technically, you can also dynamically load the shared library into your C++ "
"application at runtime in much the same way we did it in Python. On Linux, "
"`you can do this with dlopen <https://tldp.org/HOWTO/Program-Library-"
"HOWTO/dl-libraries.html>`_. There exist equivalents on other platforms."
msgstr ""
"理论上，你也可以像在 Python 中一样，在运行时将共享库动态加载到你的 C++ 应用程序中。在 Linux 上，`你可以使用 dlopen "
"实现这一点 <https://tldp.org/HOWTO/Program-Library-HOWTO/dl-"
"libraries.html>`_。其他平台也有类似的方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Building on the C++ execution tutorial linked above, let's start with a "
"minimal C++ application in one file, ``main.cpp`` in a different folder from"
" our custom operator, that loads and executes a serialized TorchScript "
"model:"
msgstr ""
"以之前链接的 C++ 执行教程为基础，让我们从一个单文件的最小 C++ 应用程序开始，它位于与我们自定义运算符不同的文件夹中，名为 "
"``main.cpp``，这个应用程序加载并执行一个序列化的 TorchScript 模型："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Along with a small ``CMakeLists.txt`` file:"
msgstr "以及一个小型的 ``CMakeLists.txt`` 文件："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "At this point, we should be able to build the application:"
msgstr "此时我们应该可以构建应用程序："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "And run it without passing a model just yet:"
msgstr "运行它，但暂时还不传递模型："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Next, let's serialize the script function we wrote earlier that uses our "
"custom operator:"
msgstr "接下来，让我们序列化之前编写使用自定义运算符的脚本函数："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The last line will serialize the script function into a file called "
"\"example.pt\". If we then pass this serialized model to our C++ "
"application, we can run it straight away:"
msgstr ""
"最后一行会将脚本函数序列化为一个名为 \"example.pt\" 的文件。如果我们将这个序列化模型传递给我们的 C++ 应用程序，我们可以立即运行它："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Or maybe not. Maybe not just yet. Of course! We haven't linked the custom "
"operator library with our application yet. Let's do this right now, and to "
"do it properly let's update our file organization slightly, to look like "
"this::"
msgstr ""
"或者也许不能。也许还不行。当然！我们还没有将自定义运算符库与应用程序链接起来。现在就让我们来做，并且为了正确完成这一步，让我们稍微更新文件组织，结构如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This will allow us to add the ``warp_perspective`` library CMake target as a"
" subdirectory of our application target. The top level ``CMakeLists.txt`` in"
" the ``example_app`` folder should look like this:"
msgstr ""
"这将允许我们将 ``warp_perspective`` 库的 CMake 目标作为我们应用程序目标的一个子目录。顶级 ``example_app`` "
"文件夹中的 ``CMakeLists.txt`` 文件应如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This basic CMake configuration looks much like before, except that we add "
"the ``warp_perspective`` CMake build as a subdirectory. Once its CMake code "
"runs, we link our ``example_app`` application with the ``warp_perspective`` "
"shared library."
msgstr ""
"这个基本的 CMake 配置看起来与之前很相似，但我们添加了 ``warp_perspective`` CMake 构建作为一个子目录。一旦它的 "
"CMake 代码运行完，我们就将 ``example_app`` 应用程序与 ``warp_perspective`` 共享库链接起来。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"There is one crucial detail embedded in the above example: The ``-Wl,--no-"
"as-needed`` prefix to the ``warp_perspective`` link line. This is required "
"because we will not actually be calling any function from the "
"``warp_perspective`` shared library in our application code. We only need "
"the ``TORCH_LIBRARY`` function to run. Inconveniently, this confuses the "
"linker and makes it think it can just skip linking against the library "
"altogether. On Linux, the ``-Wl,--no-as-needed`` flag forces the link to "
"happen (NB: this flag is specific to Linux!). There are other workarounds "
"for this. The simplest is to define *some function* in the operator library "
"that you need to call from the main application. This could be as simple as "
"a function ``void init();`` declared in some header, which is then defined "
"as ``void init() { }`` in the operator library. Calling this ``init()`` "
"function in the main application will give the linker the impression that "
"this is a library worth linking against. Unfortunately, this is outside of "
"our control, and we would rather let you know the reason and the simple "
"workaround for this than handing you some opaque macro to plop in your code."
msgstr ""
"上面示例中嵌入了一个重要细节：``warp_perspective`` 链接行的 ``-Wl,--no-as-needed`` "
"前缀。这是必要的，因为我们的应用程序代码中实际上不会调用 ``warp_perspective`` 共享库中的任何函数。我们只需要运行 "
"``TORCH_LIBRARY`` 函数。不幸的是，这会迷惑链接器，使其认为可以完全跳过链接到这个库。在 Linux 上，``-Wl,--no-as-"
"needed`` 标志强制进行链接（注意：此标志仅适用于 Linux！）。还有其他一些解决方法。最简单的是在操作符库中定义 "
"*某些函数*，以便需要在主应用程序中调用。这可以是一个简单的声明在某些头文件中的函数 ``void init();`` ，然后在运算符库中定义为 "
"``void init() { }``。在主应用程序中调用这个 ``init()`` "
"函数会让链接器认为这是一个值得链接的库。不幸的是，我们对此无能为力，所以我们更愿意告诉你背后的原因和这个简单的解决办法，而不是给你一些黑箱宏放到代码中。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, since we find the ``Torch`` package at the top level now, the "
"``CMakeLists.txt`` file in the  ``warp_perspective`` subdirectory can be "
"shortened a bit. It should look like this:"
msgstr ""
"现在，由于我们在顶级找到了 ``Torch`` 包，因此在 ``warp_perspective`` 子目录中的 ``CMakeLists.txt`` "
"文件可以稍加简化，如下所示："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Let's re-build our example app, which will also link with the custom "
"operator library. In the top level ``example_app`` directory:"
msgstr "让我们重新构建示例应用程序，它还会链接自定义运算符库。在顶级 ``example_app`` 目录中："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we now run the ``example_app`` binary and hand it our serialized model, "
"we should arrive at a happy ending:"
msgstr "如果我们现在运行 ``example_app`` 二进制文件并将其与序列化模型连接，我们应该会得到一个满意的结果："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Success! You are now ready to inference away."
msgstr "成功！现在你可以开始推理。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial walked you throw how to implement a custom TorchScript "
"operator in C++, how to build it into a shared library, how to use it in "
"Python to define TorchScript models and lastly how to load it into a C++ "
"application for inference workloads. You are now ready to extend your "
"TorchScript models with C++ operators that interface with third party C++ "
"libraries, write custom high performance CUDA kernels, or implement any "
"other use case that requires the lines between Python, TorchScript and C++ "
"to blend smoothly."
msgstr ""
"本教程向你展示了如何在 C++ 中实现一个自定义 TorchScript 运算符，如何将其构建为共享库，如何在 Python 中使用它来定义 "
"TorchScript 模型以及最后如何将其加载到一个 C++ 应用程序中以进行推理工作负载。现在，你已经准备好扩展你的 TorchScript "
"模型，使用能够与第三方 C++ 库交互的 C++ 运算符，编写自定义高性能 CUDA 内核，或实现任何需要在 Python、TorchScript 和 "
"C++ 之间平滑融合的用例。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Appendix A: More Ways of Building Custom Operators"
msgstr "附录 A: 构建自定义算子的更多方法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The section \"Building the Custom Operator\" explained how to build a custom"
" operator into a shared library using CMake. This appendix outlines two "
"further approaches for compilation. Both of them use Python as the "
"\"driver\" or \"interface\" to the compilation process. Also, both re-use "
"the `existing infrastructure "
"<https://pytorch.org/docs/stable/cpp_extension.html>`_ PyTorch provides for "
"`*C++ extensions* "
"<https://pytorch.org/tutorials/advanced/cpp_extension.html>`_, which are the"
" vanilla (eager) PyTorch equivalent of TorchScript custom operators that "
"rely on `pybind11 <https://github.com/pybind/pybind11>`_ for \"explicit\" "
"binding of functions from C++ into Python."
msgstr ""
"章节“构建自定义算子”讲解了如何使用 CMake 将自定义算子构建为一个共享库。本附录概述了另外两种编译方法。这两种方法都使用 Python "
"作为编译过程的“驱动程序”或“接口”。此外，两种方法都重新利用了 PyTorch 提供的现有基础设施，参考`C++ 扩展 "
"<https://pytorch.org/docs/stable/cpp_extension.html>`_，这实际是 TorchScript "
"自定义算子在原生（动态）PyTorch 中的等价形式，使用 `pybind11 "
"<https://github.com/pybind/pybind11>`_ 来将 C++ 的功能“显式地”绑定到 Python。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The first approach uses C++ extensions' `convenient just-in-time (JIT) "
"compilation interface "
"<https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load>`_"
" to compile your code in the background of your PyTorch script the first "
"time you run it. The second approach relies on the venerable ``setuptools`` "
"package and involves writing a separate ``setup.py`` file. This allows more "
"advanced configuration as well as integration with other "
"``setuptools``-based projects. We will explore both approaches in detail "
"below."
msgstr ""
"第一种方法使用 C++ 扩展工具的`即时 (JIT) 编译接口 "
"<https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load>`_"
" 来在首次运行时于 PyTorch 脚本后台编译你的代码。第二种方法依赖传统的 ``setuptools`` 包，需要编写一个独立的 "
"``setup.py`` 文件。这种方法允许更高级的配置，并可与基于 ``setuptools`` 的其它项目集成。我们将在下面详细探索这两种方法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building with JIT compilation"
msgstr "使用 JIT 编译构建"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The JIT compilation feature provided by the PyTorch C++ extension toolkit "
"allows embedding the compilation of your custom operator directly into your "
"Python code, e.g. at the top of your training script."
msgstr "PyTorch C++ 扩展工具提供的 JIT 编译功能允许直接在 Python 代码（比如你的训练脚本开头）中嵌入自定义算子的编译过程。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"\"JIT compilation\" here has nothing to do with the JIT compilation taking "
"place in the TorchScript compiler to optimize your program. It simply means "
"that your custom operator C++ code will be compiled in a folder under your "
"system's `/tmp` directory the first time you import it, as if you had "
"compiled it yourself beforehand."
msgstr ""
"这里的 “JIT 编译” 并非指 TorchScript 编译器中的 JIT 编译用于优化程序，而是表示自定义算子的 C++ 代码将在首次导入时于系统的"
" `/tmp` 目录下一个文件夹中被编译，就像你事先自己编译它一样。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This JIT compilation feature comes in two flavors. In the first, you still "
"keep your operator implementation in a separate file (``op.cpp``), and then "
"use ``torch.utils.cpp_extension.load()`` to compile your extension. Usually,"
" this function will return the Python module exposing your C++ extension. "
"However, since we are not compiling our custom operator into its own Python "
"module, we only want to compile a plain shared library . Fortunately, "
"``torch.utils.cpp_extension.load()`` has an argument ``is_python_module`` "
"which we can set to ``False`` to indicate that we are only interested in "
"building a shared library and not a Python module. "
"``torch.utils.cpp_extension.load()`` will then compile and also load the "
"shared library into the current process, just like "
"``torch.ops.load_library`` did before:"
msgstr ""
"此 JIT 编译功能有两种形式。第一种形式，你将算子实现保留在单独的文件中（例如 ``op.cpp``），然后使用 "
"``torch.utils.cpp_extension.load()`` 编译扩展。通常，这个函数返回一个 Python 模块，暴露你的 C++ "
"扩展。然而，由于我们不打算将自定义算子编译成一个独立的 Python "
"模块，我们只需要编译一个简单的共享库。幸运的是，``torch.utils.cpp_extension.load()`` 提供了一个参数 "
"``is_python_module``，可以将其设置为 ``False``，表示我们只对构建共享库感兴趣，而不是 Python "
"模块。然后，``torch.utils.cpp_extension.load()`` 将像以前的 ``torch.ops.load_library`` "
"一样，编译并加载共享库到当前进程中："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "This should approximately print:"
msgstr "预期输出大致如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The second flavor of JIT compilation allows you to pass the source code for "
"your custom TorchScript operator as a string. For this, use "
"``torch.utils.cpp_extension.load_inline``:"
msgstr ""
"第二种形式的 JIT 编译允许你直接以字符串形式传递自定义 TorchScript 算子的源代码。要实现此功能，请使用 "
"``torch.utils.cpp_extension.load_inline``："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Naturally, it is best practice to only use "
"``torch.utils.cpp_extension.load_inline`` if your source code is reasonably "
"short."
msgstr "自然地，最佳实践是仅在源代码相对较短时使用 ``torch.utils.cpp_extension.load_inline``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that if you're using this in a Jupyter Notebook, you should not execute"
" the cell with the registration multiple times because each execution "
"registers a new library and re-registers the custom operator. If you need to"
" re-execute it, please restart the Python kernel of your notebook "
"beforehand."
msgstr ""
"请注意，如果你在 Jupyter Notebook "
"中使用此功能，不应多次执行注册单元格，因为每次执行都会注册一个新的库，并重新注册自定义算子。如果需要重新执行，请先重启 Notebook 的 "
"Python 内核。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Building with Setuptools"
msgstr "使用 Setuptools 构建"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"The second approach to building our custom operator exclusively from Python "
"is to use ``setuptools``. This has the advantage that ``setuptools`` has a "
"quite powerful and extensive interface for building Python modules written "
"in C++. However, since ``setuptools`` is really intended for building Python"
" modules and not plain shared libraries (which do not have the necessary "
"entry points Python expects from a module), this route can be slightly "
"quirky. That said, all you need is a ``setup.py`` file in place of the "
"``CMakeLists.txt`` which looks like this:"
msgstr ""
"从纯 Python 构建自定义算子的第二种方法是使用 ``setuptools``。其优点在于 ``setuptools`` "
"提供了非常强大和全面的接口，用于构建用 C++ 编写的 Python 模块。然而，由于 ``setuptools`` 实际上是用来构建 Python "
"模块而非普通共享库（共享库缺乏 Python 模块所需的入口点），所以这种方法有点特殊。尽管如此，你只需要一个 ``setup.py`` 文件替代 "
"``CMakeLists.txt``，内容如下："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Notice that we enabled the ``no_python_abi_suffix`` option in the "
"``BuildExtension`` at the bottom. This instructs ``setuptools`` to omit any "
"Python-3 specific ABI suffixes in the name of the produced shared library. "
"Otherwise, on Python 3.7 for example, the library may be called "
"``warp_perspective.cpython-37m-x86_64-linux-gnu.so`` where "
"``cpython-37m-x86_64-linux-gnu`` is the ABI tag, but we really just want it "
"to be called ``warp_perspective.so``"
msgstr ""
"注意，我们在底部的 ``BuildExtension`` 中启用了选项 ``no_python_abi_suffix``。这指示 "
"``setuptools`` 在生成的共享库名称中省略任何与 Python-3 相关的 ABI 后缀。否则，例如在 Python 3.7 "
"中，库可能会被命名为 ``warp_perspective.cpython-37m-x86_64-linux-gnu.so``，其中 "
"``cpython-37m-x86_64-linux-gnu`` 是 ABI 标志，但我们实际上只希望它被命名为 "
"``warp_perspective.so``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"If we now run ``python setup.py build develop`` in a terminal from within "
"the folder in which ``setup.py`` is situated, we should see something like:"
msgstr ""
"如果我们现在在 ``setup.py`` 所在文件夹中从终端运行 ``python setup.py build "
"develop``，我们应该会看到如下输出："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This will produce a shared library called ``warp_perspective.so``, which we "
"can pass to ``torch.ops.load_library`` as we did earlier to make our "
"operator visible to TorchScript:"
msgstr ""
"这将产生一个名为 ``warp_perspective.so`` 的共享库，我们可以像之前一样将其传递给 "
"``torch.ops.load_library``，以使算子对 TorchScript 可见："

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Click :ref:`here <sphx_glr_download_advanced_usb_semisup_learn.py>` to "
"download the full example code"
msgstr ""
"点击 :ref:`这里 <sphx_glr_download_advanced_usb_semisup_learn.py>` 下载完整示例代码"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Semi-Supervised Learning using USB built upon PyTorch"
msgstr "使用基于 PyTorch 的 USB 进行半监督学习"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "**Author**: `Hao Chen <https://github.com/Hhhhhhao>`_"
msgstr "**作者**：`Hao Chen <https://github.com/Hhhhhhao>`_"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Unified Semi-supervised learning Benchmark (USB) is a semi-supervised "
"learning (SSL) framework built upon PyTorch. Based on Datasets and Modules "
"provided by PyTorch, USB becomes a flexible, modular, and easy-to-use "
"framework for semi-supervised learning. It supports a variety of semi-"
"supervised learning algorithms, including ``FixMatch``, ``FreeMatch``, "
"``DeFixMatch``, ``SoftMatch``, and so on. It also supports a variety of "
"imbalanced semi-supervised learning algorithms. The benchmark results across"
" different datasets of computer vision, natural language processing, and "
"speech processing are included in USB."
msgstr ""
"统一的半监督学习基准 (USB) 是一个基于 PyTorch 构建的半监督学习 (SSL) 框架。基于 PyTorch 提供的数据集和模块，USB "
"成为一个灵活、模块化且易于使用的半监督学习框架。它支持多种半监督学习算法，包括 "
"``FixMatch``、``FreeMatch``、``DeFixMatch``、``SoftMatch`` "
"等，还支持各种不平衡的半监督学习算法。USB 包含了计算机视觉、自然语言处理和语音处理等不同数据集的基准结果。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"This tutorial will walk you through the basics of using the USB lighting "
"package. Let's get started by training a ``FreeMatch``/``SoftMatch`` model "
"on CIFAR-10 using pretrained Vision Transformers (ViT)! And we will show it "
"is easy to change the semi-supervised algorithm and train on imbalanced "
"datasets."
msgstr ""
"本教程将引导你了解 USB 的基本用法。让我们首先在 CIFAR-10 数据集上使用预训练模型 Vision Transformers (ViT) "
"训练一个 ``FreeMatch``/``SoftMatch`` 模型！同时，我们会展示如何轻松切换到其他半监督算法，或在不平衡数据集上进行训练。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "USB framework illustration"
msgstr "USB 框架概述"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Introduction to ``FreeMatch`` and ``SoftMatch`` in Semi-Supervised Learning"
msgstr "半监督学习中的 ``FreeMatch`` 和 ``SoftMatch`` 简介"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Here we provide a brief introduction to ``FreeMatch`` and ``SoftMatch``. "
"First, we introduce a famous baseline for semi-supervised learning called "
"``FixMatch``. ``FixMatch`` is a very simple framework for semi-supervised "
"learning, where it utilizes a strong augmentation to generate pseudo labels "
"for unlabeled data. It adopts a confidence thresholding strategy to filter "
"out the low-confidence pseudo labels with a fixed threshold set. "
"``FreeMatch`` and ``SoftMatch`` are two algorithms that improve upon "
"``FixMatch``. ``FreeMatch`` proposes adaptive thresholding strategy to "
"replace the fixed thresholding strategy in ``FixMatch``. The adaptive "
"thresholding progressively increases the threshold according to the learning"
" status of the model on each class. ``SoftMatch`` absorbs the idea of "
"confidence thresholding as an weighting mechanism. It proposes a Gaussian "
"weighting mechanism to overcome the quantity-quality trade-off in pseudo-"
"labels. In this tutorial, we will use USB to train ``FreeMatch`` and "
"``SoftMatch``."
msgstr ""
"在此，我们简要介绍 ``FreeMatch`` 和 ``SoftMatch``。首先，我们介绍一款著名的半监督学习基线框架 "
"``FixMatch``。``FixMatch`` "
"是一个非常简单的半监督学习框架，通过强增强生成未标记数据的伪标签。它采用置信度阈值策略，根据固定阈值过滤低置信度的伪标签。而 ``FreeMatch``"
" 和 ``SoftMatch`` 是基于 ``FixMatch`` 的两种改进算法。``FreeMatch`` 提出了自适应阈值策略来替代 "
"``FixMatch`` 的固定阈值策略，自适应阈值根据模型在每个类别上的学习状态逐步提高阈值。``SoftMatch`` "
"将置信度阈值的思想吸收为一种权重机制，提出了一种高斯权重机制，克服伪标签的数量与质量权衡问题。在本教程中，我们将使用 USB 训练 "
"``FreeMatch`` 和 ``SoftMatch``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use USB to Train ``FreeMatch``/``SoftMatch`` on CIFAR-10 with only 40 labels"
msgstr "使用 USB 在 CIFAR-10 上训练仅含 40 个标签的 ``FreeMatch``/``SoftMatch``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"USB is easy to use and extend, affordable to small groups, and comprehensive"
" for developing and evaluating SSL algorithms. USB provides the "
"implementation of 14 SSL algorithms based on Consistency Regularization, and"
" 15 tasks for evaluation from CV, NLP, and Audio domain. It has a modular "
"design that allows users to easily extend the package by adding new "
"algorithms and tasks. It also supports a Python API for easier adaptation to"
" different SSL algorithms on new data."
msgstr ""
"USB 易于使用和扩展，对于小团队来说是负担得起的，同时也全面支持 SSL 算法的开发和评估。USB 提供了基于一致性正则化的 14 种 SSL "
"算法的实现，涵盖来自计算机视觉、自然语言处理和音频领域的 15 个评估任务。它采用模块化设计，使用户可以通过添加新算法和任务轻松扩展包。它还提供了 "
"Python API，以便更轻松地适配不同的 SSL 算法到新的数据上。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now, let's use USB to train ``FreeMatch`` and ``SoftMatch`` on CIFAR-10. "
"First, we need to install USB package ``semilearn`` and import necessary API"
" functions from USB. If you are running this in Google Colab, install "
"``semilearn`` by running: ``!pip install semilearn``."
msgstr ""
"现在，让我们使用 USB 在 CIFAR-10 上训练 ``FreeMatch`` 和 ``SoftMatch``。首先，我们需要安装 USB 包 "
"``semilearn`` 并导入 USB 的必要 API 功能。如果你在 Google Colab 中运行，请通过运行 ``!pip install "
"semilearn`` 安装 ``semilearn``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Below is a list of functions we will use from ``semilearn``:"
msgstr "以下是我们将从 ``semilearn`` 使用的函数列表："

#: ../../advanced/usb_semisup_learn.rst:355
msgid "``get_dataset`` to load dataset, here we use CIFAR-10"
msgstr "``get_dataset`` 用于加载数据集，这里我们使用 CIFAR-10"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"``get_data_loader`` to create train (labeled and unlabeled) and test data"
msgstr "``get_data_loader`` 用于创建训练（有标签和无标签）和测试数据"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"loaders, the train unlabeled loaders will provide both strong and weak "
"augmentation of unlabeled data - ``get_net_builder`` to create a model, here"
" we use pretrained ViT - ``get_algorithm`` to create the semi-supervised "
"learning algorithm, here we use ``FreeMatch`` and ``SoftMatch`` - "
"``get_config``: to get default configuration of the algorithm - ``Trainer``:"
" a Trainer class for training and evaluating the algorithm on dataset"
msgstr ""
"加载器，其中训练无标签加载器将同时提供未标记数据的强增强和弱增强 - ``get_net_builder`` 用于创建模型，这里我们使用预训练 ViT "
"- ``get_algorithm`` 用于创建半监督学习算法，这里我们使用 ``FreeMatch`` 和 ``SoftMatch`` - "
"``get_config``：获取算法的默认配置 - ``Trainer``：一个 Trainer 类，用于在数据集上训练和评估算法"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Note that a CUDA-enabled backend is required for training with the "
"``semilearn`` package. See `Enabling CUDA in Google Colab "
"<https://pytorch.org/tutorials/beginner/colab#enabling-cuda>`__ for "
"instructions on enabling CUDA in Google Colab."
msgstr ""
"请注意，使用 ``semilearn`` 包训练需要启用 CUDA 后端。有关在 Google Colab 中启用 CUDA 的说明，请参见 "
"`Enabling CUDA in Google Colab "
"<https://pytorch.org/tutorials/beginner/colab#enabling-cuda>`__。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"After importing necessary functions, we first set the hyper-parameters of "
"the algorithm."
msgstr "引入必要函数后，我们首先设置算法的超参数。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Then, we load the dataset and create data loaders for training and testing. "
"And we specify the model and algorithm to use."
msgstr "然后，我们加载数据集，并为训练和测试创建数据加载器。接着，我们指定要使用的模型和算法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can start training the algorithms on CIFAR-10 with 40 labels now. We "
"train for 500 iterations and evaluate every 500 iterations."
msgstr "现在可以开始在 CIFAR-10 上训练仅含 40 个标签的算法了。我们训练 500 次迭代，并每 500 次评估一次。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Finally, let's evaluate the trained model on the validation set. After "
"training 500 iterations with ``FreeMatch`` on only 40 labels of CIFAR-10, we"
" obtain a classifier that achieves around 87% accuracy on the validation "
"set."
msgstr ""
"最后，让我们在验证集上评估训练好的模型。经过 500 次迭代训练，仅用 CIFAR-10 上的 40 个标签，``FreeMatch`` "
"模型在验证集上获得大约 87% 的准确率。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Use USB to Train ``SoftMatch`` with specific imbalanced algorithm on "
"imbalanced CIFAR-10"
msgstr "使用 USB 在不平衡的 CIFAR-10 上训练带有特定不平衡算法的 ``SoftMatch``"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Now let's say we have imbalanced labeled set and unlabeled set of CIFAR-10, "
"and we want to train a ``SoftMatch`` model on it. We create an imbalanced "
"labeled set and imbalanced unlabeled set of CIFAR-10, by setting the "
"``lb_imb_ratio`` and ``ulb_imb_ratio`` to 10. Also, we replace the "
"``algorithm`` with ``softmatch`` and set the ``imbalanced`` to ``True``."
msgstr ""
"现在假设我们有 CIFAR-10 的不平衡有标签数据集和无标签数据集，希望训练一个 ``SoftMatch`` 模型。在这种情况下，我们通过将 "
"``lb_imb_ratio`` 和 ``ulb_imb_ratio`` 设置为 10 来创建不平衡有标签和无标签的 CIFAR-10 "
"数据集。此外，我们将 ``algorithm`` 替换为 ``softmatch``，并将 ``imbalanced`` 设置为 ``True``。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"Then, we re-load the dataset and create data loaders for training and "
"testing. And we specify the model and algorithm to use."
msgstr "接着，我们重新加载数据集，并为训练和测试创建数据加载器。然后，我们指定要使用的模型和算法。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"We can start Train the algorithms on CIFAR-10 with 40 labels now. We train "
"for 500 iterations and evaluate every 500 iterations."
msgstr "现在可以开始在 CIFAR-10 上训练仅含 40 个标签的算法了。我们训练 500 次迭代，并每 500 次评估一次。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid "Finally, let's evaluate the trained model on the validation set."
msgstr "最后，让我们在验证集上评估训练好的模型。"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
"References: - [1] USB: https://github.com/microsoft/Semi-supervised-learning"
" - [2] Kihyuk Sohn et al. FixMatch: Simplifying Semi-Supervised Learning "
"with Consistency and Confidence - [3] Yidong Wang et al. FreeMatch: Self-"
"adaptive Thresholding for Semi-supervised Learning - [4] Hao Chen et al. "
"SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised "
"Learning"
msgstr ""
"参考文献: - [1] USB: https://github.com/microsoft/Semi-supervised-learning - [2]"
" Kihyuk Sohn 等. FixMatch: 使用一致性和置信度简化半监督学习 - [3] Yidong Wang 等. FreeMatch: "
"为半监督学习提供自适应阈值 - [4] Hao Chen 等. SoftMatch: 解决半监督学习中的数量-质量权衡问题"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Python source code: usb_semisup_learn.py "
"<usb_semisup_learn.py>`"
msgstr ""
":download:`下载 Python 源代码: usb_semisup_learn.py <usb_semisup_learn.py>`"

#: ../../advanced/usb_semisup_learn.rst:355
msgid ""
":download:`Download Jupyter notebook: usb_semisup_learn.ipynb "
"<usb_semisup_learn.ipynb>`"
msgstr ""
":download:`下载 Jupyter notebook: usb_semisup_learn.ipynb "
"<usb_semisup_learn.ipynb>`"
